{
    "id": "2601.07060v1",
    "title": "PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation",
    "authors": [
        "Yuanzhe Liu",
        "Jingyuan Zhu",
        "Yuchen Mo",
        "Gen Li",
        "Xu Cao",
        "Jin Jin",
        "Yifan Shen",
        "Zhengyuan Li",
        "Tianjiao Yu",
        "Wenzhen Yuan",
        "Fangqiang Ding",
        "Ismini Lourentzou"
    ],
    "abstract": "ËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÈ¢ÜÂüüÁöÑÊúÄÊñ∞ËøõÂ±ïÂ±ïÁé∞Âá∫ÊΩúÂäõÔºå‰ΩÜÂÖ∂Âú®Â§ÑÁêÜÈïøÂë®ÊúüÂ§öÊ≠•È™§‰ªªÂä°Êó∂‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèËÉΩÂ§üËØÜÂà´‰ªªÂä°Áõ∏ÂÖ≥‰∫§‰∫íÁ∫øÁ¥¢ÊàñËøΩË∏™Â≠ê‰ªªÂä°ËøõÂ∫¶ÁöÑÂÜÖÈÉ®Êé®ÁêÜÊú∫Âà∂ÔºåÂØºËá¥ÈáçÂ§çÂä®‰Ωú„ÄÅÊ≠•È™§ÈÅóÊºèÂèäËøáÊó©ÁªàÊ≠¢Á≠âÂÖ≥ÈîÆÊâßË°åÈîôËØØ„ÄÇ‰∏∫Â∫îÂØπËøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫PALMÊ°ÜÊû∂‚Äî‚Äî‰∏ÄÁßçÂõ¥Áªï‰∫§‰∫íÊ†∏ÂøÉÂèØ‰æõÊÄßÊé®ÁêÜ‰∏éÂ≠ê‰ªªÂä°ËøõÂ∫¶Á∫øÁ¥¢ÊûÑÂª∫Á≠ñÁï•Â≠¶‰π†ÁöÑVLAÊû∂ÊûÑ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊèêÁÇº‰∫íË°•ÁöÑÂèØ‰æõÊÄßË°®ÂæÅÔºåÊçïÊçâÂØπË±°ÂÖ≥ËÅîÊÄß„ÄÅÊé•Ëß¶Âá†‰ΩïÁâπÂæÅ„ÄÅÁ©∫Èó¥Â∏ÉÂ±ÄÂèäËøêÂä®Âä®ÊÄÅÔºå‰∏∫ËßÜËßâËøêÂä®ÊéßÂà∂Êèê‰æõ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÈîöÁÇπ„ÄÇ‰∏∫Ëøõ‰∏ÄÊ≠•Á®≥ÂÆöÈïøÂë®Êúü‰ªªÂä°ÊâßË°åÔºåPALMÈÄöËøáÈ¢ÑÊµãËøûÁª≠ÁöÑÂ≠ê‰ªªÂä°ÂÜÖÈÉ®ËøõÂ∫¶ÂÆûÁé∞Êó†ÁºùÂ≠ê‰ªªÂä°ËΩ¨Êç¢„ÄÇÂú®Â§ßÈáè‰ªøÁúü‰∏éÁúüÂÆûÁéØÂ¢ÉÂÆûÈ™å‰∏≠ÔºåPALMÂßãÁªà‰ºò‰∫éÂü∫Á∫øÊñπÊ≥ïÔºöÂú®LIBERO-LONG‰ªªÂä°‰∏≠ËææÊàê91.8%ÊàêÂäüÁéáÔºåÂú®CALVIN ABC->D‰ªªÂä°‰∏≠Âπ≥ÂùáÊâßË°åÈïøÂ∫¶ÊèêÂçá12.5%ÔºåÂπ∂Âú®‰∏âÁßçÈïøÂë®ÊúüÊ≥õÂåñÂú∫ÊôØ‰∏≠ÂÆûÁé∞ÁúüÂÆû‰∏ñÁïåÂü∫Á∫øÊñπÊ≥ï‰∏§ÂÄçÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ",
    "url": "https://arxiv.org/abs/2601.07060v1",
    "html_url": "https://arxiv.org/html/2601.07060v1",
    "html_content": "PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation\nYuanzhe Liu\n1,2\nJingyuan Zhu\n1\nYuchen Mo\n2\nGen Li\n3\nXu Cao\n2\nJin Jin\n4\nYifan Shen\n2\nZhengyuan Li\n2\nTianjiao Yu\n2\nWenzhen Yuan\n2\nFangqiang Ding\n5\n‚Äã\n‚Ä†\n{}^{5\\text{\\textdagger}}\nIsmini Lourentzou\n2\n‚Äã\n‚Ä†\n{}^{2\\text{\\textdagger}}\n1\nUniversity of Pennsylvania\n2\nUniversity of Illinois Urbana-Champaign\n3\nNanyang Technological University\n4\nUniversity of Oxford\n5\nMassachusetts Institute of Technology\nAbstract\nRecent advancements in vision-language‚Äìaction (VLA) models have shown promise in robotic manipulation, yet they continue to struggle with long-horizon, multi-step tasks. Existing methods lack internal reasoning mechanisms that can identify task-relevant interaction cues or track progress within a subtask, leading to critical execution errors such as repeated actions, missed steps, and premature termination. To address these challenges, we introduce\nPALM\n, a VLA framework that structures policy learning around interaction-centric affordance reasoning and subtask progress cues.\nPALM\ndistills complementary affordance representations that capture object relevance, contact geometry, spatial placements, and motion dynamics, and serve as task-relevant anchors for visuomotor control. To further stabilize long-horizon execution,\nPALM\npredicts continuous within-subtask progress, enabling seamless subtask transitions. Across extensive simulation and real-world experiments,\nPALM\nconsistently outperforms baselines, achieving a 91.8% success rate on LIBERO-LONG, a 12.5% improvement in average length on CALVIN ABCD, and a 2 improvement over real-world baselines across three long-horizon generalization settings.\nFigure 1\n:\nIn contrast to vanilla VLAs that directly map inputs to actions or to predictive methods that forecast dense future images,\nPALM\nintroduces learnable queries to forecast a structured set of future affordances. Conditioned on these affordances, a diffusion-based policy jointly decodes the robot‚Äôs action and a continuous progress value, enabling temporal state tracking and seamless subtask transitions in multi-step, long-horizon tasks. Our training strategy utilizes large-scale pre-training on both robot datasets (BridgeDataV2\n[\nkhazatsky2025droidlargescaleinthewildrobot\n]\n, DROID\n[\nwalke2024bridgedatav2datasetrobot\n]\n) and long-horizon video data (EPIC-KITCHENS\n[\ndamen2020epickitchensdatasetcollectionchallenges\n]\n, RoboCerebra\n[\nhan2025robocerebralargescalebenchmarklonghorizon\n]\n), followed by fine-tuning on our collected human-annotated affordance dataset. Consequently,\nPALM\nattains state-of-the-art performance on long-horizon simulation benchmarks (CALVIN ABCD\n[\nmees2022calvinbenchmarklanguageconditionedpolicy\n]\n, LIBERO-LONG\n[\nliu2023liberobenchmarkingknowledgetransfer\n]\n) and demonstrates strong results under real-world long-horizon generalization settings.\n1\nIntroduction\nRobot learning has made significant strides in developing generalizable policies for diverse tasks and environments\n[\nbharadhwaj2024gen2acthumanvideogeneration\n,\nbrohan2023rt1roboticstransformerrealworld\n,\ncheang2024gr2generativevideolanguageactionmodel\n,\ndriess2023palmeembodiedmultimodallanguage\n,\ndu2023videolanguageplanning\n,\nfu2024mobilealohalearningbimanual\n,\nhu2025datascalinglawsimitation\n,\nzawalski2025roboticcontrolembodiedchainofthought\n,\nmu2023embodiedgptvisionlanguagepretrainingembodied\n,\nshridhar2021cliportpathwaysroboticmanipulation\n,\nwen2024anypointtrajectorymodelingpolicy\n,\nwu2023unleashinglargescalevideogenerative\n,\nyang2024pushinglimitscrossembodimentlearning\n,\nzhang2025learningmanipulationskillsrobot\n]\n. Much of this progress is driven by Vision-Language-Action (VLA) models, which leverage pre-trained vision-language backbones to map visual observations and language instructions directly to robot actions\n[\nzhen20243dvla3dvisionlanguageactiongenerative\n,\noctomodelteam2024octoopensourcegeneralistrobot\n,\nkim2024openvlaopensourcevisionlanguageactionmodel\n,\nblack2024pi0visionlanguageactionflowmodel\n,\nintelligence2025pi05visionlanguageactionmodelopenworld\n,\nchi2024diffusionpolicyvisuomotorpolicy\n,\nli2025cogvlacognitionalignedvisionlanguageactionmodel\n,\nli2025coavlaimprovingvisionlanguageactionmodels\n,\nblack2023zeroshotroboticmanipulationpretrained\n,\nbrohan2023rt1roboticstransformerrealworld\n,\nbrohan2023rt2visionlanguageactionmodelstransfer\n,\nnasiriany2024rtaffordanceaffordancesversatileintermediate\n,\ngu2023rttrajectoryrobotictaskgeneralization\n,\nli2024visionlanguagefoundationmodelseffective\n,\nxiao2022maskedvisualpretrainingmotor\n,\nzeng2024learningmanipulationpredictinginteraction\n,\nreuss2024multimodaldiffusiontransformerlearning\n]\n.\nHowever, current VLA methods are fundamentally limited to short-horizon manipulation and struggle with long-horizon, multi-step planning in dynamic scenes. For example, on ‚Äúclean a cluttered table,‚Äù state-of-the-art policies typically succeed initially but fail mid-task, unable to reliably complete the full sequence.\nA fundamental limitation is the absence of structured affordance cues\n[\nyuan2024robopointvisionlanguagemodelspatial\n,\nkuang2024ramretrievalbasedaffordancetransfer\n,\nsundaresan2023kitekeypointconditionedpoliciessemantic\n,\nhuang2024copageneralroboticmanipulation\n,\nhuang2024rekepspatiotemporalreasoningrelational\n]\nand explicit state tracking\n[\nchen2025sarmstageawarerewardmodeling\n,\nsun2025hypertasrhypernetworkdriventaskawarescene\n,\nkang2024incorporatingtaskprogressknowledge\n]\n. Although existing models\n[\nzhang2025upvlaunifiedunderstandingprediction\n,\nzhang2025dreamvlavisionlanguageactionmodeldreamed\n,\nhu2025videopredictionpolicygeneralist\n,\ntian2024predictiveinversedynamicsmodels\n,\nchen2025villaxenhancinglatentaction\n,\nzhao2025cotvlavisualchainofthoughtreasoning\n]\nmay infer the final goal and produce intermediate actions, they lack internal representations that disambiguate which object should be targeted next, which part or region is relevant for interaction, where items should be placed or moved, or what motion is appropriate for the upcoming step. Consequently, many visually similar states become ambiguous, obscuring the underlying task stage and destabilizing long-horizon control.\nIn addition, existing VLAs lack mechanisms for continuously estimating progress within a subtask. Without a persistent online notion of advancement, the policy cannot reliably decide whether to continue, switch stages, or terminate. This absence of temporal grounding leads to characteristic long-horizon failure modes: repeated or unnecessary actions, skipped required subtasks, premature termination, and even declaring success in incorrect states.\nTo address these gaps, we introduce\nPALM\n, a novel end-to-end framework for learning scalable, long-horizon manipulation. As illustrated in\nFig.\n1\n,\nPALM\nintegrates perception, action, and progress within a closed loop and is built around two complementary capabilities. First,\nPALM\npredicts structured future affordances encoded as latent queries, which are refined through a block-wise structured attention mechanism and specialize into four types, capturing object relevance (global), contact-level geometry (local), candidate placement regions (spatial), and plausible motion trajectories for the next interaction step (dynamic). These latents form a compact and task-relevant representation of the evolving scene state.\nSecond,\nPALM\nestimates fine-grained subtask progress through a continuous progress signal. Conditioned on both the affordance latents and the current multimodal context, the action policy employs a diffusion transformer to jointly predict action and progress sequences.\nBy combining structured affordance prediction with progress-aware action generation,\nPALM\nmaintains stable, coherent behavior across long, multi-step manipulation sequences.\nWe conduct large-scale pre-training on both robot datasets\n[\nkhazatsky2025droidlargescaleinthewildrobot\n,\nwalke2024bridgedatav2datasetrobot\n]\nand long-horizon video data\n[\ndamen2020epickitchensdatasetcollectionchallenges\n,\nhan2025robocerebralargescalebenchmarklonghorizon\n]\n, and then fine-tune the model on human-annotated robot trajectories. Through extensive experiments in both simulation and real-world settings,\nPALM\ndemonstrates clear and consistent gains in long-horizon manipulation. On two widely used long-horizon simulation benchmarks, it improves the success rate on CALVIN ABCD\n[\nmees2022calvinbenchmarklanguageconditionedpolicy\n]\nby 12.5% over prior state-of-the-art baselines and reaches 91.8% success on LIBERO-LONG\n[\nliu2023liberobenchmarkingknowledgetransfer\n]\n. Beyond simulation, we design three types of challenging real-world generalization tests for long-horizon scenarios that vary in localization, lighting, and visual distractors. Under all conditions,\nPALM\nconsistently demonstrates strong performance and robustness with limited fine-tuning data. Our contributions are as follows:\n‚Ä¢\nWe introduce\nPALM\n, a unified VLA framework that integrates structured affordance reasoning and progress-aware policy generation to enable reliable execution across long-horizon, contact-rich manipulation tasks.\n‚Ä¢\nWe propose two novel complementary modules: (1) a fine-grained affordance predictor that acts as an intermediate implicit reasoning step, producing structured and task-relevant representations, and (2) a progress-aware inverse-dynamics module that couples action generation with subtask progress estimation, ensuring temporally coherent execution over long action sequences.\n‚Ä¢\nWe conduct comprehensive evaluations both in simulation and the real world, achieving a 12.5% improvement on CALVIN ABCD over prior state-of-the-art, 91.8% success on LIBERO-LONG, and consistent gains across three real-world long-horizon generalization tests.\n2\nRelated Works\nVision-Language-Action Models.\nSeveral works repurpose pre-trained Vision-Language Models (VLMs) into VLA policies that map visual observations and language instructions to low-level robot actions\n[\nshridhar2021cliportpathwaysroboticmanipulation\n,\ndeng2025graspvlagraspingfoundationmodel\n,\nreed2022generalistagent\n,\nzhao2025vlasvisionlanguageactionmodelspeech\n,\nsong2025acceleratingvisionlanguageactionmodelintegrated\n,\nding2025quarvlavisionlanguageactionmodelquadruped\n,\nkaramcheti2023languagedrivenrepresentationlearningrobotics\n,\ndriess2023palmeembodiedmultimodallanguage\n,\ndu2023visionlanguagemodelssuccessdetectors\n]\nby fine-tuning on large-scale robotics datasets\n[\nembodimentcollaboration2025openxembodimentroboticlearning\n,\nebert2021bridgedataboostinggeneralization\n,\nkhazatsky2025droidlargescaleinthewildrobot\n]\n. A prominent paradigm, pioneered by the RT series\n[\nbrohan2023rt1roboticstransformerrealworld\n,\nbrohan2023rt2visionlanguageactionmodelstransfer\n,\nbelkhale2024rthactionhierarchiesusing\n]\n, formulates action generation as autoregressive prediction over tokenized sequences\n[\nblattmann2023stablevideodiffusionscaling\n,\nzhu2024visionmambaefficientvisual\n,\nniu2024llarvavisionactioninstructiontuning\n,\nli2025llarasuperchargingrobotlearning\n,\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\n.\nIn parallel, diffusion-based action generators\n[\nchi2024diffusionpolicyvisuomotorpolicy\n,\nhou2025ditascalingdiffusiontransformer\n,\nliu2025rdt1bdiffusionfoundationmodel\n,\nke20243ddiffuseractorpolicy\n,\nze20243ddiffusionpolicygeneralizable\n]\ntreat control as a denoising process in continuous trajectory spaces, producing smoother temporal dynamics. Despite their success, both paradigms rely on direct action prediction, which lacks explicit reasoning and fine-grained representations of spatial or physical dynamics. Subsequent works address these limitations by incorporating future prediction through goal-image generation\n[\ndu2023learninguniversalpoliciestextguided\n,\nzawalski2025roboticcontrolembodiedchainofthought\n,\nzhen20243dvla3dvisionlanguageactiongenerative\n,\nnasiriany2024pivotiterativevisualprompting\n,\ngu2023rttrajectoryrobotictaskgeneralization\n,\nzhang2024pivotrprimitivedrivenwaypointawareworld\n,\nwen2024anypointtrajectorymodelingpolicy\n,\nhu2025videopredictionpolicygeneralist\n]\nor integrated forecasting\n[\ntian2024predictiveinversedynamicsmodels\n,\nzhang2025upvlaunifiedunderstandingprediction\n,\nzhao2025cotvlavisualchainofthoughtreasoning\n,\nyang2025gripperkeyposeobjectpointflow\n,\nzhu2025unifiedworldmodelscoupling\n,\nzhang2025reinbotamplifyingrobotvisuallanguage\n]\n, or by enhancing spatio-temporal grounding via keypoint prediction\n[\nyuan2024robopointvisionlanguagemodelspatial\n,\nhuang2024rekepspatiotemporalreasoningrelational\n]\nand historical visual traces\n[\nniu2024llarvavisionactioninstructiontuning\n]\n. In contrast, our work introduces a closed perception‚Äìaction‚Äìprogress loop that improves long-horizon manipulation by integrating affordance and subtask progress reasoning into the VLA.\nFigure 2\n:\nPALM\nOverview.\n(a) Model Architecture:\nGiven a language instruction\nl\nl\n, observation\no\nt\no_{t}\n, and robot state\ns\nt\ns_{t}\n,\nPALM\nencodes each modality using frozen encoders to obtain text, visual, and state tokens.\nThese tokens are fused by a GPT-style transformer with\nunidirectional attention\nand two specialized query sets:\nfine-grained affordance\nand\naction‚Äìprogress\n.\nDuring training, affordance queries attend to context tokens to predict foresight\nùêÖ\n^\nt\n+\nn\n\\hat{\\mathbf{F}}_{t+n}\nwith four supervised heads (\n<Global>\n,\n<Local>\n,\n<Spatial>\n,\n<Dynamic>\n) that ground future scene understanding.\nAt inference, the affordance heads are removed; the action‚Äìprogress query attends to both context and affordance foresight to condition a diffusion transformer that predicts action\na\n^\nt\n:\nt\n+\nn\n‚àí\n1\n\\hat{a}_{t:t+n-1}\nand progress\np\n^\nt\n:\nt\n+\nn\n‚àí\n1\n\\hat{p}_{t:t+n-1}\ntrajectories for continuous control.\n(b) Structured Attention:\nAffordance subqueries attend only to shared context tokens to stay disentangled, while both query types use causal attention to preserve temporal consistency.\nImitation Learning with Progress Supervision.\nEarly imitation learning approaches for long-horizon tasks relied on explicit task decomposition, such as symbolic planning\n[\ngarrett2020pddlstreamintegratingsymbolicplanners\n,\njiang2019taskplanningroboticsempirical\n]\nor predefined visual keyframes\n[\nchanesane2021goalconditionedreinforcementlearningimagined\n,\npertsch2020keyframingfuturekeyframediscovery\n]\n.\nRecent works have shifted towards learning continuous progress representations directly from large-scale video data\n[\nnair2022r3muniversalvisualrepresentation\n,\nma2023livlanguageimagerepresentationsrewards\n,\nmajumdar2024searchartificialvisualcortex\n]\n. These learned progress metrics can be used to reweight behavior cloning\n[\nchen2020bailbestactionimitationlearning\n,\nsiegel2020doingworkedbehavioralmodelling\n,\nxu2022discriminatorweightedofflineimitationlearning\n]\n, assigning higher importance to demonstrations that yield greater progress.\nCompared to computationally expensive bootstrapping methods\n[\nxu2022discriminatorweightedofflineimitationlearning\n]\nor strategies that rely on a small, high-quality subset for data labeling\n[\nwu2019imitationlearningimperfectdemonstration\n,\nwang2023improvingbehaviouralcloningpositive\n]\n, this intrinsic progress weighting offers improved scalability and stability.\nOur approach augments the action space with a scalar progress indicator, which is used to supervise stage-wise policy learning for improved long-horizon manipulation consistency.\nAffordance Representations for Robotic Manipulation.\nRepresenting affordances is a core problem in robot learning. Early work generated structured visual outputs from sensor data, predicting affordances as heatmaps over interaction regions\n[\nbahl2023affordanceshumanvideosversatile\n,\nchen2023affordancegroundingdemonstrationvideo\n,\ngeng2022endtoendaffordancelearningrobotic\n,\nling2024articulatedobjectmanipulationcoarsetofine\n,\nmo2021where2actpixelsactionsarticulated\n,\nqian2024affordancellmgroundingaffordancevision\n,\nli2023locate\n,\nli2024one\n]\nor as bounding boxes and keypoints to localize relevant parts\n[\nhuang2024a3vlmactionablearticulationawarevision\n,\nhuang2024rekepspatiotemporalreasoningrelational\n,\nliu2024mokaopenworldroboticmanipulation\n,\nmanuelli2019kpamkeypointaffordancescategorylevel\n,\nqin2019ketolearningkeypointrepresentations\n,\nmo2021where2actpixelsactionsarticulated\n,\nli2025learning\n]\n. With VLMs, affordance reasoning became open-vocabulary, either via retrieval from knowledge bases\n[\nju2024roboabcaffordancegeneralizationcategories\n,\nkuang2024ramretrievalbasedaffordancetransfer\n]\nor via fine-tuning on affordance datasets\n[\nqian2024affordancellmgroundingaffordancevision\n,\nma2025glovergeneralizableopenvocabularyaffordance\n,\nma2025gloverunleashingpotentialaffordance\n]\n. Recent VLA models further integrate affordance into the policy loop by injecting visual interaction traces\n[\nzheng2025tracevlavisualtraceprompting\n]\n, predicting affordance plans\n[\nnasiriany2025rt\n]\n, or encoding affordance types into reasoning chains\n[\nli2025coavlaimprovingvisionlanguageactionmodels\n]\n. Our approach predicts multiple affordance types as structured latent tokens that provide implicit task-aware representations for progress estimation and subtask policy learning.\n3\nMethod\n3.1\nProblem Formulation\nLong-horizon, language-conditioned manipulation requires policies that remain temporally consistent across sub-tasks with heterogeneous dynamics.\nLet a robot operate over a distribution of tasks\nùíØ\n=\n{\nœÑ\nk\n}\nk\n=\n1\nK\n\\mathcal{T}\\!=\\!\\left\\{\\tau_{k}\\right\\}_{k=1}^{K}\n. Each task\nœÑ\n‚Äã\nùíØ\n\\tau\\in\\mathcal{T}\ndefines an observation‚Äìaction distribution\np\n‚Äã\n(\no\nt\n,\na\nt\n‚Äã\nœÑ\n)\np\\left({o}_{t},{a}_{t}\\mid\\tau\\right)\nand an implicit temporal phase progression. Differences in background, lighting, object configuration, and viewpoint, compounded by cumulative control errors, alter the observation space. As a result, when sub-policies are trained in isolation, the terminal state of one often deviates from the expected start state of the next, leading to unstable transitions.\nConventional behavior cloning that directly fits\nœÄ\n‚Äã\n(\na\nt\n‚Äã\no\nt\n)\n\\pi\\left({a}_{t}\\mid{o}_{t}\\right)\non a mixture of demonstrations collapses these distinct phases, causing repetition or skipped subtasks during rollout.\nAs illustrated in\nFig.\n2\n,\nPALM\naddresses long-horizon stability through two tightly coupled components:\n\\lxSVG@picture\n1\n\\endlxSVG@picture\na fine-grained affordance prediction module\nthat anticipates future interaction cues by estimating an affordance-centric latent at future offsets\nt\n+\nn\nt+n\n,\nf\naff\n:\nùí™\n‚Äã\nùíØ\n‚Äã\n‚Ñ±\nf_{\\mathrm{aff}}\\!:\\!\\mathcal{O}\\!\\times\\mathcal{T}\\!\\rightarrow\\!\\mathcal{F}\n, serving as a task-relevant intermediate representation that stabilizes perception (¬ß\n3.3\n), and\n\\lxSVG@picture\n2\n\\endlxSVG@picture\na progress-aware policy\nœÄ\n:\nùí™\n‚Äã\nùíØ\n‚Äã\nùíú\n~\n\\pi\\!:\\!\\mathcal{O}\\!\\times\\!\\mathcal{T}\\!\\rightarrow\\!\\tilde{\\mathcal{A}}\n,\nùíú\n~\n=\nùíú\n‚Äã\nùí´\n\\tilde{\\mathcal{A}}\\!=\\!\\mathcal{A}\\!\\times\\!\\mathcal{P}\n, where\nùí´\n‚Äã\n[\n0\n,\n1\n]\n\\mathcal{P}\\subset[0,1]\nis a continuous progress space.\nAt time\nt\nt\n, given observations\no\nt\n‚Äã\nùí™\n{o}_{t}\\!\\in\\!\\mathcal{O}\n, and task specification\nœÑ\n‚Äã\nùíØ\n\\tau\\!\\in\\!\\mathcal{T}\n, and conditioned on the predicted affordance latent, the policy jointly decodes an action\na\nt\n‚Äã\nùíú\n{a}_{t}\\!\\in\\!\\mathcal{A}\nalongside a scalar\np\nt\n‚Äã\nùí´\np_{t}\\!\\in\\!\\mathcal{P}\nthat encodes progress within the current subtask and serves as a temporal regularizer that mitigates phase ambiguity and enables smooth subtask transitions without relying on separate planners or hierarchical controllers (¬ß\n3.4\n).\n3.2\nPALM\nArchitecture\nMulti-Modal Encoders.\nPALM\nprocesses three synchronized inputs: a language instruction\nl\nl\n, an image observation\no\nt\no_{t}\n, and a robot state\ns\nt\ns_{t}\n. Instructions are embedded using a CLIP text encoder\n[\nradford2021learningtransferablevisualmodels\n]\n, observations are encoded with a Masked Autoencoder\n[\nhe2021maskedautoencodersscalablevision\n]\nand downsampled by a Perceiver Resampler\n[\njaegle2021perceivergeneralperceptioniterative\n]\nto retain task-relevant visual tokens. The robot state is projected through a lightweight MLP. The resulting tokens are concatenated into a single sequence with time index\nt\nt\nto form a unified multimodal sequence passed to the transformer backbone.\nBackbone and Learnable Queries.\nThe backbone follows a GPT-2-style transformer\n[\nradford2019language\n]\nto integrate the token sequence with causal and cross-modal attention.\nOn top of this backbone,\nPALM\nintroduces two learnable query sets that instantiate the affordance and progress-aware policy components:\nFine-grained affordance queries\ncomprise four subqueries\n<Global>\n,\n<Local>\n,\n<Spatial>\n, and\n<Dynamic>\n, that attend over language, vision, and state tokens to extract task-relevant representation at complementary scales and produce the affordance-centric latent\nùêÖ\n^\nt\n+\nn\n=\nf\naff\n‚Äã\n(\nl\n,\no\nt\n,\ns\nt\n)\n‚Äã\n‚Ñ±\n.\n\\hat{\\mathbf{F}}_{t+n}=f_{\\mathrm{aff}}\\!\\left(l,o_{t},s_{t}\\right)\\in\\mathcal{F}.\n(1)\nAction‚Äìprogress queries\npool control-relevant context and condition on\nùêÖ\n^\nt\n+\nn\n\\hat{\\mathbf{F}}_{t+n}\nto support progress-aware inverse-dynamics decoding for temporally consistent long-horizon control.\nBuilding on prior inverse-dynamics formulations\n[\nchen2025villaxenhancinglatentaction\n,\ntian2024predictiveinversedynamicsmodels\n,\nhu2025videopredictionpolicygeneralist\n]\n, these queries aggregate current observations with the predicted affordance latent to infer action sequences that align with the forecasted interaction state.\nDecoders.\nThe action head is a denoising diffusion transformer\n[\npeebles2023scalablediffusionmodelstransformers\n]\nthat conditions on the action‚Äìprogress queries and the affordance latent to generate a horizon-\nn\nn\ntrajectory:\n(\na\n^\nt\n:\nt\n+\nn\n‚àí\n1\n,\np\n^\nt\n:\nt\n+\nn\n‚àí\n1\n)\n=\nDiT\n‚Å°\n(\nl\n,\no\nt\n,\ns\nt\n,\nùêÖ\n^\nt\n+\nn\n)\n,\n(\\hat{a}_{t:t+n-1},\\hat{p}_{t:t+n-1})\\ =\\operatorname{DiT}\\!\\left(l,o_{t},s_{t},\\hat{\\mathbf{F}}_{t+n}\\right),\n(2)\nwhere\na\n^\nt\n:\nt\n+\nn\n‚àí\n1\n\\hat{{a}}_{t:t+n-1}\nis the predicted action sequence and\np\n^\nt\n:\nt\n+\nn\n‚àí\n1\n\\hat{p}_{t:t+n-1}\nthe predicted sequence of scalar progress values at each step. Details can be found in the supplementary material.\n3.3\nFine-Grained Affordance Prediction\nGlobal Prediction.\nGlobal affordance provides a high-level semantic prior that identifies\nwhich\nobject in the scene is relevant to the instruction and\nwhere\nit is located.\nGiven a command such as ‚Äúpick up the cup,‚Äù the model parses the referent (\ni.e\n.\n, the instruction-specified object) at time\nt\nt\nand predicts, for a future offset\nt\n+\nn\nt\\!+\\!n\n, an object-centric prior that marks the referred instance and its approximate region.\nThis global prior anchors all subsequent affordance cues and constrains local reasoning to the correct region of interest.\nTargets are obtained by first resolving the referent via Grounding DINO\n[\nliu2024groundingdinomarryingdino\n]\nand then segmenting the instance with SAM\n[\nkirillov2023segment\n]\nto obtain a binary mask on the observation lattice. An object representation is further extracted by masked pooling over a frozen image encoder within this region. During training, the predicted future mask and object feature are supervised against these targets. The training objective for global affordance tokens is then formulated as:\n‚Ñí\nglobal\n\\displaystyle\\mathcal{L}_{\\text{global}}\n=\n‚Ñí\nFL\n‚Äã\n(\n‚Ñ≥\nt\n+\nn\nglobal\n,\n‚Ñ≥\n^\nt\n+\nn\nglobal\n)\n\\displaystyle=\\mathcal{L}_{\\text{FL}}\\!\\left(\\mathcal{M}_{t+n}^{\\text{global}},\\,\\hat{\\mathcal{M}}_{t+n}^{\\text{global}}\\right)\n(3)\n+\n‚Ñí\nDice\n‚Äã\n(\n‚Ñ≥\nt\n+\nn\nglobal\n,\n‚Ñ≥\n^\nt\n+\nn\nglobal\n)\n\\displaystyle\\quad+\\mathcal{L}_{\\text{Dice}}\\!\\left(\\mathcal{M}_{t+n}^{\\text{global}},\\,\\hat{\\mathcal{M}}_{t+n}^{\\text{global}}\\right)\nwhere\n‚Ñ≥\n^\nt\n+\nn\nglobal\n=\nf\nglobal\n‚Äã\n(\nl\n,\no\nt\n,\ns\nt\n)\n\\hat{\\mathcal{M}}_{t+n}^{\\text{global}}\\!=\\!f_{\\text{global}}\\left(l,o_{t},s_{t}\\right)\ndenotes the predicted global affordance mask at time\nt\n+\nn\nt\\!+\\!n\n, and\n‚Ñ≥\nt\n+\nn\nglobal\n\\mathcal{M}_{t+n}^{\\text{global }}\nis the corresponding binary target mask on the image (observation) lattice. Here,\n‚Ñí\nFL\n\\mathcal{L}_{\\text{FL}}\ndenotes the pixel-wise focal loss computed over image domain\nŒ©\n\\Omega\n, and\n‚Ñí\nDice\n\\mathcal{L}_{\\text{Dice }}\ndenotes the soft Dice loss.\nLocal Prediction.\nLocal affordance captures fine-grained geometric localization affordance cues necessary for precise contact reasoning. Conditioned on the region highlighted by the global prior, it analyzes high-frequency local visual structure, such as edges, textures, and part geometry, to predict, at time\nt\n+\nn\nt\\!+\\!n\n, a dense contact-likelihood distribution over the object of interest. Following GLOVER++\n[\nma2025gloverunleashingpotentialaffordance\n]\n, annotated contact points are converted into Gaussian heatmaps that serve as continuous supervision targets. The model is trained to match this target heatmap distribution.\nThe training objective for local affordance tokens is thus formulated as:\n‚Ñí\nlocal\n\\displaystyle\\mathcal{L}_{\\text{local}}\n=\n‚Ñí\nFL\n‚Äã\n(\n‚Ñ≥\nt\n+\nn\nlocal\n,\n‚Ñ≥\n^\nt\n+\nn\nlocal\n)\n\\displaystyle=\\mathcal{L}_{\\text{FL}}\\!\\left(\\mathcal{M}_{t+n}^{\\text{local}},\\,\\hat{\\mathcal{M}}_{t+n}^{\\text{local}}\\right)\n(4)\n+\n‚Ñí\nKL\n‚Äã\n(\n‚Ñ≥\n~\nt\n+\nn\nlocal\n,\n‚Ñ≥\n^\n~\nt\n+\nn\nlocal\n)\n,\n\\displaystyle\\quad+\\mathcal{L}_{\\text{KL}}\\!\\left(\\tilde{\\mathcal{M}}_{t+n}^{\\text{local}},\\,\\tilde{\\hat{\\mathcal{M}}}_{t+n}^{\\text{local}}\\right),\n‚Ñ≥\n~\n=\n‚Ñ≥\n\\slimits@\n(\ni\n,\nj\n)\n‚Äã\nŒ©\n‚Äã\n‚Ñ≥\n(\ni\n,\nj\n)\n+\nŒµ\n,\n\\tilde{\\mathcal{M}}=\\frac{\\mathcal{M}}{\\tsum\\slimits@_{(i,j)\\in\\Omega}\\mathcal{M}^{(i,j)}+\\varepsilon},\n(5)\nwhere\n‚Ñ≥\n^\nt\n+\nn\nlocal\n=\nf\nlocal\n‚Äã\n(\nl\n,\no\nt\n,\ns\nt\n)\n\\hat{\\mathcal{M}}_{t+n}^{\\text{local }}\\!=\\!f_{\\text{local }}\\left(l,o_{t},s_{t}\\right)\nis the predicted future local contact-likelihood map at\nt\n+\nn\n,\n‚Ñ≥\nt\n+\nn\nlocal\nt\\!+\\!n,\\mathcal{M}_{t+n}^{\\text{local }}\nis the soft Gaussian target heatmap, and\n‚Ñ≥\n~\n\\tilde{\\mathcal{M}}\ndenotes the\n‚Ñì\n1\n\\ell_{1}\n-normalized map used in the KL term with a small\nŒµ\n>\n0\n\\varepsilon>0\n.\nSpatial Prediction.\nSpatial affordance converts underspecified spatial language into executable placement proposals that remain robust under layout changes.\nInstead of memorizing a single coordinate, the model predicts a small set of candidate placement points at time\nt\n+\nn\nt\\!+\\!n\n, each representing a plausible region for placing the object.\nGround-truth targets are obtained by first converting the instruction into spatial semantics using SpatialVLM\n[\nchen2024spatialvlmendowingvisionlanguagemodels\n]\nand then sampling executable 2D coordinates with RoboPoint\n[\nyuan2024robopointvisionlanguagemodelspatial\n]\n.\nThe training objective minimizes a set-matching objective that aligns each target point with its closest predicted candidate:\n‚Ñí\nspatial\n=\n1\nC\nt\n+\nn\n‚Äã\n\\slimits@\nc\n=\n1\nC\nt\n+\nn\n‚Äã\nmin\n1\n‚Äã\nm\n‚Äã\nM\n‚Å°\n\\|\n‚Äã\nùê©\n^\nt\n+\nn\n(\nm\n)\n‚àí\nùê©\nt\n+\nn\n(\nc\n)\n‚Äã\n\\|\n2\n2\n\\mathcal{L}_{\\text{spatial}}=\\frac{1}{C_{t+n}}\\tsum\\slimits@_{c=1}^{C_{t+n}}\\min_{1\\leq m\\leq M}\\left\\|\\hat{\\mathbf{p}}_{t+n}^{(m)}-\\mathbf{p}_{t+n}^{(c)}\\right\\|_{2}^{2}\n(6)\nwhere\nùíÆ\n^\nt\n+\nn\n=\n{\nùê©\n^\nt\n+\nn\n(\nm\n)\n}\nm\n=\n1\nM\n\\hat{\\mathcal{S}}_{t+n}\\!=\\!\\{\\hat{\\mathbf{p}}_{t+n}^{(m)}\\}_{m=1}^{M}\nand\nùíÆ\nt\n+\nn\n=\n{\nùê©\nt\n+\nn\n(\nc\n)\n}\nc\n=\n1\nC\nt\n+\nn\n\\mathcal{S}_{t+n}\\!=\\!\\{{\\mathbf{p}}_{t+n}^{(c)}\\}_{c=1}^{C_{t+n}}\nare the set of\nM\nM\npredicted normalized 2D candidates and\nC\nt\n+\nn\nC_{t+n}\ntarget placement points at\nt\n+\nn\nt\\!+\\!n\n, respectively.\nDynamic Prediction.\nDynamic affordance identifies the pixels corresponding to the robot gripper and movable objects, and predicts how these regions will evolve over time. Its goal is to predict and establish the statistical associations among the current scene, the language instruction, and the actions required to realize the predicted motion.\nTo construct supervision, we apply a grid-based tracking protocol: an\nN\n‚Äã\nN\nN\\!\\times\\!N\nvisual grid of query points is placed on the first frame of a short history at\nt\n‚àí\nŒ¥\nt-\\delta\n. CoTracker\n[\nkaraev2024cotrackerbettertrack\n]\nfollows each point forward in time. We compute cumulative displacement for each trajectory and retain those that exceed a threshold, ensuring that the dynamic mask captures genuine motion rather than static background or tracker jitter. The selected trajectories are then rasterized at\nt\n+\nn\nt\\!+\\!n\ninto a dynamic region. Given inputs at time\nt\nt\n, the model predicts a future dynamic probability map that highlights pixels likely to belong to the gripper or other moving objects at\nt\n+\nn\nt\\!+\\!n\n. Training encourages calibrated per-pixel probabilities and alignment with the tracked motion regions. The training objective for dynamic affordance tokens is formulated as a masked reconstruction loss using a latent-variable model:\n‚Ñí\ndynamic\n=\nùîº\nùê≥\n‚Äã\nQ\nœï\n‚Äã\n(\nùê≥\n‚Äã\nx\nt\n+\nn\n‚Ñ≥\n)\n‚Äã\n[\n‚àí\nlog\n‚Å°\nP\nœà\n‚Äã\n(\nx\nt\n+\nn\n‚Ñ≥\n‚Äã\nùê≥\n)\n]\n+\nŒ≤\n‚Äã\nKL\n‚Å°\n(\nQ\nœï\n‚Äã\n(\nùê≥\n‚Äã\nx\nt\n+\nn\n‚Ñ≥\n)\n‚Äã\n\\|\n‚Äã\np\n‚Äã\n(\nùê≥\n)\n)\n\\begin{split}\\mathcal{L}_{\\text{dynamic}}=\\mathbb{E}_{\\mathbf{z}\\sim Q_{\\phi}\\!\\left(\\mathbf{z}\\mid x_{t+n}^{\\mathcal{M}}\\right)}\\!\\left[-\\log P_{\\psi}\\!\\left(x_{t+n}^{\\mathcal{M}}\\mid\\mathbf{z}\\right)\\right]\\\\\n\\qquad+\\beta\\operatorname{KL}\\!\\Big(Q_{\\phi}\\!\\left(\\mathbf{z}\\mid x_{t+n}^{\\mathcal{M}}\\right)\\,\\|\\,p(\\mathbf{z})\\Big)\\end{split}\n(7)\nwhere\n‚Ñ≥\nt\n+\nn\n\\mathcal{M}_{t+n}\nis the dynamic region mask from tracking,\nx\nt\n+\nn\n‚Ñ≥\nx_{t+n}^{\\mathcal{M}}\nis the future frame restricted to those masked pixels,\nQ\nœï\n‚Äã\n(\nùê≥\n‚Äã\nx\nt\n+\nn\n‚Ñ≥\n)\nQ_{\\phi}(\\mathbf{z}\\mid x_{t+n}^{\\mathcal{M}})\nis the posterior,\nP\nœà\n‚Äã\n(\nx\nt\n+\nn\n‚Ñ≥\n‚Äã\nùê≥\n)\nP_{\\psi}(x_{t+n}^{\\mathcal{M}}\\mid\\mathbf{z})\nthe decoder likelihood,\np\n‚Äã\n(\nùê≥\n)\np(\\mathbf{z})\nthe prior, and\nŒ≤\n\\beta\nthe KL weight.\n3.4\nProgress-aware Policy via Inverse Dynamics\nIn addition to predicting where to act via affordances, we introduce a progress-aware prediction task that estimates how far execution has advanced within the current subtask. At each time step\nPALM\nfirst infers the active subtask stage from the affordance latent and derives a stage embedding. Conditioned on this embedding, the model predicts a scalar\np\nt\n‚Äã\n[\n0\n,\n1\n]\np_{t}\\in[0,1]\nthat quantifies within-stage completion.\nWe append this scalar to the action output so the policy jointly predicts\n(\na\nt\n,\np\nt\n)\n\\left(a_{t},p_{t}\\right)\nunder a shared multimodal context. This explicit progress signal reduces ambiguity in long-horizon control: visually similar observations may correspond to different actions depending on stage, and\np\nt\np_{t}\ndisambiguates these cases by providing a continuous indicator of ‚Äúwhere we are‚Äù, stabilizing learning and execution by encouraging monotonic, stage-consistent evolution of the latent state and by smoothing transitions at sub-policy boundaries without relying on separate high-level controllers.\nClassical inverse dynamics predicts the action\na\n^\nt\n\\hat{a}_{t}\nbridging two temporally ordered observations\n(\no\nt\n,\no\nt\n+\n1\n)\n(o_{t},o_{t+1})\n. We extend this to predict an\nn\nn\n-step action‚Äìprogress sequence conditioned on the current inputs and a single-step affordance latent. We instantiate\nf\ninv\nf_{\\mathrm{inv}}\nas a denoising diffusion transformer that conditions on the current observation\no\nt\no_{t}\n, the instruction\nl\nl\n, the robot state\ns\nt\ns_{t}\n, and the predicted affordance latent\nùêÖ\n^\nt\n+\nn\n=\nf\naff\n‚Äã\n(\nl\n,\no\nt\n,\ns\nt\n)\n\\hat{\\mathbf{F}}_{t+n}\\!=\\!f_{\\mathrm{aff}}(l,o_{t},s_{t})\nto generate\n(\na\n^\nt\n:\nt\n+\nn\n‚àí\n1\n,\np\n^\nt\n:\nt\n+\nn\n‚àí\n1\n)\n=\nf\ninv\n‚Äã\n(\nl\n,\no\nt\n,\ns\nt\n,\nùêÖ\n^\nt\n+\nn\n)\n.\n\\left(\\hat{a}_{t:t+n-1},\\,\\hat{p}_{t:t+n-1}\\right)=f_{\\mathrm{inv}}\\!\\left(l,o_{t},s_{t},\\hat{\\mathbf{F}}_{t+n}\\right).\n(8)\nThe DiT head jointly models action distributions and progress values, capturing correlations among these cross-modal inputs across the trajectory. Training follows the standard diffusion objective:\nùê≤\n~\nt\n:\nt\n+\nn\n‚àí\n1\n,\nt\nd\n=\nŒ±\n¬Ø\nt\nd\n‚Äã\nùê≤\nt\n:\nt\n+\nn\n‚àí\n1\n+\n1\n‚àí\nŒ±\n¬Ø\nt\nd\n‚Äã\nœµ\n\\tilde{\\mathbf{y}}_{t:t+n-1,t_{d}}=\\sqrt{\\bar{\\alpha}_{t_{d}}}\\,\\mathbf{y}_{t:t+n-1}+\\sqrt{1-\\bar{\\alpha}_{t_{d}}}\\,\\boldsymbol{\\epsilon}\n(9)\n‚Ñí\nDiT\n=\nùîº\nt\nd\n,\nœµ\n\\|\nœµ\n‚àí\nœµ\nŒ∏\n(\nùê≤\n~\nt\n:\nt\n+\nn\n‚àí\n1\n,\nt\nd\n|\nl\n,\no\nt\n,\ns\nt\n,\nùêÖ\n^\nt\n+\nn\n,\nt\nd\n)\n\\|\n2\n2\n\\mathcal{L}_{\\text{DiT}}=\\mathbb{E}_{t_{d},\\,\\boldsymbol{\\epsilon}}\\big\\|\\boldsymbol{\\epsilon}-\\epsilon_{\\theta}\\!\\left(\\tilde{\\mathbf{y}}_{t:t+n-1,t_{d}}\\,\\middle|\\,l,o_{t},s_{t},\\hat{\\mathbf{F}}_{t+n},t_{d}\\right)\\big\\|_{2}^{2}\n(10)\nwhere\nùê≤\nt\n:\nt\n+\nn\n‚àí\n1\n\\mathbf{y}_{t:t+n-1}\nis the target action‚Äìprogress vector over steps\nt\nt\nto\nt\n+\nn\n‚àí\n1\nt{+}n{-}1\n,\nœµ\n‚Äã\nùí©\n‚Äã\n(\nùüé\n,\nùêà\n)\n\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\nis Gaussian noise,\nt\nd\nt_{d}\nis the diffusion time with\nŒ±\n¬Ø\nt\nd\n\\bar{\\alpha}_{t_{d}}\nthe cumulative noise schedule,\nùê≤\n~\nt\n:\nt\n+\nn\n‚àí\n1\n,\nt\nd\n\\tilde{\\mathbf{y}}_{t:t+n-1,t_{d}}\nis the noised target, and\nœµ\nŒ∏\n‚Äã\n(\n)\n\\epsilon_{\\theta}(\\cdot)\nis the noise predictor.\nTable 1\n:\nCALVIN ABCD experimental results.\nWe group the baselines into four types and report the average success rate of the top three checkpoints, computed over 1,000 rollouts per task, as well as the average number of consecutively completed tasks to solve 5 instructions (Avg. Len.). PALM consistently and substantially outperforms all baselines. Best results are shown in\nbold\n.\nMethod\nType\nTask completed in a row\n1\n2\n3\n4\n5\nAvg. Len.\n\\uparrow\n\\uparrow\nRT-1\n[\nbrohan2023rt1roboticstransformerrealworld\n]\nAutoregressive\n53.3%\n22.2%\n9.40%\n3.80%\n1.30%\n0.90\nRobo-Flamingo\n[\nli2024visionlanguagefoundationmodelseffective\n]\nAutoregressive\n82.4%\n61.9%\n46.6%\n33.1%\n23.5%\n2.47\nOpenVLA\n[\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\nAutoregressive\n91.3%\n77.8%\n62.0%\n52.1%\n43.5%\n3.27\nDiffusion Policy\n[\nchi2024diffusionpolicyvisuomotorpolicy\n]\nDiffusion-based\n40.2%\n12.3%\n2.60%\n0.80%\n0.00%\n0.56\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi0visionlanguageactionflowmodel\n]\nDiffusion-based\n93.8%\n85.0%\n76.7%\n68.1%\n59.9%\n3.92\n3D-VLA\n[\nzhen20243dvla3dvisionlanguageactiongenerative\n]\n3D-Aware\n44.7%\n16.3%\n8.10%\n1.60%\n0.00%\n0.71\n3D Diffuser Actor\n[\nke20243ddiffuseractorpolicy\n]\n3D-Aware\n92.2%\n78.7%\n63.9%\n51.2%\n41.2%\n3.27\nRoboUniview\n[\nliu2024robouniviewvisuallanguagemodelunified\n]\n3D-Aware\n94.2%\n84.2%\n73.4%\n62.2%\n50.7%\n3.65\nSusie\n[\nblack2023zeroshotroboticmanipulationpretrained\n]\nPrediction\n87.0%\n69.0%\n49.0%\n38.0%\n26.0%\n2.69\nGR-1\n[\nwu2023unleashinglargescalevideogenerative\n]\nPrediction\n85.4%\n71.2%\n59.6%\n49.7%\n40.1%\n3.06\nSeer\n[\ntian2024predictiveinversedynamicsmodels\n]\nPrediction\n94.4%\n87.2%\n79.9%\n72.2%\n64.3%\n3.98\nPALM\n(\n‚úó\nprogress)\nPrediction\n95.3%\n85.6%\n79.5%\n74.3%\n67.0%\n4.02\nPALM\nPrediction + Progress\n96.9%\n93.8%\n89.3%\n85.9%\n82.0%\n4.48\n4\nExperiments\nTraining Details.\nOur training process consists of a pre-training and a fine-tuning stage. For pre-training, we utilize a mixed dataset from the DROID\n[\nkhazatsky2025droidlargescaleinthewildrobot\n]\nand BridgeData V2\n[\nwalke2024bridgedatav2datasetrobot\n]\ndatasets, which together provide large-scale, in-the-wild robotic arm demonstrations to build a foundational understanding of diverse real-world tasks, and EPIC-KITCHENS\n[\ndamen2020epickitchensdatasetcollectionchallenges\n]\nand RoboCerebra\n[\nhan2025robocerebralargescalebenchmarklonghorizon\n]\n, which provide fine-grained sub-steps and time-segment annotations to supervise the learning of semantic progress estimation in long-horizon, contact-rich scenarios. For fine-tuning, we select 942 trajectories from robot data and annotate them with affordance data and continuous progress labels using a semi-automated method. Details can be found in the supplementary material.\n4.1\nSimulation Experiments\nBenchmarks.\nWe conduct evaluations across two simulation benchmarks: the\nLIBERO\n[\nliu2023liberobenchmarkingknowledgetransfer\n]\nbenchmark comprising\nfour distinct task suites (Spatial, Object, Goal, Long), each with 10 tasks and 50 demonstrations for evaluating the robot‚Äôs comprehension of spatial relationships, and the\nCALVIN\n[\nmees2022calvinbenchmarklanguageconditionedpolicy\n]\nbenchmark, designed to evaluate the instruction-following capabilities of robotic policies on long-horizon language-conditioned tasks, comprising 34 tasks across four distinct environments. We focus on the challenging ABCD setting, where we pre-train in the ABC environments and evaluate in the unseen D environment.\nTable 2\n:\nLIBERO experimental results.\nFor each task suite (Spatial, Object, Goal, Long), we report the average success rate and standard error across\n3\n3\nseeds with\n500\n500\nepisodes each. PALM achieves the best performance over previous methods. Best results in\nbold\n.\nAverage (\n\\uparrow\n\\uparrow\n)\nSpatial (\n\\uparrow\n\\uparrow\n)\nObject (\n\\uparrow\n\\uparrow\n)\nGoal (\n\\uparrow\n\\uparrow\n)\nLong (\n\\uparrow\n\\uparrow\n)\nOpenVLA\n[\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\n76.5  0.6%\n84.7  0.9%\n88.4  0.8%\n79.2  1.0%\n53.7  1.3%\nDiffusion Policy\n[\nchi2024diffusionpolicyvisuomotorpolicy\n]\n72.4  0.7%\n78.3  1.1%\n92.5  0.7%\n68.3  1.2%\n50.5  1.3%\nOcto fine-tuned\n[\noctomodelteam2024octoopensourcegeneralistrobot\n]\n75.1  0.6%\n78.9  1.0%\n85.7  0.9%\n84.6  0.9%\n51.1  1.3%\nSpatialVLA\n[\nqu2025spatialvlaexploringspatialrepresentations\n]\n69.0  1.2%\n88.2  0.7%\n89.9  1.3%\n78.6  0.9%\n55.5  1.5%\nCoT-VLA\n[\nzhao2025cotvlavisualchainofthoughtreasoning\n]\n81.1  0.6 %\n87.5  1.4%\n91.6  0.5%\n87.6  0.6%\n69.0  0.8%\nTraceVLA\n[\nzheng2025tracevlavisualtraceprompting\n]\n74.8  0.9%\n84.6  1.0%\n85.2  0.6%\n75.1  1.4%\n54.1  1.0%\nCoA-VLA\n[\nli2025coavlaimprovingvisionlanguageactionmodels\n]\n79.8  0.5%\n85.3  0.9%\n93.1  1.0%\n85.8  0.9%\n55.0  1.2%\nPALM\n94.5  1.0%\n95.2  1.2 %\n96.7  0.7%\n94.3  1.6%\n91.8  0.8%\nBaselines.\nWe compare\nPALM\nwith four types of baselines: autoregressive methods, diffusion-based methods, 3D-aware methods and prediction methods. For the CALVIN ABCD benchmark, we compare to RT-1\n[\nbrohan2023rt1roboticstransformerrealworld\n]\n, Robo-Flamingo\n[\nli2024visionlanguagefoundationmodelseffective\n]\n, and OpenVLA\n[\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\n, which are autoregressive action models that generate actions from pre-trained VLMs. Diffusion Policy\n[\nchi2024diffusionpolicyvisuomotorpolicy\n]\nand\nœÄ\n0\n\\pi_{0}\n[\nblack2024pi0visionlanguageactionflowmodel\n]\nare selected as representative diffusion-based methods that use a denoising diffusion process to model high-dimensional action distributions, while 3D-VLA\n[\nzhen20243dvla3dvisionlanguageactiongenerative\n]\n, 3D Diffuser Actor\n[\nke20243ddiffuseractorpolicy\n]\n, and RoboUniview\n[\nliu2024robouniviewvisuallanguagemodelunified\n]\nspecialize in capturing 3D-aware representations to enhance manipulation. Prediction-based methods, represented by Susie\n[\nblack2023zeroshotroboticmanipulationpretrained\n]\n, GR-1\n[\nwu2023unleashinglargescalevideogenerative\n]\n, and Seer\n[\ntian2024predictiveinversedynamicsmodels\n]\n, merge visual foresight as a future representation to enhance performance in multi-task robot manipulation. For LIBERO, we compare with OpenVLA, Diffusion Policy, and Seer. Additionally, we compare against Octo\n[\noctomodelteam2024octoopensourcegeneralistrobot\n]\n, which pre-trains robot policies on diverse datasets to enhance generalization. SpatialVLA\n[\nqu2025spatialvlaexploringspatialrepresentations\n]\n, CoT-VLA\n[\nzhao2025cotvlavisualchainofthoughtreasoning\n]\n, TraceVLA\n[\nzheng2025tracevlavisualtraceprompting\n]\n, and CoA-VLA\n[\nli2025coavlaimprovingvisionlanguageactionmodels\n]\nare included to compare diverse task representation methods.\nResults.\nTable\n1\nsummarizes performance on the CALVIN ABCD long-horizon benchmark, demonstrating that\nPALM\nachieves state-of-the-art results across all metrics and outperforms all baselines. First,\nPALM\n(Prediction + Progress) reaches a 96.9% success rate on the first subtask and maintains strong performance as horizon length increases (\ne.g\n.\n, 82.0% for five consecutive subtasks). This is a +17.7% absolute improvement over the strongest prior baseline (Seer at 64.3%)\nat the 5-task horizon.\nPALM\nalso yields the longest average task trajectory (4.48), exceeding Seer (3.98) and\nœÄ\n0\n\\pi_{0}\n(3.92), indicating significantly more stable execution over extended sequences.\nImportantly, results confirm that the progress-aware policy is critical for long-horizon generalization, as removing the progress prediction (\nPALM\n‚úó\nprogress) reduces performance consistently across horizons (\ne.g\n.\n, average length 4.02  4.48).\nMoreover, as shown in\nTable\n2\n, across all four LIBERO suites,\nPALM\nachieves state-of-the-art performance with an average success rate of 94.5%. The largest gain is in LIBERO-LONG, where\nPALM\nreaches 91.8%, outperforming the strongest baseline (CoT-VLA at 69.0%) by 22.8%.\n4.2\nAblation Studies\nFigure 3\n:\nAblation studies of affordance components.\non CALVIN ABCD and LIBERO-LONG benchmarks demonstrate the effectiveness of the four components of affordance prediction.\nHow do the components of the fine-grained affordance module affect performance?\nWe evaluate the four types of predictable affordance latents:\nGlobal\n,\nLocal\n,\nSpatial\n, and\nDynamic\nto measure their respective contributions. We train models by cumulatively adding these affordances to a vanilla VLA. As illustrated in\nFigure\n3\n, adding Global affordance (G) already yields a consistent improvement on both CALVIN and LIBERO-LONG, indicating that coarse object-centric cues help stabilize long-horizon reasoning. Incorporating Local affordance (L) provides additional gains on CALVIN but introduces a slight dip on LIBERO-LONG, likely due to viewpoint-induced geometric bias that affects fine-grained edge-based features. Adding Spatial affordance (S) restores and further improves performance across both benchmarks by providing robust placement priors that generalize across layout variations. Finally, adding Dynamic affordance (D) to the full set (\nPALM\n) yields the best performance, indicating that combining motion cues with structured spatial reasoning produces the most reliable long-horizon behavior.\nHow do the proposed\nPALM\nmodules affect performance?\nWe study the contributions of\nPALM\n‚Äôs three core components (affordance prediction module, inverse dynamics prediction, and progress-aware prediction) by conducting ablations during both pre-training and fine-tuning on the CALVIN ABCD benchmark.\nTable\n4.2\nshows that all three\nPALM\ncomponents contribute complementary capabilities and collectively result in the best performance. Removing affordance foresight produces the largest drop in fine-tuning performance (4.48  3.58), confirming that structured future affordance prediction is essential for precise long-horizon planning once the policy is adapted to downstream robot data. In contrast, removing progress prediction causes the largest degradation during pre-training (4.48  3.73), highlighting that large-scale long-horizon datasets are particularly valuable for learning a strong progress prior that stabilizes temporal reasoning. Eliminating inverse dynamics prediction also reduces performance in both stages, showing that predicting multi-step action trajectories conditioned on affordance latents provides an important training signal beyond direct behavior cloning.\nTable 3\n:\nAblation studies of\nPALM\ncomponents.\nResults on the CALVIN ABCD benchmark demonstrate the effectiveness of each training module under both pre-training and fine-tuning.\nAblation Type\nPre-training\nFine-tuning\nAvg. Len.\n\\uparrow\n\\uparrow\nAvg. Len.\n\\uparrow\n\\uparrow\nPALM\n4.48\n4.48\n‚úó\nAffordance Foresight\n3.90\n3.58\n‚úó\nInverse Dynamic Prediction\n4.17\n3.92\n‚úó\nProgress Prediction\n3.73\n4.02\nTable 4\n:\nAblation studies on training data composition.\nResults on the CALVIN ABCD and LIBERO-LONG benchmarks demonstrate the data efficiency of each source type.\nAblation Type\nCALVIN ABCD\nLIBERO-LONG\nAvg. Len.\n\\uparrow\n\\uparrow\nSR (%)\n\\uparrow\n\\uparrow\nPALM\n4.48\n91.8\n‚úó\nIn-the-Wild Data\n3.90\n73.5\n‚úó\nLong-Horizon Video Data\n3.73\n84.5\n‚úó\nHuman Annotated Data\n3.58\n76.5\n‚úó\nSimulation Data (Pretrain)\n3.96\n81.0\nFigure 4\n:\nReal-world experimental setup and task design.\nLeft:\nWe use a UFACTORY xArm6 robot with the matched Gripper G2 and two RealSense D455 cameras.\nRight:\nWe design a real-world long-horizon manipulation task consisting of six consecutive subtasks, driven by a single high-level instruction.\nTable 5\n:\nReal-world experimental results\non long-horizon task under different generalization settings.\nType\nMethod\nTask completed in a row\nAvg. Len.\n\\uparrow\n\\uparrow\n1\n2\n3\n4\n5\n6\nRandom Localization\nOpenVLA\n[\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\n0.45\n0.30\n0.15\n0.05\n0.00\n0.00\n0.95\nOcto\n[\noctomodelteam2024octoopensourcegeneralistrobot\n]\n0.35\n0.20\n0.10\n0.00\n0.00\n0.00\n0.65\nPALM\n0.70\n0.65\n0.55\n0.45\n0.40\n0.30\n3.05\nVisual Distraction\nOpenVLA\n[\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\n0.65\n0.50\n0.25\n0.15\n0.05\n0.00\n1.60\nOcto\n[\noctomodelteam2024octoopensourcegeneralistrobot\n]\n0.45\n0.35\n0.15\n0.00\n0.00\n0.00\n0.95\nPALM\n0.85\n0.80\n0.65\n0.60\n0.50\n0.40\n3.80\nUnseen Lighting\nOpenVLA\n[\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\n0.55\n0.35\n0.25\n0.10\n0.00\n0.00\n1.25\nOcto\n[\noctomodelteam2024octoopensourcegeneralistrobot\n]\n0.50\n0.35\n0.15\n0.05\n0.00\n0.00\n1.05\nPALM\n0.80\n0.70\n0.60\n0.60\n0.45\n0.40\n3.55\nEffectiveness of training data composition for robot manipulation.\nTraining data is a critical factor influencing the performance of robot policies. Accordingly, we classify our training data into four primary types based on their task orientation: (1) In-the-wild datasets, represented by DROID\n[\nkhazatsky2025droidlargescaleinthewildrobot\n]\nand BridgeData V2\n[\nwalke2024bridgedatav2datasetrobot\n]\ndatasets for the pre-training stage, (2) Long-horizon video datasets, consisting of EPIC-KITCHENS\n[\ndamen2020epickitchensdatasetcollectionchallenges\n]\nand RoboCerebra\n[\nhan2025robocerebralargescalebenchmarklonghorizon\n]\n, which are also utilized during pre-training to learn the understanding of semantic progress in complex, multi-step scenarios, (3) Human-annotated datasets, employed in the fine-tuning stage to train affordance and progress-aware policy, and (4) Simulation datasets for post-training evaluation (CALVIN ABCD\n[\nmees2022calvinbenchmarklanguageconditionedpolicy\n]\nand LIBERO-LONG\n[\nliu2023liberobenchmarkingknowledgetransfer\n]\n).\nAs illustrated in\nSection\n4.2\n, excluding any single data type invariably degrades model performance, though to varying extents.\nThis degradation is most pronounced for the In-the-Wild and Human-Annotated datasets. A substantial performance decrease also occurs when removing the pre-training data corresponding to the simulation benchmarks.\n4.3\nReal-World Experiments\nExperimental Setup.\nAs shown in Figure\n4\n, we use a UFACTORY xArm6 with a Gripper G2 to conduct real-world experiments. For visual input, we use two RealSense D455 cameras configured as eye-on-hand and eye-on-base to capture RGB images. We design several types of generalization tests for long-horizon tasks, including Visual Distraction, Random Localization, and Unseen Lighting.\nWe select a mixed pre-training dataset composed of DROID\n[\nkhazatsky2025droidlargescaleinthewildrobot\n]\nand BridgeData V2\n[\nwalke2024bridgedatav2datasetrobot\n]\n, while the fine-tuning dataset consists of 200 demonstrations collected on the xArm with RGB images, robot states, and actions.\nBaselines and Metrics.\nWe select OpenVLA\n[\nkim2024openvlaopensourcevisionlanguageactionmodel\n]\nand Octo\n[\noctomodelteam2024octoopensourcegeneralistrobot\n]\nas baselines. We report the success rate (SR) and average length for each task over 20 real-world rollouts. For each rollout, each method is permitted a maximum of three execution attempts. To ensure fairness, all models are fine-tuned on our training dataset, trained for an equal number of iterations, and evaluated with the final checkpoint.\nGeneralization Evaluation for Long-Horizon Task.\nTo evaluate generalization in long-horizon tasks, we construct a sequential pick-and-place task consisting of 6 consecutive subtasks, driven by a single high-level instruction, as shown in Table\n5\n. We consider three generalization settings:\n\\lxSVG@picture\n1\n\\endlxSVG@picture\nvarying the target object pose,\n\\lxSVG@picture\n2\n\\endlxSVG@picture\nchanging the scene lighting to unseen conditions, and\n\\lxSVG@picture\n3\n\\endlxSVG@picture\nadding multiple distractor objects to induce visual clutter. The results demonstrate\nPALM\n‚Äôs superior generalization over baselines as the task sequence length increases, showing its robustness in long-horizon settings. Details are available in the Appendix.\n5\nConclusion\nIn this work, we introduce\nPALM\n, a novel end-to-end vision‚Äìlanguage‚Äìaction (VLA) approach for long-horizon robotic manipulation by coupling structured future affordance prediction with continuous progress estimation in a closed loop.\nPALM\nachieves state-of-the-art results on two simulation benchmarks, with a 12.5% improvement on CALVIN ABCD and 91.8% success on LIBERO-LONG, and shows significant robustness in real-world experiments across long-horizon generalization settings.\nReferences\n\\thetitle\nSupplementary Material\nAppendix A\nImplementation Details\nA.1\nPALM\nModel Details\nVision.\nVisual encoding employs a MAE-pretrained ViT-B\n[\nhe2021maskedautoencodersscalablevision\n]\nbackbone, which serves as the primary feature extractor. For each timestep, the model processes images from two viewpoints: a static eye-on-base camera for global context and an eye-on-hand camera for local, gripper-centric views. The ViT transforms each image into 196 patch embeddings plus a [CLS] token. To maintain computational tractability over long temporal sequences, we employ a Perceiver Resampler\n[\njaegle2021perceivergeneralperceptioniterative\n]\n. This module uses a set of learnable latent vectors and cross-attention to distill the initial 197 high-dimensional tokens into a compact, fixed-size set of task-relevant visual embeddings, which are then fed to the main backbone.\nText.\nTo ground the policy in natural language, we encode task instructions using a pretrained CLIP text encoder\n[\nradford2021learningtransferablevisualmodels\n]\n. This module converts the input instruction into a fixed-length embedding that captures its semantic intent. A subsequent linear projection maps this embedding into the model‚Äôs shared latent space, allowing effective integration with visual and state information.\nRobot State.\nThe model receives proprioceptive feedback describing the robot‚Äôs physical configuration. This state is represented by a six-dimensional vector for the end-effector‚Äôs 6-DoF Cartesian pose (position and Euler angles) and a binary value for the gripper‚Äôs open/closed status. For processing, the binary gripper value is first transformed into a two-dimensional one-hot vector. Both the 6-D pose and the one-hot gripper vector are projected through separate linear layers, then concatenated and passed through a final MLP to generate a single, unified state token.\nLearnable Queries.\nWe introduce two sets of learnable query tokens that are appended to the multimodal token sequence at each step\nt\nt\nand updated inside the transformer via cross-attention under causal masking. Each query set extracts and integrates information from multimodal inputs to enable joint prediction.\n‚Ä¢\nFine-grained affordance queries\nare specialized tokens that extract a structured view of future interaction. Organized into four sub-queries (\n<Global>\n,\n<Local>\n,\n<Spatial>\n,\n<Dynamic>\n), they attend to the multimodal context to produce a disentangled, affordance-centric latent\nùêÖ\n^\nt\n+\nn\n\\hat{\\mathbf{F}}_{t+n}\nthat guides downstream control.\n‚Ä¢\nAction‚Äìprogress queries\ngenerate the final control sequence. They pool control-relevant information from current observations and explicitly condition on\nùêÖ\n^\nt\n+\nn\n\\hat{\\mathbf{F}}_{t+n}\n, enabling a progress-aware inverse-dynamics formulation so actions remain temporally consistent and aligned with the predicted future state.\nBackbone.\nThe core of our model is a GPT-2 style transformer\n[\nradford2019language\n]\n, which functions as the central multi-modal reasoning engine. It takes as input a concatenated sequence containing the encoded vision, text, and state tokens, alongside the learnable affordance and action-progress queries. By applying causal and cross-modal attention, the backbone fuses these diverse inputs into a coherent latent representation of the world state.\nDecoders.\nWe preserve spatial correspondence by adding fixed 2D sine‚Äìcosine positional encodings to the image patch tokens and propagating these coordinates through a stack of Transformer encoder layers. After this shared encoding, modality-specific heads decode the four affordance latents.\n<Global>\nand\n<Spatial>\nuse lightweight 2-layer MLP heads, producing respectively a future object mask and a set of candidate placement points.\n<Local>\nand\n<Dynamic>\nuse 2-layer Transformer blocks followed by linear projections to produce a future contact heatmap and a dynamic region. These targets are defined at\nt\n+\nn\nt+n\nto supervise the affordance latents and are used only during training. The primary model output is produced by the action‚Äìprogress decoder, which jointly predicts actions and scalar progress. Configuration details for each module are given in Table\n6\n.\nTable 6\n:\nKey parameters of each module in\nPALM\n.\nType\nHidden Size\nNumber of Layers\nNumber of Heads\nImage Encoder\n768\n12\n12\nPerceiver Resampler\n768\n3\n8\nGPT-2 (LLM Backbone)\n384\n24\n12\nGlobal Decoder\n384\n2\n16\nLocal Decoder\n384\n2\n16\nSpatial Decoder\n384\n2\n16\nDynamic Decoder\n384\n2\n16\nPrediction with Diffusion Transformer.\nWe formulate action‚Äìprogress generation as a conditional denoising task and employ a Diffusion Transformer (DiT) decoder\n[\npeebles2023scalablediffusionmodelstransformers\n]\n. The decoder conditions on the latent embedding from the action‚Äìprogress queries, which already integrates the predicted affordance information. By iteratively reversing a Gaussian noise process over a sequence of latent vectors, the DiT models complex, multimodal action distributions and yields a temporally coherent trajectory. It produces a joint sequence of 7-DoF actions and the corresponding scalar progress values associated with subtask completion. Configuration details of the DiT are provided in Table\n7\n.\nTable 7\n:\nConfiguration of the Diffusion Transformer\nused for action-progress prediction.\nParameter\nValue\nHidden Size\n384\nNumber of Layers\n12\nNumber of Heads\n12\nSampling Steps\n10\nNoise Schedule\nCosine\nAction Prediction Steps\n3\nLoss Function\nMSE (\nL\n2\nL_{2}\nloss)\nPrecision\nfp32\nA.2\nTraining Details\nDatasets.\nOur training process consists of a pre-training and a fine-tuning stage. For pre-training, we utilize a mixed dataset from two domains. One part is drawn from the DROID\n[\nkhazatsky2025droidlargescaleinthewildrobot\n]\ndataset and BridgeData V2\n[\nwalke2024bridgedatav2datasetrobot\n]\n, which together provide large-scale, in-the-wild robotic arm demonstrations to build a foundational understanding of diverse real-world tasks. Another part is from EPIC-KITCHENS\n[\ndamen2020epickitchensdatasetcollectionchallenges\n]\nand RoboCerebra\n[\nhan2025robocerebralargescalebenchmarklonghorizon\n]\n, which provides fine-grained sub-steps and time-segment annotations to supervise the learning of semantic progress estimation in long-horizon, contact-rich scenarios. To keep storage and computation requirements manageable during pre-training, the model predicts entire frames rather than fine-grained affordances. For the fine-tuning stage, we select 942 trajectories from robot data and annotate them with affordance data and continuous progress labels using a semi-automated method. We then fine-tuned model on these annotated trajectories to learn conditional affordance foresight for inverse dynamics prediction, ultimately outputting an action-progress pair. For the post-training stage, we adopt dataset-specific schedules. For LIBERO\n[\nliu2023liberobenchmarkingknowledgetransfer\n]\n, we pre-train on LIBERO-90, which contains fully annotated demonstrations for 90 short-horizon tasks, and then fine-tune and evaluate on LIBERO-LONG, which features long-horizon tasks. For CALVIN ABC\nD\n[\nmees2022calvinbenchmarklanguageconditionedpolicy\n]\n, we first pre-train on the official robot play data without language instructions; the remaining language-conditioned data with full annotations is used for fine-tuning.\nHyperparameters.\nWe perform training on 8 NVIDIA A100 GPUs, and set an initial learning rate of 1e-3, a weight decay of 1e-4, and a batch size of 64. Throughout the entire training process, the pre-trained visual and text encoders are kept frozen. All models are trained for a total of 30 epochs. Our model has 68 million trainable parameters. This lightweight architecture offers a compact and flexible base for fine-tuning, allowing it to be performed on smaller GPUs such as the RTX 4090. Details are illustrated in Table\n8\n.\nTable 8\n:\nTraining hyperparameters.\nHyperparameters\nPre-training\nFine-tuning\nNumber of GPUs\n8\n8\nBatch Size\n80 / GPU\n64 / GPU\nLearning Rate\n1e-4\n1e-3\nWeight Decay\n1e-4\n1e-4\nOptimizer\nAdamW\nAdamW\nLearning Rate Schedule\nCosine decay\nCosine decay\nTraining Epochs\n30\n40\nHistorical Sequence Length\n7\n7\nAction Prediction Length\n3\n3\nA.3\nPolicy Roll-out Details\nTo ensure efficiency during inference, we set the number of DiT diffusion steps to 10, the number of observation steps to 7, and the number of future prediction steps to 3. This configuration results in a rapid sampling time of approximately 40 ms. Our approach uses fine-grained affordances for conditional visual foresight, which avoids the need for explicit image decoding. This enables a closed-loop frequency of 10-15 Hz, with each decision cycle taking less than 80 ms.\nAppendix B\nLong-Horizon Real-World Task Details\nB.1\nTask Setup and Success Criteria\nWe design a long-horizon manipulation task to evaluate the model‚Äôs ability to follow complex, sequential instructions. The experimental setup consists of a UFACTORY xArm 6 robot arm equipped with a Gripper G2. Visual perception is provided by two RealSense D455 cameras: one mounted on the robot‚Äôs wrist for an eye-in-hand perspective, and another positioned statically for a third-person, eye-on-base view. The task is driven by a single, high-level language instruction:\n\"Put the pineapple on the white plate, the grape on the white bowl, and the orange on the blue bowl.\"\nThe scene contains three toy fruits (pineapple, grape, orange), two white containers (a plate and a bowl), and one blue bowl. For each trial, the initial positions of all objects are randomized within a predefined workspace to test policy robustness.\nThe full task is considered a success only when all three fruits are correctly placed in their designated containers as specified by the instructions. The success criteria for each of the six subtasks are detailed below.\nSubtask 1. Pick up pineapple:\nThe robot must successfully grasp the toy pineapple from its initial randomized location. Success for this subtask is defined as the robot securely gripping the pineapple and lifting it clear of the table surface without dropping it. This initial step challenges the policy‚Äôs ability to generalize its grasping strategy to objects with irregular shapes and textures, evaluating its local geometric reasoning for identifying stable contact points.\nSubtask 2. Place on white plate:\nThe robot must transport the grasped pineapple and place it onto the white plate. Success is achieved if the pineapple is fully supported by the plate and remains stable after the gripper retracts. This step tests both precise spatial targeting and the model‚Äôs capacity for language-based disambiguation, as it must correctly identify the ‚Äúplate‚Äù from two similar white containers.\nSubtask 3. Pick up grape:\nThe robot must successfully grasp the toy grape from its initial position. The success criteria are identical to Subtask 1. This action requires fine-grained motor control, evaluating the policy‚Äôs precision when interacting with small objects where the margin for positional error is minimal.\nSubtask 4. Place on white bowl:\nThe robot must transport the grasped grape and place it inside the white bowl. Success is achieved if the grape is fully contained within the bowl after the gripper retracts. This subtask again evaluates language disambiguation (‚Äúbowl‚Äù vs. ‚Äúplate‚Äù) and tests the model‚Äôs understanding of different placement affordances, specifically placing an object\ninto\na concave container versus\nonto\na flat surface.\nSubtask 5. Pick up orange:\nThe robot must successfully grasp the toy orange from its initial position. The success criteria are identical to Subtask 1. This action serves as a baseline evaluation of the model‚Äôs core grasping capability on a simple, regular object shape.\nSubtask 6. Place on blue bowl:\nThe robot must transport the grasped orange and place it inside the blue bowl. Success is achieved if the orange is fully contained within the bowl after the gripper retracts. This explicitly tests the model‚Äôs language grounding for object attributes, as it must correctly associate the color ‚Äúblue‚Äù with the appropriate target container.\nB.2\nRobustness of Progress Estimates\nTo evaluate the reliability and physical grounding of the learned self-progress indicator\np\nt\np_{t}\n, we subject a representative subtask to three categories of dynamic perturbations injected twice per episode at random timesteps. Specifically, we test:\n\\lxSVG@picture\n1\n\\endlxSVG@picture\nrandom object relocations,\ni.e\n.\n, changing the target‚Äôs pose or position mid-execution to require geometric reactivity;\n\\lxSVG@picture\n2\n\\endlxSVG@picture\nunseen lighting,\ni.e\n.\n, switching illumination to out-of-distribution conditions (\ne.g\n.\n, sudden dimming or color-temperature shifts) to require perceptual invariance; and\n\\lxSVG@picture\n3\n\\endlxSVG@picture\nvisual distractions,\ni.e\n.\n, introducing additional distractor objects to induce workspace clutter and occlusions. This analysis verifies that the progress estimate tracks the underlying task state rather than spurious visual correlations, supporting robust closed-loop behavior in unstructured environments. The details and results are as follows.\nRandom Relocation Disturbances.\nAs illustrated in\nFigure\n5\n, during the subtask of\npick up grape\n,\nwe relocate the grape to a new pose twice mid-execution\n(vertical dashed lines). The predicted progress increases smoothly over time and, at each relocation event, exhibits a transient deviation followed by a rapid recovery that quickly returns to the previous upward trend rather than resetting or collapsing. This behavior indicates that the progress signal is robust to substantial changes in the target object‚Äôs pose and continues to track subtask completion rather than the instantaneous geometric configuration of the scene.\nFigure 5\n:\nRandom Relocation Disturbances.\nPredicted progress in the\npick up grape\nsubtask under two random grape relocations.\nUnseen Lighting Disturbances.\nAs illustrated in\nFigure\n6\n, during the subtask of\npick up grape\nwe introduce two abrupt switches to unseen illumination conditions (vertical dashed lines), substantially altering global brightness and shadows while leaving the physical scene unchanged. The predicted progress continues to increase over time, with only mild local deviations at the change points before returning to its prior upward trend. This indicates that the progress estimator is robust to severe photometric disturbances, exhibiting stable behavior under illumination changes.\nFigure 6\n:\nUnseen Lighting Disturbances.\nPredicted progress in the\npick up grape\nsubtask under two unseen lighting changes.\nMulti-Object Visual Distractions.\nAs illustrated in\nFigure\n7\n, during the subtask of\npick up grape\nwe introduce\nmultiple additional objects into the scene twice\n(vertical dashed lines), placing them near the target and within the camera‚Äôs field of view to create visual clutter. The predicted progress keeps increasing over time, with localized deviations at the distraction events that promptly recover and return to the overall upward trend. This indicates that the progress estimator remains stable under multi-object visual distractions, with limited sensitivity to distractors.\nFigure 7\n:\nMulti-Object Visual Distractions.\nPredicted progress in\npick up grape\nsubtask under two injected visual distraction events.\nAppendix C\nMore Experiments\nC.1\nAblation on Progress Threshold\nThe threshold\nœï\n\\phi\nserves as the decision boundary for terminating the current sub-policy and triggering the subsequent phase based on the predicted progress signal\np\nt\np_{t}\n. Since\np\nt\np_{t}\nserves as a continuous indicator of the sub-task‚Äôs temporal progress, the setting of this threshold directly governs the transition timing: a lower threshold risks premature termination of the action, while an excessively high threshold may induce execution stagnation due to signal non-saturation. To investigate the impact of this parameter on final manipulation performance, we conduct an ablation study on the CALVIN ABC\nD benchmark across\nœï\n‚Äã\n{\n70\n%\n,\n80\n%\n,\n90\n%\n,\n100\n%\n}\n\\phi\\in\\{70\\%,80\\%,90\\%,100\\%\\}\n, with\nœï\n=\n90\n%\n\\phi=90\\%\nas the default setting. This analysis aims to validate the effectiveness of the switching logic in long-horizon tasks by quantifying this trade-off.\nAs illustrated in Table\n9\n, the default threshold of\n90\n%\n90\\%\nyields the highest success rates across all chain lengths, achieving an average sequence length of 4.48. Reducing\nœï\n\\phi\nto\n70\n%\n70\\%\nsignificantly degrades performance due to premature transitions, whereas increasing it to 100% causes a slight decline, confirming that a strict saturation requirement can hinder task completion.\nTable 9\n:\nAblation studies on the progress threshold\nœï\n\\phi\n.\nResults on the CALVIN ABCD benchmark demonstrate the effect of the threshold setting on long-horizon task performance.\nThreshold\nœï\n\\phi\nTask completed in a row\n1\n2\n3\n4\n5\nAvg. Len.\n70%\n93.4\n86.3\n78.4\n71.9\n65.5\n3.96\n80%\n95.3\n90.2\n84.2\n79.7\n74.1\n4.24\n90%\n96.9\n93.8\n89.3\n85.9\n82.0\n4.48\n100%\n95.2\n89.7\n84.4\n78.3\n73.0\n4.21\nC.2\nAblation on Prediction vs. Reconstruction\nWe perform an ablation on prediction targets vs. reconstruction to understand which form of foresight best supports long-horizon control. With the backbone and task-conditioned policy fixed, we compare: (1) Affordance Foresight, which predicts future affordance maps to highlight actionable regions and interaction points; (2) Image/Video Foresight, which predicts future RGB observations as a purely pixellevel forecasting objective; and (3) Auxiliary or Reconstruction, which reconstructs the current observation at time\nt\nt\nas a representation-learning signal without future prediction. All variants are evaluated on CALVIN ABC\nD using the average number of consecutively completed subtasks (Avg. Len.) and inference latency.\nAs shown in Table\nC.2\n, affordance foresight attains the highest average length (4.48) with moderate latency (\n70 ms), while future RGB prediction yields lower performance (4.17) at higher latency (\n90 ms) and auxiliary reconstruction is fastest (\n55 ms) but degrades performance the most (3.58). This pattern indicates that structured, action-centric affordance prediction yields substantial long-horizon gains without prohibitive computational cost, and provides the most effective trade-off among the tested prediction types.\nTable 10\n:\nAblation on prediction vs. reconstruction.\nResults on CALVIN ABCD and inference latency, demonstrating the impact of different prediction and reconstruction objectives on long-horizon task performance.\nAblation Type\nCALVIN ABC\nD\nLatency\nAvg. Len.\n\\uparrow\n\\uparrow\n(ms)\n\\downarrow\n\\downarrow\nAffordance\n(\nPALM\n)\n4.48\n70\nImage / Video\n4.17\n90\nAuxiliary\n3.58\n55\nAppendix D\nQualitative Results and Visualization\nFigure 8\n:\nVisualization of affordance predictions.\nAcross sequential progress steps, the model predicts four complementary affordances to guide policy generation:\nGlobal Affordance\nsegments task-relevant objects and goals;\nLocal Affordance\ngenerates heatmaps for precise contact points;\nSpatial Affordance\npredicts candidate placement regions; and\nDynamic Affordance\nforecasts motion trajectories.\nIn this section, we visualize the internal reasoning process of\nPALM\nduring the execution of a long-horizon manipulation task: ‚ÄúSlide the pick block into the drawer.‚Äù As illustrated in Figure\n8\n, our model actively predicts structured affordances to guide its decision-making. We display the rollout over five timesteps, visualizing the four distinct affordance outputs that the model generates:\n‚Ä¢\nGlobal Affordance:\nThe model correctly segments the target object (the pink block) and the goal region (the open drawer), demonstrating semantic understanding of the instruction.\n‚Ä¢\nLocal Affordance:\nThe heatmaps focus precisely on the interaction points, shifting from the block‚Äôs graspable surface to the handle of the drawer as the task progresses.\n‚Ä¢\nSpatial Affordance:\nThe predicted yellow keypoints indicate valid placement candidates, guiding the robot to move the block towards the drawer‚Äôs opening.\n‚Ä¢\nDynamic Affordance:\nThe arrows visualize the predicted motion of the end-effector and the object, showing a clear trajectory for sliding the block forward.\nThese visualizations confirm that\nPALM\nbuilds a comprehensive, structured representation of the task. By explicitly forecasting\nwhat\nto interact with (Global/Local),\nwhere\nto move (Spatial), and\nhow\nthe scene will evolve (Dynamic), the model achieves precise and robust control, successfully completing the task where baselines often fail due to ambiguity or lack of spatial reasoning.\nAppendix E\nBroader Impacts\nPALM\nadvances robotic manipulation by enhancing intermediate reasoning to mitigate common failures in long-horizon tasks. Rather than direct sensorimotor mapping, the model first anticipates a structured set of future affordances that encode task-relevant object identities, interaction points, spatial goal regions, and motion patterns. It then couples these predictions with a mechanism for estimating progress within the current subtask. This combination of structured affordance prediction and progress-aware state tracking yields an internal representation that helps policies remain coherent and effective across complex multi-stage activities where traditional models often fail.\nAt the application level, progress-aware, affordance-based policies can help move long-horizon manipulation from controlled labs to open-world settings (\ne.g\n.\n, homes, warehouses, hospitals) when paired with sufficient data and stronger inductive priors. Because\nPALM\nexposes explicit affordance and progress signals, it integrates more naturally with safety monitors, constraint checks, and human-in-the-loop control, and it simplifies visualization and debugging relative to black-box policies.\nHowever, these structured representations may reflect dataset biases (\ne.g\n.\n, which objects are prioritized or what counts as success), and premature deployment in safety-critical environments could amplify such risks. Realizing the benefits, therefore, requires standardized evaluation, robust sim-to-real transfer, and careful safety and societal-impact assessments so that capability gains do not come at the expense of safety, employment, or privacy.",
    "preview_text": "Recent advancements in vision-language-action (VLA) models have shown promise in robotic manipulation, yet they continue to struggle with long-horizon, multi-step tasks. Existing methods lack internal reasoning mechanisms that can identify task-relevant interaction cues or track progress within a subtask, leading to critical execution errors such as repeated actions, missed steps, and premature termination. To address these challenges, we introduce PALM, a VLA framework that structures policy learning around interaction-centric affordance reasoning and subtask progress cues. PALM distills complementary affordance representations that capture object relevance, contact geometry, spatial placements, and motion dynamics, and serve as task-relevant anchors for visuomotor control. To further stabilize long-horizon execution, PALM predicts continuous within-subtask progress, enabling seamless subtask transitions. Across extensive simulation and real-world experiments, PALM consistently outperforms baselines, achieving a 91.8% success rate on LIBERO-LONG, a 12.5% improvement in average length on CALVIN ABC->D, and a 2x improvement over real-world baselines across three long-horizon generalization settings.\n\nPALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation\nYuanzhe Liu\n1,2\nJingyuan Zhu\n1\nYuchen Mo\n2\nGen Li\n3\nXu Cao\n2\nJin Jin\n4\nYifan Shen\n2\nZhengyuan Li\n2\nTianjiao Yu\n2\nWenzhen Yuan\n2\nFangqiang Ding\n5\n‚Äã\n‚Ä†\n{}^{5\\text{\\textdagger}}\nIsmini Lourentzou\n2\n‚Äã\n‚Ä†\n{}^{2\\text{\\textdagger}}\n1\nUniversity of Pennsylvania\n2\nUniversity of Illinois Urbana-Champaign\n3\nNanyang Technological University\n4\nUniversity of Oxford\n5\nMassachusetts Institute of Technology\nAbstract\nRecent advancements in vision-language‚Äìaction (VLA) models have shown promise in robotic manipulation, yet they continue to struggle with long-horizon, multi-step tasks. Existing methods lack internal reasoning mechanisms that can identify task-relevant interaction cues or track ",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "VLA",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "PALMÊòØ‰∏Ä‰∏™Âü∫‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÊ®°ÂûãÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊ°ÜÊû∂ÔºåÈÄöËøá‰∫§‰∫í‰∏≠ÂøÉÁöÑÂèØ‰æõÊÄßÊé®ÁêÜÂíåÂ≠ê‰ªªÂä°ËøõÂ∫¶Ë∑üË∏™ÔºåÊèêÂçáÈïøÊó∂Á®ãÂ§öÊ≠•‰ªªÂä°ÁöÑÊâßË°åÊàêÂäüÁéá„ÄÇ",
    "detailed_summary": "## PALM ËÆ∫ÊñáËØ¶ÁªÜÊëòË¶Å\n\n### 1. Á†îÁ©∂ËÉåÊôØÂíåÂä®Êú∫\nËøëÂπ¥Êù•ÔºåËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ®°ÂûãÂú®Êú∫Âô®‰∫∫Êìç‰ΩúÈ¢ÜÂüüÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®Â§ÑÁêÜÈúÄË¶ÅÂ§öÊ≠•È™§ËßÑÂàíÁöÑ**ÈïøËßÜÈáé‰ªªÂä°**Êó∂‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÁº∫‰πèÂÜÖÈÉ®Êé®ÁêÜÊú∫Âà∂ÔºåÊó†Ê≥ïËØÜÂà´‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰∫§‰∫íÁ∫øÁ¥¢ÊàñË∑üË∏™Â≠ê‰ªªÂä°ËøõÂ∫¶ÔºåÂØºËá¥ÈáçÂ§çÂä®‰Ωú„ÄÅÈÅóÊºèÊ≠•È™§ÂíåËøáÊó©ÁªàÊ≠¢Á≠âÂÖ≥ÈîÆÊâßË°åÈîôËØØ„ÄÇ‰∏∫Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü**PALM**Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÁªìÊûÑÂåñÁöÑ**ÂèØ‰æõÊÄßÊé®ÁêÜ**Âíå**ËøõÂ∫¶ÊÑüÁü•**Á≠ñÁï•Â≠¶‰π†ÔºåÂÆûÁé∞ÂèØÈù†ÁöÑÈïøËßÜÈáéÊìç‰Ωú„ÄÇ\n\n### 2. Ê†∏ÂøÉÊñπÊ≥ïÂíåÊäÄÊúØÂàõÊñ∞\nPALMÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÈõÜÊàê‰∫ÜÊÑüÁü•„ÄÅÂä®‰Ωú‰∏éËøõÂ∫¶ÁöÑÈó≠ÁéØÊ°ÜÊû∂Ôºå‰∏ªË¶ÅÂåÖÊã¨‰∏§‰∏™‰∫íË°•Ê®°ÂùóÔºö\n- **ÁªÜÁ≤íÂ∫¶ÂèØ‰æõÊÄßÈ¢ÑÊµãÊ®°Âùó**ÔºöËØ•Ê®°ÂùóÈÄöËøá‰∏ÄÁªÑÂèØÂ≠¶‰π†ÁöÑÊü•ËØ¢ÔºåÈ¢ÑÊµãÊú™Êù•Êó∂ÂàªÁöÑÁªìÊûÑÂåñÂèØ‰æõÊÄßË°®ÂæÅ„ÄÇËøô‰∫õË°®ÂæÅÂàÜ‰∏∫ÂõõÁßç‰∫íË°•Á±ªÂûãÔºåÂàÜÂà´ÊçïÊçâ**ÂØπË±°Áõ∏ÂÖ≥ÊÄß**„ÄÅ**Êé•Ëß¶Âá†‰Ωï**„ÄÅ**Á©∫Èó¥ÊîæÁΩÆÂå∫Âüü**Âíå**ËøêÂä®Âä®ÊÄÅ**Ôºå‰∏∫ËßÜËßâËøêÂä®ÊéßÂà∂Êèê‰æõ‰∫Ü‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÈîöÁÇπ„ÄÇ\n- **ËøõÂ∫¶ÊÑüÁü•Á≠ñÁï•Ê®°Âùó**ÔºöËØ•Ê®°ÂùóÂü∫‰∫éÊâ©Êï£ÂèòÊç¢Âô®ÔºåÂú®È¢ÑÊµãÂä®‰ΩúÂ∫èÂàóÁöÑÂêåÊó∂Ôºå**ËÅîÂêàÈ¢ÑÊµã‰∏Ä‰∏™ËøûÁª≠ÁöÑËøõÂ∫¶Ê†áÈáè**„ÄÇËøô‰∏™ËøõÂ∫¶‰ø°Âè∑Áî®‰∫éË∑üË∏™ÂΩìÂâçÂ≠ê‰ªªÂä°ÁöÑÂÆåÊàêÂ∫¶Ôºå‰ªéËÄåÂÆûÁé∞Â≠ê‰ªªÂä°Èó¥ÁöÑÊó†Áºù„ÄÅÁ®≥ÂÆöËøáÊ∏°ÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüË°å‰∏∫ÂÖãÈöÜÊñπÊ≥ïÂú®ÈïøËßÜÈáé‰ªªÂä°‰∏≠Â∏∏ËßÅÁöÑÈò∂ÊÆµÊ®°Á≥äÈóÆÈ¢ò„ÄÇ\n\n### 3. ‰∏ªË¶ÅÂÆûÈ™åÁªìÊûú\nPALMÂú®‰ªøÁúüÂíåÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÂùáÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºö\n- **‰ªøÁúüÂü∫ÂáÜÊµãËØï**ÔºöÂú®CALVIN ABCDÂü∫ÂáÜ‰∏äÔºåÂπ≥ÂùáËøûÁª≠ÂÆåÊàê‰ªªÂä°Êï∞ËææÂà∞**4.48**ÔºåÊØî‰πãÂâçÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá‰∫Ü12.5%„ÄÇÂú®LIBERO-LONGÂü∫ÂáÜ‰∏äÔºåÊàêÂäüÁéáÈ´òËææ**91.8%**ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ\n- **ÁúüÂÆû‰∏ñÁïåÊ≥õÂåñÊµãËØï**ÔºöÂú®ÂåÖÂê´ÈöèÊú∫ÂÆö‰Ωç„ÄÅËßÜËßâÂπ≤Êâ∞ÂíåÊú™Áü•ÂÖâÁÖß‰∏âÁßçÊåëÊàòÊÄßÊ≥õÂåñËÆæÁΩÆÁöÑÈïøËßÜÈáé‰ªªÂä°‰∏≠ÔºåPALMÁöÑË°®Áé∞ÂùáÂ§ßÂπÖ‰ºò‰∫éOpenVLAÂíåOctoÁ≠âÂü∫Á∫øÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ\n- **Ê∂àËûçÁ†îÁ©∂**ÔºöÂÆûÈ™åËØÅÂÆû‰∫ÜÂõõÁßçÂèØ‰æõÊÄßÁ±ªÂûãÂíåËøõÂ∫¶È¢ÑÊµãÊ®°ÂùóÂêÑËá™ÁöÑÈáçË¶ÅË¥°ÁåÆÔºåÂπ∂ËØÅÊòé‰∫ÜÁªìÊûÑÂåñÂèØ‰æõÊÄßÈ¢ÑÊµãÁõ∏ÊØîÊú™Êù•ÂõæÂÉèÈ¢ÑÊµãÊàñÈáçÂª∫‰ªªÂä°ÔºåÂú®ÊÄßËÉΩ‰∏éÊïàÁéá‰∏äÂÖ∑ÊúâÊõ¥‰ºòÁöÑÊùÉË°°„ÄÇ\n\n### 4. Á†îÁ©∂ÊÑè‰πâÂíå‰ª∑ÂÄº\nPALMÈÄöËøáÂ∞ÜÁªìÊûÑÂåñÁöÑ‰∏≠Èó¥Êé®ÁêÜÔºàÂèØ‰æõÊÄßÈ¢ÑÊµãÔºâ‰∏éÊòæÂºèÁöÑÊó∂Â∫èÁä∂ÊÄÅË∑üË∏™ÔºàËøõÂ∫¶‰º∞ËÆ°ÔºâÊ∑±Â∫¶ËûçÂêàÔºå‰∏∫ÈïøËßÜÈáéÊú∫Âô®‰∫∫Êìç‰ΩúÊèê‰æõ‰∫Ü‰∏Ä‰∏™Êñ∞È¢ñ‰∏îÈ´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇÂÖ∂ÊÑè‰πâÂú®‰∫éÔºö\n- **ÊäÄÊúØ‰ª∑ÂÄº**ÔºöÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑVLAÊ°ÜÊû∂ÔºåÊúâÊïàËß£ÂÜ≥‰∫ÜÈïøËßÜÈáé‰ªªÂä°‰∏≠ÁöÑÁä∂ÊÄÅÊ≠ß‰πâÂíåÊó∂Â∫è‰∏ç‰∏ÄËá¥ÈóÆÈ¢òÔºåÊé®Âä®‰∫Ü‰ªé‚ÄúÁ´ØÂà∞Á´ØÊò†Â∞Ñ‚ÄùÂà∞‚ÄúÁªìÊûÑÂåñÊé®ÁêÜ‚ÄùÁöÑËåÉÂºèËΩ¨Âèò„ÄÇ\n- **Â∫îÁî®‰ª∑ÂÄº**ÔºöÊ®°ÂûãËæìÂá∫ÁöÑÊòæÂºèÂèØ‰æõÊÄßÂíåËøõÂ∫¶‰ø°Âè∑Êõ¥Êòì‰∫é‰∏éÂÆâÂÖ®ÁõëÊéß„ÄÅ‰∫∫Êú∫‰∫§‰∫íÈõÜÊàêÔºåÂπ∂ÁÆÄÂåñ‰∫ÜÁ≠ñÁï•Ë∞ÉËØïËøáÁ®ãÔºåÊúâÂä©‰∫éÂ∞ÜÂ§çÊùÇÁöÑÂ§öÊ≠•È™§Êìç‰ΩúËÉΩÂäõ‰ªéÂÆûÈ™åÂÆ§Êé®ÂπøÂà∞Êõ¥ÂºÄÊîæÁöÑÁúüÂÆû‰∏ñÁïåÂú∫ÊôØÔºàÂ¶ÇÂÆ∂Â∫≠„ÄÅ‰ªìÂ∫ìÔºâ„ÄÇÂêåÊó∂ÔºåÁ†îÁ©∂‰πüÊåáÂá∫‰∫ÜÈúÄÂÖ≥Ê≥®Êï∞ÊçÆÂÅèËßÅÂíåÂÆâÂÖ®ÊÄßËØÑ‰º∞Á≠âÊõ¥ÂπøÊ≥õÁöÑÂΩ±Âìç„ÄÇ",
    "qa_pairs": [
        {
            "question": "ËøôÁØáËÆ∫ÊñáÁöÑÊ†∏ÂøÉÂàõÊñ∞ÁÇπÊòØ‰ªÄ‰πàÔºå‰ªñÊÉ≥Ëß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢òÔºåÊÄé‰πàËß£ÂÜ≥ÁöÑÔºü",
            "answer": "## ËÆ∫ÊñáÊ†∏ÂøÉÂàÜÊûêÔºöPALM\n\n### **‰∏Ä„ÄÅ Ê†∏ÂøÉÈóÆÈ¢ò**\nÂΩìÂâçÂü∫‰∫éËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÔºàVLAÔºâÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÊ®°ÂûãÂú®**ÈïøËßÜÈáé„ÄÅÂ§öÊ≠•È™§‰ªªÂä°**‰∏≠Â≠òÂú®Ê†πÊú¨ÊÄßÁº∫Èô∑„ÄÇÂÆÉ‰ª¨Áº∫‰πèÂÜÖÈÉ®Êé®ÁêÜÊú∫Âà∂ÔºåÂØºËá¥Ôºö\n1.  **Êó†Ê≥ïËØÜÂà´‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰∫§‰∫íÁ∫øÁ¥¢**ÔºöÂàÜ‰∏çÊ∏Ö‰∏ã‰∏ÄÊ≠•ËØ•Êìç‰ΩúÂì™‰∏™Áâ©‰Ωì„ÄÅÂì™‰∏™ÈÉ®‰Ωç„ÄÅÂ¶Ç‰ΩïÊîæÁΩÆÊàñÁßªÂä®„ÄÇ\n2.  **Êó†Ê≥ïË∑üË∏™Â≠ê‰ªªÂä°ÂÜÖÁöÑÊâßË°åËøõÂ∫¶**ÔºöÂØºËá¥ÈáçÂ§çÂä®‰Ωú„ÄÅË∑≥ËøáÂøÖË¶ÅÊ≠•È™§„ÄÅËøáÊó©ÁªàÊ≠¢Á≠âÂÖ∏ÂûãÂ§±Ë¥•Ê®°Âºè„ÄÇ\n\n### **‰∫å„ÄÅ Ê†∏ÂøÉÂàõÊñ∞ÁÇπ**\nPALM ÊèêÂá∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑ VLA Ê°ÜÊû∂ÔºåÈÄöËøá**ÁªìÊûÑÂåñÂèØ‰æõÊÄßÊé®ÁêÜ**Âíå**ËøõÂ∫¶ÊÑüÁü•Á≠ñÁï•ÁîüÊàê**Êù•Ëß£ÂÜ≥‰∏äËø∞ÈóÆÈ¢ò„ÄÇÂÖ∂ÂàõÊñ∞‰∏ªË¶Å‰ΩìÁé∞Âú®‰∏§‰∏™Á¥ßÂØÜËÄ¶ÂêàÁöÑÊ®°ÂùóÔºö\n\n1.  **ÁªìÊûÑÂåñÊú™Êù•ÂèØ‰æõÊÄßÈ¢ÑÊµãÊ®°Âùó**\n    *   **ÂàõÊñ∞**Ôºö‰∏çÊòØÁõ¥Êé•È¢ÑÊµãÂä®‰ΩúÊàñÊú™Êù•ÂõæÂÉèÔºåËÄåÊòØÈ¢ÑÊµã‰∏ÄÁªÑ**ÁªìÊûÑÂåñÁöÑ„ÄÅÈù¢ÂêëÊú™Êù•ÁöÑÂèØ‰æõÊÄßÊΩúÂú®Ë°®Á§∫**Ôºå‰Ωú‰∏∫‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰∏≠Èó¥Êé®ÁêÜÈîöÁÇπ„ÄÇ\n    *   **ÂÖ∑‰ΩìÂÆûÁé∞**ÔºöÈÄöËøáÂõõÁ±ª‰∏ìÈó®ÁöÑÊü•ËØ¢ÔºàQueriesÔºâÈ¢ÑÊµã‰∫íË°•ÁöÑÂèØ‰æõÊÄßÔºö\n        *   **ÂÖ®Â±ÄÂèØ‰æõÊÄß**ÔºöËØÜÂà´‰∏éÊåá‰ª§Áõ∏ÂÖ≥ÁöÑ**ÁõÆÊ†áÁâ©‰ΩìÂèäÂÖ∂Â§ßËá¥Âå∫Âüü**„ÄÇ\n        *   **Â±ÄÈÉ®ÂèØ‰æõÊÄß**ÔºöÂú®ÂÖ®Â±ÄÂå∫ÂüüÂÜÖÈ¢ÑÊµã**Á≤æÁªÜÁöÑÊé•Ëß¶ÁÇπÂá†‰ΩïÁÉ≠Âõæ**ÔºåÁî®‰∫éÁ≤æÁ°Æ‰∫§‰∫í„ÄÇ\n        *   **Á©∫Èó¥ÂèØ‰æõÊÄß**ÔºöÂ∞ÜÊ®°Á≥äÁöÑÁ©∫Èó¥Êåá‰ª§ÔºàÂ¶Ç‚ÄúÊîæÂú®ÊóÅËæπ‚ÄùÔºâËΩ¨Âåñ‰∏∫‰∏ÄÁªÑ**ÂÄôÈÄâÊîæÁΩÆÁÇπ**ÔºåÂ¢ûÂº∫Â∏ÉÂ±ÄÂèòÂåñÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\n        *   **Âä®ÊÄÅÂèØ‰æõÊÄß**ÔºöÈ¢ÑÊµã**Êú™Êù•ÂèØËÉΩÁßªÂä®ÁöÑÂå∫ÂüüÔºàÂ¶ÇÂ§πÁà™„ÄÅÁâ©‰ΩìÔºâ**ÔºåÊçïÊçâËøêÂä®Âä®ÊÄÅ„ÄÇ\n\n2.  **ËøõÂ∫¶ÊÑüÁü•ÁöÑÈÄÜÂä®ÂäõÂ≠¶Á≠ñÁï•Ê®°Âùó**\n    *   **ÂàõÊñ∞**ÔºöÂ∞ÜÂä®‰ΩúÈ¢ÑÊµã‰∏é**ËøûÁª≠ÁöÑÂ≠ê‰ªªÂä°ËøõÂ∫¶‰º∞ËÆ°**ËøõË°åËÅîÂêàËß£Á†Å„ÄÇÁ≠ñÁï•‰∏ç‰ªÖËæìÂá∫Âä®‰ΩúÔºåËøòËæìÂá∫‰∏Ä‰∏™Ê†áÈáèËøõÂ∫¶ÂÄº `p_t ‚àà [0,1]`„ÄÇ\n    *   **ÂÖ∑‰ΩìÂÆûÁé∞**Ôºö‰ΩøÁî®**Êâ©Êï£ÂèòÊç¢Âô®**Ôºå‰ª•È¢ÑÊµãÁöÑÂèØ‰æõÊÄßÊΩúÂèòÈáè‰∏∫Êù°‰ª∂ÔºåËÅîÂêàÁîüÊàêÊú™Êù•Â§öÊ≠•ÁöÑ`ÔºàÂä®‰ΩúÔºåËøõÂ∫¶Ôºâ`Â∫èÂàó„ÄÇËøõÂ∫¶‰ø°Âè∑‰Ωú‰∏∫Êó∂Èó¥Ê≠£ÂàôÂåñÂô®ÔºåÂ∏ÆÂä©Ê®°ÂûãÂå∫ÂàÜËßÜËßâÁõ∏‰ºº‰ΩÜÂ§Ñ‰∫é‰∏çÂêå‰ªªÂä°Èò∂ÊÆµÁöÑÁä∂ÊÄÅÔºå‰ªéËÄåÂÆûÁé∞**Âπ≥ÊªëÁöÑÂ≠ê‰ªªÂä°ËΩ¨Êç¢**„ÄÇ\n\n### **‰∏â„ÄÅ Ëß£ÂÜ≥ÊñπÊ°àÊû∂ÊûÑ**\nPALM ÈááÁî®Á´ØÂà∞Á´ØËÆ≠ÁªÉÔºåÂÖ∂Â∑•‰ΩúÊµÅÁ®ãÂ¶Ç‰∏ãÔºö\n\n```mermaid\ngraph TD\n    A[ËæìÂÖ•: ËØ≠Ë®ÄÊåá‰ª§l, ËßÜËßâËßÇÊµão_t, Êú∫Âô®‰∫∫Áä∂ÊÄÅs_t] --> B[Â§öÊ®°ÊÄÅÁºñÁ†ÅÂô®<br/>ÔºàCLIPÊñáÊú¨ÁºñÁ†ÅÂô® + MAEËßÜËßâÁºñÁ†ÅÂô®Ôºâ]\n    B --> C[GPT-2È£éÊ†ºTransformer‰∏ªÂπ≤]\n    C --> D{‰∏§Á±ªÂèØÂ≠¶‰π†Êü•ËØ¢}\n    D --> E[‚Äú**ÂèØ‰æõÊÄßÊü•ËØ¢**<br/>ÔºàGlobal, Local, Spatial, DynamicÔºâ‚Äù]\n    D --> F[‚Äú**Âä®‰Ωú-ËøõÂ∫¶Êü•ËØ¢**‚Äù]\n    E --> G[È¢ÑÊµãÁªìÊûÑÂåñÊú™Êù•ÂèØ‰æõÊÄßÊΩúÂèòÈáè F_hat_t+n]\n    G --> F\n    F --> H[Êâ©Êï£ÂèòÊç¢Âô®Ëß£Á†ÅÂô®]\n    H --> I[‚Äú**ËÅîÂêàËæìÂá∫**<br/>Âä®‰ΩúÂ∫èÂàó a_hat ‰∏é ËøõÂ∫¶Â∫èÂàó p_hat‚Äù]\n```\n\n**ËÆ≠ÁªÉÁ≠ñÁï•**Ôºö\n1.  **Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉ**ÔºöÊ∑∑Âêà‰ΩøÁî®Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜÔºàDROID, BridgeDataV2ÔºâÂíåÈïøËßÜÈáéËßÜÈ¢ëÊï∞ÊçÆÔºàEPIC-KITCHENS, RoboCerebraÔºâËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†ÈÄöÁî®Ë°®Á§∫ÂíåËøõÂ∫¶Ê¶ÇÂøµ„ÄÇ\n2.  **Á≤æÁªÜÂåñÂæÆË∞É**ÔºöÂú®‰∫∫Â∑•Ê†áÊ≥®‰∫ÜÂèØ‰æõÊÄßÂíåËøõÂ∫¶Ê†áÁ≠æÁöÑÊú∫Âô®‰∫∫ËΩ®ËøπÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞ÉÔºå‰ΩøÊ®°ÂûãÂ≠¶‰ºöÂü∫‰∫éÂèØ‰æõÊÄßËøõË°åÁ≤æÁ°ÆÁöÑÈÄÜÂä®ÂäõÂ≠¶È¢ÑÊµã„ÄÇ\n\n### **Âõõ„ÄÅ ÂÆûÈôÖ‰ª∑ÂÄº‰∏éÊäÄÊúØ‰ºòÂäø**\n*   **ÊÄßËÉΩÊèêÂçáÊòæËëó**ÔºöÂú®‰∏ªÊµÅ‰ªøÁúüÂü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæóSOTA„ÄÇ\n    *   **CALVIN ABCD**ÔºöÂπ≥ÂùáËøûÁª≠ÂÆåÊàê‰ªªÂä°Êï∞Ëææ **4.48**ÔºåÊØî‰πãÂâçÊúÄ‰Ω≥ÊñπÊ≥ïÊèêÂçá **12.5%**„ÄÇ\n    *   **LIBERO-LONG**ÔºöÊàêÂäüÁéáÈ´òËææ **91.8%**ÔºåÊØîÊúÄÂº∫Âü∫Á∫øÊèêÂçá **22.8%**„ÄÇ\n*   **Ê≥õÂåñ‰∏éÈ≤ÅÊ£íÊÄßÂº∫**ÔºöÂú®ÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÔºåÂú®**ÈöèÊú∫ÂÆö‰Ωç„ÄÅËßÜËßâÂπ≤Êâ∞„ÄÅÊú™Áü•ÂÖâÁÖß**‰∏âÁßçÊ≥õÂåñËÆæÁΩÆ‰∏ãÔºåÊÄßËÉΩÂùáÂ§ßÂπÖË∂ÖËøáÂü∫Á∫øÔºàOpenVLA, OctoÔºâÔºåÂ±ïÁ§∫‰∫ÜÂá∫Ëâ≤ÁöÑÁéØÂ¢ÉÈÄÇÂ∫îËÉΩÂäõ„ÄÇ\n*   **ÂèØËß£ÈáäÊÄß‰∏éÊ®°ÂùóÂåñ**ÔºöÁªìÊûÑÂåñÁöÑÂèØ‰æõÊÄßÈ¢ÑÊµãÔºàÂ¶ÇÁÉ≠Âõæ„ÄÅÂÖ≥ÈîÆÁÇπÔºâÊèê‰æõ‰∫ÜÁ≠ñÁï•ÂÜ≥Á≠ñÁöÑ**‰∏≠Èó¥ÂèØËßÜÂåñ‰æùÊçÆ**ÔºåÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÂèØËß£ÈáäÊÄßÂíåË∞ÉËØï‰æøÂà©ÊÄß„ÄÇ\n*   **È´òÊïàÊé®ÁêÜ**ÔºöÈÄöËøáÈ¢ÑÊµãÁ¥ßÂáëÁöÑÂèØ‰æõÊÄßÊΩúÂèòÈáèËÄåÈùûÂÆåÊï¥ÂõæÂÉèÔºåÂÆûÁé∞‰∫Ü **10-15 Hz** ÁöÑÈó≠ÁéØÊéßÂà∂È¢ëÁéáÔºåÊª°Ë∂≥ÂÆûÊó∂ÊÄßË¶ÅÊ±Ç„ÄÇ\n\n**ÊÄªÁªì**ÔºöPALM ÁöÑÊ†∏ÂøÉÁ™ÅÁ†¥Âú®‰∫éÂ∞Ü**Âü∫‰∫éÂèØ‰æõÊÄßÁöÑÁªìÊûÑÂåñÂú∫ÊôØÁêÜËß£**‰∏é**ÊòæÂºèÁöÑÊó∂Â∫èËøõÂ∫¶Ë∑üË∏™**Ê∑±Â∫¶ËûçÂêàÂà∞VLAÁ≠ñÁï•‰∏≠Ôºå‰∏∫ÈïøËßÜÈáéÊú∫Âô®‰∫∫Êìç‰ΩúÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂÖºÂÖ∑**Êé®ÁêÜËÉΩÂäõ„ÄÅÊó∂Â∫è‰∏ÄËá¥ÊÄßÂíåÂº∫Ê≥õÂåñÊÄß**ÁöÑÁ´ØÂà∞Á´ØËß£ÂÜ≥ÊñπÊ°à„ÄÇ",
            "timestamp": "2026-01-21T12:10:16.918326",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "Âü∫‰∫é‰ªñÁöÑÂâç‰ΩúÔºåÊ¢≥ÁêÜËøô‰∏™ÊñπÂêëÁöÑÊï¥‰∏™ÂèëÂ±ïËÑâÁªúÔºåÊØè‰∏ÄÊ≠•Áõ∏ÊØî‰∫é‰πãÂâçÁöÑÂ∑•‰ΩúÈÉΩÊîπËøõ‰∫Ü‰ªÄ‰πàÔºåÁùÄÈáç‰∫éÂá†‰∏™‰∏çÂêåÁöÑÂèëÂ±ïÊñπÂêë„ÄÇ",
            "answer": "## PALMËÆ∫ÊñáÂàÜÊûêÔºöÂü∫‰∫éÂâç‰ΩúÁöÑÂèëÂ±ïËÑâÁªúÊ¢≥ÁêÜ\n\nPALMÔºàProgress-Aware Policy Learning via Affordance ReasoningÔºâÊòØÊú∫Âô®‰∫∫ÈïøÊó∂Á®ãÊìç‰ΩúÈ¢ÜÂüüÁöÑ‰∏ÄÈ°πÊúÄÊñ∞Â∑•‰Ωú„ÄÇË¶ÅÁêÜËß£ÂÖ∂ÂàõÊñ∞ÊÄßÔºåÈúÄË¶ÅÊ¢≥ÁêÜÂÖ∂ÊâÄÂú®ÊñπÂêëÔºàVision-Language-ActionÊ®°ÂûãÁî®‰∫éÈïøÊó∂Á®ãÊìç‰ΩúÔºâÁöÑÂèëÂ±ïËÑâÁªú„ÄÇËØ•ËÑâÁªúÂèØ‰ª•Ê¶ÇÊã¨‰∏∫**‰ªéÁõ¥Êé•Êò†Â∞ÑÂà∞ÁªìÊûÑÂåñÊé®ÁêÜ**ÁöÑÊºîËøõÔºå‰∏ªË¶ÅÊ≤øÁùÄ**‰∏â‰∏™ÂèëÂ±ïÊñπÂêë**Â±ïÂºÄÔºö\n\n### ÂèëÂ±ïËÑâÁªúÊÄªËßà\n\n| ÂèëÂ±ïÈò∂ÊÆµ | Ê†∏ÂøÉÊÄùÊÉ≥ | ‰ª£Ë°®ÊÄßÂ∑•‰Ωú | ‰∏ªË¶ÅÊîπËøõ | ÈÅóÁïôÈóÆÈ¢ò |\n| :--- | :--- | :--- | :--- | :--- |\n| **1. Âü∫Á°ÄVLAÊ®°Âûã** | Â∞ÜÈ¢ÑËÆ≠ÁªÉÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁõ¥Êé•ÂæÆË∞ÉÔºåÂÆûÁé∞‰ªéËßÇÊµãÂíåÊåá‰ª§Âà∞Âä®‰ΩúÁöÑÁ´ØÂà∞Á´ØÊò†Â∞Ñ„ÄÇ | RT-1/RT-2, OpenVLA | Âà©Áî®Â§ßËßÑÊ®°‰∫íËÅîÁΩëÈ¢ÑËÆ≠ÁªÉÁü•ËØÜÔºåÂÆûÁé∞ÂºÄÈõÜÊåá‰ª§Ë∑üÈöèÂíå‰∏ÄÂÆöÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ | Áº∫‰πèÂÜÖÈÉ®Êé®ÁêÜÔºåÂØπÈïøÊó∂Á®ã„ÄÅÂ§öÊ≠•È™§‰ªªÂä°Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊòìÂá∫Áé∞ÈáçÂ§ç„ÄÅÈÅóÊºè„ÄÅÊèêÂâçÁªàÊ≠¢Á≠âÈîôËØØ„ÄÇ |\n| **2. ÂºïÂÖ•Êú™Êù•È¢ÑÊµã** | Âú®Âä®‰ΩúÈ¢ÑÊµã‰πãÂ§ñÔºåÂ¢ûÂä†ÂØπÊú™Êù•ËßÇÊµãÔºàÂ¶ÇÂõæÂÉè„ÄÅÁä∂ÊÄÅÔºâÁöÑÈ¢ÑÊµãÔºå‰ª•Êèê‰æõÊõ¥‰∏∞ÂØåÁöÑËßÑÂàí‰ø°Âè∑„ÄÇ | Seer, GR-1, 3D-VLA | ÈÄöËøáÈ¢ÑÊµãÊú™Êù•Áä∂ÊÄÅÔºå‰ΩøÁ≠ñÁï•ÂÖ∑Â§á‰∏ÄÂÆöÁöÑÂâçÁûªÊÄßÔºåÊîπÂñÑ‰∫ÜÂ§öÊ≠•È™§‰ªªÂä°ÁöÑËøûË¥ØÊÄß„ÄÇ | È¢ÑÊµãÁõÆÊ†áÈÄöÂ∏∏ÊòØÂØÜÈõÜÁöÑÂÉèÁ¥†Á∫ßÂõæÂÉèÔºåËÆ°ÁÆóÂºÄÈîÄÂ§ßÔºå‰∏î‰∏éÂÖ∑‰ΩìÁöÑ‰∫§‰∫íÊÑèÂõæÔºà‚ÄúÂú®Âì™Èáå„ÄÅÂ¶Ç‰ΩïÊìç‰Ωú‚ÄùÔºâÂÖ≥ËÅî‰∏çÂ§üÁõ¥Êé•„ÄÇ |\n| **3. ÈõÜÊàêÁªìÊûÑÂåñË°®ÂæÅ** | ÂºïÂÖ•Êõ¥ÁªìÊûÑÂåñ„ÄÅ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰∏≠Èó¥Ë°®ÂæÅÔºàÂ¶ÇÂÖ≥ÈîÆÁÇπ„ÄÅËΩ®Ëøπ„ÄÅaffordanceÔºâÊù•ÂºïÂØºÁ≠ñÁï•„ÄÇ | RoboPoint, CoT-VLA, RT-Affordance | Êèê‰æõ‰∫ÜÊõ¥ÊòéÁ°ÆÁöÑÁ©∫Èó¥„ÄÅÂá†‰ΩïÊàñËØ≠‰πâ groundingÔºåÂáèÂ∞ë‰∫ÜÁä∂ÊÄÅÊ≠ß‰πâÔºåÊèêÂçá‰∫ÜÊìç‰ΩúÁ≤æÂ∫¶„ÄÇ | Ë°®ÂæÅÈÄöÂ∏∏ÊòØÈùôÊÄÅÁöÑÊàñÂçï‰∏ÄÂ∞∫Â∫¶ÁöÑÔºåÁº∫‰πèÂØπ‰ªªÂä°**Êó∂Â∫èËøõÂ±ï**ÁöÑÊòæÂºèÂª∫Ê®°ÔºåÂú®ÈïøÂ∫èÂàó‰∏≠‰ªçÂèØËÉΩËø∑Â§±Èò∂ÊÆµ„ÄÇ |\n| **4. PALM (Êú¨Â∑•‰Ωú)** | **ËÄ¶ÂêàÁªìÊûÑÂåñAffordanceÊé®ÁêÜ‰∏éËøûÁª≠ËøõÂ∫¶ÊÑüÁü•**ÔºåÂΩ¢Êàê‰∏Ä‰∏™Èó≠ÁéØÁöÑÊÑüÁü•-Âä®‰Ωú-ËøõÂ∫¶Á≥ªÁªü„ÄÇ | PALM | 1. **Â§öÁ≤íÂ∫¶AffordanceÈ¢ÑÊµã**ÔºöÂêåÊó∂È¢ÑÊµãÁâ©‰Ωì„ÄÅÊé•Ëß¶ÁÇπ„ÄÅÊîæÁΩÆÂå∫Âüü„ÄÅËøêÂä®ËΩ®ËøπÂõõÁßç‰∫íË°•affordanceÔºåÊèê‰æõÂÖ®Èù¢ÁöÑ‰∫§‰∫íÁ∫øÁ¥¢„ÄÇ<br>2. **ËøûÁª≠ËøõÂ∫¶‰º∞ËÆ°**ÔºöËÅîÂêàÈ¢ÑÊµãÂä®‰ΩúÂíåÊ†áÈáèËøõÂ∫¶ÂÄºÔºåÊòæÂºèË∑üË∏™Â≠ê‰ªªÂä°ÂÜÖÈÉ®ËøõÂ±ïÔºåÂÆûÁé∞Âπ≥ÊªëËøáÊ∏°„ÄÇ<br>3. **Áªü‰∏ÄÊ°ÜÊû∂**ÔºöÂ∞Üaffordance‰Ωú‰∏∫Êù°‰ª∂ÔºåÈÄöËøáÊâ©Êï£TransformerËÅîÂêàËß£Á†ÅÂä®‰ΩúÂíåËøõÂ∫¶ÔºåÂÆûÁé∞Á´ØÂà∞Á´ØËÆ≠ÁªÉ‰∏éÊé®ÁêÜ„ÄÇ | Ê®°ÂûãÁõ∏ÂØπÂ§çÊùÇÔºåÈúÄË¶ÅÂ§öÈò∂ÊÆµËÆ≠ÁªÉÂíåÁâπÂÆöÊ†áÊ≥®Êï∞ÊçÆ„ÄÇ |\n\n---\n\n### ÁùÄÈáçÂàÜÊûêÁöÑÂá†‰∏™ÂèëÂ±ïÊñπÂêë\n\n#### ÂèëÂ±ïÊñπÂêë‰∏ÄÔºö**‰ªéÁõ¥Êé•Âä®‰ΩúÈ¢ÑÊµãÂà∞‰ª•Êú™Êù•È¢ÑÊµã‰∏∫ÂºïÂØº**\n\n*   **Ââç‰ΩúÔºàÂ¶ÇSeer, GR-1ÔºâÁöÑÊîπËøõ**ÔºöËÆ§ËØÜÂà∞‰ªÖÊò†Â∞ÑÂΩìÂâçÁä∂ÊÄÅÂà∞Âä®‰ΩúÁöÑÂ±ÄÈôêÊÄßÔºåÂºïÂÖ•‰∫Ü**Êú™Êù•Áä∂ÊÄÅÈ¢ÑÊµã**‰Ωú‰∏∫ËæÖÂä©‰ªªÂä°„ÄÇËøôÁõ∏ÂΩì‰∫éËÆ©Ê®°ÂûãËøõË°å‚ÄúÂøÉÁêÜÊ®°Êãü‚ÄùÔºåÈ¢ÑÊµãÊâßË°åÂä®‰ΩúÂêéÁöÑÁªìÊûúÔºå‰ªéËÄåÈÄâÊã©ËÉΩÂØºÂêëÈ¢ÑÊúüÊú™Êù•ÁöÑÂä®‰Ωú„ÄÇËøôÊèêÂçá‰∫ÜÁ≠ñÁï•Âú®Â§öÊ≠•‰ªªÂä°‰∏≠ÁöÑËøûË¥ØÊÄßÂíåÁõÆÁöÑÊÄß„ÄÇ\n*   **PALMÁöÑËøõ‰∏ÄÊ≠•Êé®Ëøõ**ÔºöPALMËÆ§‰∏∫È¢ÑÊµãÊï¥‰∏™Êú™Êù•ÂõæÂÉèÊòØ‰ΩéÊïà‰∏î‰∏éÂä®‰ΩúËß£ËÄ¶ÁöÑ„ÄÇÂÆÉËΩ¨ËÄåÈ¢ÑÊµã**ÁªìÊûÑÂåñÁöÑÊú™Êù•affordance**„ÄÇËøôÊúâ‰∏§Â§ß‰ºòÂäøÔºö\n    1.  **È´òÊïà‰∏éÁõ∏ÂÖ≥**ÔºöAffordance latentÊòØÁ¥ßÂáëÁöÑ„ÄÅ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑË°®ÂæÅÔºåÁõ¥Êé•ÂõûÁ≠î‰∫Ü‚Äú‰∏ã‰∏ÄÊ≠•ÂèØËÉΩÂú®Âì™Èáå„ÄÅÂ¶Ç‰Ωï‰∫§‰∫í‚ÄùÔºåËÆ°ÁÆóÊàêÊú¨Ëøú‰Ωé‰∫éÂõæÂÉèÁîüÊàê„ÄÇ\n    2.  **ÁªìÊûÑÂåñ‰ø°ÊÅØ**ÔºöÂõõÁßçaffordanceÂàÜÂà´‰ªéÂÖ®Â±ÄËØ≠‰πâ„ÄÅÂ±ÄÈÉ®Âá†‰Ωï„ÄÅÁ©∫Èó¥ÂÖ≥Á≥ªÂíåËøêÂä®Âä®ÊÄÅÊèê‰æõ‰ø°ÊÅØÔºåÊØîÂçï‰∏ÄÁöÑÂõæÂÉèÊàñÁä∂ÊÄÅÈ¢ÑÊµãÂåÖÂê´Êõ¥‰∏∞ÂØåÁöÑÂºïÂØº‰ø°Âè∑„ÄÇ\n\n#### ÂèëÂ±ïÊñπÂêë‰∫åÔºö**‰ªéÁ≤óÁ≥ôËØ≠‰πâÁêÜËß£Âà∞ÁªÜÁ≤íÂ∫¶Á©∫Èó¥/Âá†‰Ωï grounding**\n\n*   **Ââç‰ΩúÔºàÂ¶ÇRoboPoint, CoT-VLAÔºâÁöÑÊîπËøõ**ÔºöÊó©ÊúüVLAÊ®°ÂûãÁöÑÁ©∫Èó¥ÁêÜËß£ËæÉÁ≤óÁ≥ô„ÄÇËøô‰∫õÂ∑•‰ΩúÂºïÂÖ•‰∫Ü**ÂÖ≥ÈîÆÁÇπÊ£ÄÊµã„ÄÅËßÜËßâÈìæÂºèÊé®ÁêÜÔºàCoTÔºâÊàñaffordanceÁÉ≠Âõæ**Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂÖ≥Ê≥®Âà∞Âú∫ÊôØ‰∏≠‰∏é‰ªªÂä°ÊâßË°åÂÖ∑‰ΩìÁõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÔºàÂ¶ÇÊääÊâã„ÄÅÊîæÁΩÆÈù¢ÔºâÔºåÊòæËëóÊèêÂçá‰∫ÜÊìç‰ΩúÁöÑÁ≤æÁ°ÆÂ∫¶„ÄÇ\n*   **PALMÁöÑËøõ‰∏ÄÊ≠•Êé®Ëøõ**ÔºöPALMÊ≤°Êúâ‰ΩøÁî®Âçï‰∏ÄÁöÑ grounding ÊñπÂºèÔºåËÄåÊòØÊèêÂá∫‰∫Ü‰∏Ä‰∏™**Â§öÁ≤íÂ∫¶„ÄÅ‰∫íË°•ÁöÑaffordanceÈ¢ÑÊµãÊ°ÜÊû∂**Ôºö\n    *   **Global**ÔºöËØÜÂà´ÂíåÂàÜÂâ≤‰ªªÂä°Áõ∏ÂÖ≥Áâ©‰ΩìÔºà‚ÄúÊìç‰Ωú‰ªÄ‰πà‚ÄùÔºâ„ÄÇ\n    *   **Local**ÔºöÈ¢ÑÊµãÁ≤æÁ°ÆÁöÑÊé•Ëß¶ÁÇπÁÉ≠ÂõæÔºà‚ÄúÂÖ∑‰ΩìÁ¢∞Âì™Èáå‚ÄùÔºâ„ÄÇ\n    *   **Spatial**ÔºöÁîüÊàêÂ§ö‰∏™ÂÄôÈÄâÊîæÁΩÆÂå∫ÂüüÔºà‚ÄúÊîæÂà∞Âì™Èáå‚ÄùÔºâ„ÄÇ\n    *   **Dynamic**ÔºöÈ¢ÑÊµãÂèØËÉΩÂèëÁîüËøêÂä®ÁöÑÂå∫ÂüüÔºà‚Äú‰ªÄ‰πà‰ºöÂä®‚ÄùÔºâ„ÄÇ\n    ËøôÁßçÁªÑÂêàÊèê‰æõ‰∫Ü‰ªéÂÆèËßÇÊÑèÂõæÂà∞ÂæÆËßÇÊâßË°åÁöÑÂÖ®Êñπ‰Ωç groundingÔºåÊØîÂçï‰∏ÄÁöÑÂÖ≥ÈîÆÁÇπÊàñÁÉ≠ÂõæÊõ¥ÂÖ®Èù¢„ÄÅÊõ¥È≤ÅÊ£í„ÄÇ\n\n#### ÂèëÂ±ïÊñπÂêë‰∏âÔºö**‰ªéÈöêÂºèÈò∂ÊÆµÁÆ°ÁêÜÂà∞ÊòæÂºèËøõÂ∫¶ÊÑüÁü•**\n\n*   **Ââç‰ΩúÔºàÂ¶ÇÂü∫‰∫éHierarchical RLÊàñ‰ªªÂä°ÂàÜËß£ÁöÑÊñπÊ≥ïÔºâÁöÑÊîπËøõ**Ôºö‰º†ÁªüÊñπÊ≥ï‰ΩøÁî®ÊòæÂºèÁöÑÂàÜÂ±ÇÁ≠ñÁï•ÊàñÁ¨¶Âè∑ËßÑÂàíÊù•ÁÆ°ÁêÜÈïøÊó∂Á®ã‰ªªÂä°ÁöÑ‰∏çÂêåÈò∂ÊÆµÔºå‰ΩÜ‰æùËµñ‰∫∫Â∑•ÂÆö‰πâÊàñÈöæ‰ª•Â≠¶‰π†„ÄÇËøëÊúü‰∏Ä‰∫õÂ∑•‰ΩúÂ∞ùËØï‰ªéËßÜÈ¢ëÊï∞ÊçÆ‰∏≠Â≠¶‰π†**ËøõÂ±ïÂ•ñÂä±ÊàñË°®ÂæÅ**„ÄÇ\n*   **PALMÁöÑËøõ‰∏ÄÊ≠•Êé®Ëøõ**ÔºöPALMÂàõÊñ∞ÊÄßÂú∞Â∞Ü**ËøûÁª≠ËøõÂ∫¶‰º∞ËÆ°**‰Ωú‰∏∫Á≠ñÁï•ÁöÑ‰∏Ä‰∏™Ê†∏ÂøÉËæìÂá∫Ôºå‰∏éÂä®‰ΩúËÅîÂêàÈ¢ÑÊµã„ÄÇ\n    *   **ÂÖ≥ÈîÆÂàõÊñ∞**Ôºö`(a_t, p_t)` ÁöÑËÅîÂêàËæìÂá∫„ÄÇÊ†áÈáè `p_t` Ë°®Á§∫ÂΩìÂâçÂ≠ê‰ªªÂä°ÂÆåÊàêÁöÑÁôæÂàÜÊØî„ÄÇ\n    *   **‰ª∑ÂÄº**Ôºö\n        1.  **Ê∂àÈô§Ê≠ß‰πâ**ÔºöËßÜËßâ‰∏äÁõ∏‰ººÁöÑËßÇÊµãÂèØËÉΩÂ±û‰∫é‰ªªÂä°ÁöÑ‰∏çÂêåÈò∂ÊÆµÔºåÈúÄË¶Å‰∏çÂêåÁöÑÂä®‰Ωú„ÄÇËøõÂ∫¶ÂÄº `p_t` Êèê‰æõ‰∫ÜÈ¢ùÂ§ñÁöÑÊó∂Â∫è‰∏ä‰∏ãÊñáÊù•Ê∂àÈô§ËøôÁßçÊ≠ß‰πâ„ÄÇ\n        2.  **Âπ≥ÊªëËøáÊ∏°**ÔºöËøõÂ∫¶ÂÄºËá™ÁÑ∂‰∏äÂçáËá≥ÈòàÂÄºÔºàÂ¶Ç90%ÔºâÊó∂Ëß¶ÂèëÂ≠ê‰ªªÂä°ÂàáÊç¢ÔºåÂÆûÁé∞‰∫ÜÊó†ÈúÄÈ´òÂ±ÇËßÑÂàíÂô®ÁöÑ„ÄÅÊï∞ÊçÆÈ©±Âä®ÁöÑÂπ≥ÊªëÈò∂ÊÆµËøáÊ∏°„ÄÇ\n        3.  **Á®≥ÂÆöËÆ≠ÁªÉ**ÔºöËøõÂ∫¶‰ø°Âè∑‰Ωú‰∏∫‰∏ÄÁßçÊó∂Â∫èÊ≠£ÂàôÂåñÔºåÈºìÂä±Á≠ñÁï•‰∫ßÁîüÂçïË∞É„ÄÅËøûË¥ØÁöÑË°å‰∏∫Â∫èÂàóÔºåÈÅøÂÖç‰∫ÜË°å‰∏∫ÂÖãÈöÜ‰∏≠Â∏∏ËßÅÁöÑÈò∂ÊÆµÊ∑∑Ê∑ÜÈóÆÈ¢ò„ÄÇ\n\n### ÊÄªÁªì\n\nPALMÁöÑÂ∑•‰ΩúÁ´ôÂú®‰∫ÜVLAÊ®°ÂûãÁî®‰∫éÈïøÊó∂Á®ãÊìç‰ΩúÁ†îÁ©∂ÁöÑÂâçÊ≤øÔºåÂÆÉ**Á≥ªÁªüÊÄßÂú∞Êï¥ÂêàÂπ∂Êé®Ëøõ‰∫ÜÂ§ö‰∏™ÂèëÂ±ïÊñπÂêë**Ôºö\n1.  ÂÆÉÂ∞Ü**Êú™Êù•È¢ÑÊµã**‰ªé‚ÄúÈ¢ÑÊµãÂÉèÁ¥†‚ÄùÂçáÁ∫ß‰∏∫‚ÄúÈ¢ÑÊµãÁªìÊûÑÂåñ‰∫§‰∫íÂèØËÉΩÊÄßÔºàaffordanceÔºâ‚Äù„ÄÇ\n2.  ÂÆÉÂ∞Ü**Á©∫Èó¥ grounding** ‰ªéÂçï‰∏ÄÂΩ¢ÂºèÊâ©Â±ï‰∏∫Â§öÁ≤íÂ∫¶‰∫íË°•ÁöÑËÅîÂêàÈ¢ÑÊµã„ÄÇ\n3.  ÂÆÉÂºÄÂàõÊÄßÂú∞Â∞Ü**ÊòæÂºè„ÄÅËøûÁª≠ÁöÑËøõÂ∫¶ÊÑüÁü•**Ê∑±Â∫¶ÈõÜÊàêÂà∞Á≠ñÁï•Â≠¶‰π†‰∏≠ÔºåËß£ÂÜ≥‰∫ÜÈïøÊó∂Á®ã‰ªªÂä°‰∏≠ÊúÄÊ†∏ÂøÉÁöÑÊó∂Â∫è‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇ\n\nÂõ†Ê≠§ÔºåPALMÁöÑÊ†∏ÂøÉË¥°ÁåÆÂú®‰∫éÊûÑÂª∫‰∫Ü‰∏Ä‰∏™**‚ÄúÊÑüÁü•ÔºàÂ§öÁ≤íÂ∫¶affordanceÔºâ-Êé®ÁêÜÔºàËøõÂ∫¶Ë∑üË∏™Ôºâ-Âä®‰ΩúÔºàÊù°‰ª∂ÁîüÊàêÔºâ‚ÄùÁöÑÈó≠ÁéØ**ÔºåÈÄöËøáÂÜÖÈÉ®ÁöÑÁªìÊûÑÂåñÊé®ÁêÜÂíåÊòæÂºèÁä∂ÊÄÅË∑üË∏™ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇ„ÄÅÂ§öÊ≠•È™§Êìç‰Ωú‰ªªÂä°‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊàêÂäüÁéá„ÄÇÂÖ∂ÂÆûÈ™åÁªìÊûúÂú®‰ªøÁúüÂíåÁé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÊòæËëó‰ºòÂäøÔºåÈ™åËØÅ‰∫ÜËøôÊù°ÊäÄÊúØË∑ØÁ∫øÁöÑÊúâÊïàÊÄßÂíåÂÆûÈôÖ‰ª∑ÂÄº„ÄÇ",
            "timestamp": "2026-01-21T12:11:09.118776",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñÁöÑÂâç‰ΩúÊúâÂì™‰∫õÔºü‰ΩøÁî®Ë°®Ê†º‰ªîÁªÜËÆ≤ËÆ≤‰ªñÁöÑÊØèÁØáÂâç‰ΩúÔºå‰ªñÂíåÂâç‰ΩúÁöÑÂå∫Âà´ÊòØ‰ªÄ‰πàÔºå‰∏ªË¶ÅÊîπÂñÑÊòØ‰ªÄ‰πàÔºüÁùÄÈáç‰∫éÂÖ∑‰ΩìÁõ∏ÊØî‰∫é‰πãÂâçÊñáÁ´†ÁöÑÊîπÂä®",
            "answer": "## PALM ÁöÑÂâç‰ΩúÂàÜÊûê\n\nÊ†πÊçÆËÆ∫ÊñáÂÜÖÂÆπÔºåPALM ÁöÑÁ†îÁ©∂Âõ¢ÈòüÔºàÂ∞§ÂÖ∂ÊòØ‰ΩúËÄÖ Yuanzhe Liu Á≠â‰∫∫ÔºâÂú®Áõ∏ÂÖ≥È¢ÜÂüüÊúâÂ§öÁØáÂâçÊúüÂ∑•‰Ωú„ÄÇËøô‰∫õÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠Âú®**ËßÜËßâËØ≠Ë®ÄÂä®‰ΩúÊ®°Âûã**„ÄÅ**ÂÖ∑Ë∫´Êô∫ËÉΩ**Âíå**ÈïøÊó∂Á®ãÊìç‰Ωú**Á≠âÊñπÂêë„ÄÇ‰ª•‰∏ãË°®Ê†ºËØ¶ÁªÜÊ¢≥ÁêÜ‰∫ÜÂÖ∂‰∏ªË¶ÅÂâç‰ΩúÔºåÂπ∂ÈáçÁÇπÂàÜÊûê‰∫Ü PALM ‰∏éËøô‰∫õÂâç‰ΩúÁöÑÂå∫Âà´‰∏éÊîπËøõ„ÄÇ\n\n| Ââç‰ΩúËÆ∫ÊñáÊ†áÈ¢ò | Ê†∏ÂøÉË¥°ÁåÆ / ÊñπÊ≥ï | ‰∏é PALM ÁöÑ‰∏ªË¶ÅÂå∫Âà´ | PALM ÁöÑ‰∏ªË¶ÅÊîπËøõ / ÂàõÊñ∞ÁÇπ |\n| :--- | :--- | :--- | :--- |\n| **RoboPoint: Vision-Language Models as Spatial Reasoners for Robotic Manipulation** (Yuanzhe Liu Á≠â, 2024) | ÊèêÂá∫ **RoboPoint**ÔºåÂà©Áî®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãËøõË°å**Á©∫Èó¥Êé®ÁêÜ**ÔºåÂ∞ÜÊ®°Á≥äÁöÑÁ©∫Èó¥ËØ≠Ë®ÄÊåá‰ª§ÔºàÂ¶Ç‚ÄúÈù†Ëøë‚Äù„ÄÅ‚ÄúÂ∑¶Ëæπ‚ÄùÔºâËΩ¨Âåñ‰∏∫Êú∫Âô®‰∫∫ÂèØÊâßË°åÁöÑ2DÂùêÊ†áÁÇπ„ÄÇ | 1. **ËåÉÂõ¥ËÅöÁÑ¶**Ôºö‰∏ìÊ≥®‰∫é**Á©∫Èó¥Êåá‰ª§Ëß£Êûê**Ëøô‰∏ÄÁâπÂÆöÂ≠êÈóÆÈ¢ò„ÄÇ<br>2. **Ë°®Á§∫Âçï‰∏Ä**ÔºöËæìÂá∫‰∏ªË¶ÅÊòØ2DÂùêÊ†áÁÇπÔºåÁº∫‰πèÂØπ‰∫§‰∫íÂá†‰Ωï„ÄÅËøêÂä®Âä®ÊÄÅÁ≠âÂ§öÁª¥Â∫¶Á∫øÁ¥¢ÁöÑÊçïÊçâ„ÄÇ<br>3. **Êó†Áä∂ÊÄÅË∑üË∏™**ÔºöÊ≤°ÊúâÊòæÂºèÂú∞Âª∫Ê®°‰ªªÂä°ËøõÂ∫¶ÊàñÂ≠ê‰ªªÂä°Áä∂ÊÄÅËΩ¨Áßª„ÄÇ | 1. **Áªü‰∏ÄÊ°ÜÊû∂**ÔºöPALM Â∞ÜÁ©∫Èó¥Êé®ÁêÜÔºàSpatial AffordanceÔºâ**Êï¥Âêà**‰∏∫‰∏Ä‰∏™Êõ¥ÂπøÊ≥õÁöÑ„ÄÅÁªìÊûÑÂåñÁöÑ**ÂèØÈ¢ÑËßÅÊÄßÊé®ÁêÜ**Ê°ÜÊû∂‰∏≠ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ<br>2. **Â§öÁª¥Â∫¶Ë°®ÂæÅ**ÔºöÈô§‰∫ÜÁ©∫Èó¥‰ΩçÁΩÆÔºåPALM ËøòÈ¢ÑÊµã**ÂÖ®Â±ÄÁâ©‰ΩìÁõ∏ÂÖ≥ÊÄß„ÄÅÂ±ÄÈÉ®Êé•Ëß¶Âá†‰Ωï„ÄÅÂä®ÊÄÅËøêÂä®ËΩ®Ëøπ**ÔºåÂΩ¢Êàê‰∫íË°•ÁöÑ„ÄÅ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰∏≠Èó¥Ë°®ÂæÅ„ÄÇ<br>3. **ËøõÂ∫¶ÊÑüÁü•**ÔºöÂºïÂÖ•‰∫Ü**ËøûÁª≠ÁöÑÂ≠ê‰ªªÂä°ËøõÂ∫¶‰º∞ËÆ°**ÔºåÂÆûÁé∞‰∫ÜÊòæÂºèÁöÑÁä∂ÊÄÅË∑üË∏™ÂíåÊµÅÁïÖÁöÑÂ≠ê‰ªªÂä°ËΩ¨Êç¢ÔºåËøôÊòØ RoboPoint ÊâÄ‰∏çÂÖ∑Â§áÁöÑ„ÄÇ |\n| **MOKA: Open-World Robotic Manipulation through Spatial and Visual Goal Abstraction** (Liu Á≠â, 2024) | ÊèêÂá∫ **MOKA**ÔºåÈÄöËøá**Á©∫Èó¥ÂíåËßÜËßâÁõÆÊ†áÊäΩË±°**Êù•ÂÆûÁé∞ÂºÄÊîæ‰∏ñÁïåÁöÑÊú∫Âô®‰∫∫Êìç‰Ωú„ÄÇÂÆÉÂèØËÉΩÊ∂âÂèä‰ΩøÁî®ÂÖ≥ÈîÆÁÇπÊàñÊäΩË±°Ë°®Á§∫Êù•Ê≥õÂåñÂà∞Êñ∞Áâ©‰ΩìÂíåÊñ∞Âú∫ÊôØ„ÄÇ | 1. **ÁõÆÊ†áÊäΩË±°**ÔºöMOKA ‰æßÈáç‰∫éÂØπ**ÊúÄÁªàÁõÆÊ†á**ËøõË°åÊäΩË±°Ë°®Á§∫ÔºàÂ¶ÇÂÖ≥ÈîÆÁÇπÔºâÔºå‰ª•ÊîØÊåÅÈõ∂Ê†∑Êú¨Ê≥õÂåñ„ÄÇ<br>2. **Êé®ÁêÜËøáÁ®ã**ÔºöÂèØËÉΩÁº∫‰πèÂØπ**ÊâßË°åËøáÁ®ã‰∏≠Èó¥Ê≠•È™§**ÁöÑÁªÜÁ≤íÂ∫¶„ÄÅÁªìÊûÑÂåñÁöÑÂâçÁûªÊÄßÊé®ÁêÜ„ÄÇ<br>3. **Êó∂Â∫è‰∏ÄËá¥ÊÄß**ÔºöÊú™Âº∫Ë∞ÉÂú®ÈïøÊó∂Á®ã‰ªªÂä°‰∏≠Áª¥ÊåÅ**Ë∑®Â≠ê‰ªªÂä°ÁöÑÊó∂Â∫è‰∏ÄËá¥ÊÄß**ÂíåÁä∂ÊÄÅË∑üË∏™„ÄÇ | 1. **ËøáÁ®ãÊé®ÁêÜ**ÔºöPALM ÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫éÂØπ**Êú™Êù•‰∫§‰∫íÁ∫øÁ¥¢**ÔºàAffordanceÔºâËøõË°åÈ¢ÑÊµãÔºåËøô‰∏∫**ÂΩìÂâçÂä®‰ΩúÁîüÊàê**Êèê‰æõ‰∫ÜÁõ¥Êé•ÁöÑ„ÄÅÁªìÊûÑÂåñÁöÑÊù°‰ª∂‰ø°ÊÅØÔºåÊòØ‰∏ÄÁßç‚ÄúÂêëÂâçÁúã‚ÄùÁöÑÊé®ÁêÜÊú∫Âà∂„ÄÇ<br>2. **ÁªìÊûÑÂåñÈîöÁÇπ**ÔºöPALM ÁöÑÂõõÁßçÂèØÈ¢ÑËßÅÊÄßÔºàÂÖ®Â±Ä„ÄÅÂ±ÄÈÉ®„ÄÅÁ©∫Èó¥„ÄÅÂä®ÊÄÅÔºâ‰Ωú‰∏∫Á≠ñÁï•Â≠¶‰π†ÁöÑ**ÁªìÊûÑÂåñÈîöÁÇπ**ÔºåÊèê‰æõ‰∫ÜÊØîÂçï‰∏ÄÁõÆÊ†áÊäΩË±°Êõ¥‰∏∞ÂØå„ÄÅÊõ¥Ëß£ËÄ¶ÁöÑÂú∫ÊôØÁêÜËß£„ÄÇ<br>3. **Èó≠ÁéØÈõÜÊàê**ÔºöPALM Â∞ÜÂèØÈ¢ÑËßÅÊÄßÊé®ÁêÜ‰∏éËøõÂ∫¶ÊÑüÁü•Á≠ñÁï•**Á¥ßÂØÜËÄ¶ÂêàÂú®‰∏Ä‰∏™Á´ØÂà∞Á´ØÊ°ÜÊû∂**‰∏≠ÔºåÂΩ¢Êàê‰∫Ü‚ÄúÊÑüÁü•-Âä®‰Ωú-ËøõÂ∫¶‚ÄùÁöÑÈó≠ÁéØÔºå‰∏ìÈó®ÈíàÂØπÈïøÊó∂Á®ã‰ªªÂä°ÁöÑÁ®≥ÂÆöÊÄßËøõË°å‰ºòÂåñ„ÄÇ |\n| **Áõ∏ÂÖ≥Â∑•‰ΩúÔºàÈùûÁõ¥Êé•Ââç‰ΩúÔºå‰ΩÜ‰ª£Ë°®ÊäÄÊúØËÉåÊôØÔºâ**Ôºö **CoA-VLA**, **TraceVLA**, **Seer**, **3D-VLA** Á≠â | Ëøô‰∫õÂ∑•‰Ωú‰ª£Ë°®‰∫ÜVLAÈ¢ÜÂüüÁöÑ‰∏çÂêåÊîπËøõÊñπÂêëÔºö<br>- **CoA-VLA**: ÂºïÂÖ•ÊÄùÁª¥ÈìæÊé®ÁêÜ„ÄÇ<br>- **TraceVLA**: Ê≥®ÂÖ•ËßÜËßâ‰∫§‰∫íËΩ®Ëøπ‰Ωú‰∏∫ÊèêÁ§∫„ÄÇ<br>- **Seer**: ÈõÜÊàêÊú™Êù•È¢ÑÊµã‰Ωú‰∏∫‰∏≠Èó¥Ë°®Á§∫„ÄÇ<br>- **3D-VLA**: Âà©Áî®3DÊÑüÁü•Â¢ûÂº∫Êìç‰Ωú„ÄÇ | 1. **Ë°®Á§∫ÂΩ¢Âºè**ÔºöËøô‰∫õÊñπÊ≥ïÊàñ‰ΩøÁî®ËØ≠Ë®ÄÈìæ„ÄÅÊàñ‰ΩøÁî®ÂéÜÂè≤ÂõæÂÉè„ÄÅÊàñÈ¢ÑÊµãÊú™Êù•RGBÂõæÂÉè„ÄÅÊàñ‰æùËµñ3DÂá†‰ΩïÔºå‰ΩÜÈÉΩ**Ê≤°ÊúâÊòéÁ°ÆÊèêÂá∫‰∏ÄÂ•óÁªìÊûÑÂåñÁöÑ„ÄÅÂ§öÁ≤íÂ∫¶ÁöÑ‚ÄúÂèØÈ¢ÑËßÅÊÄß‚Äù‰Ωú‰∏∫Ê†∏ÂøÉ‰∏≠Èó¥Ë°®ÂæÅ**„ÄÇ<br>2. **ËøõÂ∫¶Âª∫Ê®°**ÔºöÂÆÉ‰ª¨ÊôÆÈÅç**Áº∫‰πèÂØπÂ≠ê‰ªªÂä°ÂÜÖÈÉ®ËøûÁª≠ËøõÂ∫¶ÁöÑÊòæÂºèÂª∫Ê®°ÂíåÂà©Áî®**ÔºåÊù•ÊåáÂØºÂä®‰ΩúÁîüÊàêÂíåÂ≠ê‰ªªÂä°ÂàáÊç¢„ÄÇ | 1. **ÂèØÈ¢ÑËßÅÊÄß‰Ωú‰∏∫Áªü‰∏ÄË°®ÂæÅ**ÔºöPALM ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞Ü‚Äú**Âú®Âì™ÈáåË°åÂä®„ÄÅÂ¶Ç‰ΩïË°åÂä®**‚ÄùÁöÑÁâ©ÁêÜÂèØËÉΩÊÄßÔºàÂç≥ÂèØÈ¢ÑËßÅÊÄßÔºâÊèêÁÇº‰∏∫‰∏ÄÁßç**ÂèØÂ≠¶‰π†ÁöÑ„ÄÅÁªìÊûÑÂåñÁöÑÊü•ËØ¢ÂíåÈ¢ÑÊµãÁõÆÊ†á**„ÄÇËøôÊØîÈ¢ÑÊµãÊï¥Âº†Êú™Êù•ÂõæÂÉèÊõ¥Á¥ßÂáë„ÄÅÊõ¥‰ªªÂä°Áõ∏ÂÖ≥ÔºåÊØîËØ≠Ë®ÄÈìæÊõ¥ grounded ‰∫éËßÜËßâÂá†‰Ωï„ÄÇ<br>2. **ËøõÂ∫¶‰Ωú‰∏∫Êó∂Â∫èÊ≠£ÂàôÂô®**ÔºöPALM **Âú®Âä®‰ΩúÁ©∫Èó¥È¢ùÂ§ñÂ¢ûÂä†‰∫Ü‰∏Ä‰∏™ËøûÁª≠ÁöÑËøõÂ∫¶Ê†áÈáè**ÔºåÂπ∂‰∏éÂä®‰ΩúÈÄöËøáÊâ©Êï£ÂèòÊç¢Âô®ËÅîÂêàÈ¢ÑÊµã„ÄÇËøô‰∏™ËøõÂ∫¶‰ø°Âè∑‰Ωú‰∏∫‰∏Ä‰∏™ÂÜÖÂú®ÁöÑÊó∂Â∫èÊ≠£ÂàôÂô®ÔºåÊúâÊïàÊ∂àÈô§‰∫ÜÈïøÊó∂Á®ã‰ªªÂä°‰∏≠ËßÜËßâÁõ∏‰ººÁä∂ÊÄÅ‰∏ãÁöÑÂä®‰ΩúÊ≠ß‰πâÔºåÈò≤Ê≠¢‰∫ÜÈáçÂ§ç„ÄÅË∑≥ËøáÊàñÊèêÂâçÁªàÊ≠¢Á≠âÂÖ∏ÂûãÈîôËØØ„ÄÇ |\n\n### ÊÄªÁªìÔºöPALM Áõ∏ÂØπ‰∫éÂâç‰ΩúÁöÑÊ†∏ÂøÉÁêÜÂøµÊèêÂçá\n\n1.  **‰ªé‚ÄúËß£ÂÜ≥Â≠êÈóÆÈ¢ò‚ÄùÂà∞‚ÄúÊûÑÂª∫Áªü‰∏ÄÊé®ÁêÜÊ°ÜÊû∂‚Äù**ÔºöÂâç‰ΩúÔºàÂ¶ÇRoboPointÔºâËá¥Âäõ‰∫éËß£ÂÜ≥ÈïøÊó∂Á®ãÊìç‰Ωú‰∏≠ÁöÑÊüê‰∏™ÂÖ∑‰ΩìÊåëÊàòÔºàÂ¶ÇÁ©∫Èó¥Êåá‰ª§Ëß£ÊûêÔºâ„ÄÇËÄåPALMÊó®Âú®ÊûÑÂª∫‰∏Ä‰∏™**Áªü‰∏ÄÁöÑ„ÄÅÂÜÖÁúÅÁöÑÊé®ÁêÜÊ°ÜÊû∂**ÔºåËØ•Ê°ÜÊû∂ËÉΩËá™Âä®ÂàÜËß£‰ªªÂä°ÔºåÂπ∂È¢ÑÊµãÊâßË°å‰∏ã‰∏ÄÊ≠•ÊâÄÈúÄÁöÑÊâÄÊúâÂÖ≥ÈîÆ‰∫§‰∫íÁ∫øÁ¥¢ÔºàÂèØÈ¢ÑËßÅÊÄßÔºâ„ÄÇ\n\n2.  **‰ªé‚ÄúÈùôÊÄÅÁõÆÊ†áË°®Á§∫‚ÄùÂà∞‚ÄúÂä®ÊÄÅËøáÁ®ãÊé®ÁêÜ‚Äù**ÔºöÂâç‰ΩúÔºàÂ¶ÇMOKAÂèäÁõ∏ÂÖ≥Â∑•‰ΩúÔºâ‰æßÈáç‰∫éÂØπ‰ªªÂä°ÁõÆÊ†áÊàñÂéÜÂè≤Áä∂ÊÄÅËøõË°åË°®ÂæÅ„ÄÇPALMÂàôÂº∫Ë∞ÉÂØπ**Êú™Êù•‰∫§‰∫íÁä∂ÊÄÅÁöÑÈ¢ÑÊµã**ÔºåËøôÊòØ‰∏ÄÁßç‰∏ªÂä®ÁöÑ„ÄÅÈù¢ÂêëËøáÁ®ãÁöÑÊé®ÁêÜÔºåËÉΩ‰∏∫Âç≥Êó∂Âä®‰ΩúÁîüÊàêÊèê‰æõÊõ¥Áõ¥Êé•ÁöÑÊåáÂØº„ÄÇ\n\n3.  **ÂºïÂÖ•‚ÄúËøõÂ∫¶‚Äù‰Ωú‰∏∫Ê†∏ÂøÉÁä∂ÊÄÅÂèòÈáè**ÔºöËøôÊòØPALMÊúÄÊòæËëóÁöÑÂàõÊñ∞‰πã‰∏Ä„ÄÇ‰πãÂâçÁöÑVLAÊ®°ÂûãÂ§ßÂ§öÈöêÂºèÂú∞Â§ÑÁêÜ‰ªªÂä°Èò∂ÊÆµÔºåËÄåPALM**ÊòæÂºèÂú∞Âª∫Ê®°Âπ∂È¢ÑÊµãÂ≠ê‰ªªÂä°ËøõÂ∫¶**ÔºåÂ∞ÜÂÖ∂‰Ωú‰∏∫Á≠ñÁï•ÁöÑ‰∏ÄÈÉ®ÂàÜËæìÂá∫„ÄÇËøôÁõ∏ÂΩì‰∫é‰∏∫Á≠ñÁï•ÈÖçÂ§á‰∫Ü‰∏Ä‰∏™‚ÄúÊàëÂú®Âì™‰∏ÄÈò∂ÊÆµ‚ÄùÁöÑÊåÅÁª≠Ëá™ÁúÅ‰ø°Âè∑ÔºåÊûÅÂ§ßÂú∞Â¢ûÂº∫‰∫ÜÈïøÊó∂Á®ãÊâßË°åÁöÑ**Êó∂Â∫èËøûË¥ØÊÄßÂíåÈ≤ÅÊ£íÊÄß**„ÄÇ\n\n4.  **ÊäÄÊúØÂÆûÁé∞‰∏äÁöÑÈõÜÊàê‰∏éÊ∑±Âåñ**ÔºöPALMÂú®Êû∂ÊûÑ‰∏äÈõÜÊàê‰∫ÜÂ§öÁßçÂÖàËøõÁªÑ‰ª∂ÔºàÂ¶ÇGPTÈ£éÊ†ºTransformer„ÄÅÊâ©Êï£ÂèòÊç¢Âô®„ÄÅÂ§öÁßçÈ¢ÑËÆ≠ÁªÉÁºñÁ†ÅÂô®ÔºâÔºåÂπ∂ÂàõÈÄ†ÊÄßÂú∞ËÆæËÆ°‰∫Ü**‰∏§Â•óÂèØÂ≠¶‰π†Êü•ËØ¢**ÔºàÂèØÈ¢ÑËßÅÊÄßÊü•ËØ¢„ÄÅÂä®‰Ωú-ËøõÂ∫¶Êü•ËØ¢ÔºâÂíå**ÂùóÁä∂ÁªìÊûÑÂåñÊ≥®ÊÑèÂäõÊú∫Âà∂**ÔºåÊù•ÂÆûÁé∞ÂÖ∂Ê†∏ÂøÉÁêÜÂøµ„ÄÇÂÆÉ‰∏çÊòØÂú®Êüê‰∏™Âçï‰∏ÄÊäÄÊúØ‰∏äÁ™ÅÁ†¥ÔºåËÄåÊòØÂú®**Á≥ªÁªüËÆæËÆ°Â±ÇÈù¢**ËøõË°å‰∫ÜÊ∑±Â∫¶Êï¥Âêà‰∏éÂàõÊñ∞„ÄÇ\n\n**ÊÄªËÄåË®Ä‰πãÔºåPALM ÁöÑÊ†∏ÂøÉÊîπÂñÑÂú®‰∫éÔºöÂÆÉÈÄöËøáÁªìÊûÑÂåñÂèØÈ¢ÑËßÅÊÄßÊé®ÁêÜÂíåËøûÁª≠ËøõÂ∫¶‰º∞ËÆ°Ôºå‰∏∫VLAÊ®°ÂûãËµã‰∫à‰∫ÜÁ±ª‰ºº‚ÄúÂÜÖÁúÅ‚ÄùÂíå‚ÄúÂâçÁûª‚ÄùÁöÑËÉΩÂäõÔºå‰ªéËÄåÁ≥ªÁªüÊÄßÂú∞Ëß£ÂÜ≥‰∫ÜÈïøÊó∂Á®ãÊìç‰Ωú‰∏≠Âõ†Áº∫‰πè‰∏≠Èó¥Ë°®Á§∫ÂíåÁä∂ÊÄÅË∑üË∏™ËÄåÂØºËá¥ÁöÑÊâßË°åÈîôËØØÈóÆÈ¢ò„ÄÇ** ÂÖ∂ÂÆûÈ™åÁªìÊûúÂú®‰ªøÁúüÂíåÁé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÊòæËëóÊèêÂçáÔºåÈ™åËØÅ‰∫ÜËøôÁßçÁ≥ªÁªüÁ∫ßËÆæËÆ°ÂàõÊñ∞ÁöÑÊúâÊïàÊÄß„ÄÇ",
            "timestamp": "2026-01-21T12:12:00.927781",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÊèêÂá∫‰∫ÜÂì™‰∫õÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÔºåËØ∑ÂàóË°®Ê†ºÂÖ∑‰ΩìËØ¶ÁªÜËØ¥ÊòéÊäÄÊúØÁªÜËäÇÔºåÈúÄË¶ÅÂåÖÂê´ÂÖ∑‰ΩìÁöÑÊï∞Â≠¶ÂéüÁêÜÊé®ÂØºÔºå‰ª•ÂèäÂÖ∑‰ΩìÂèÇÊï∞„ÄÇ",
            "answer": "Â•ΩÁöÑÔºå‰Ωú‰∏∫‰∏ÄÂêç‰∏ì‰∏öÁöÑÂ≠¶ÊúØËÆ∫ÊñáÂàÜÊûêÂä©ÊâãÔºåÊàëÂ∞Ü‰∏∫ÊÇ®Ê∑±ÂÖ•ÂàÜÊûêËÆ∫Êñá„ÄäPALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation„ÄãÊèêÂá∫ÁöÑÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ï„ÄÇ\n\n## ËÆ∫ÊñáÊ†∏ÂøÉÊäÄÊúØÂàõÊñ∞\n\nPALM ÁöÑÊ†∏ÂøÉÂàõÊñ∞Âú®‰∫é‰∏∫Ëß£ÂÜ≥ÈïøËßÜÈáé„ÄÅÂ§öÊ≠•È™§Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°‰∏≠ÁöÑ‰∏§Â§ßÂÖ≥ÈîÆÊåëÊàò‚Äî‚Äî**‰ªªÂä°Áõ∏ÂÖ≥‰∫§‰∫íÁ∫øÁ¥¢Ê®°Á≥ä**Âíå**Â≠ê‰ªªÂä°ÂÜÖÈÉ®Áä∂ÊÄÅË∑üË∏™Áº∫Â§±**‚Äî‚ÄîËÄåËÆæËÆ°‰∫Ü‰∏Ä‰∏™Áªü‰∏ÄÁöÑÁ´ØÂà∞Á´ØÊ°ÜÊû∂„ÄÇÂÖ∂ÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïÂèØÂΩíÁ∫≥‰∏∫‰ª•‰∏ã‰∏§Â§ßÊ®°ÂùóÔºö\n\n1.  **ÁªìÊûÑÂåñ„ÄÅÁªÜÁ≤íÂ∫¶ÁöÑÂèØÊìç‰ΩúÊÄßÈ¢ÑÊµãÊ®°Âùó**Ôºö‰Ωú‰∏∫ÈöêÂºèÁöÑ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§ÔºåÈ¢ÑÊµãÊú™Êù•‰∫§‰∫íÁä∂ÊÄÅÁöÑÁªìÊûÑÂåñË°®ÂæÅ„ÄÇ\n2.  **Âü∫‰∫éÈÄÜÂä®ÂäõÂ≠¶ÁöÑËøõÂ∫¶ÊÑüÁü•Á≠ñÁï•Ê®°Âùó**ÔºöÂ∞ÜÂä®‰ΩúÁîüÊàê‰∏éÂ≠ê‰ªªÂä°ËøõÂ∫¶‰º∞ËÆ°ËÄ¶ÂêàÔºåÁ°Æ‰øùÈïøÂ∫èÂàóÊâßË°åÁöÑÊó∂Èó¥‰∏ÄËá¥ÊÄß„ÄÇ\n\n‰∏ãÈù¢‰ª•Ë°®Ê†ºÂΩ¢ÂºèËØ¶ÁªÜËØ¥ÊòéÂêÑÈ°πÊäÄÊúØÁªÜËäÇÔºåÂåÖÊã¨Êï∞Â≠¶ÂéüÁêÜÂíåÂÖ∑‰ΩìÂèÇÊï∞„ÄÇ\n\n### ÂÖ≥ÈîÆÊäÄÊúØÊñπÊ≥ïËØ¶Ëø∞\n\n| Ê®°Âùó | Â≠êÁªÑ‰ª∂/ÊñπÊ≥ï | ÊäÄÊúØÁªÜËäÇ‰∏éÊï∞Â≠¶ÂéüÁêÜ | ÂÖ∑‰ΩìÂèÇÊï∞‰∏éÂÆûÁé∞ |\n| :--- | :--- | :--- | :--- |\n| **1. ÁªÜÁ≤íÂ∫¶ÂèØÊìç‰ΩúÊÄßÈ¢ÑÊµã** | **Êï¥‰ΩìÊû∂ÊûÑ** | ÂºïÂÖ•‰∏ÄÁªÑ**ÂèØÂ≠¶‰π†ÁöÑÂèØÊìç‰ΩúÊÄßÊü•ËØ¢**ÔºåÈÄöËøáÂùóÁä∂ÁªìÊûÑÂåñÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åÁ≤æÁÇºÔºåÂπ∂ÁâπÂåñ‰∏∫ÂõõÁßçÁ±ªÂûã„ÄÇËøô‰∫õÊü•ËØ¢ÈÄöËøáÂõ†ÊûúÊ≥®ÊÑèÂäõ‰∏éÂ§öÊ®°ÊÄÅ‰∏ä‰∏ãÊñáÔºàËØ≠Ë®Ä„ÄÅËßÜËßâ„ÄÅÁä∂ÊÄÅÔºâ‰∫§‰∫íÔºåÈ¢ÑÊµãÊú™Êù•Êó∂Âàª `t+n` ÁöÑÁªìÊûÑÂåñÂèØÊìç‰ΩúÊÄßÊΩúÂú®Ë°®Á§∫ `ùêÖÃÇ_{t+n}`„ÄÇ | ‚Ä¢ **Êü•ËØ¢Á±ªÂûã**Ôºö4ÁßçÔºàGlobal, Local, Spatial, DynamicÔºâ<br>‚Ä¢ **È¢ÑÊµãÂÅèÁßª**Ôºö`n` Ê≠•ÔºàÊú™Êù•Ôºâ<br>‚Ä¢ **Ê≥®ÊÑèÂäõÊú∫Âà∂**ÔºöÂõ†ÊûúÊ≥®ÊÑèÂäõÔºåÁ°Æ‰øùÊó∂Â∫è‰∏ÄËá¥ÊÄß |\n| | **ÂÖ®Â±ÄÂèØÊìç‰ΩúÊÄß** | **ÁõÆÊ†á**ÔºöËØÜÂà´‰∏éÊåá‰ª§Áõ∏ÂÖ≥ÁöÑ**ÂØπË±°**ÂèäÂÖ∂**Â§ßËá¥Âå∫Âüü**„ÄÇ<br>**Êï∞Â≠¶ÂéüÁêÜ**Ôºö<br>1. **ÁõëÁù£ÁõÆÊ†áÁîüÊàê**Ôºö‰ΩøÁî® Grounding DINO Ëß£ÊûêÊåá‰ª£ÂØπË±°ÔºåSAM ÁîüÊàêÂÆû‰æãÂàÜÂâ≤Êé©Á†Å `‚Ñ≥_{t+n}^{global}`„ÄÇ<br>2. **ÊçüÂ§±ÂáΩÊï∞**ÔºöÁªìÂêà Focal Loss Âíå Dice Loss ÁõëÁù£È¢ÑÊµãÁöÑÊé©Á†Å `‚Ñ≥ÃÇ_{t+n}^{global}`„ÄÇ<br>`‚Ñí_global = ‚Ñí_FL(‚Ñ≥_{t+n}^{global}, ‚Ñ≥ÃÇ_{t+n}^{global}) + ‚Ñí_Dice(‚Ñ≥_{t+n}^{global}, ‚Ñ≥ÃÇ_{t+n}^{global})`<br>ÂÖ∂‰∏≠ÔºåFocal Loss Â§ÑÁêÜÁ±ªÂà´‰∏çÂπ≥Ë°°ÔºåDice Loss ‰ºòÂåñÊé©Á†ÅÈáçÂè†Âå∫Âüü„ÄÇ | ‚Ä¢ **Ëß£Á†ÅÂô®**Ôºö2Â±Ç MLP<br>‚Ä¢ **ÊçüÂ§±**ÔºöFocal Loss + Dice Loss<br>‚Ä¢ **ËæÖÂä©ÁâπÂæÅ**ÔºöÂØπÊé©Á†ÅÂå∫ÂüüËøõË°åÊé©Á†ÅÊ±†ÂåñÔºåÊèêÂèñÂØπË±°ÁâπÂæÅ |\n| | **Â±ÄÈÉ®ÂèØÊìç‰ΩúÊÄß** | **ÁõÆÊ†á**ÔºöÂú®ÂÖ®Â±ÄÂå∫ÂüüÂü∫Á°Ä‰∏äÔºåÈ¢ÑÊµã**Á≤æÁªÜÁöÑÊé•Ëß¶ÁÇπÂá†‰ΩïÂàÜÂ∏É**ÔºàÁÉ≠ÂäõÂõæÔºâ„ÄÇ<br>**Êï∞Â≠¶ÂéüÁêÜ**Ôºö<br>1. **ÁõëÁù£ÁõÆÊ†áÁîüÊàê**ÔºöÂ∞Ü‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊé•Ëß¶ÁÇπËΩ¨Êç¢‰∏∫È´òÊñØÁÉ≠ÂäõÂõæ `‚Ñ≥_{t+n}^{local}`„ÄÇ<br>2. **ÊçüÂ§±ÂáΩÊï∞**Ôºö‰ΩøÁî® Focal Loss Âíå KL Êï£Â∫¶„ÄÇÈ¶ñÂÖàÂØπÁÉ≠ÂäõÂõæËøõË°å L1 ÂΩí‰∏ÄÂåñÔºö`‚Ñ≥ÃÉ = ‚Ñ≥ / (‚àë_{(i,j)‚ààŒ©} ‚Ñ≥^{(i,j)} + Œµ)`ÔºåÁÑ∂ÂêéËÆ°ÁÆó KL Êï£Â∫¶„ÄÇ<br>`‚Ñí_local = ‚Ñí_FL(‚Ñ≥_{t+n}^{local}, ‚Ñ≥ÃÇ_{t+n}^{local}) + ‚Ñí_KL(‚Ñ≥ÃÉ_{t+n}^{local}, ‚Ñ≥ÃÇÃÉ_{t+n}^{local})` | ‚Ä¢ **Ëß£Á†ÅÂô®**Ôºö2Â±Ç Transformer Block + Á∫øÊÄßÊäïÂΩ±<br>‚Ä¢ **ÊçüÂ§±**ÔºöFocal Loss + KL Divergence<br>‚Ä¢ **ÂΩí‰∏ÄÂåñ**Ôºö`Œµ` ‰∏∫Â∞èÂ∏∏Êï∞Èò≤Ê≠¢Èô§Èõ∂ |\n| | **Á©∫Èó¥ÂèØÊìç‰ΩúÊÄß** | **ÁõÆÊ†á**ÔºöÂ∞ÜÊ®°Á≥äÁöÑÁ©∫Èó¥ËØ≠Ë®ÄÔºàÂ¶Ç‚ÄúÊîæÂú®ÊóÅËæπ‚ÄùÔºâËΩ¨Âåñ‰∏∫‰∏ÄÁªÑ**ÂèØË°åÁöÑÊîæÁΩÆ‰ΩçÁΩÆÂÄôÈÄâÁÇπ**„ÄÇ<br>**Êï∞Â≠¶ÂéüÁêÜ**Ôºö<br>1. **ÁõëÁù£ÁõÆÊ†áÁîüÊàê**Ôºö‰ΩøÁî® SpatialVLM Ëß£ÊûêÁ©∫Èó¥ËØ≠‰πâÔºåRoboPoint ÈááÊ†∑ÂæóÂà∞‰∏ÄÁªÑÁõÆÊ†áÁÇπ `ùíÆ_{t+n} = {ùê©_{t+n}^{(c)}}`„ÄÇ<br>2. **ÊçüÂ§±ÂáΩÊï∞**ÔºöÈááÁî®**ÈõÜÂêàÂåπÈÖçÊçüÂ§±**ÔºàSet-Matching LossÔºâÔºåÊúÄÂ∞èÂåñÊØè‰∏™ÁõÆÊ†áÁÇπÂà∞ÂÖ∂ÊúÄËøëÈ¢ÑÊµãÁÇπÁöÑË∑ùÁ¶ª„ÄÇ<br>`‚Ñí_spatial = (1/C_{t+n}) ‚àë_{c=1}^{C_{t+n}} min_{1‚â§m‚â§M} ‚Äñ ùê©ÃÇ_{t+n}^{(m)} - ùê©_{t+n}^{(c)} ‚Äñ_2^2`<br>ÂÖ∂‰∏≠ `M` ÊòØÈ¢ÑÊµãÁÇπÊï∞Ôºå`C_{t+n}` ÊòØÁõÆÊ†áÁÇπÊï∞„ÄÇ | ‚Ä¢ **Ëß£Á†ÅÂô®**Ôºö2Â±Ç MLP<br>‚Ä¢ **ËæìÂá∫**Ôºö‰∏ÄÁªÑ M ‰∏™ÂΩí‰∏ÄÂåñÁöÑ 2D ÂùêÊ†áÁÇπ<br>‚Ä¢ **ÊçüÂ§±**ÔºöÈõÜÂêàÂåπÈÖçÊçüÂ§±ÔºàÂåàÁâôÂà©ÁÆóÊ≥ïÁöÑ‰∏ÄÁßçÁÆÄÂåñÂΩ¢ÂºèÔºâ |\n| | **Âä®ÊÄÅÂèØÊìç‰ΩúÊÄß** | **ÁõÆÊ†á**ÔºöÈ¢ÑÊµãÊú™Êù•Âì™‰∫õÂÉèÁ¥†ÔºàÊú∫Ê¢∞Áà™„ÄÅÂèØÁßªÂä®Áâ©‰ΩìÔºâÂ∞ÜÂèëÁîü**ËøêÂä®**ÔºåÂª∫Á´ãÂú∫ÊôØ„ÄÅËØ≠Ë®Ä‰∏éÊâÄÈúÄÂä®‰ΩúÁöÑÁªüËÆ°ÂÖ≥ËÅî„ÄÇ<br>**Êï∞Â≠¶ÂéüÁêÜ**Ôºö<br>1. **ÁõëÁù£ÁõÆÊ†áÁîüÊàê**Ôºö‰ΩøÁî® CoTracker Âú®ÂéÜÂè≤Â∏ß‰∏äË∑üË∏™ÁΩëÊ†ºÁÇπÔºåÂ∞Ü‰ΩçÁßªË∂ÖËøáÈòàÂÄºÁöÑËΩ®ËøπÂÖâÊ†ÖÂåñ‰∏∫Êú™Êù•Âä®ÊÄÅÂå∫ÂüüÊé©Á†Å `‚Ñ≥_{t+n}`„ÄÇ<br>2. **ÊçüÂ§±ÂáΩÊï∞**ÔºöÈááÁî®**ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâ** ÂΩ¢ÂºèÁöÑÊé©Á†ÅÈáçÂª∫ÊçüÂ§±ÔºåÈºìÂä±Ê®°ÂûãÂ≠¶‰π†‰∫ßÁîüÂä®ÊÄÅÂå∫ÂüüÁöÑÊΩúÂèòÈáèÂàÜÂ∏É„ÄÇ<br>`‚Ñí_dynamic = ùîº_{ùê≥‚àºQ_œï(ùê≥‚à£x_{t+n}^‚Ñ≥)}[-log P_œà(x_{t+n}^‚Ñ≥‚à£ùê≥)] + Œ≤ KL( Q_œï(ùê≥‚à£x_{t+n}^‚Ñ≥) ‚Äñ p(ùê≥) )`<br>ÂÖ∂‰∏≠ `x_{t+n}^‚Ñ≥` ÊòØÊú™Êù•Â∏ßÂú®Âä®ÊÄÅÊé©Á†ÅÂå∫ÂüüÂÜÖÁöÑÂÉèÁ¥†Ôºå`Œ≤` ÊòØ KL ÊùÉÈáç„ÄÇ | ‚Ä¢ **Ëß£Á†ÅÂô®**Ôºö2Â±Ç Transformer Block + Á∫øÊÄßÊäïÂΩ±ÔºàÊé®Êµã‰∏∫ VAE ÁöÑ decoderÔºâ<br>‚Ä¢ **ÊçüÂ§±**ÔºöVAE ÁöÑ ELBOÔºàÈáçÊûÑÊçüÂ§± + Œ≤ * KL Êï£Â∫¶Ôºâ |\n| **2. ËøõÂ∫¶ÊÑüÁü•Á≠ñÁï•** | **Âü∫‰∫éÊâ©Êï£ÁöÑÈÄÜÂä®ÂäõÂ≠¶** | **ÁõÆÊ†á**ÔºöËÅîÂêàÈ¢ÑÊµãÊú™Êù• `n` Ê≠•ÁöÑ**Âä®‰ΩúÂ∫èÂàó**Âíå**ËøõÂ∫¶Ê†áÈáèÂ∫èÂàó**„ÄÇ<br>**Êï∞Â≠¶ÂéüÁêÜ**Ôºö<br>1. **Êù°‰ª∂ËæìÂÖ•**ÔºöÂΩìÂâçËßÇÊµã `o_t`„ÄÅÊåá‰ª§ `l`„ÄÅÁä∂ÊÄÅ `s_t` ‰ª•ÂèäÈ¢ÑÊµãÁöÑÂèØÊìç‰ΩúÊÄßÊΩúÂú®Ë°®Á§∫ `ùêÖÃÇ_{t+n}`„ÄÇ<br>2. **Êâ©Êï£ËøáÁ®ã**ÔºöÈááÁî® Diffusion Transformer (DiT)„ÄÇÈ¶ñÂÖàÂÆö‰πâÁõÆÊ†áÂêëÈáè `ùê≤_{t:t+n-1} = [a_t, p_t, ..., a_{t+n-1}, p_{t+n-1}]`„ÄÇÂú®ËÆ≠ÁªÉÊó∂ÔºåÂêëÁõÆÊ†áÊ∑ªÂä†Âô™Â£∞Ôºö<br>`ùê≤ÃÉ_{t:t+n-1, t_d} = ‚àö(Œ±ÃÑ_{t_d}) ùê≤_{t:t+n-1} + ‚àö(1-Œ±ÃÑ_{t_d}) œµ`ÔºåÂÖ∂‰∏≠ `œµ ‚àº ùí©(0, I)`Ôºå`t_d` ÊòØÊâ©Êï£Êó∂Èó¥Ê≠•„ÄÇ<br>3. **ËÆ≠ÁªÉÁõÆÊ†á**ÔºöËÆ≠ÁªÉ‰∏Ä‰∏™ÂéªÂô™ÁΩëÁªú `œµ_Œ∏` Êù•È¢ÑÊµãÊ∑ªÂä†ÁöÑÂô™Â£∞„ÄÇ<br>`‚Ñí_DiT = ùîº_{t_d, œµ} ‚Äñ œµ - œµ_Œ∏(ùê≤ÃÉ_{t:t+n-1, t_d} ‚à£ l, o_t, s_t, ùêÖÃÇ_{t+n}, t_d) ‚Äñ_2^2`<br>4. **Êé®ÁêÜ**Ôºö‰ªéÁ∫ØÂô™Â£∞ÂºÄÂßãÔºåÈÄöËøá `œµ_Œ∏` Ëø≠‰ª£ÂéªÂô™ÔºåÁîüÊàêÂä®‰Ωú-ËøõÂ∫¶Â∫èÂàó„ÄÇ | ‚Ä¢ **Ëß£Á†ÅÂô®**ÔºöDiffusion Transformer (DiT)<br>‚Ä¢ **ÈöêËóèÂ±ÇÂ§ßÂ∞è**Ôºö384<br>‚Ä¢ **Â±ÇÊï∞/Â§¥Êï∞**Ôºö12Â±ÇÔºå12Â§¥<br>‚Ä¢ **ÈááÊ†∑Ê≠•Êï∞**Ôºö10<br>‚Ä¢ **Âô™Â£∞ËÆ°Âàí**ÔºöCosine<br>‚Ä¢ **È¢ÑÊµãÊ≠•Èïø**Ôºö`n=3`<br>‚Ä¢ **ËøõÂ∫¶ËåÉÂõ¥**Ôºö`p_t ‚àà [0, 1]` |\n| | **ËøõÂ∫¶‰ø°Âè∑ÁöÑ‰ΩúÁî®** | **Êï∞Â≠¶ÂéüÁêÜ**ÔºöËøõÂ∫¶Ê†áÈáè `p_t` ‰Ωú‰∏∫**Êó∂Èó¥Ê≠£ÂàôÂô®**„ÄÇÂú®ËßÜËßâÁõ∏‰ºº‰ΩÜÂ±û‰∫é‰∏çÂêåÂ≠ê‰ªªÂä°Èò∂ÊÆµÁöÑËßÇÊµã‰∏≠Ôºå`p_t` Êèê‰æõ‰∫ÜÈ¢ùÂ§ñÁöÑ**Êó∂Èó¥‰∏ä‰∏ãÊñá**ÔºåÂ∏ÆÂä©Á≠ñÁï• `œÄ(a_t ‚à£ o_t, l, p_t)` Ê∂àÈô§Ê≠ß‰πâ„ÄÇÂÆÉÈºìÂä±ÊΩúÂú®Áä∂ÊÄÅÈöè‰ªªÂä°Êé®ËøõËÄåÂçïË∞ÉÊºîÂåñÔºå‰ªéËÄåÂπ≥ÊªëÂ≠êÁ≠ñÁï•ËæπÁïåÂ§ÑÁöÑËøáÊ∏°ÔºåÊó†ÈúÄ‰æùËµñÂçïÁã¨ÁöÑÈ´òÂ±ÇÊéßÂà∂Âô®„ÄÇ | ‚Ä¢ **ÂÜ≥Á≠ñÈòàÂÄº**ÔºöÈªòËÆ§ `œÜ = 90%`„ÄÇÂΩì `p_t > œÜ` Êó∂ÔºåËÆ§‰∏∫ÂΩìÂâçÂ≠ê‰ªªÂä°ÂÆåÊàêÔºåÂèØÂáÜÂ§áËøáÊ∏°„ÄÇ<br>‚Ä¢ **ÁõëÁù£Êù•Ê∫ê**Ôºö‰ªéÈïøËßÜÈáéËßÜÈ¢ëÊï∞ÊçÆÔºàEPIC-KITCHENS, RoboCerebraÔºâ‰∏≠Â≠¶‰π†ËØ≠‰πâËøõÂ∫¶ÔºåÂπ∂Âú®Êú∫Âô®‰∫∫Êï∞ÊçÆ‰∏äÂæÆË∞É„ÄÇ |\n| **3. Ê®°ÂûãÊû∂ÊûÑ‰∏éËÆ≠ÁªÉ** | **Â§öÊ®°ÊÄÅÁºñÁ†ÅÂô®** | ‚Ä¢ **ËßÜËßâ**ÔºöMAE",
            "timestamp": "2026-01-21T12:13:05.393909",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "‰ªñ‰ΩøÁî®‰∫ÜÂì™‰∫õËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÔºåÂàóË°®Ê†ºÂÖ∑‰ΩìËÆ≤ËÆ≤‰ªñÁöÑËØÑ‰ª∑ÊåáÊ†áÁöÑÁªÜËäÇ‰∏éÊï∞ÊçÆÈõÜÁöÑÁªÜËäÇ",
            "answer": "## PALMËÆ∫ÊñáËØÑ‰ª∑ÊåáÊ†á‰∏éÊï∞ÊçÆÈõÜÂàÜÊûê\n\n### ‰∏Ä„ÄÅ ËØÑ‰ª∑ÊåáÊ†á\n\nËÆ∫ÊñáÂú®‰ªøÁúüÂíåÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠‰ΩøÁî®‰∫Ü‰ª•‰∏ãÊ†∏ÂøÉËØÑ‰ª∑ÊåáÊ†áÔºåÂÖ∑‰ΩìÁªÜËäÇÂ¶Ç‰∏ãÔºö\n\n| ÊåáÊ†áÂêçÁß∞ | ËÆ°ÁÆóÊñπÂºè‰∏éÁªÜËäÇ | ËØÑ‰º∞ÁõÆÁöÑ |\n| :--- | :--- | :--- |\n| **‰ªªÂä°ÊàêÂäüÁéá** | Âú®Âõ∫ÂÆöÊ¨°Êï∞ÁöÑÊµãËØïÂõûÂêàÔºàRolloutÔºâ‰∏≠ÔºåÊàêÂäüÂÆåÊàêÊåáÂÆö‰ªªÂä°ÁöÑÊØîÁéá„ÄÇ <br> - **CALVIN ABCD**: Êä•ÂëäËøûÁª≠ÂÆåÊàê1Ëá≥5‰∏™Â≠ê‰ªªÂä°ÁöÑÊàêÂäüÁéá„ÄÇ <br> - **LIBERO**: Êä•ÂëäÂõõ‰∏™‰ªªÂä°Â•ó‰ª∂ÔºàSpatial, Object, Goal, LongÔºâÂêÑËá™ÁöÑÂπ≥ÂùáÊàêÂäüÁéá„ÄÇ <br> - **ÁúüÂÆû‰∏ñÁïå**: Êä•ÂëäÂÆåÊàê6Ê≠•ÈïøÁ®ã‰ªªÂä°‰∏≠ÊØè‰∏™Â≠ê‰ªªÂä°ÁöÑÊàêÂäüÁéá„ÄÇ | Ë°°ÈáèÊ®°ÂûãÂú®ÂçïÊ≠•ÂíåÂ§öÊ≠•ÈïøÁ®ã‰ªªÂä°‰∏≠ÁöÑ**ÁªùÂØπÊÄßËÉΩ**Âíå**ÂèØÈù†ÊÄß**„ÄÇ |\n| **Âπ≥ÂùáËøûÁª≠ÂÆåÊàêÈïøÂ∫¶** | Âú®CALVIN ABCDÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÊ®°ÂûãËÉΩÂ§üËøûÁª≠ÊàêÂäüÂÆåÊàêÁöÑÂ≠ê‰ªªÂä°ÁöÑÂπ≥ÂùáÊï∞Èáè„ÄÇËÆ°ÁÆóÂü∫‰∫éËß£ÂÜ≥5Êù°Êåá‰ª§ÁöÑÊµãËØïÁªìÊûú„ÄÇ | ÈáèÂåñÊ®°ÂûãÂú®**ÈïøÁ®ãÂ∫èÂàó**‰∏≠ÁöÑ**Á®≥ÂÆöÊÄß**Âíå**ËøûË¥ØÊÄß**„ÄÇÈÅøÂÖçÂõ†Âçï‰∏™Â≠ê‰ªªÂä°Â§±Ë¥•ÂØºËá¥Êï¥‰∏™ÈïøÁ®ã‰ªªÂä°‰∏≠Êñ≠„ÄÇ |\n| **Êé®ÁêÜÂª∂Ëøü** | Ê®°ÂûãÂú®Âçï‰∏™ÂÜ≥Á≠ñÂë®ÊúüÂÜÖÔºà‰ªéÊé•Êî∂ËßÇÂØüÂà∞ËæìÂá∫Âä®‰ΩúÔºâÊâÄËä±Ë¥πÁöÑÊó∂Èó¥ÔºàÊØ´ÁßíÔºåmsÔºâ„ÄÇËÆ∫Êñá‰∏≠PALMÁöÑÈó≠ÁéØÈ¢ëÁéá‰∏∫10-15 HzÔºàÁ∫¶66-100 ms/Âë®ÊúüÔºâ„ÄÇ | ËØÑ‰º∞Ê®°ÂûãÁöÑ**ÂÆûÊó∂ÊÄß**Âíå**ËÆ°ÁÆóÊïàÁéá**ÔºåËøôÂØπÁúüÂÆû‰∏ñÁïåÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ |\n\n**ÊåáÊ†áÁöÑÊäÄÊúØÂÜÖÊ∂µ**:\n- **ÈïøÁ®ãËØÑ‰º∞Ê†∏ÂøÉ**: ËÆ∫ÊñáÁâπÂà´Âº∫Ë∞É**Â§öÊ≠•ËøûÁª≠ÊàêÂäüÁéá**Âíå**Âπ≥ÂùáÈïøÂ∫¶**ÔºåËÄåÈùûÂçïÊ≠•ÊàêÂäüÁéá„ÄÇËøôÁõ¥Êé•ÈíàÂØπÁé∞ÊúâVLAÊ®°ÂûãÂú®ÈïøÁ®ã‰ªªÂä°‰∏≠‚Äú‰∏≠ÈÄîÂ§±Ë¥•‚ÄùÁöÑÁóõÁÇπÔºåÂ¶ÇÈáçÂ§çÂä®‰Ωú„ÄÅË∑≥ËøáÊ≠•È™§„ÄÅÊèêÂâçÁªàÊ≠¢Á≠â„ÄÇ\n- **Á®≥ÂÅ•ÊÄßÊåáÊ†á**: Âú®ÁúüÂÆû‰∏ñÁïåÂÆûÈ™å‰∏≠ÔºåÈÄöËøáÊîπÂèòÁâ©‰Ωì‰ΩçÂßøÔºàRandom LocalizationÔºâ„ÄÅÂÖâÁÖßÔºàUnseen LightingÔºâÂíåÊ∑ªÂä†ËßÜËßâÂπ≤Êâ∞Áâ©ÔºàVisual DistractionÔºâÊù•ÊµãËØï**Ê≥õÂåñËÉΩÂäõ**ÔºåÊàêÂäüÁéáÂú®Ëøô‰∫õÊâ∞Âä®‰∏ãÁöÑ‰øùÊåÅÂ∫¶ÊòØÈöêÂê´ÁöÑÁ®≥ÂÅ•ÊÄßÊåáÊ†á„ÄÇ\n\n### ‰∫å„ÄÅ Êï∞ÊçÆÈõÜ\n\nËÆ∫ÊñáÈááÁî®‰∫ÜÂàÜÈò∂ÊÆµÁöÑËÆ≠ÁªÉÁ≠ñÁï•Ôºå‰ΩøÁî®‰∫ÜÂ§öÁßçÁ±ªÂûãÁöÑÊï∞ÊçÆÈõÜÔºåÁªÜËäÇÂ¶Ç‰∏ãÔºö\n\n| Êï∞ÊçÆÈõÜÁ±ªÂûã | ÂÖ∑‰ΩìÊï∞ÊçÆÈõÜ | Áî®ÈÄî‰∏éÁªÜËäÇ | Ë¥°ÁåÆ‰∏é‰ª∑ÂÄº |\n| :--- | :--- | :--- | :--- |\n| **È¢ÑËÆ≠ÁªÉ - Êú∫Âô®‰∫∫Êï∞ÊçÆ** | **DROID** [khazatsky2025droid] <br> **BridgeData V2** [walke2024bridgedatav2] | - **Áî®ÈÄî**: Â§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÔºåÂ≠¶‰π†Â§öÊ†∑ÂåñÁöÑÁúüÂÆû‰∏ñÁïåÊú∫Âô®‰∫∫Êìç‰ΩúÊäÄËÉΩ„ÄÇ <br> - **ÁªÜËäÇ**: Êèê‰æõ‚ÄúÈáéÂ§ñ‚ÄùÁéØÂ¢É‰∏ãÁöÑÊú∫Ê¢∞ËáÇÊºîÁ§∫Êï∞ÊçÆÔºåË¶ÜÁõñÂπøÊ≥õÁöÑ‰ªªÂä°ÂíåÂú∫ÊôØ„ÄÇ | Âª∫Á´ã**Êú∫Âô®‰∫∫Êìç‰ΩúÁöÑÂü∫Á°Ä‰∏ñÁïåÊ®°Âûã**Âíå**Âä®‰ΩúÂÖàÈ™å**Ôºå‰∏∫Ê®°ÂûãÊèê‰æõ‰∏∞ÂØåÁöÑÂä®‰Ωú-ËßÇÂØüÈÖçÂØπÊï∞ÊçÆ„ÄÇ |\n| **È¢ÑËÆ≠ÁªÉ - ÈïøÁ®ãËßÜÈ¢ëÊï∞ÊçÆ** | **EPIC-KITCHENS** [damen2020epickitchens] <br> **RoboCerebra** [han2025robocerebra] | - **Áî®ÈÄî**: È¢ÑËÆ≠ÁªÉÔºå‰∏ìÈó®Áî®‰∫éÂ≠¶‰π†**ËØ≠‰πâËøõÂ∫¶‰º∞ËÆ°**„ÄÇ <br> - **ÁªÜËäÇ**: Êèê‰æõÁ≤æÁªÜÊ†áÊ≥®ÁöÑÂ≠êÊ≠•È™§ÂíåÊó∂Èó¥ÂàÜÊÆµ‰ø°ÊÅØÔºåÂåÖÂê´‰∫∫Á±ªÊó•Â∏∏Ê¥ªÂä®‰∏≠ÈïøÁ®ã„ÄÅÊé•Ëß¶‰∏∞ÂØåÁöÑÂ§çÊùÇ‰ªªÂä°ËßÜÈ¢ë„ÄÇ | Ëµã‰∫àÊ®°ÂûãÁêÜËß£**ÈïøÁ®ã‰ªªÂä°Êó∂Â∫èÁªìÊûÑ**Âíå**Èò∂ÊÆµËøõÂ±ï**ÁöÑËÉΩÂäõÔºåËøôÊòØ‰º†ÁªüÊú∫Âô®‰∫∫Êï∞ÊçÆÈõÜÊâÄÁº∫‰πèÁöÑ„ÄÇ |\n| **ÂæÆË∞É - ‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆ** | ËÆ∫ÊñáËá™Ë°åÊî∂ÈõÜ | - **Áî®ÈÄî**: Ê®°ÂûãÂæÆË∞ÉÔºåÂ≠¶‰π†**ÂÖ∑Ë∫´ÂåñÁöÑÂèØÊìç‰ΩúÊÄßÈ¢ÑÊµã**Âíå**ËøõÂ∫¶ÊÑüÁü•Á≠ñÁï•**„ÄÇ <br> - **ÁªÜËäÇ**: ‰ªéÊú∫Âô®‰∫∫Êï∞ÊçÆ‰∏≠ÈÄâÂèñ942Êù°ËΩ®ËøπÔºåÈÄöËøáÂçäËá™Âä®ÊñπÊ≥ïÊ†áÊ≥®‰∫Ü**ÂõõÁßçÂèØÊìç‰ΩúÊÄß**ÔºàÂÖ®Â±Ä„ÄÅÂ±ÄÈÉ®„ÄÅÁ©∫Èó¥„ÄÅÂä®ÊÄÅÔºâÂíå**ËøûÁª≠ËøõÂ∫¶Ê†áÁ≠æ**„ÄÇ | Â∞ÜÈ¢ÑËÆ≠ÁªÉËé∑ÂæóÁöÑÈÄöÁî®Áü•ËØÜ**ÂØπÈΩêÂà∞ÂÖ∑‰ΩìÁöÑÊú∫Âô®‰∫∫ÊéßÂà∂‰ªªÂä°**‰∏äÔºåÊòØÂÆûÁé∞È´òÊÄßËÉΩÈïøÁ®ãÊìç‰ΩúÁöÑÂÖ≥ÈîÆÁéØËäÇ„ÄÇ |\n| **ËØÑ‰º∞ - ‰ªøÁúüÂü∫ÂáÜ** | **CALVIN ABCD** [mees2022calvin] | - **Áî®ÈÄî**: ‰∏ªË¶ÅÈïøÁ®ãËØÑ‰º∞Âü∫ÂáÜ„ÄÇ <br> - **ÁªÜËäÇ**: ÂåÖÂê´34‰∏™ËØ≠Ë®ÄÊåá‰ª§‰ªªÂä°ÔºåÂàÜÂ∏ÉÂú®A„ÄÅB„ÄÅC„ÄÅDÂõõ‰∏™‰∏çÂêåÁéØÂ¢É‰∏≠„ÄÇÈááÁî®**ABCDËÆæÁΩÆ**ÔºöÂú®A„ÄÅB„ÄÅCÁéØÂ¢É‰∏äÈ¢ÑËÆ≠ÁªÉ/ÂæÆË∞ÉÔºåÂú®**Êú™ËßÅËøáÁöÑDÁéØÂ¢É**‰∏äÊµãËØïÔºåËØÑ‰º∞**Ë∑®Âú∫ÊôØÊ≥õÂåñ**ËÉΩÂäõ„ÄÇ | Ê†áÂáÜÂåñÁöÑ**ÈïøÁ®ã„ÄÅËØ≠Ë®ÄÊù°‰ª∂**Êìç‰ΩúÊµãËØïÂπ≥Âè∞ÔºåÁî®‰∫éÂÖ¨Âπ≥ÊØîËæÉ‰∏çÂêåÊñπÊ≥ïÁöÑÊÄßËÉΩ„ÄÇ |\n| **ËØÑ‰º∞ - ‰ªøÁúüÂü∫ÂáÜ** | **LIBERO-LONG** [liu2023libero] | - **Áî®ÈÄî**: ÈïøÁ®ãËØÑ‰º∞Âü∫ÂáÜ„ÄÇ <br> - **ÁªÜËäÇ**: LIBEROÂü∫ÂáÜÁöÑ‚ÄúLong‚ÄùÂ•ó‰ª∂Ôºå‰∏ìÊ≥®‰∫éËØÑ‰º∞ÂØπ**Áâ©‰ΩìÂ±ûÊÄß„ÄÅÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÅÁõÆÊ†áÁä∂ÊÄÅ**ÁöÑÈïøÊúüÊé®ÁêÜÂíåÁü•ËØÜËøÅÁßªËÉΩÂäõ„ÄÇ | ‰∏ìÈó®‰∏∫ËØÑ‰º∞**Áü•ËØÜËøÅÁßª**Âíå**ÈïøÁ®ãÈÄªËæëÊé®ÁêÜ**ËÄåËÆæËÆ°ÔºåÊåëÊàòÊÄßÊûÅÈ´ò„ÄÇ |\n| **ËØÑ‰º∞ - ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆ** | ËÆ∫ÊñáËá™Ë°åÊê≠Âª∫ | - **Áî®ÈÄî**: ÁúüÂÆû‰∏ñÁïåÈïøÁ®ãÊ≥õÂåñÊµãËØï„ÄÇ <br> - **ÁªÜËäÇ**: ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÂåÖÂê´6‰∏™ËøûÁª≠Â≠ê‰ªªÂä°ÔºàÂ¶Ç‚ÄúÂ∞ÜËè†ËêùÊîæÂú®ÁôΩÁõòÂ≠ê‰∏äÔºåËë°ËêÑÊîæÂú®ÁôΩÁ¢óÈáå‚Ä¶‚ÄùÔºâÁöÑÂ§çÊùÇÊìç‰Ωú‰ªªÂä°„ÄÇÂú®**ÈöèÊú∫ÂàùÂßã‰ΩçÂßø„ÄÅÊú™Áü•ÂÖâÁÖß„ÄÅËßÜËßâÂπ≤Êâ∞**‰∏âÁßçËÆæÁΩÆ‰∏ãËøõË°åËØÑ‰º∞„ÄÇ | È™åËØÅÊ®°Âûã‰ªé‰ªøÁúüÂà∞ÁúüÂÆû‰∏ñÁïåÁöÑ**Èõ∂Ê†∑Êú¨/Â∞ëÊ†∑Êú¨Ê≥õÂåñËÉΩÂäõ**Ôºå‰ª•ÂèäÂú®ÂÆûÈôÖÁâ©ÁêÜÁéØÂ¢É‰∏≠ÁöÑ**È≤ÅÊ£íÊÄß**ÔºåÊòØÊäÄÊúØÂÆûÁî®ÊÄßÁöÑÊúÄÁªàÊ£ÄÈ™å„ÄÇ |\n\n**Êï∞ÊçÆÈõÜ‰ΩøÁî®ÁöÑÊäÄÊúØÂàõÊñ∞**:\n1.  **Â§öÊ∫êÂºÇÊûÑÊï∞ÊçÆËûçÂêà**: ÂàõÊñ∞ÊÄßÂú∞Â∞Ü**Êú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆ**ÔºàÊèê‰æõÂä®‰ΩúÂä®ÂäõÂ≠¶Ôºâ‰∏é**‰∫∫Á±ªÊó•Â∏∏ËßÜÈ¢ëÊï∞ÊçÆ**ÔºàÊèê‰æõ‰∏∞ÂØåÁöÑÈïøÁ®ã‰ªªÂä°ËØ≠‰πâÂíåËøõÂ∫¶‰ø°ÊÅØÔºâÁªìÂêàËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÁ™ÅÁ†¥‰∫ÜÂçï‰∏ÄÊï∞ÊçÆÊ∫êÁöÑÂ±ÄÈôê„ÄÇ\n2.  **ÂèØÊìç‰ΩúÊÄßÊ†áÊ≥®**: ÊûÑÂª∫‰∫ÜÂåÖÂê´Â§öÁ≤íÂ∫¶ÂèØÊìç‰ΩúÊÄßÔºà‰ªéÁâ©‰ΩìÁ∫ßÂà∞ÂÉèÁ¥†Á∫ßÔºå‰ªéÈùôÊÄÅÂà∞Âä®ÊÄÅÔºâÁöÑÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºå‰∏∫Ê®°ÂûãÊèê‰æõ‰∫ÜÂØÜÈõÜÁöÑ„ÄÅ‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁõëÁù£‰ø°Âè∑ÔºåÈ©±Âä®ÂÖ∂ËøõË°åÁªìÊûÑÂåñÊé®ÁêÜ„ÄÇ\n3.  **‰∏•Ê†ºÁöÑËØÑ‰º∞ÂçèËÆÆ**: ÈááÁî® **‚ÄúËÆ≠ÁªÉÂüü‰∏éÊµãËØïÂüüÂàÜÁ¶ª‚Äù**ÔºàÂ¶ÇCALVIN ABCDÔºâÂíå**Â§öÁª¥Â∫¶Êâ∞Âä®ÊµãËØï**ÔºàÁúüÂÆû‰∏ñÁïåÔºâÔºåÁ°Æ‰øù‰∫ÜËØÑ‰º∞ÁªìÊûúÁöÑ**Ê≥õÂåñÊÄß**Âíå**ÂèØ‰ø°Â∫¶**ÔºåÈÅøÂÖç‰∫ÜËøáÊãüÂêàÂà∞ÁâπÂÆöËÆ≠ÁªÉÁéØÂ¢É„ÄÇ",
            "timestamp": "2026-01-21T12:13:50.657851",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÂú®Âì™‰∫õÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÂÆûÈ™åÔºü‰∏ªË¶ÅÁöÑËØÑ‰º∞ÊåáÊ†áÂíåÊÄßËÉΩÊèêÂçáÊòØÂ§öÂ∞ëÔºü",
            "answer": "## ËÆ∫ÊñáÂÆûÈ™åÊï∞ÊçÆÈõÜ„ÄÅËØÑ‰º∞ÊåáÊ†á‰∏éÊÄßËÉΩÊèêÂçáÂàÜÊûê\n\n### ‰∏Ä„ÄÅÂÆûÈ™åÊï∞ÊçÆÈõÜ\nËÆ∫ÊñáÂú®**‰ªøÁúü**Âíå**ÁúüÂÆû‰∏ñÁïå**‰∏§‰∏™Â±ÇÈù¢‰ΩøÁî®‰∫ÜÂ§ö‰∏™Êï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇ\n\n#### 1. ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ\n- **È¢ÑËÆ≠ÁªÉÈò∂ÊÆµ**Ôºö\n    - **Êú∫Âô®‰∫∫Êï∞ÊçÆÈõÜ**Ôºö`DROID` Âíå `BridgeData V2`ÔºåÊèê‰æõÂ§ßËßÑÊ®°„ÄÅÁúüÂÆû‰∏ñÁïåÁöÑÊú∫Âô®‰∫∫ÊºîÁ§∫Êï∞ÊçÆÔºåÁî®‰∫éÂ≠¶‰π†Â§öÊ†∑ÂåñÁöÑ‰ªªÂä°Âü∫Á°Ä„ÄÇ\n    - **ÈïøÊó∂Á®ãËßÜÈ¢ëÊï∞ÊçÆÈõÜ**Ôºö`EPIC-KITCHENS` Âíå `RoboCerebra`ÔºåÊèê‰æõÁªÜÁ≤íÂ∫¶ÁöÑÂ≠êÊ≠•È™§ÂíåÊó∂Èó¥ÊÆµÊ†áÊ≥®ÔºåÁî®‰∫éÂ≠¶‰π†Â§çÊùÇ„ÄÅÂ§öÊ≠•È™§Âú∫ÊôØ‰∏≠ÁöÑËØ≠‰πâËøõÂ∫¶‰º∞ËÆ°„ÄÇ\n- **ÂæÆË∞ÉÈò∂ÊÆµ**Ôºö\n    - **‰∫∫Â∑•Ê†áÊ≥®Êï∞ÊçÆÈõÜ**Ôºö‰ªéÊú∫Âô®‰∫∫Êï∞ÊçÆ‰∏≠ÈÄâÂèñ **942 Êù°ËΩ®Ëøπ**ÔºåÈÄöËøáÂçäËá™Âä®ÊñπÊ≥ïÊ†áÊ≥®‰∫Ü**ÂèØÊâøÂèóÊÄßÔºàaffordanceÔºâÊï∞ÊçÆ**Âíå**ËøûÁª≠ËøõÂ∫¶Ê†áÁ≠æ**ÔºåÁî®‰∫éËÆ≠ÁªÉÂèØÊâøÂèóÊÄßÊÑüÁü•ÂíåËøõÂ∫¶ÊÑüÁü•Á≠ñÁï•„ÄÇ\n\n#### 2. ËØÑ‰º∞Âü∫ÂáÜÔºà‰ªøÁúüÔºâ\n- **CALVIN ABCD**Ôºö‰∏Ä‰∏™ÈïøÊó∂Á®ã„ÄÅËØ≠Ë®ÄÊù°‰ª∂ÂåñÁöÑÊú∫Âô®‰∫∫Êìç‰ΩúÂü∫ÂáÜÔºåÂåÖÂê´ **34 ‰∏™‰ªªÂä°**ÔºåÂàÜÂ∏ÉÂú®Âõõ‰∏™‰∏çÂêåÁéØÂ¢É‰∏≠„ÄÇËÆ∫ÊñáÂú® **ABCD ËÆæÁΩÆ**‰∏ãËØÑ‰º∞ÔºåÂç≥Âú® ABC ÁéØÂ¢É‰∏≠È¢ÑËÆ≠ÁªÉÔºåÂú®Êú™ËßÅËøáÁöÑ D ÁéØÂ¢É‰∏≠ÊµãËØï„ÄÇ\n- **LIBERO**ÔºöÂåÖÂê´Âõõ‰∏™‰ªªÂä°Â•ó‰ª∂ÔºàSpatial, Object, Goal, LongÔºâÔºåÊØè‰∏™Â•ó‰ª∂Êúâ **10 ‰∏™‰ªªÂä°**ÔºåÁî®‰∫éËØÑ‰º∞Êú∫Âô®‰∫∫ÂØπÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÅÁâ©‰Ωì„ÄÅÁõÆÊ†áÂíåÈïøÊó∂Á®ã‰ªªÂä°ÁöÑÁêÜËß£„ÄÇÈáçÁÇπËØÑ‰º∞‰∫Ü **LIBERO-LONG**„ÄÇ\n\n#### 3. ÁúüÂÆû‰∏ñÁïåÂÆûÈ™å\n- ‰ΩøÁî® **UFACTORY xArm6 Êú∫Âô®‰∫∫**Âíå **Gripper G2**ÔºåÈÖçÂ§á‰∏§‰∏™ **RealSense D455 Áõ∏Êú∫**ÔºàÁúºÂú®ÊâãÂíåÁúºÂú®Âü∫Â∫ßÔºâ„ÄÇ\n- ËÆæËÆ°‰∫ÜÂåÖÂê´ **6 ‰∏™ËøûÁª≠Â≠ê‰ªªÂä°**ÁöÑÈïøÊó∂Á®ãÊìç‰Ωú‰ªªÂä°Ôºà‰æãÂ¶ÇÔºå‚ÄúÂ∞ÜËè†ËêùÊîæÂú®ÁôΩÁõòÂ≠ê‰∏äÔºåËë°ËêÑÊîæÂú®ÁôΩÁ¢óÈáåÔºåÊ©ôÂ≠êÊîæÂú®ËìùÁ¢óÈáå‚ÄùÔºâ„ÄÇ\n- Âú®‰∏âÁßçÊ≥õÂåñËÆæÁΩÆ‰∏ãÊµãËØïÔºö**ÈöèÊú∫ÂÆö‰Ωç**„ÄÅ**ËßÜËßâÂπ≤Êâ∞**Âíå**Êú™ËßÅËøáÁöÑÂÖâÁÖß**„ÄÇ\n\n### ‰∫å„ÄÅ‰∏ªË¶ÅËØÑ‰º∞ÊåáÊ†á\n1. **ÊàêÂäüÁéáÔºàSuccess Rate, SRÔºâ**ÔºöÊàêÂäüÂÆåÊàêÔºàÂ≠êÔºâ‰ªªÂä°ÁöÑÊØîÁéá„ÄÇ\n2. **Âπ≥ÂùáËøûÁª≠ÂÆåÊàêÈïøÂ∫¶ÔºàAvg. Len.Ôºâ**ÔºöÂú®ÈïøÊó∂Á®ã‰ªªÂä°‰∏≠ÔºåÂπ≥ÂùáËÉΩËøûÁª≠ÊàêÂäüÂÆåÊàêÁöÑ‰ªªÂä°Êï∞ÈáèÔºà‰æãÂ¶ÇÔºåÂú® CALVIN ‰∏≠ÊúÄÂ§öËøûÁª≠ÂÆåÊàê 5 ‰∏™Â≠ê‰ªªÂä°Ôºâ„ÄÇ\n3. **Âπ≥ÂùáÊàêÂäüÁéá**ÔºöÂú® LIBERO ÁöÑ‰∏çÂêå‰ªªÂä°Â•ó‰ª∂‰∏äÁöÑÂπ≥ÂùáÊàêÂäüÁéá„ÄÇ\n\n### ‰∏â„ÄÅÊÄßËÉΩÊèêÂçá‰∏éÂÖ≥ÈîÆÁªìÊûú\n\n#### 1. CALVIN ABCD Âü∫ÂáÜ\n- **PALMÔºàÂÆåÊï¥Ê®°ÂûãÔºâ** ÂèñÂæó‰∫Ü **SOTA ÊÄßËÉΩ**Ôºö\n    - **5 ‰∏™ËøûÁª≠‰ªªÂä°ÁöÑÊàêÂäüÁéá**Ôºö**82.0%**ÔºåÊØî‰πãÂâçÊúÄÂº∫ÁöÑÂü∫Á∫øÔºàSeerÔºå64.3%Ôºâ**ÁªùÂØπÊèêÂçá‰∫Ü 17.7%**„ÄÇ\n    - **Âπ≥ÂùáËøûÁª≠ÂÆåÊàêÈïøÂ∫¶**Ôºö**4.48**ÔºåÊòæËëóÈ´ò‰∫é SeerÔºà3.98ÔºâÂíå œÄ‚ÇÄÔºà3.92Ôºâ„ÄÇ\n    - **ÂÖ≥ÈîÆÂØπÊØî**ÔºöÁßªÈô§ËøõÂ∫¶È¢ÑÊµãÁöÑ PALMÔºà‚úó progressÔºâÊÄßËÉΩ‰∏ãÈôçÔºàÂπ≥ÂùáÈïøÂ∫¶ 4.02 ‚Üí 4.48ÔºâÔºåÂá∏Êòæ‰∫ÜËøõÂ∫¶ÊÑüÁü•ÂØπÈïøÊó∂Á®ãÊ≥õÂåñÁöÑÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇ\n\n#### 2. LIBERO Âü∫ÂáÜ\n- **PALM Âú®Âõõ‰∏™‰ªªÂä°Â•ó‰ª∂‰∏äÂùáËææÂà∞ SOTA**Ôºö\n    - **Âπ≥ÂùáÊàêÂäüÁéá**Ôºö**94.5%**„ÄÇ\n    - **LIBERO-LONG ÊàêÂäüÁéá**Ôºö**91.8%**ÔºåÊØîÊúÄÂº∫Âü∫Á∫øÔºàCoT-VLAÔºå69.0%Ôºâ**ÁªùÂØπÊèêÂçá‰∫Ü 22.8%**„ÄÇ\n\n#### 3. ÁúüÂÆû‰∏ñÁïåÂÆûÈ™å\n- Âú®‰∏âÁßçÊ≥õÂåñËÆæÁΩÆ‰∏ãÔºåPALM ÂùáÊòæËëó‰ºò‰∫éÂü∫Á∫øÔºàOpenVLA Âíå OctoÔºâÔºö\n    - **ÈöèÊú∫ÂÆö‰Ωç**ÔºöÂπ≥ÂùáËøûÁª≠ÂÆåÊàêÈïøÂ∫¶ **3.05**ÔºàÂü∫Á∫øÊúÄÈ´ò 0.95Ôºâ„ÄÇ\n    - **ËßÜËßâÂπ≤Êâ∞**ÔºöÂπ≥ÂùáËøûÁª≠ÂÆåÊàêÈïøÂ∫¶ **3.80**ÔºàÂü∫Á∫øÊúÄÈ´ò 1.60Ôºâ„ÄÇ\n    - **Êú™ËßÅËøáÁöÑÂÖâÁÖß**ÔºöÂπ≥ÂùáËøûÁª≠ÂÆåÊàêÈïøÂ∫¶ **3.55**ÔºàÂü∫Á∫øÊúÄÈ´ò 1.25Ôºâ„ÄÇ\n\n### Âõõ„ÄÅÊÄªÁªì\n- **ÊäÄÊúØÂàõÊñ∞**ÔºöÈÄöËøáÁªìÂêà**ÁªìÊûÑÂåñÂèØÊâøÂèóÊÄßÈ¢ÑÊµã**‰∏é**ËøûÁª≠ËøõÂ∫¶‰º∞ËÆ°**ÔºåPALM Ëß£ÂÜ≥‰∫ÜÈïøÊó∂Á®ãÊìç‰Ωú‰∏≠ÁöÑÁä∂ÊÄÅÊ®°Á≥äÂíåÊó∂Â∫è‰∏ç‰∏ÄËá¥ÈóÆÈ¢ò„ÄÇ\n- **ÂÆûÈôÖ‰ª∑ÂÄº**ÔºöÂú®‰ªøÁúüÂíåÁúüÂÆû‰∏ñÁïå‰∏≠ÂùáË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõÂíåÈ≤ÅÊ£íÊÄßÔºå‰∏∫Â∞ÜÂ§çÊùÇÂ§öÊ≠•È™§‰ªªÂä°ÈÉ®ÁΩ≤Âà∞Âä®ÊÄÅ„ÄÅÂºÄÊîæ‰∏ñÁïåÁéØÂ¢ÉÔºàÂ¶ÇÂÆ∂Â∫≠„ÄÅ‰ªìÂ∫ìÔºâÊèê‰æõ‰∫ÜÊõ¥ÂèØÈù†ÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ",
            "timestamp": "2026-01-21T12:14:21.900430",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ËÆ∫ÊñáÁöÑ‰∏ªË¶ÅÂ±ÄÈôêÊÄßÊúâÂì™‰∫õÔºüÊú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêëÊòØ‰ªÄ‰πàÔºü",
            "answer": "## ËÆ∫Êñá‰∏ªË¶ÅÂ±ÄÈôêÊÄßÂàÜÊûê\n\nÂü∫‰∫éÂØπËÆ∫ÊñáÂÜÖÂÆπÁöÑÂàÜÊûêÔºå**PALM** Ê°ÜÊû∂ËôΩÁÑ∂Âú®ÈïøËßÜÈáéÊìç‰Ωú‰ªªÂä°‰∏äÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜ‰ªçÂ≠òÂú®‰ª•‰∏ãÂá†‰∏™‰∏ªË¶ÅÂ±ÄÈôêÊÄßÔºö\n\n- **Êï∞ÊçÆ‰æùËµñ‰∏éÊ†áÊ≥®ÊàêÊú¨**Ôºö\n    - Ê®°ÂûãËÆ≠ÁªÉ‰∏•Èáç‰æùËµñÂ§ßËßÑÊ®°„ÄÅÂ§öÊ∫êÊï∞ÊçÆÈõÜÔºàÂ¶ÇÊú∫Âô®‰∫∫Êï∞ÊçÆ„ÄÅÈïøËßÜÈáéËßÜÈ¢ëÊï∞ÊçÆÔºâ‰ª•Âèä**‰∫∫Â∑•Ê†áÊ≥®ÁöÑÁ§∫ÊïôËΩ®Ëøπ**Ôºàfine-tuningÈò∂ÊÆµ‰ΩøÁî®‰∫Ü942Êù°‰∫∫Â∑•Ê†áÊ≥®ËΩ®ËøπÔºâ„ÄÇ\n    - Ëé∑ÂèñÈ´òË¥®Èáè„ÄÅÁªÜÁ≤íÂ∫¶ÁöÑ**ÂèØÊìç‰ΩúÊÄßÔºàaffordanceÔºâÊ†áÊ≥®**ÔºàÂ¶ÇÊé•Ëß¶ÁÇπ„ÄÅÊîæÁΩÆÂå∫Âüü„ÄÅÂä®ÊÄÅÂå∫ÂüüÔºâÂíå**ËøûÁª≠ËøõÂ∫¶Ê†áÁ≠æ**ÊàêÊú¨È´òÊòÇÔºåÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑÂø´ÈÄüÊâ©Â±ïÂíåÊñ∞‰ªªÂä°ÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇ\n    - ËÆ∫ÊñáÊèêÂà∞‰ΩøÁî®‚ÄúÂçäËá™Âä®ÂåñÊñπÊ≥ï‚ÄùËøõË°åÊ†áÊ≥®Ôºå‰ΩÜËØ•ÊñπÊ≥ïÁöÑÂÖ∑‰ΩìÁªÜËäÇ„ÄÅÂèØÊâ©Â±ïÊÄßÂíåÈÄöÁî®ÊÄßÊú™ÂÖÖÂàÜËØ¥Êòé„ÄÇ\n\n- **Ê≥õÂåñËÉΩÂäõÁöÑËæπÁïå**Ôºö\n    - Â∞ΩÁÆ°Âú®**‰ªøÁúüÂü∫ÂáÜÔºàCALVIN, LIBEROÔºâ** ÂíåËÆæËÆ°ÁöÑ**ÁúüÂÆû‰∏ñÁïåÊ≥õÂåñÊµãËØï**ÔºàÈöèÊú∫ÂÆö‰Ωç„ÄÅËßÜËßâÂπ≤Êâ∞„ÄÅÊú™Áü•ÂÖâÁÖßÔºâ‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜËøô‰∫õÊµãËØïÂú∫ÊôØ‰ªçÊòØÂèóÊéßÂíåÁªìÊûÑÂåñÁöÑ„ÄÇ\n    - Ê®°ÂûãÂú®**ÊûÅÁ´ØÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÂú∫ÊôØ**‰∏ãÁöÑÈ≤ÅÊ£íÊÄßÊú™ÁªèÂÖÖÂàÜÈ™åËØÅÔºå‰æãÂ¶ÇÔºöÂÖ®Êñ∞ÁöÑÁâ©‰ΩìÁ±ªÂà´„ÄÅÂÆåÂÖ®Êú™Áü•ÁöÑÁâ©‰ΩìÊùêË¥®„ÄÅÂä®ÊÄÅÂπ≤Êâ∞ÔºàÂ¶ÇÁßªÂä®ÁöÑÈöúÁ¢çÁâ©Ôºâ„ÄÅÊàñÊåá‰ª§ËØ≠‰πâÊûÅÂ∫¶Ê®°Á≥äÁöÑÊÉÖÂÜµ„ÄÇ\n    - ËøõÂ∫¶‰º∞ËÆ°Ê®°ÂùóÔºà`p_t`ÔºâÁöÑÈ≤ÅÊ£íÊÄßËôΩÁÑ∂Âú®ÈôÑÂΩï‰∏≠ÈíàÂØπÁâπÂÆöÊâ∞Âä®ËøõË°å‰∫ÜÊµãËØïÔºå‰ΩÜÂÖ∂Âú®Êõ¥Â§çÊùÇ„ÄÅËøûÁª≠Êâ∞Âä®‰∏ãÁöÑÈïøÊúüÁ®≥ÂÆöÊÄß‰ªçÊúâÂæÖËÄÉÂØü„ÄÇ\n\n- **ËÆ°ÁÆóÊïàÁéá‰∏éÂÆûÊó∂ÊÄß**Ôºö\n    - Ê®°ÂûãÊû∂ÊûÑÈõÜÊàê‰∫ÜÂ§ö‰∏™Â§çÊùÇÊ®°ÂùóÔºàÂ§öÊ®°ÊÄÅÁºñÁ†ÅÂô®„ÄÅGPTÈ£éÊ†ºTransformer„ÄÅÂõõ‰∏™ÂèØÊìç‰ΩúÊÄßÈ¢ÑÊµãÂ§¥„ÄÅÊâ©Êï£TransformerËß£Á†ÅÂô®ÔºâÔºåÂ∞ΩÁÆ°ÈÄöËøáPerceiver ResamplerÁ≠âÊäÄÊúØËøõË°å‰∫ÜÂéãÁº©Ôºå‰ΩÜÊï¥‰ΩìËÆ°ÁÆóÂºÄÈîÄ‰ªçÁÑ∂ËæÉÂ§ß„ÄÇ\n    - Êé®ÁêÜÊó∂ÔºåÊâ©Êï£TransformerÈúÄË¶Å**10Ê≠•ÂéªÂô™**ÊâçËÉΩÁîüÊàêÂä®‰ΩúÔºåËôΩÁÑ∂ËÆ∫ÊñáÁß∞Èó≠ÁéØÈ¢ëÁéáÂèØËææ10-15HzÔºàÁ∫¶70-100ms/Ê≠•ÔºâÔºå‰ΩÜËøôÂú®ÈúÄË¶ÅÊõ¥È´òÈ¢ëÁéáÔºàÂ¶Ç>30HzÔºâÁöÑ**Âä®ÊÄÅ„ÄÅÈ´òÈÄüÊìç‰Ωú‰ªªÂä°**‰∏≠ÂèØËÉΩÊàê‰∏∫Áì∂È¢à„ÄÇ\n    - ËÆ≠ÁªÉÈúÄË¶Å**8ÂùóA100 GPU**ÔºåËµÑÊ∫êË¶ÅÊ±ÇËæÉÈ´ò„ÄÇ\n\n- **Ê®°ÂùóÂåñËÆæËÆ°‰∏éËØØÂ∑Æ‰º†Êí≠**Ôºö\n    - Ê°ÜÊû∂ÈááÁî®‰∫Ü**Á∫ßËÅîÈ¢ÑÊµã**ÁªìÊûÑÔºöÂÖàÈ¢ÑÊµãÊú™Êù•ÂèØÊìç‰ΩúÊÄßÔºåÂÜçÂü∫‰∫éÊ≠§ÁîüÊàêÂä®‰ΩúÂíåËøõÂ∫¶„ÄÇËøôÁßçËÆæËÆ°ËôΩÁÑ∂Êèê‰æõ‰∫ÜÁªìÊûÑÂåñÊé®ÁêÜÔºå‰ΩÜ‰πüÂºïÂÖ•‰∫Ü**ËØØÂ∑Æ‰º†Êí≠È£éÈô©**„ÄÇÂ¶ÇÊûúÂèØÊìç‰ΩúÊÄßÈ¢ÑÊµãÂá∫Áé∞ÂÅèÂ∑ÆÔºà‰æãÂ¶ÇÔºåÂú®ÊùÇ‰π±Âú∫ÊôØ‰∏≠ÈîôËØØËØÜÂà´‰∫ÜÁõ∏ÂÖ≥Áâ©‰ΩìÔºâÔºåÂêéÁª≠ÁöÑÂä®‰ΩúÁîüÊàêÂ∞ÜÂü∫‰∫éÈîôËØØÁöÑÂâçÊèêÔºåÂèØËÉΩÂØºËá¥‰ªªÂä°Â§±Ë¥•„ÄÇ\n    - Âõõ‰∏™ÂèØÊìç‰ΩúÊÄßÈ¢ÑÊµãÂ§¥ÔºàÂÖ®Â±Ä„ÄÅÂ±ÄÈÉ®„ÄÅÁ©∫Èó¥„ÄÅÂä®ÊÄÅÔºâÊòØÁã¨Á´ãÁõëÁù£ÁöÑÔºåÂÆÉ‰ª¨‰πãÈó¥ÁöÑ**ÂçèÂêå‰ºòÂåñÂíå‰∏ÄËá¥ÊÄß**ÂèØËÉΩ‰∏çË∂≥ÔºåÁâπÂà´ÊòØÂú®Êï∞ÊçÆÊúâÈôêÁöÑÂú∫ÊôØ‰∏ã„ÄÇ\n\n- **ÂØπ‰ªøÁúü‰∏éÁâπÂÆöÁ°¨‰ª∂ÁöÑ‰æùËµñ**Ôºö\n    - Â§ßÈÉ®ÂàÜÊÄßËÉΩËØÑ‰º∞Âú®‰ªøÁúüÁéØÂ¢É‰∏≠ÂÆåÊàê„ÄÇËôΩÁÑ∂ËøõË°å‰∫ÜÁúüÂÆû‰∏ñÁïåÂÆûÈ™åÔºå‰ΩÜ‰ªªÂä°ËÆæÁΩÆÔºàÂÖ≠Ê≠•È°∫Â∫èÊîæÁΩÆÔºâÂíåÊú∫Âô®‰∫∫Âπ≥Âè∞ÔºàxArm6ÔºâÁõ∏ÂØπÁâπÂÆö„ÄÇ\n    - Ê®°ÂûãÂØπ**ËßÜËßâÁºñÁ†ÅÂô®ÔºàMAE ViTÔºâÂíåÊñáÊú¨ÁºñÁ†ÅÂô®ÔºàCLIPÔºâÁöÑÂÜªÁªìÊùÉÈáç**Â≠òÂú®‰æùËµñÔºåËøô‰∫õÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÂÅèÂ∑ÆÂíåËÉΩÂäõ‰∏äÈôê‰ºöÁõ¥Êé•ÂΩ±ÂìçPALMÁöÑÊÄßËÉΩ„ÄÇ\n\n---\n\n## Êú™Êù•ÂèØËÉΩÁöÑÊîπËøõÊñπÂêë\n\nÈíàÂØπ‰∏äËø∞Â±ÄÈôêÊÄßÔºåÊú™Êù•ÁöÑÁ†îÁ©∂Â∑•‰ΩúÂèØ‰ª•‰ªé‰ª•‰∏ãÂá†‰∏™ÊñπÂêëÂ±ïÂºÄÔºö\n\n- **Èôç‰ΩéÊï∞ÊçÆ‰æùËµñ‰∏éÂºÄÂèëËá™ÁõëÁù£ÊñπÊ≥ï**Ôºö\n    - Êé¢Á¥¢Êõ¥Âº∫ÁöÑ**Ëá™ÁõëÁù£ÊàñÂº±ÁõëÁù£Â≠¶‰π†ËåÉÂºè**Ôºå‰ª•ÂáèÂ∞ëÂØπÊòÇË¥µ‰∫∫Â∑•Ê†áÊ≥®ÁöÑ‰æùËµñ„ÄÇ‰æãÂ¶ÇÔºåÂà©Áî®Â§ßËßÑÊ®°Êú™Ê†áÊ≥®ËßÜÈ¢ëÊï∞ÊçÆÔºåÈÄöËøá**ÂØπÊØîÂ≠¶‰π†„ÄÅÊé©Á†ÅÂª∫Ê®°ÊàñÊó∂Èó¥‰∏ÄËá¥ÊÄßÁ∫¶Êùü**Êù•Ëá™Âä®Â≠¶‰π†ÂèØÊìç‰ΩúÊÄßÂíåËøõÂ∫¶Ë°®Á§∫„ÄÇ\n    - Á†îÁ©∂**‰ªøÁúüÂà∞ÁúüÂÆûÔºàSim2RealÔºâÁöÑËøÅÁßªÊäÄÊúØ**ÔºåÂà©Áî®‰∏∞ÂØåÁöÑ‰ªøÁúüÊï∞ÊçÆÁîüÊàêÂèØÊìç‰ΩúÊÄßÊ†áÊ≥®ÔºåÂπ∂ËÆæËÆ°ÂüüÈÄÇÂ∫îÊñπÊ≥ï‰ª•Èôç‰ΩéÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÁöÑÈúÄÊ±Ç„ÄÇ\n\n- **Â¢ûÂº∫Ê≥õÂåñ‰∏éÈ≤ÅÊ£íÊÄß**Ôºö\n    - Â∞Ü**Áâ©ÁêÜÂ∏∏ËØÜÂíå‰∏ñÁïåÊ®°Âûã**Êõ¥ÊòæÂºèÂú∞ËûçÂÖ•Ê°ÜÊû∂„ÄÇ‰æãÂ¶ÇÔºåÂºïÂÖ•ËΩªÈáèÁ∫ßÁöÑÁâ©ÁêÜÂºïÊìéÊé®ÁêÜÊàñÂü∫‰∫éÁâ©ÁêÜÁöÑÁ∫¶ÊùüÔºå‰ΩøÊ®°ÂûãÂØπÁâ©‰ΩìÂ±ûÊÄßÔºàË¥®Èáè„ÄÅÊë©Êì¶Ôºâ„ÄÅÁâ©ÁêÜ‰∫§‰∫íÔºàÁ¢∞Êíû„ÄÅÊîØÊíëÔºâÊúâÊõ¥Â•ΩÁöÑÁêÜËß£Ôºå‰ªéËÄåÂ∫îÂØπÊõ¥Â§çÊùÇÁöÑOODÂú∫ÊôØ„ÄÇ\n    - ÂºÄÂèë**Êõ¥ÂÅ•Â£ÆÁöÑËøõÂ∫¶‰º∞ËÆ°Âô®**Ôºå‰ΩøÂÖ∂‰∏ç‰ªÖËÉΩÂü∫‰∫éËßÜËßâÁ∫øÁ¥¢ÔºåËøòËÉΩÁªìÂêà**ÂäõËßâ„ÄÅËß¶ËßâÁ≠âÂ§öÊ®°ÊÄÅ‰º†ÊÑü‰ø°ÊÅØ**ÔºåÂú®ËßÜËßâ‰ø°ÊÅØÁº∫Â§±Êàñ‰∏çÂèØÈù†Êó∂ÔºàÂ¶Ç‰∏•ÈáçÈÅÆÊå°Ôºâ‰ªçËÉΩÂèØÈù†Ë∑üË∏™‰ªªÂä°Áä∂ÊÄÅ„ÄÇ\n\n- **ÊèêÂçáËÆ°ÁÆó‰∏éÊû∂ÊûÑÊïàÁéá**Ôºö\n    - ÂØπÊ®°ÂûãËøõË°å**ÂéãÁº©‰∏éËí∏È¶è**ÔºåÊé¢Á¥¢Êõ¥ËΩªÈáèÁ∫ßÁöÑÊû∂ÊûÑÔºàÂ¶ÇÈ´òÊïàÁöÑTransformerÂèò‰Ωì„ÄÅÊõ¥Â∞èÁöÑÊâ©Êï£Ê≠•Êï∞ÔºâÔºåÂú®‰øùÊåÅÊÄßËÉΩÁöÑÂêåÊó∂ÊèêÂçáÊé®ÁêÜÈÄüÂ∫¶Ôºå‰ª•Êª°Ë∂≥Êõ¥È´òÈ¢ëÁéáÁöÑÂÆûÊó∂ÊéßÂà∂ÈúÄÊ±Ç„ÄÇ\n    - Á†îÁ©∂**ÂèØÊìç‰ΩúÊÄßÈ¢ÑÊµã‰∏éÂä®‰ΩúÁîüÊàêÁöÑËÅîÂêà‰ºòÂåñÊàñÁ´ØÂà∞Á´ØËÆ≠ÁªÉ**ÊñπÊ≥ïÔºåÂáèÂ∞ëÊ®°ÂùóÈó¥ÁöÑËØØÂ∑Æ‰º†Êí≠ÔºåÂπ∂ÂèØËÉΩÊèêÂçáÊï¥‰ΩìÊïàÁéá„ÄÇ\n\n- **Êâ©Â±ï‰ªªÂä°ËåÉÂõ¥‰∏é‰∫§‰∫íËÉΩÂäõ**Ôºö\n    - Â∞ÜÊ°ÜÊû∂Êâ©Â±ïÂà∞**ÈùûÂàöÊÄßÁâ©‰ΩìÊìç‰Ωú„ÄÅÂèåÊâãÊìç‰Ωú„ÄÅ‰∫∫Êú∫Âçè‰Ωú**Á≠âÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°È¢ÜÂüü„ÄÇ\n    - Êé¢Á¥¢**‰∏ªÂä®ÊÑüÁü•‰∏éËßÑÂàí**ÁöÑÁªìÂêàÔºå‰ΩøÊú∫Âô®‰∫∫‰∏ç‰ªÖËÉΩÈ¢ÑÊµãÂèØÊìç‰ΩúÊÄßÔºåËøòËÉΩÂú®‰∏çÁ°ÆÂÆöÊó∂‰∏ªÂä®Ë∞ÉÊï¥ËßÜËßíÊàñËøõË°åÊé¢Á¥¢ÊÄßÂä®‰Ωú‰ª•Ëé∑ÂèñÊõ¥Â•Ω‰ø°ÊÅØ„ÄÇ\n    - Á†îÁ©∂**‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊàñËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑÊõ¥Ê∑±Â∫¶ÈõÜÊàê**ÔºåÂà©Áî®ÂÖ∂Âº∫Â§ßÁöÑÂ∏∏ËØÜÂíåÊé®ÁêÜËÉΩÂäõËøõË°åÊõ¥È´òÂ±ÇÊ¨°ÁöÑ‰ªªÂä°ÂàÜËß£ÂíåÊÑèÂõæÁêÜËß£ÔºåËÄåPALM‰∏ìÊ≥®‰∫é‰ΩéÂ±ÇÁöÑ„ÄÅÂü∫‰∫éÁâ©ÁêÜÁöÑÁªÜÁ≤íÂ∫¶Êé®ÁêÜ‰∏éÊéßÂà∂„ÄÇ\n\n- **ÂºÄÊ∫ê‰∏éÁîüÊÄÅÂª∫ËÆæ**Ôºö\n    - **ÂºÄÊ∫ê‰ª£Á†Å„ÄÅÊ®°ÂûãÂíåÊ†áÊ≥®Â∑•ÂÖ∑Èìæ**Ôºå‰ª•‰øÉËøõÁ§æÂå∫Á†îÁ©∂ÔºåÂÖ±ÂêåÊîªÂÖãÊï∞ÊçÆÁì∂È¢àÂíåÊ≥õÂåñÊåëÊàò„ÄÇ\n    - Âª∫Á´ãÊõ¥**Â§öÊ†∑Âåñ„ÄÅÊõ¥ÂÖ∑ÊåëÊàòÊÄßÁöÑÈïøËßÜÈáéÊìç‰ΩúÂü∫ÂáÜÊµãËØï**ÔºåÊ∂µÁõñÊõ¥ÂπøÊ≥õÁöÑÁâ©‰Ωì„ÄÅÂú∫ÊôØ„ÄÅÂπ≤Êâ∞Á±ªÂûãÂíå‰ªªÂä°Â§çÊùÇÂ∫¶Ôºå‰ª•Êé®Âä®È¢ÜÂüüÂèëÂ±ï„ÄÇ\n\n**ÊÄªÁªì**ÔºöPALMÁöÑÊ†∏ÂøÉ‰ª∑ÂÄºÂú®‰∫éÈÄöËøá**ÁªìÊûÑÂåñÁöÑÂèØÊìç‰ΩúÊÄßÊé®ÁêÜ**Âíå**ËøûÁª≠ÁöÑËøõÂ∫¶ÊÑüÁü•**Ôºå‰∏∫ÈïøËßÜÈáéÊìç‰ΩúÊèê‰æõ‰∫ÜÂÖ≥ÈîÆÁöÑÂÜÖÈÉ®Áä∂ÊÄÅË°®Á§∫ÂíåÊó∂Â∫è grounding„ÄÇÊú™Êù•ÁöÑÊîπËøõÂ∞ÜÂõ¥ÁªïÂ¶Ç‰Ωï‰ª•Êõ¥‰ΩéÁöÑÊàêÊú¨ÔºàÊï∞ÊçÆ„ÄÅËÆ°ÁÆóÔºâËé∑ÂæóÊõ¥Âº∫Â§ß„ÄÅÊõ¥ÈÄöÁî®„ÄÅÊõ¥È≤ÅÊ£íÁöÑËÉΩÂäõÔºåÊúÄÁªàÂÆûÁé∞Êú∫Âô®‰∫∫Âú®ÂºÄÊîæ‰∏ñÁïå‰∏≠ÁöÑËá™‰∏ª„ÄÅÂèØÈù†Êìç‰Ωú„ÄÇ",
            "timestamp": "2026-01-21T12:15:06.171895",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-11T21:00:58Z",
    "created_at": "2026-01-21T12:09:07.021874",
    "updated_at": "2026-01-21T12:09:07.021880",
    "recommend": 0
}