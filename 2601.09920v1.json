{
    "id": "2601.09920v1",
    "title": "SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping",
    "authors": [
        "Ruopeng Huang",
        "Boyu Yang",
        "Wenlong Gui",
        "Jeremy Morgan",
        "Erdem Biyik",
        "Jiachen Li"
    ],
    "abstract": "åœ¨åŠ¨æ€å’Œè§†è§‰é®æŒ¡æ¡ä»¶ä¸‹å®ç°ç²¾ç¡®ä¸”å®‰å…¨çš„æŠ“å–ï¼Œä»ç„¶æ˜¯ç°å®ä¸–ç•Œæœºå™¨äººæ“ä½œä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†SyncTwinâ€”â€”ä¸€ä¸ªæ•°å­—å­ªç”Ÿæ¡†æ¶ï¼Œå®ƒé›†æˆäº†å¿«é€Ÿä¸‰ç»´åœºæ™¯é‡å»ºä¸è™šå®åŒæ­¥æŠ€æœ¯ï¼Œæ—¨åœ¨ä¸ºæ­¤ç±»ç¯å¢ƒæä¾›é²æ£’ä¸”å®‰å…¨æ„ŸçŸ¥çš„æŠ“å–è§£å†³æ–¹æ¡ˆã€‚åœ¨ç¦»çº¿é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨VGGTä»RGBå›¾åƒå¿«é€Ÿé‡å»ºç‰©ä½“çº§ä¸‰ç»´èµ„äº§ï¼Œæ„å»ºç”¨äºä»¿çœŸçš„å¯é‡ç”¨å‡ ä½•åº“ã€‚åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼ŒSyncTwiné€šè¿‡ç‚¹äº‘åˆ†å‰²æ›´æ–°è·Ÿè¸ªç°å®ä¸–ç•Œç‰©ä½“çŠ¶æ€ï¼Œå¹¶å€ŸåŠ©å½©è‰²ICPé…å‡†è¿›è¡Œå¯¹é½ï¼Œä»è€ŒæŒç»­åŒæ­¥æ•°å­—å­ªç”Ÿä½“ã€‚æ›´æ–°åçš„å­ªç”Ÿä½“ä½¿è¿åŠ¨è§„åˆ’å™¨èƒ½å¤Ÿåœ¨ä»¿çœŸä¸­è®¡ç®—æ— ç¢°æ’ä¸”åŠ¨æ€å¯è¡Œçš„è½¨è¿¹ï¼Œè¿™äº›è½¨è¿¹é€šè¿‡é—­ç¯çš„è™šå®-å®å¾ªç¯åœ¨çœŸå®æœºå™¨äººä¸Šå®‰å…¨æ‰§è¡Œã€‚åœ¨åŠ¨æ€å’Œé®æŒ¡åœºæ™¯ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒSyncTwinæé«˜äº†æŠ“å–ç²¾åº¦å’Œè¿åŠ¨å®‰å…¨æ€§ï¼Œè¯æ˜äº†æ•°å­—å­ªç”ŸåŒæ­¥æŠ€æœ¯å¯¹äºç°å®ä¸–ç•Œæœºå™¨äººæ‰§è¡Œä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.09920v1",
    "html_url": "https://arxiv.org/html/2601.09920v1",
    "html_content": "SyncTwin: Fast Digital Twin Construction\nand Synchronization for Safe Robotic Grasping\nRuopeng Huang\n1,2\nBoyu Yang\n1\nWenlong Gui\n1\nJeremy Morgan\n1\nErdem Biyik\n1\nJiachen Li\n2\n1\nUniversity of Southern California\n2\nUniversity of California, Riverside\nAbstract\nAccurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We introduce SyncTwin, a novel digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.\n1\nIntroduction\nAchieving accurate and safe robotic grasping in dynamic real-world environments remains a long-standing challenge, due to incomplete perception and dynamic scenes.\nWithout an accurate understanding of their surroundings, robots risk colliding with the environment, which may damage hardware or even endanger humans\n[\n58\n]\n.\nThus, ensuring that robots can safely plan and execute motions amid dynamic scene changes is a prerequisite for reliable real-world deployment.\nUnlike simulation, where complete scene geometry and object states are fully accessible, real-world perception often only offers partial, occluded observations.\nAs a result, motion-planning algorithms\n[\n33\n,\n16\n,\n36\n]\nthat rely on full and accurate environment models in simulation face significant challenges when deployed in the real world.\nSeveral recent efforts have attempted to bridge the gap between perception and control.\nVoxel-based mapping systems like NVBlox\n[\n23\n]\nenable online obstacle reconstruction, yet they only provide a voxel grid, which is a representation too coarse for reliable manipulation.\nEnd-to-end reactive policies like DRP\n[\n53\n]\nhandle dynamics through continuous control but require robot-specific retraining and lack generalization across new hardware or environments.\nThus, all of these methods share a critical limitation: they operate without a consistent and complete model of the scene, leading to unsafe or unreliable executions in the real world.\nBridging this gap between simulation and the real world, a robot needs a perception model that can efficiently perceive what objects exist in the environment and also track how the real-world scene evolves over time.\nIn other words, instead of planning in a static or outdated simulation, the robot should plan within a dynamic digital twin that mirrors the physical world in real time, where perception and control are tightly coupled through continuous real-to-sim synchronization.\nHowever, real-world perception is inherently partialâ€”single-view occlusions often reveal only fragments of object geometry, making grasping and motion planning unreliable. Inspired by SAM4D\n[\n51\n]\n, which maintains a memory bank of object assets, we incorporate the idea of leveraging object-level memories to complete partial observations at execution time. We develop\nSyncTwin\n, a digital-twin framework equipped with an object memory bank that performs real-time object tracking from point clouds, injects accurate poses and geometries into simulation, and closes the loop by executing planned trajectories back on the real robot, as illustrated in Figure\nLABEL:fig:overview\n.\nSyncTwin operates in two stages.\nIn StageÂ I, VGGT\n[\n46\n]\ngenerates scene-level point clouds from RGB inputs. Object-level point clouds are extracted via projection, segmentation, and denoising, then converted into lightweight meshes and stored in a memory bank.\nIn StageÂ II, the digital twin is continuously synchronizes with the real world using SAM2 for object tracking and GPU-accelerated colored-ICP\n[\n42\n]\n. Combined, these two methods align partial observations with stored assets to maintain a consistent and complete scene representation.\nThe updated digital twin is streamed into Isaac Sim\n[\n24\n]\n, allowing cuRobo\n[\n41\n]\nto perform motion planning and generate collision-aware trajectories. Moreover, this architecture can be transferred to different real robots without any retraining.\nThe main contributions of this paper are as follows:\nâ€¢\nWe present the first digital twin framework that tracks 3D objects in real time from point clouds and updates their poses and geometries in a synchronized simulation, enabling collision-aware planning and a closed real-to-sim-to-real loop for dynamic, partially observed scenes.\nâ€¢\nWe introduce a fast, low-cost RGB-only method that constructs 3D geometry assets using learning-based geometry estimation and projection-based segmentation.\nâ€¢\nWe develop a real-time 3D segmentation and tracking module that processes streaming RGB-D camera data.\nâ€¢\nOur experiments demonstrate that the proposed system improves grasp accuracy and safety in single-view and occluded settings, while also achieving state-of-the-art efficiency in 3D geometry asset reconstruction.\nFigure 2\n:\nFramework of the SyncTwin\n.\nStageÂ I reconstructs simulation-ready 3D assets from RGB images using VGGT and SAM2. Multi-view masks are unprojected into point clouds, then denoised, scaled, and meshed into clean object assets stored in a memory bank.\nStage II performs real-time object segmentation, pose tracking, and asset-based completion, enabling grasp generation and reactive motion planning in a closed real2sim2real loop. By continuously updating the digital twin and leveraging simulation for decision making, the system ensures safe and adaptive execution under dynamic and partially occluded environments.\n2\nRelated Work\n3D Scene Reconstruction and Segmentation.\nTraditional 3D scene reconstruction approaches rely on RGB-D input from depth sensors to estimate geometry\n[\n28\n,\n38\n,\n7\n,\n5\n]\n.\nMore recent approaches operate purely on RGB images, either through optimization-based pipelines such as Structure-from-Motion (SfM)\n[\n30\n,\n35\n,\n31\n,\n14\n]\nand Multi-View Stereo (MVS)\n[\n37\n,\n10\n,\n11\n]\n, or through learning-based frameworks such as MVSNet\n[\n54\n]\n, NeRF\n[\n22\n]\n, and 3D Gaussian Splatting\n[\n20\n]\n. While these methods can recover visually plausible geometry, they often require substantial computation time to produce complete reconstructions.\nFor robotics applications, object-centric 3D segmentation is important as well. Prior work has explored segmentation directly on point clouds using learned clustering and aggregation strategies\n[\n57\n,\n47\n,\n15\n]\n, as well as multi-view projection of 2D masks for 3D instance segmentation\n[\n34\n,\n21\n,\n56\n]\n.\nHowever, projection-based methods typically require accurate camera extrinsics\n[\n52\n,\n2\n]\n, and little research has examined how such strategies perform when applied to point clouds produced by learning-based reconstruction methods. Our work addresses this gap by unifying learned 3D reconstruction with mask-guided multi-view segmentation, enabling efficient and robust object asset generation for downstream robotic manipulation.\nDigital Twin for Robotic Manipulation.\nDigital twin systems have become increasingly popular for sim-to-real transfer, particularly in training reinforcement learning and imitation learning policies\n[\n4\n,\n50\n,\n55\n,\n45\n]\n. And several studies leverage generated digital twins as a form of data augmentation to enhance the generalization of downstream models\n[\n25\n,\n19\n]\n.\nHowever, existing frameworks typically reconstruct static scenes once before training\n[\n29\n]\nand do not maintain continuous synchronization with the evolving physical world.\nSome recent efforts attempt real-time tracking by detecting object locations with 2D detectors\n[\n40\n]\n, but they often lack precise pose estimation or geometry updates.\nIn contrast, our system enables continuous synchronization of the digital twin with real-world online perception, which enables accurate tracking and reliable manipulation in dynamic environments under occlusion.\nSafe Motion Planning for Manipulation.\nSafe and robust motion planning remains a critical challenge for robotic manipulation in unstructured environments\n[\n39\n,\n18\n]\n.\nClassical and learning-based planners\n[\n33\n,\n16\n,\n36\n]\ntypically operate in simulation, where the environment is fully accessible. When deployed in the real world, however, they must handle partial observations. To narrow this gap, several methods construct static point cloud maps and import them into simulation for offline planning\n[\n3\n]\n, though such maps struggle to support real-time adaptation.\nOther approaches aim to ensure safety by predicting collision-free actions directly from images in latent space\n[\n27\n,\n1\n]\n, using force-sensing-based control\n[\n48\n,\n17\n]\n, or distilling planning policies from point cloud observations\n[\n6\n,\n53\n,\n9\n]\n.\nNVBlox improves online safety by voxelizing scene geometry and segmenting the robot in real time\n[\n23\n]\n.\nIn contrast, our method maintains a dynamically synchronized digital twin that continuously provides updated scene geometry to the planner, enabling safe execution in dynamic, cluttered environments.\n3\nMethod\nSyncTwin consists of two stages: (1) fast digital twin construction, and (2) digital twin synchronization. An overall framework is provided in Figure\n2\n.\n3.1\nProblem Formulation\nWe aim to enable safe robotic grasping in dynamic, partially observable real-world environments by maintaining a\ncontinuously synchronized digital twin\n.\nThis problem can be decomposed into the following components:\nStageÂ I: Fast Digital Twin Construction.\nThe system receives a small set of RGB images\n{\nğˆ\ni\n}\ni\n=\n1\nN\n\\{\\mathbf{I}_{i}\\}_{i=1}^{N}\nalong with camera intrinsics\nK\nK\nand estimated extrinsics\n{\nğ“\ni\nworld\n}\ni\n=\n1\nN\n\\{\\mathbf{T}_{i}^{\\text{world}}\\}_{i=1}^{N}\n.\nThe goal is to produce object-level, simulation-ready 3D assets\nğ”¹\n=\n{\nğ’³\nj\n,\nâ„³\nj\n3\nâ€‹\nD\n}\n\\mathbb{B}=\\{\\mathcal{X}_{j},\\mathcal{M}_{j}^{3D}\\}\nfrom these images.\nThe main challenge is that learning-based extrinsics contain unstable errors, causing maskâ€“projection misalignment and tableâ€“object mixing in the reconstructed point cloud, which must be addressed to obtain clean object geometry.\nStageÂ II: Online Digital Twin Synchronization.\nDuring execution, the system receives streaming RGB-D frames and corresponding partial point clouds\nğ’³\np\n\\mathcal{X}_{p}\n.\nThe objective is to maintain accurate object poses\nğ“\nj\nworld\n\\mathbf{T}_{j}^{\\text{world}}\nin the simulator by aligning\nğ’³\np\n\\mathcal{X}_{p}\nwith their complete assets\nğ’³\nm\nâˆˆ\nğ”¹\n\\mathcal{X}_{m}\\in\\mathbb{B}\n.\nThis synchronized scene is streamed into Isaac Sim\n[\n24\n]\n,\nwhere cuRoboâ€™s MPC planner\n[\n41\n]\nproduces short-horizon, collision-free trajectories\nğ€\nt\n:\nt\n+\nH\n=\n{\nğš\n0\n,\nâ€¦\n,\nğš\nH\n}\n\\mathbf{A}_{t:t+H}=\\{\\mathbf{a}_{0},\\ldots,\\mathbf{a}_{H}\\}\n.\nThe key challenge is robustly tracking objects under occlusion and partial observation, while ensuring that the object poses and geometries can be accurately updated into the simulator to enable safe real-to-sim-to-real planning.\n3.2\nFast Digital Twin Construction\nThe first stage aims to rapidly reconstruct the 3D environment and extract object-level representations suitable for real-time simulation, where the focus is to achieve accurate geometric perception for motion planning, rather than photorealistic reconstruction.\nWe employ VGGT\n[\n46\n]\nto reconstruct dense scene point clouds directly from a small number of RGB images, which enables efficient extraction of object-level geometry in a fast and low-cost manner without depth sensors or multi-view optimization.\nNevertheless, there are two major practical challenges.\nFirst, the camera extrinsics estimated by VGGT are often inaccurate, which leads to projection misalignment during mask-based segmentation.\nSecond, the generated point cloud is not in a true metric world scale, causing the imported assets to appear incorrectly sized relative to the robot in the simulator.\nTherefore, Stage I focuses on producing simulation-ready 3D digital assets from VGGT outputs by addressing these limitations. We design a four-step pipeline that corrects extrinsic inaccuracies, enforces scale consistency, and generates clean object meshes suitable for downstream planning, which is introduced as follows.\nMask Projection Expansion.\nTo mitigate inaccuracies in VGGT-estimated camera extrinsics, each 2D segmentation mask\nğ’®\ni\n\\mathcal{S}_{i}\nis spatially expanded before projection, which ensures full coverage of object boundaries and prevents missing edge regions during 3D reconstruction.\nWhile mask expansion compensates for projection drift, it also introduces floating outliers (e.g., background points above the object) and merged support-plane regions (e.g., table surfaces).\nTo address this, we apply our point clouds denoising mechanism to isolate the true object shape.\nFigure 3\n:\nSupporting-plane noise removal mechanism.\nA virtual light sphere expands from the object center to identify openings and boundary points, enabling filtering of table-plane noise.\nPoint Clouds Denoising.\nDetecting openings or cavities in 3D point clouds is a fundamental step in shape understanding for denoising table noise.\nWe propose a purely geometric method that progressively expands a virtual light sphere from the objectâ€™s center and tracks uncovered regions on the spherical sampling space.\nThe algorithm automatically detects openings and extracts rim points around their boundaries without requiring mesh topology or prior segmentation. Figure\n3\nshows the overall process.\nGiven a point cloud\nğ’«\n\\mathcal{P}\n, we first estimate a geometric center\nğœ\n=\nmean\nâ€‹\n(\nğ’«\n)\n\\mathbf{c}=\\mathrm{mean}(\\mathcal{P})\n.\nWe then discretize the unit sphere into\nF\nF\ndirections\n{\nğ\ni\n}\ni\n=\n1\nF\n\\{\\mathbf{d}_{i}\\}_{i=1}^{F}\nusing a Fibonacci spiral distribution\n[\n13\n]\n, forming a uniform sampling domain\nğ’Ÿ\n\\mathcal{D}\n. Each point\nğ©\ni\n\\mathbf{p}_{i}\ndefines a normalized direction\nğ¯\n^\ni\n=\n(\nğ©\ni\nâˆ’\nğœ\n)\n/\nâ€–\nğ©\ni\nâˆ’\nğœ\nâ€–\n\\hat{\\mathbf{v}}_{i}=(\\mathbf{p}_{i}-\\mathbf{c})/\\|\\mathbf{p}_{i}-\\mathbf{c}\\|\nand is assigned to its nearest angular bucket direction\nğ\nj\n\\mathbf{d}_{j}\nif\nğ¯\n^\ni\nâ‹…\nğ\nj\nâ‰¥\ncos\nâ¡\n(\nÎ¸\ntolerance\n)\n\\hat{\\mathbf{v}}_{i}\\cdot\\mathbf{d}_{j}\\geq\\cos(\\theta_{\\text{tolerance}})\n,\nWe then iteratively expand a virtual sphere centered at\nğœ\n\\mathbf{c}\nwith radius\nr\nt\nr_{t}\n:\nr\nt\n+\n1\n=\nr\nt\n+\nÎ”\nâ€‹\nr\n,\nr\nmin\nâ€‹\n(\nj\n)\n=\nmin\ni\nâˆˆ\nbucket\nâ€‹\nj\nâ¡\nâ€–\nğ©\ni\nâˆ’\nğœ\nâ€–\n.\nr_{t+1}=r_{t}+\\Delta r,\\quad r_{\\min}(j)=\\min_{i\\in\\text{bucket }j}\\|\\mathbf{p}_{i}-\\mathbf{c}\\|.\n(1)\nA direction\nğ\nj\n\\mathbf{d}_{j}\nis marked as\nhit\nonce any assigned point enters the sphere,\nand unhit directions form a binary mask\nğ’°\nt\n\\mathcal{U}_{t}\n.\nStable uncovered regions (openings or cavities) are detected when the largest unhit component\nremains consistent over iterations. We denote by\nğ’©\nâ€‹\n(\nj\n)\n=\n{\nk\nâˆ£\n(\nj\n,\nk\n)\nâˆˆ\nğ’«\n}\n\\mathcal{N}(j)=\\{k\\mid(j,k)\\in\\mathcal{P}\\}\nthe neighborhood of bucket\nj\nj\n.\nBoundary buckets are defined as unhit directions adjacent to hit ones:\nâ„¬\n=\n{\nj\nâˆˆ\nğ’°\nâˆ£\nâˆƒ\nk\nâˆˆ\nğ’©\nâ€‹\n(\nj\n)\n,\nhit\nâ€‹\n(\nk\n)\n=\n1\n}\n.\n\\mathcal{B}=\\{j\\in\\mathcal{U}\\mid\\exists\\,k\\in\\mathcal{N}(j),\\,\\text{hit}(k)=1\\}.\n(2)\nFor each boundary\nğ\nj\n\\mathbf{d}_{j}\n, the farthest point within tolerance is chosen as a rim sample\nğ©\nj\nâˆ—\n\\mathbf{p}^{*}_{j}\n.\nConnected components on the spherical adjacency graph are extracted to identify large uncovered regions.\nLet\n{\nğ’\nm\n}\n\\{\\mathcal{C}_{m}\\}\ndenote all connected components of the unhit set\nğ’°\nt\n,\nğ’\nmax\n\\mathcal{U}_{t},\\mathcal{C}_{\\max}\nis the largest connected region.\nThe principal opening direction is then computed by averaging the largest unhit component:\nğ§\nopen\n=\nâˆ‘\nj\nâˆˆ\nğ’\nmax\nğ\nj\nâ€–\nâˆ‘\nj\nâˆˆ\nğ’\nmax\nğ\nj\nâ€–\n.\n\\mathbf{n}_{\\text{open}}=\\frac{\\sum_{j\\in\\mathcal{C}_{\\max}}\\mathbf{d}_{j}}{\\|\\sum_{j\\in\\mathcal{C}_{\\max}}\\mathbf{d}_{j}\\|}.\nFinally, the rim points\n{\nğ©\nj\nâˆ—\n}\n\\{\\mathbf{p}^{*}_{j}\\}\nare fitted with a plane using SVD\n[\n12\n]\n,\nyielding the opening orientation and visualizable boundary.\nCompared with RANSAC plane fitting\n[\n8\n]\n, which can only segment dominant planes, our method can detect cavity openings, enabling the identification of ring-shaped planes surrounding the opening\nReal-World Scale Alignment.\nSince VGGT produces point clouds that are not in a world-scale metric,\nwe estimate a global scale factor to align the reconstructions with world coordinates.\nEven with known intrinsic parameters, according to the pinhole camera model,\nmonocular geometry cannot determine absolute scale\n[\n14\n]\n.\nTherefore, we calibrate the scale using a reference object or markers of known physical dimensions within the scene. Implementation details are provided in supplementary.\nMesh Simplification.\nTo maintain real-time performance in the digital twin, we apply an adaptive mesh decimation that reduces vertex count while preserving geometric fidelity and collision boundaries.\nTo avoid over-smoothing across sharp edges, we use an angle-based gating weight\nw\ni\nâ€‹\nj\n=\n1\nw_{ij}=1\nif\nÎ¸\ni\nâ€‹\nj\nâ‰¤\nÎ¸\nth\n\\theta_{ij}\\leq\\theta_{\\mathrm{th}}\nand\n0\notherwise,\nwhere\nÎ¸\nth\n\\theta_{\\mathrm{th}}\nis a feature threshold (e.g.,\n30\nâˆ˜\n30^{\\circ}\n) and\nÎ¸\ni\nâ€‹\nj\n=\narccos\nâ¡\n(\nğ§\ni\nâŠ¤\nâ€‹\nğ§\nj\n)\n\\theta_{ij}=\\arccos(\\mathbf{n}_{i}^{\\top}\\mathbf{n}_{j})\nfor all\nj\nâˆˆ\nğ’©\nâ€‹\n(\ni\n)\nj\\in\\mathcal{N}(i)\n.\nEach vertex is then updated via a selective Laplacian step\n[\n44\n]\n. Compared to uniform mesh decimation, this feature-aware smoothing preserves sharp edges around handles and object rims, which are critical for accurate grasp planning.\nAll processed point clouds, meshes, and their id are stored in a memory bank, which serves for object recognition and scene synchronization in StageÂ II.\n3.3\nOnline Digital Twin Synchronization\nThe second stage of our system focuses on real-time object tracking and safe grasp execution through continuous perception, planning synchronization between the real and digital environments. This stage consists of four tightly integrated modules: real-time point cloud segmentation, GPU-accelerated colored-ICP registration, grasp pose generation from complete object models, and dynamic motion planning with cuRobo MPC.\nReal-time Point Clouds Segmentation.\nTo achieve real-time segmentation on incoming RGB-D streams, we build a module shown in Fig\n4\n, that performs continuous inference on camera frames and outputs segmentation masks\nğ’®\np\n\\mathcal{S}_{p}\n,\nand then projects the mask onto the full point cloud to obtain the corresponding partial object point clouds\nğ’³\np\n\\mathcal{X}_{p}\n. Compared with traditional offline SAM2 inference, we design a sliding window mechanism that enables SAM2 to process camera streams in real time, maintaining temporal consistency in object masks for continuous 3D segmentation and tracking under occlusions. At image size 640Ã—480, our method runs at 15 Hz on RTX 4090.\nColored-ICP Registration.\nFor aligning partial point clouds\nğ’³\np\n\\mathcal{X}_{p}\nobtained from the camera with the corresponding full object model\nğ’³\nm\n\\mathcal{X}_{m}\nstored in the memory bank, we employ a colored-ICP algorithm\n[\n32\n]\nimplemented on the GPU via the cupoch library\n[\n42\n]\n.\nUnlike traditional geometric ICP, which minimizes only spatial distance, colored-ICP jointly minimizes geometric and color residuals:\nE\nâ€‹\n(\nR\n,\nt\n)\n=\nâˆ‘\ni\n[\nÎ»\ng\nâ€‹\nâ€–\nR\nâ€‹\nğ±\ni\n+\nt\nâˆ’\nğ²\ni\nâ€–\n2\n+\nÎ»\nc\nâ€‹\nâ€–\nI\nâ€‹\n(\nğ±\ni\n)\nâˆ’\nI\nâ€‹\n(\nğ²\ni\n)\nâ€–\n2\n]\nE(R,t)=\\sum_{i}[\\lambda_{g}\\|R\\mathbf{x}_{i}+t-\\mathbf{y}_{i}\\|^{2}+\\lambda_{c}\\|I(\\mathbf{x}_{i})-I(\\mathbf{y}_{i})\\|^{2}]\nwhere\n(\nR\n,\nt\n)\n(R,t)\ndenotes the transformation between\nğ’³\np\n\\mathcal{X}_{p}\nand\nğ’³\nm\n\\mathcal{X}_{m}\n, and\nI\nâ€‹\n(\nâ‹…\n)\nI(\\cdot)\nrepresents point color intensity. The weighting factors\nÎ»\ng\n\\lambda_{g}\nand\nÎ»\nc\n\\lambda_{c}\nbalance geometric and color terms.\nFigure 4\n:\nOverview of the camera predictor module\n.\nThe red solid line indicates that the first frame is persistently stored in the frame memory.\nThe yellow dashed line represents the operation of saving frames from the previous time step into the memory.\nThe blue and green solid lines denote the data flow and processing steps for the current frame.\nTogether, the sliding-window mechanism enables real-time video segmentation and object-level point cloud tracking with temporally consistent memory updates.\nGrasp Pose Generation and Obstacle Representation.\nFor unseen target objects, we directly apply GraspGen to the partial point cloud.\nIn contrast, once a seen target object is registered, we replace its partial observation with the complete point cloud\nğ’³\nm\n\\mathcal{X}_{m}\nfrom the memory bank and feed it into GraspGen to predict grasp poses\n{\nğ“\nğ ğ«ğ¢ğ©ğ©ğğ«\n=\nf\nGraspGen\nâ€‹\n(\nğ’³\nm\n)\n}\n\\{\\mathbf{T_{gripper}}=f_{\\text{GraspGen}}(\\mathcal{X}_{m})\\}\n,This replacement mitigates the uncertainty from occlusion and single-view perception, yielding more stable and accurate grasp pose estimation.\nFor unseen obstacles, we dynamically generate multi-convex hulls (V-HACD\n[\n49\n]\n) from the segmented point cloud for collision modeling.\nFor known obstacles, the aligned object meshes and poses are directly imported into the digital twin for real-time collision checking.\nMotion Planning with Sim-to-real Synchronization.\nFinally, motion planning and control are performed using the cuRoboâ€™s model predictive control (MPC) framework.\nAt each control step, the robotâ€™s joint states are synchronized with the digital twin,\nwhere cuRobo computes an optimized short-horizon trajectory under real-time collision constraints.\nOnly the first control action\n{\nğš\n0\n}\n\\{\\mathbf{a}_{0}\\}\nfrom the predicted trajectory\nğ€\nt\n:\nt\n+\nH\n=\n{\nğš\n0\n,\nğš\n1\n,\nâ€¦\n,\nğš\nH\n}\n\\mathbf{A}_{t:t+H}=\\{\\mathbf{a}_{0},\\mathbf{a}_{1},\\ldots,\\mathbf{a}_{H}\\}\nis executed on the real robot, followed by continuous replanning with the environment updates,\nachieving closed-loop synchronization between simulation and reality.\n4\nExperiments\nWe evaluate SyncTwin across three dimensions:\n(1) the efficiency of the fast 3D reconstruction pipeline;\n(2) obstacle avoidance performance under dynamic and single-view occluded conditions; and\n(3) grasp success rate under single-view occlusion.\nAll experiments are designed to validate both the offline and online components of our framework.\n4.1\nExperiment Setup\nAll experiments are conducted with a Franka Emika Panda robotic arm equipped with an Intel RealSense D455 RGB-D camera mounted above and in front of the workspace. And iPhone 12 for RGB images.\nWe apply voxel-based downsampling to the input point cloud with a voxel size of 3Â mm.\nThis is motivated by the fact that the computational complexity of point cloud segmentation algorithms scales linearly with the number of points.\nThe digital twin is implemented in Isaac SimÂ 4.0 and integrated with the cuRobo MPC motion planning framework, running on a single NVIDIA RTXÂ 4090 GPU.\nBoth perception and planning run on the same GPU, enabling a closed-loop update rate of up to 5Â Hz. Motion planning runs at 10Â Hz, and the robotâ€™s velocity scaling is set to 0.2. The test objects include bottles, cans, cups, and boxes of various shapes.\nFigure 5\n:\nThe comparison of processing time given different numbers of input images.\nThe processing time covers both reconstruction and segmentation. The 5 and 10 images do not apply to the baselines because of fail to estimate the camera extrinsic.\n4.2\nBaselines and Metrics\nBaselines.\n(1) For\n3D reconstruction\n: we compare against Photogrammetry\n[\n35\n]\n, NeRF (Nerfstudio\n[\n43\n]\n), and Gaussian Splatting (3DGS\n[\n20\n]\n). (2) For\nobstacle avoidance\n: we adopt NVBlox\n[\n23\n]\nas the baseline voxel-mapping method.\nFor the ablation studies, we analyze performance across the following settings:\n(1)\nMask Expansion and Denoising\n: using variants without mask expansion and without denoising as baselines, evaluating their reconstruction quality against our full segmentation mechanism.\n(2)\nObject Completion\n: we use GraspGen\n[\n26\n]\nas the baseline grasp generator, comparing grasp poses predicted from single-view partial point clouds (baseline) versus from complete, asset-retrieved geometry produced by SyncTwin (ours).\nEvaluation Metrics.\nDepending on the experiment, we report the following metrics:\nreconstruction time (min), dependency on the number of input images\nN\nmin\nN_{\\min}\n, obstacle avoidance success rate (%), and grasp success rate (%).\nFor avoidance tests, we define 3-level outcome levels: FA (full avoidance, no contact with the object, 1.0),\nEA (edge avoidance, slight contact without displacing the object, 0.8),\nand CO (collision, noticeable contact that significantly moves the object, 0.0). Weighted success rate:\nSR\n=\nN\nFA\n+\n0.8\nâ€‹\nN\nEA\n+\n0.0\nâ€‹\nN\nCO\nN\nÃ—\n100\n%\n.\n\\text{SR}=\\frac{N_{\\mathrm{FA}}+0.8\\,N_{\\mathrm{EA}}+0.0\\,N_{\\mathrm{CO}}}{N}\\times 100\\%.\nFigure 6\n:\nReconstruction comparison given different numbers of input images\n. In each column, the left figure shows the point clouds, and the right one shows the untextured mesh.\nTable 1\n:\nComparison of obstacle-avoidance performance between NVBlox and SyncTwin in dynamic environments\n.\nUnseen objects (left) are not present in the asset memory, whereas starred objects in the Seen category (right) are stored in the memory bank.\nSyncTwin achieves significantly higher success rates in both settings, with particularly strong gains when complete object assets are available.\nMethod\nMotion\nUnseen\nSeen (Memory Bank)\nBox1\nBox2\nBox3\nBox4\nSR_unseen (%)\nBox1*\nBox2*\nBox3*\nBox4*\nSR_seen (%)\nFA\nEA\nCO\nFA\nEA\nCO\nFA\nEA\nCO\nFA\nEA\nCO\nFA\nEA\nCO\nFA\nEA\nCO\nFA\nEA\nCO\nFA\nEA\nCO\nNVBlox\nSelfRot\n6\n4\n10\n9\n5\n6\n3\n4\n13\n3\n11\n6\n50.3%\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nEnterTraj\n3\n9\n8\n4\n9\n7\n1\n6\n13\n0\n3\n17\n37.0%\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nâ€“\nSyncTwin\nSelfRot\n12\n7\n1\n14\n5\n1\n11\n7\n2\n9\n9\n2\n85.5%\n18\n1\n1\n16\n4\n0\n15\n5\n0\n13\n6\n1\n93.5%\nEnterTraj\n8\n8\n4\n11\n6\n3\n8\n7\n5\n7\n8\n5\n71.5%\n12\n7\n3\n12\n5\n3\n10\n6\n4\n9\n7\n4\n78.8%\n4.3\nFast 3D Reconstruction\nWe evaluate the efficiency and input-image dependency of SyncTwinâ€™s 3D reconstruction module, comparing against Photogrammetry, 3DGS, and Nerfstudio. Each method reconstructs the same object using 5, 10 ,20, 40, 60 RGB images, and all approaches obtain object assets through multi-view projection based segmentation. The experimental results are summarized as follows.\nReconstruction Time.\nAs shown in Figure\n5\n, SyncTwin achieves the shortest reconstruction time across all settings.\nIt produces a simulation-ready mesh in only about 1â€“2 minutes using 5â€“10 input images, while Photogrammetry, 3DGS, and Nerfstudio require at least 4â€“7 minutes even with more frames. This enables significantly faster digital-twin construction in a new real-world scenarios.\nDependency on the Number of Input Images.\nSyncTwin also exhibits the lowest dependency on input-image count.\nIt generates usable meshes from as few as 5â€“10 images, whereas competing approaches typically require 20+ images to avoid failure caused by unstable optimization of camera extrinsics.\nThis low image dependency substantially accelerates asset generation and improves robustness under limited views, especially because the reported processing time excludes image-capture time, which becomes additional and unpredictable when more images are required.\nFurthermore, the qualitative comparison in Figure\n6\nshows that our method preserves fine-grained geometric details even with very few input images, as our mesh simplification algorithm maintains high geometric fidelity while reducing mesh vertices, enabling faster simulation performance. For instance, the\nbearâ€™s ear shape\nremains well preserved with only 5â€“10 frames, demonstrating both low image dependency and strong geometric consistency.\nThese properties make SyncTwin a state-of-the-art solution for fast 3D geometry asset generation: the system can construct high-quality, simulation-ready object meshes in about 1 minute, enabling rapid reconstruction of object assets for downstream digital-twin synchronization.\n4.4\nObstacle Avoidance under Occlusion\nFigure 7\n:\nExamples of SyncTwinâ€™s dynamic obstacle avoidance\n.\nGreen dashed lines indicate collision-free robot trajectories, while red lines mark trajectories that result in collisions.\nBlue arrows denote the motion of dynamic obstacles.\nFor unseen objects (top-left), the robot collides with unobserved regions, whereas the same object in the seen case (top-right) is successfully avoided. SyncTwin also handles dynamic obstacles (bottom-left) and small objects (bottom-right) effectively.\nWe adopt the built-in obstacle avoidance benchmark in cuRobo to evaluate the success rate of obstacle avoidance, where the robot repeatedly moves between two target points while avoiding obstacles along its path.\nBoth NVBlox and SyncTwin are tested on unseen (not stored in memory bank) obstacles to evaluate avoidance performance. Additionally, SyncTwin is evaluated on seen (stored in memory bank) objects recorded in its asset memory to study how prior geometry improves safety and reactivity under single-view occlusion.\nThe experiment tests under two motion patterns:\nSelfRot\nindicates in-place rotation;\nEnterTraj\nblocks motion into the predicted trajectory.\nEach condition is repeated for 20 trials, total\nN\n=\n20\nÃ—\n4\n=\n80\nN=20\\times 4=80\n.\nWe test four boxes with sizes of\nBox1:\n10\nÃ—\n10\nÃ—\n10\n10{\\times}10{\\times}10\ncm,\nBox2:\n20\nÃ—\n20\nÃ—\n20\n20{\\times}20{\\times}20\ncm,\nBox3:\n10\nÃ—\n20\nÃ—\n30\n10{\\times}20{\\times}30\ncm, and\nBox4:\n20\nÃ—\n22\nÃ—\n35\n20{\\times}22{\\times}35\ncm as obstacles.\nThrough experimental observation, the results are as follows:\nUnseen Object Performance.\nAs shown in Table\n1\n, SyncTwin consistently outperforms NVBlox when encountering unseen obstacles under single-view dynamic occlusion. NVBlox exhibits failure modes such as: (1) unstable voxelization for small objects (e.g., Box1) due to sparse depth returns; (2) misclassification of limited height objects (e.g., Box3) as part of the tabletop caused by inconsistent depth estimation; and (3) trajectory intersections when large obstacles (e.g., Box4) fall outside the sensorâ€™s visible region.\nAcross all these cases, SyncTwin maintains markedly higher obstacle-avoidance success rates, demonstrating stronger robustness to object scale, occlusion, and limited viewpoint coverage.\nSeen Object Performance.\nIn Table\n1\n, when objects are stored in the asset memory, SyncTwinâ€™s performance improves even further. Complete geometric priors obtained in StageÂ I resolve the challenges posed by small or thin objects, eliminate many failure modes, and convert numerous edge-avoidance cases into full avoidance.\nThese results indicate that SyncTwin not only generalizes better on unseen objects but also achieves substantially higher reliability when the objects have been previously reconstructed. These results highlight the importance of SyncTwinâ€™s Stage I memory construction, which enables complete geometric reasoning even under partial observability. Furthermore, Figure\n7\nillustrates representative avoidance examples achieved by SyncTwin.\n4.5\nAblation Studies\nMask Expansion and Denoising.\nDue to the inaccuracy of VGGT-estimated camera extrinsics, multi-view mask projections often become misaligned and fail to fully cover the object.\nOur ablation study evaluates reconstruction results\nw/o mask expansion\nand\nw/o denoising\n.\nAs shown in Figure\n8\n, removing margin expansion causes inconsistent projections across viewpoints, leaving parts of the object missing.\nLikewise, disabling supporting-plane points denoising preserves table artifacts in the point cloud, resulting in noisy meshes.\nThese comparisons demonstrate that segmentation-aware expansion and support-plane denoising are essential for obtaining clean and stable 3D assets.\nFigure 8\n:\nComparison of segmentation-based 3D reconstruction results\n.\nLeft: Our method generates a complete and clean object mesh without support-plane noise.\nMiddle: w/o mask expansion, multi-view projections (red) fail to fully overlap, and their intersection leads to missing object geometry.\nRight: w/o denoising, the intersection of projected regions (red) incorrectly preserves support-plane points, introducing significant noise into the reconstructed point cloud.\nTable 2\n:\nComparison of grasp success rates before and after geometry completion\n.\nAsset-based completion significantly improves performance across all objects by producing more accurate and safer grasp pose candidates, with the largest gains observed for partially occluded items such as the cup (with handle).\nCondition\nBottle\nCup (Handle)\nCookie Box\nChips Can\nBefore Completion (%)\n78.3\n65.0\n81.7\n80.0\nAfter Completion (%)\n90.0\n86.7\n93.3\n95.0\nImprovement\n+11.7\n+21.7\n+11.6\n+15.0\nObject Completion.\nWe evaluate the effect of object completeness on grasp generation.\nWhen grasp poses are generated from single-view partial point clouds, the limited geometry often leads to incomplete or incorrect grasp configurations, for example. As shown in Figure\n9\n, the cup handle is misinterpreted, resulting in unsafe or colliding grasps.\nIn contrast, after SyncTwin aligns the observed object with the corresponding complete mesh from the asset library, the grasp generator produces denser, more accurate, and physically feasible grasp candidates.\nThis demonstrates that asset-based completion is essential for reliable and collision-free grasp execution in the real world.\nAs shown in Table\n2\n, across 60 evaluation trials, grasp success rates increase substantially after geometry completion. The improvement is most pronounced for partially occluded or asymmetric objects, such as the cup and chips can, where asset-based completion provides the missing structure needed for reliable planning.\nFigure 9\n:\nComparison of grasp candidates generated from Ours (w/o Completion) and Ours.\nThe example objects include a handled cup (left pairs) and a bottle (right pairs).\nGreen lines visualize predicted gripper poses.\n4.6\nDiscussions\nIn our experiments, StageÂ I reconstruction occasionally fails when the object lacks a clear supporting surface or when the support plane is weakly connected to the object (e.g., a tall cup).\nIn these cases, inaccurate camera extrinsics cause the 2D segmentation masks to be incorrectly projected into 3D, resulting in broken or incomplete object point clouds and missing geometry.\nSuch extrinsic errors produced by learning-based reconstruction remain an open challenge.\n5\nConclusion\nWe introduced SyncTwin, a digital-twin framework that unifies fast RGB-only 3D reconstruction with real-time scene synchronization for safe and robust grasping in dynamic, partially occluded environments.\nBy leveraging VGGT-based reconstruction, segmentation-aware denoising, and memory-driven geometry completion, SyncTwin provides accurate object geometry and reliable grasp generation from limited visual input.\nBy bridging the sim-to-real gap through consistent digital-twin updates, the system enables simulation-based planners to execute safe, collision-aware trajectories on real robots without retraining.\nOur experiments demonstrate substantial improvements over existing baselines such as NVBlox, including higher obstacle-avoidance success rates, more stable behavior in a dynamic environment, and improved grasp performance under single-view occlusion.\nThese results highlight the advantage of combining fast asset generation with persistent memory and real-time synchronization, enabling safe and accurate execution in real-world environments.\nLooking forward, two extensions appear particularly promising.\nFirst, enabling online asset expansion by integrating StageÂ I construction into StageÂ II synchronization would allow the system to incrementally acquire new objects as they appear.\nSecond, distributed or multi-GPU system designs that decouple perception, synchronization, and planning could further improve responsiveness under fast dynamic scenes.\nConsequently, these enable more adaptive, real-worldâ€“oriented digital-twin synchronization.\nReferences\n[1]\nA. Bahety, A. Balaji, B. Abbatematteo, and R. MartÃ­n-MartÃ­n\n(2025)\nSafeMimic: towards safe and autonomous human-to-robot imitation for mobile manipulation\n.\narXiv preprint arXiv:2506.15847\n.\nCited by:\nÂ§2\n.\n[2]\nM. E. A. Boudjoghra, A. Dai, J. Lahoud, H. Cholakkal, R. M. Anwer, S. Khan, and F. S. Khan\n(2024)\nOpen-yolo 3d: towards fast and accurate open-vocabulary 3d instance segmentation\n.\narXiv preprint arXiv:2406.02548\n.\nCited by:\nÂ§2\n.\n[3]\nL. Brunke, Y. Zhang, R. RÃ¶mer, J. Naimer, N. Staykov, S. Zhou, and A. P. Schoellig\n(2025)\nSemantically safe robot manipulation: from semantic scene understanding to motion safeguards\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§2\n.\n[4]\nY. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox\n(2019)\nClosing the sim-to-real loop: adapting simulation randomization with real world experience\n.\nIn\n2019 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 8973â€“8979\n.\nCited by:\nÂ§2\n.\n[5]\nA. Dai, M. NieÃŸner, M. ZollhÃ¶fer, S. Izadi, and C. Theobalt\n(2017)\nBundlefusion: real-time globally consistent 3d reconstruction using on-the-fly surface reintegration\n.\nACM Transactions on Graphics (ToG)\n36\n(\n4\n),\npp.Â 1\n.\nCited by:\nÂ§2\n.\n[6]\nM. Dalal, J. Yang, R. Mendonca, Y. Khaky, R. Salakhutdinov, and D. Pathak\n(2024)\nNeural mp: a generalist neural motion planner\n.\narXiv preprint arXiv:2409.05864\n.\nCited by:\nÂ§2\n.\n[7]\nM. Dou, L. Guan, J. Frahm, and H. Fuchs\n(2012)\nExploring high-level plane primitives for indoor 3d reconstruction with a hand-held rgb-d camera\n.\nIn\nAsian Conference on Computer Vision\n,\npp.Â 94â€“108\n.\nCited by:\nÂ§2\n.\n[8]\nM. A. Fischler and R. C. Bolles\n(1981)\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography\n.\nCommunications of the ACM\n24\n(\n6\n),\npp.Â 381â€“395\n.\nCited by:\nÂ§3.2\n.\n[9]\nA. Fishman, A. Murali, C. Eppner, B. Peele, B. Boots, and D. Fox\n(2023)\nMotion policy networks\n.\nIn\nconference on Robot Learning\n,\npp.Â 967â€“977\n.\nCited by:\nÂ§2\n.\n[10]\nY. Furukawa and J. Ponce\n(2009)\nAccurate, dense, and robust multiview stereopsis\n.\nIEEE transactions on pattern analysis and machine intelligence\n32\n(\n8\n),\npp.Â 1362â€“1376\n.\nCited by:\nÂ§2\n.\n[11]\nS. Galliani, K. Lasinger, and K. Schindler\n(2015)\nMassively parallel multiview stereopsis by surface normal diffusion\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp.Â 873â€“881\n.\nCited by:\nÂ§2\n.\n[12]\nG. H. Golub and C. Reinsch\n(1971)\nSingular value decomposition and least squares solutions\n.\nIn\nLinear algebra\n,\npp.Â 134â€“151\n.\nCited by:\nÂ§3.2\n.\n[13]\nÃ. GonzÃ¡lez\n(2010)\nMeasurement of areas on a sphere using fibonacci and latitudeâ€“longitude lattices\n.\nMathematical geosciences\n42\n(\n1\n),\npp.Â 49â€“64\n.\nCited by:\nÂ§3.2\n.\n[14]\nR. Hartley and A. Zisserman\n(2003)\nMultiple view geometry in computer vision\n.\nCambridge university press\n.\nCited by:\nÂ§2\n,\nÂ§3.2\n.\n[15]\nL. Jiang, H. Zhao, S. Shi, S. Liu, C. Fu, and J. Jia\n(2020)\nPointgroup: dual-set point grouping for 3d instance segmentation\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and Pattern recognition\n,\npp.Â 4867â€“4876\n.\nCited by:\nÂ§2\n.\n[16]\nM. Kalakrishnan, S. Chitta, E. Theodorou, P. Pastor, and S. Schaal\n(2011)\nSTOMP: stochastic trajectory optimization for motion planning\n.\nIn\n2011 IEEE international conference on robotics and automation\n,\npp.Â 4569â€“4574\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\n[17]\nJ. H. Kang, S. Joshi, R. Huang, and S. K. Gupta\n(2025)\nRobotic compliant object prying using diffusion policy guided by vision and force observations\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§2\n.\n[18]\nS. Karaman and E. Frazzoli\n(2011)\nSampling-based algorithms for optimal motion planning\n.\nThe international journal of robotics research\n30\n(\n7\n),\npp.Â 846â€“894\n.\nCited by:\nÂ§2\n.\n[19]\nP. Katara, Z. Xian, and K. Fragkiadaki\n(2024)\nGen2sim: scaling up robot learning in simulation with generative models\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6672â€“6679\n.\nCited by:\nÂ§2\n.\n[20]\nB. Kerbl, G. Kopanas, T. LeimkÃ¼hler, and G. Drettakis\n(2023)\n3D gaussian splatting for real-time radiance field rendering.\n.\nACM Trans. Graph.\n42\n(\n4\n),\npp.Â 139â€“1\n.\nCited by:\nÂ§2\n,\nÂ§4.2\n.\n[21]\nJ. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger\n(2018)\nFusion++: volumetric object-level slam\n.\nIn\n2018 international conference on 3D vision (3DV)\n,\npp.Â 32â€“41\n.\nCited by:\nÂ§2\n.\n[22]\nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng\n(2021)\nNerf: representing scenes as neural radiance fields for view synthesis\n.\nCommunications of the ACM\n65\n(\n1\n),\npp.Â 99â€“106\n.\nCited by:\nÂ§2\n.\n[23]\nA. Millane, H. Oleynikova, E. Wirbel, R. Steiner, V. Ramasamy, D. Tingdahl, and R. Siegwart\n(2024)\nNvblox: gpu-accelerated incremental signed distance field mapping\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 2698â€“2705\n.\nCited by:\nÂ§1\n,\nÂ§2\n,\nÂ§4.2\n.\n[24]\nM. Mittal, C. Yu, Q. Yu, J. Liu, N. Rudin, D. Hoeller, J. L. Yuan, R. Singh, Y. Guo, H. Mazhar,\net al.\n(2023)\nOrbit: a unified simulation framework for interactive robot learning environments\n.\nIEEE Robotics and Automation Letters\n8\n(\n6\n),\npp.Â 3740â€“3747\n.\nCited by:\nÂ§1\n,\nÂ§3.1\n.\n[25]\nY. Mu, T. Chen, Z. Chen, S. Peng, Z. Lan, Z. Gao, Z. Liang, Q. Yu, Y. Zou, M. Xu,\net al.\n(2025)\nRobotwin: dual-arm robot benchmark with generative digital twins\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 27649â€“27660\n.\nCited by:\nÂ§2\n.\n[26]\nA. Murali, B. Sundaralingam, Y. Chao, W. Yuan, J. Yamada, M. Carlson, F. Ramos, S. Birchfield, D. Fox, and C. Eppner\n(2025)\nGraspgen: a diffusion-based framework for 6-dof grasping with on-generator training\n.\narXiv preprint arXiv:2507.13097\n.\nCited by:\nÂ§4.2\n.\n[27]\nK. Nakamura, L. Peters, and A. Bajcsy\n(2025)\nGeneralizing safety beyond collision-avoidance via latent-space reachability analysis\n.\narXiv preprint arXiv:2502.00935\n.\nCited by:\nÂ§2\n.\n[28]\nR. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon\n(2011)\nKinectfusion: real-time dense surface mapping and tracking\n.\nIn\n2011 10th IEEE international symposium on mixed and augmented reality\n,\npp.Â 127â€“136\n.\nCited by:\nÂ§2\n.\n[29]\nC. Ning, K. Fang, and W. Ma\n(2025)\nPrompting with the future: open-world model predictive control with interactive digital twins\n.\narXiv preprint arXiv:2506.13761\n.\nCited by:\nÂ§2\n.\n[30]\nJ. Oliensis\n(2000)\nA critique of structure-from-motion algorithms\n.\nComputer Vision and Image Understanding\n80\n(\n2\n),\npp.Â 172â€“214\n.\nCited by:\nÂ§2\n.\n[31]\nO. Ã–zyeÅŸil, V. Voroninski, R. Basri, and A. Singer\n(2017)\nA survey of structure from motion*.\n.\nActa Numerica\n26\n,\npp.Â 305â€“364\n.\nCited by:\nÂ§2\n.\n[32]\nJ. Park, Q. Zhou, and V. Koltun\n(2017)\nColored point cloud registration revisited\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp.Â 143â€“152\n.\nCited by:\nÂ§3.3\n.\n[33]\nN. Ratliff, M. Zucker, J. A. Bagnell, and S. Srinivasa\n(2009)\nCHOMP: gradient optimization techniques for efficient motion planning\n.\nIn\n2009 IEEE international conference on robotics and automation\n,\npp.Â 489â€“494\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\n[34]\nM. Runz, M. Buffier, and L. Agapito\n(2018)\nMaskfusion: real-time recognition, tracking and reconstruction of multiple moving objects\n.\nIn\n2018 IEEE international symposium on mixed and augmented reality (ISMAR)\n,\npp.Â 10â€“20\n.\nCited by:\nÂ§2\n.\n[35]\nJ. L. Schonberger and J. Frahm\n(2016)\nStructure-from-motion revisited\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 4104â€“4113\n.\nCited by:\nÂ§2\n,\nÂ§4.2\n.\n[36]\nJ. Schulman, J. Ho, A. X. Lee, I. Awwal, H. Bradlow, and P. Abbeel\n(2013)\nFinding locally optimal, collision-free trajectories with sequential convex optimization.\n.\nIn\nRobotics: science and systems\n,\nVol.\n9\n,\npp.Â 1â€“10\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\n[37]\nS. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski\n(2006)\nA comparison and evaluation of multi-view stereo reconstruction algorithms\n.\nIn\n2006 IEEE computer society conference on computer vision and pattern recognition (CVPRâ€™06)\n,\nVol.\n1\n,\npp.Â 519â€“528\n.\nCited by:\nÂ§2\n.\n[38]\nF. SteinbrÃ¼cker, J. Sturm, and D. Cremers\n(2011)\nReal-time visual odometry from dense rgb-d images\n.\nIn\n2011 IEEE international conference on computer vision workshops (ICCV Workshops)\n,\npp.Â 719â€“722\n.\nCited by:\nÂ§2\n.\n[39]\nI. A. Sucan, M. Moll, and L. E. Kavraki\n(2012)\nThe open motion planning library\n.\nIEEE Robotics & Automation Magazine\n19\n(\n4\n),\npp.Â 72â€“82\n.\nCited by:\nÂ§2\n.\n[40]\nY. Sun, M. Van, S. McIlvanna, N. M. Nhat, K. Olayemi, J. Close, and S. McLoone\n(2024)\nDigital twin-driven reinforcement learning for obstacle avoidance in robot manipulators: a self-improving online training framework\n.\narXiv preprint arXiv:2403.13090\n.\nCited by:\nÂ§2\n.\n[41]\nB. Sundaralingam, S. K. S. Hari, A. Fishman, C. Garrett, K. Van Wyk, V. Blukis, A. Millane, H. Oleynikova, A. Handa, F. Ramos,\net al.\n(2023)\nCurobo: parallelized collision-free robot motion generation\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 8112â€“8119\n.\nCited by:\nÂ§1\n,\nÂ§3.1\n.\n[42]\nK. Tanaka\n(2020)\nCupoch â€“ robotics with gpu computing\n.\nNote:\nhttps://github.com/neka-nat/cupoch\nCited by:\nÂ§1\n,\nÂ§3.3\n.\n[43]\nM. Tancik, E. Weber, E. Ng, R. Li, B. Yi, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja,\net al.\n(2023)\nNerfstudio: a modular framework for neural radiance field development\n.\nIn\nACM SIGGRAPH 2023 conference proceedings\n,\npp.Â 1â€“12\n.\nCited by:\nÂ§4.2\n.\n[44]\nG. Taubin\n(1995)\nA signal processing approach to fair surface design\n.\nIn\nProceedings of the 22nd annual conference on Computer graphics and interactive techniques\n,\npp.Â 351â€“358\n.\nCited by:\nÂ§3.2\n.\n[45]\nM. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, and P. Agrawal\n(2024)\nReconciling reality through simulation: a real-to-sim-to-real approach for robust manipulation\n.\narXiv preprint arXiv:2403.03949\n.\nCited by:\nÂ§2\n.\n[46]\nJ. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny\n(2025)\nVggt: visual geometry grounded transformer\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 5294â€“5306\n.\nCited by:\nÂ§1\n,\nÂ§3.2\n.\n[47]\nW. Wang, R. Yu, Q. Huang, and U. Neumann\n(2018)\nSgpn: similarity group proposal network for 3d point cloud instance segmentation\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 2569â€“2578\n.\nCited by:\nÂ§2\n.\n[48]\nL. Wei, J. Ma, Y. Hu, and R. Zhang\n(2024)\nEnsuring force safety in vision-guided robotic manipulation via implicit tactile calibration\n.\narXiv preprint arXiv:2412.10349\n.\nCited by:\nÂ§2\n.\n[49]\nX. Wei, M. Liu, Z. Ling, and H. Su\n(2022)\nApproximate convex decomposition for 3d meshes with collision-aware concavity and tree search\n.\nACM Transactions on Graphics (TOG)\n41\n(\n4\n),\npp.Â 1â€“18\n.\nCited by:\nÂ§3.3\n.\n[50]\nY. Wu, L. Pan, W. Wu, G. Wang, Y. Miao, F. Xu, and H. Wang\n(2025)\nRl-gsbridge: 3d gaussian splatting based real2sim2real method for robotic manipulation learning\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 192â€“198\n.\nCited by:\nÂ§2\n.\n[51]\nJ. Xu, S. Wang, Z. Ni, C. Hu, S. Yang, J. Zhu, and Q. Li\n(2025)\nSAM4D: segment anything in camera and lidar streams\n.\narXiv preprint arXiv:2506.21547\n.\nCited by:\nÂ§1\n.\n[52]\nM. Xu, X. Yin, L. Qiu, Y. Liu, X. Tong, and X. Han\n(2025)\nSampro3d: locating sam prompts in 3d for zero-shot instance segmentation\n.\nIn\n2025 International Conference on 3D Vision (3DV)\n,\npp.Â 1222â€“1232\n.\nCited by:\nÂ§2\n.\n[53]\nJ. Yang, J. J. Liu, Y. Li, Y. Khaky, K. Shaw, and D. Pathak\n(2025)\nDeep reactive policy: learning reactive manipulator motion planning for dynamic environments\n.\narXiv preprint arXiv:2509.06953\n.\nCited by:\nÂ§1\n,\nÂ§2\n.\n[54]\nY. Yao, Z. Luo, S. Li, T. Fang, and L. Quan\n(2018)\nMvsnet: depth inference for unstructured multi-view stereo\n.\nIn\nProceedings of the European conference on computer vision (ECCV)\n,\npp.Â 767â€“783\n.\nCited by:\nÂ§2\n.\n[55]\nH. Yuan, Z. Huang, Y. Wang, C. Mao, C. Xu, and Z. Lu\n(2025)\nDemograsp: universal dexterous grasping from a single demonstration\n.\narXiv preprint arXiv:2509.22149\n.\nCited by:\nÂ§2\n.\n[56]\nD. Zhang, D. Liang, H. Yang, Z. Zou, X. Ye, Z. Liu, and X. Bai\n(2023)\nSam3d: zero-shot 3d object detection via segment anything model\n.\narXiv preprint arXiv:2306.02245\n.\nCited by:\nÂ§2\n.\n[57]\nY. Zhou, J. Gu, T. Y. Chiang, F. Xiang, and H. Su\n(2024)\nPoint-sam: promptable 3d segmentation model for point clouds\n.\narXiv preprint arXiv:2406.17741\n.\nCited by:\nÂ§2\n.\n[58]\nZ. Zhou, X. Yang, H. Wang, and X. Zhang\n(2022)\nDigital twin with integrated robot-human/environment interaction dynamics for an industrial mobile manipulator\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 5041â€“5047\n.\nCited by:\nÂ§1\n.\n\\thetitle\nSupplementary Material\n6\nDenoising Algorithms Details\n1) Robust Center Estimation.\nGiven a point cloud\nğ’«\n=\n{\nğ©\ni\nâˆˆ\nâ„\n3\n}\n\\mathcal{P}=\\{\\mathbf{p}_{i}\\in\\mathbb{R}^{3}\\}\n,\nwe compute a robust geometric center\nğœ\n\\mathbf{c}\nto reduce bias from uneven sampling:\nğœ\n=\nmean\nâ€‹\n(\nğ’«\n)\n.\n\\mathbf{c}=\\mathrm{mean}(\\mathcal{P}).\n(3)\n2) Directional Discretization via Fibonacci Sphere.\nWe uniformly sample\nK\nK\ndirections\n{\nğ\ni\n}\ni\n=\n1\nK\n\\{\\mathbf{d}_{i}\\}_{i=1}^{K}\non the unit sphere using the Fibonacci spiral:\nÎ¸\ni\n=\narccos\nâ¡\n(\n1\nâˆ’\n2\nâ€‹\ni\nK\n)\n,\nÏ•\ni\n=\nÏ€\nâ€‹\n(\n1\n+\n5\n)\nâ€‹\ni\n,\n\\theta_{i}=\\arccos\\!\\left(1-\\frac{2i}{K}\\right),\\qquad\\phi_{i}=\\pi(1+\\sqrt{5})\\,i,\n(4)\nğ\ni\n=\n[\nsin\nâ¡\nÎ¸\ni\nâ€‹\ncos\nâ¡\nÏ•\ni\n,\nsin\nâ¡\nÎ¸\ni\nâ€‹\nsin\nâ¡\nÏ•\ni\n,\ncos\nâ¡\nÎ¸\ni\n]\nâŠ¤\n.\n\\mathbf{d}_{i}=[\\sin\\theta_{i}\\cos\\phi_{i},\\,\\sin\\theta_{i}\\sin\\phi_{i},\\,\\cos\\theta_{i}]^{\\top}.\n(5)\nThese directions form a discrete spherical domain\nğ’Ÿ\n\\mathcal{D}\nfor ray-wise accumulation.\n3) Point-to-Bucket Assignment.\nEach point defines a normalized direction from the center:\nğ¯\n^\ni\n=\nğ©\ni\nâˆ’\nğœ\nâ€–\nğ©\ni\nâˆ’\nğœ\nâ€–\n.\n\\hat{\\mathbf{v}}_{i}=\\frac{\\mathbf{p}_{i}-\\mathbf{c}}{\\|\\mathbf{p}_{i}-\\mathbf{c}\\|}.\n(6)\nA point is assigned to direction bucket\nğ\nj\n\\mathbf{d}_{j}\nif the angular deviation satisfies:\nğ¯\n^\ni\nâ‹…\nğ\nj\nâ‰¥\ncos\nâ¡\n(\nÎ¸\ntolerance\n)\n,\n\\hat{\\mathbf{v}}_{i}\\cdot\\mathbf{d}_{j}\\geq\\cos(\\theta_{\\text{tolerance}}),\n(7)\nwhere\nÎ¸\ntol\n\\theta_{\\text{tol}}\nis the angular tolerance.\n4) Progressive Sphere Expansion.\nWe iteratively expand a sphere centered at\nğœ\n\\mathbf{c}\nwith radius\nr\nt\nr_{t}\n:\nr\nt\n+\n1\n=\nr\nt\n+\nÎ”\nâ€‹\nr\n.\nr_{t+1}=r_{t}+\\Delta r.\n(8)\nA direction\nğ\nj\n\\mathbf{d}_{j}\nis marked as\nhit\nwhen any assigned point enters the sphere:\nr\nmin\nâ€‹\n(\nj\n)\n=\nmin\ni\nâˆˆ\nbucket\nâ€‹\nj\nâ¡\nâ€–\nğ©\ni\nâˆ’\nğœ\nâ€–\n.\nr_{\\min}(j)=\\min_{i\\in\\text{bucket }j}\\|\\mathbf{p}_{i}-\\mathbf{c}\\|.\n(9)\nUnhit directions form a binary mask\nğ’°\nt\n=\n{\nj\nâˆ£\nhit\nâ€‹\n(\nj\n)\n=\n0\n}\n\\mathcal{U}_{t}=\\{j\\mid\\text{hit}(j)=0\\}\n.\nConnected components on the spherical adjacency graph are extracted to identify large uncovered regions.\nStability is detected when the largest unhit component remains consistent over multiple iterations:\nmax\nâ¡\n(\n|\nğ’\nmax\n(\nt\nâˆ’\nk\n:\nt\n)\n|\n)\nâˆ’\nmin\nâ¡\n(\n|\nğ’\nmax\n(\nt\nâˆ’\nk\n:\nt\n)\n|\n)\nmax\nâ¡\n(\n|\nğ’\nmax\n(\nt\nâˆ’\nk\n:\nt\n)\n|\n)\n<\nÏµ\n.\n\\frac{\\max(|\\mathcal{C}_{\\max}^{(t-k:t)}|)-\\min(|\\mathcal{C}_{\\max}^{(t-k:t)}|)}{\\max(|\\mathcal{C}_{\\max}^{(t-k:t)}|)}<\\epsilon.\n(10)\n5) Boundary and Rim Extraction.\nBoundary buckets are defined as unhit directions adjacent to hit ones:\nâ„¬\n=\n{\nj\nâˆˆ\nğ’°\nâˆ£\nâˆƒ\nk\nâˆˆ\nğ’©\nâ€‹\n(\nj\n)\n,\nhit\nâ€‹\n(\nk\n)\n=\n1\n}\n.\n\\mathcal{B}=\\{j\\in\\mathcal{U}\\mid\\exists\\,k\\in\\mathcal{N}(j),\\,\\text{hit}(k)=1\\}.\n(11)\nFor each boundary direction\nğ\nj\n\\mathbf{d}_{j}\n, the farthest point within angular tolerance is selected as a rim point:\nğ©\nj\nâˆ—\n=\narg\nâ¡\nmax\nğ©\ni\nâˆˆ\nğ’«\nâ¡\nâ€–\nğ©\ni\nâˆ’\nğœ\nâ€–\n,\ns.t.\nâ€‹\nğ¯\n^\ni\nâ‹…\nğ\nj\nâ‰¥\ncos\nâ¡\n(\nÎ¸\ntol\n)\n.\n\\mathbf{p}^{*}_{j}=\\arg\\max_{\\mathbf{p}_{i}\\in\\mathcal{P}}\\|\\mathbf{p}_{i}-\\mathbf{c}\\|,\\qquad\\text{s.t. }\\hat{\\mathbf{v}}_{i}\\cdot\\mathbf{d}_{j}\\geq\\cos(\\theta_{\\text{tol}}).\n(12)\nIf no candidate is found, finer sub-buckets around\nğ\nj\n\\mathbf{d}_{j}\nare generated to refine sampling.\n6) Opening Axis and Plane Fitting.\nThe principal opening direction is computed by averaging the largest unhit component:\nğ§\nopen\n=\nâˆ‘\nj\nâˆˆ\nğ’\nmax\nğ\nj\nâ€–\nâˆ‘\nj\nâˆˆ\nğ’\nmax\nğ\nj\nâ€–\n.\n\\mathbf{n}_{\\text{open}}=\\frac{\\sum_{j\\in\\mathcal{C}_{\\max}}\\mathbf{d}_{j}}{\\left\\|\\sum_{j\\in\\mathcal{C}_{\\max}}\\mathbf{d}_{j}\\right\\|}.\n(13)\nRim points\n{\nğ©\nj\nâˆ—\n}\n\\{\\mathbf{p}^{*}_{j}\\}\nare used to fit an opening plane via SVD, providing a visualizable orientation and boundary.\nFigure 10\n:\nSegmented 3D point clouds in our experiments.\n7\nGreen Cube-Based Scale Estimation Algorithm\nTo determine the scale between VGGT-normalized coordinates and real-world measurements, we identify a green-colored reference cube embedded in the scene. The process involves the following steps:\n1) Point Cloud and Color Extraction.\nThe GLB file is loaded, and 3D point coordinates along with RGB vertex colors are extracted from the embedded mesh.\n2) Green Point Detection.\nWe employ multiple criteria to detect green points:\nâ€¢\nChannel thresholding: strong green with low red/blue values.\nâ€¢\nChannel dominance ratio: green significantly dominates red and blue.\nâ€¢\nHSV filtering: hue and saturation conditions for green.\nâ€¢\nPCA-based refinement and outlier filtering.\nAmong several detection masks, the method yielding a reasonable number of points (\n1000\n1000\nâ€“\n50000\n50000\n) is selected to ensure tight coverage of the cube. The result is shown in Figure\n11\n.\n3) Cube Dimension Analysis.\nThe bounding box of detected green points is computed to estimate cube dimensions.\nA refined analysis using PCA is then applied to:\nâ€¢\nExtract principal directions.\nâ€¢\nProject points onto the main 2D plane.\nâ€¢\nCompute four edge lengths of the square face from percentiles.\n4) Scale Factor Estimation.\nThe average of the four edge lengths (in VGGT units) is compared with the known real-world cube size (e.g.,\n0.1\nâ€‹\nm\n0.1\\,\\mathrm{m}\n) to compute the scale factor:\nScale Factor\n=\nReal Cube Size\nAverage Measured Length\n\\text{Scale Factor}=\\frac{\\text{Real Cube Size}}{\\text{Average Measured Length}}\nFigure 11\n:\nGreen cubeâ€“based scale estimation.\nThe top image shows the whole reconstructed point cloud.\nThe bottom illustrates the isolated cube points and the fitted red bounding box.\nBy measuring the cubeâ€™s average edge length in VGGT-normalized coordinates and comparing it to the known real-world cube size (e.g.,\n0.1\nâ€‹\nm\n0.1\\,\\mathrm{m}\n), the scale factor between VGGT coordinates and real-world metric space is computed.\nThis algorithm provides a robust estimation of coordinate scale using only geometric and color cues, without requiring prior calibration.\n8\n3D Asset Visualization\nFigure\n10\nshows the reconstructed assets used in our experiments, including a chip can, a cookie box, a Coke can, a spam can, a regular box, and a ketchup bottle. These objects have relatively regular and mostly convex shapes, making them representative of common household items.\nWe also include more irregular objects, such as the toy bear and the cup with a thin handle. Despite their asymmetric geometry and concave regions, the reconstruction pipeline can recover complete object meshes and segment them cleanly from the background, providing reliable assets for downstream synchronization and planning.\n9\nTips for Practical Implementation\nWe summarize several engineering considerations that are important for reproducing SyncTwin in practice.\nThese remarks are not essential to the core algorithmic contributions, but help avoid common pitfalls during system integration. To preserve anonymity during the review process, the implementation will be considered for full release after the review cycle.\n1) VGGT Extrinsics Are Not Stored in the GLB File.\nVGGT predicts per-image camera poses, but these extrinsics are\nnot\nembedded in the exported GLB mesh.\nInstead, they are stored in the accompanying\npredictions.npz\nfile and must be explicitly loaded for projection-based segmentation.\n2) Mask Ordering for Projection Segmentation.\nWhen projecting masks onto the reconstructed point cloud, the ordering of masks must match the ordering of input images and camera poses.\nFor improved efficiency, one may first downsample the global point cloud before performing projection-based segmentation.\n3) Asset Centering for Consistent Pose Alignment.\nICP alignment uses\n4\nÃ—\n4\n4\\times 4\nhomogeneous matrices, whereas Isaac Sim stores object poses using quaternions.\nTo maintain one-to-one correspondence, each 3D asset must be centered at its geometric centroid; otherwise, rotation pivots differ between ICP and the simulator.\nNote also that Isaac Sim uses the quaternion ordering\n(w,â€‰x,â€‰y,â€‰z)\nrather than\n(x,â€‰y,â€‰z,â€‰w)\n.\n4) Distinguishing\ncamera_optical\nfrom RealSense Physical Extrinsics.\nFrames labeled\ncamera_optical\nfollow the optical-frame convention and are not identical to the physical RealSense camera frames.\nCare must be taken when converting between real-world and simulator coordinate systems.\n5) Point Cloud Tracking and RealSense Reset Behavior.\nOur implementation resolves the issue where restarting the tracking node may cause hardware resource conflicts.\nIf users still encounter RealSense access errors, replugging the device typically resolves the problem.\nWhen storing per-vertex color, ensure correct RGB ordering, as OpenCV may need BGR while Open3D is RGB.\n6) Using GraspGen with SyncTwin.\nGraspGen requires converting point clouds into its JSON-based format before inference.\nWhen deployed via Docker, users must ensure that communication ports are properly exposed for message passing between SyncTwin modules.\n7) NVBlox Filtering Range Adjustment.\nThe default NVBlox depth-integration range may be too small for table-top scenes, potentially causing incomplete or missing geometry.\nIncreasing the truncation or bounding-volume range is recommended for robotic manipulation tasks.\n8) Synchronization Requirements for cuRobo.\ncuRobo planning depends on strict real-to-sim synchronization.\nIf the simulated state is not updated in sync with the real environment, planning may diverge and lead to execution failure.\nMaintaining accurate digital-twin updates is essential for safe and stable planning.",
    "preview_text": "Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.\n\nSyncTwin: Fast Digital Twin Construction\nand Synchronization for Safe Robotic Grasping\nRuopeng Huang\n1,2\nBoyu Yang\n1\nWenlong Gui\n1\nJeremy Morgan\n1\nErdem Biyik\n1\nJiachen Li\n2\n1\nUniversity of Southern California\n2\nUniversity of California, Riverside\nAbstract\nAccurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We introduce SyncTwin, a novel digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. ",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "digital twin",
        "robotic grasping",
        "3D reconstruction",
        "synchronization",
        "point cloud",
        "ICP registration",
        "motion planning"
    ],
    "one_line_summary": "SyncTwinæ˜¯ä¸€ä¸ªæ•°å­—å­ªç”Ÿæ¡†æ¶ï¼Œé€šè¿‡å¿«é€Ÿ3Dåœºæ™¯é‡å»ºå’Œå®æ—¶åŒæ­¥ï¼Œåœ¨åŠ¨æ€å’Œé®æŒ¡ç¯å¢ƒä¸‹å®ç°å®‰å…¨æœºå™¨äººæŠ“å–ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-14T23:03:43Z",
    "created_at": "2026-01-20T17:49:52.489867",
    "updated_at": "2026-01-20T17:49:52.489875"
}