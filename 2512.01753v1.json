{
    "id": "2512.01753v1",
    "title": "AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields",
    "authors": [
        "Zhihao Zhan",
        "Yuhang Ming",
        "Shaobin Li",
        "Jie Yuan"
    ],
    "abstract": "多传感器同步定位与建图（SLAM）对于执行喷洒、测绘与巡检等农业任务的无人机至关重要。然而，目前能够支撑稳健作业研究的真实世界多模态农业无人机数据集仍十分匮乏。为填补这一空白，我们推出了AgriLiRa4D——一个专为复杂户外农业环境设计的多模态无人机数据集。该数据集涵盖平坦、丘陵与梯田三种典型农田类型，同时包含边界巡检与覆盖作业两种飞行模式，共形成六组飞行序列。数据集提供了基于光纤惯性导航系统与实时动态定位技术（FINS_RTK）的高精度真值轨迹，并同步采集了三维激光雷达、四维成像雷达及惯性测量单元（IMU）的多源传感器数据，且附有完整的传感器内参与外参标定文件。凭借其全面的传感器配置与多样化的真实场景，AgriLiRa4D能够支持多种SLAM与定位研究，并可用于系统评估算法在低纹理作物、重复性结构、动态植被等真实农业环境挑战下的鲁棒性。为验证其实用价值，我们在不同传感器组合上对四种前沿多传感器SLAM算法进行了基准测试，结果既凸显了所提序列的挑战性，也印证了多模态融合方案对实现可靠无人机定位的必要性。通过填补农业SLAM数据集的关键空白，AgriLiRa4D为学界提供了重要的研究基准，有助于推动农业无人机自主导航技术的发展。数据集可通过以下链接获取：https://zhan994.github.io/AgriLiRa4D。",
    "url": "https://arxiv.org/abs/2512.01753v1",
    "html_url": "https://arxiv.org/html/2512.01753v1",
    "html_content": "\\corrauth\nYuhang Ming,\nSchool of Computer Science,\nHangzhou Dianzi University,\nHangzhou, 310018, China.\nJie Yuan,\nSchool of Electronic Science and Engineering,\nNanjing University,\nNanjing, 210023, China.\nAgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields\nZhihao Zhan\n1\n1\naffiliationmark:\nYuhang Ming\n2\n2\naffiliationmark:\nShaobin Li\n1\n1\naffiliationmark:\nand Jie Yuan\n3\n3\naffiliationmark:\n1\n1\naffiliationmark:\nTopXGun Robotics, Nanjing, 211100, China\n2\n2\naffiliationmark:\nSchool of Computer Science, Hangzhou Dianzi University, Hangzhou, 310018, China\n3\n3\naffiliationmark:\nSchool of Electronic Science and Engineering, Nanjing University, Nanjing, 210023, China\nyuhang.ming@hdu.edu.cn, yuanjie@nju.edu.cn\nAbstract\nMulti-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection. However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce. To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments. AgriLiRa4D spans three representative farmland types—flat, hilly, and terraced—and includes both boundary and coverage operation modes, resulting in six flight sequence groups. The dataset provides high-accuracy ground-truth trajectories from a Fiber Optic Inertial Navigation System with Real-Time Kinematic capability (FINS_RTK), along with synchronized measurements from a 3D LiDAR, a 4D Radar, and an Inertial Measurement Unit (IMU), accompanied by complete intrinsic and extrinsic calibrations.\nLeveraging its comprehensive sensor suite and diverse real-world scenarios, AgriLiRa4D supports diverse SLAM and localization studies and enables rigorous robustness evaluation against low-texture crops, repetitive patterns, dynamic vegetation, and other challenges of real agricultural environments. To further demonstrate its utility, we benchmark four state-of-the-art multi-sensor SLAM algorithms across different sensor combinations, highlighting the difficulty of the proposed sequences and the necessity of multi-modal approaches for reliable UAV localization.\nBy filling a critical gap in agricultural SLAM datasets, AgriLiRa4D provides a valuable benchmark for the research community and contributes to advancing autonomous navigation technologies for agricultural UAVs.\nThe dataset can be downloaded from:\nhttps://zhan994.github.io/AgriLiRa4D\n.\nkeywords:\nDataset, Aerial Robots, Multi-Sensor Fusion, Agricultural Fields, Simultaneous Localization and Mapping, 4D Radar, LiDAR, Inertial Measurement Unit.\n1\nIntroduction\nThe rapid advancement of agricultural technology has positioned Unmanned Aerial Vehicles (UAVs) as essential platforms for precision farming tasks such as crop monitoring, pesticide spraying, and large-scale farmland surveying\nRadoglou-Grammatikis et al. (\n2020\n); Cheng et al. (\n2023\n); He et al. (\n2025\n); Wang et al. (\n2025b\n)\n.\nTo operate safely and efficiently in these complex outdoor environments, agricultural UAVs require robust and accurate localization as a fundamental capability for autonomous navigation and task execution.\nAlthough Global Navigation Satellite System (GNSS) positioning is widely used in agricultural applications, its performance often degrades severely due to vegetation occlusion, atmospheric disturbances, and multipath effects in structured farmlands\nPini et al. (\n2020\n)\n. As a result, GNSS alone is insufficient for reliable localization, particularly when UAVs operate near crop canopies or close to the terrain. These limitations motivate the adoption of Simultaneous Localization And Mapping (SLAM)-based approaches, which can provide continuous pose estimation when GNSS becomes unreliable.\nHowever, conventional visual SLAM systems face their own challenges in agricultural environments. Dynamic illumination, repetitive and texture-sparse crop patterns, and low-light or low-temperature operating conditions—commonly encountered during pesticide spraying—reduce the availability of stable visual features. In addition, high-speed UAV flight induces motion blur, while strong downwash airflow from heavy payloads causes vegetation motion, further destabilizing feature tracking and leading to frequent failures\nCadena et al. (\n2017\n); Mur-Artal and Tardós (\n2017\n)\n.\nThese factors collectively limit the robustness of pure visual SLAM, underscoring the need for multi-sensor SLAM systems that integrate LiDAR, Radar, inertial sensing, and other modalities to achieve reliable localization in challenging agricultural settings.\nRecent advances in multi-modal sensor fusion have demonstrated promising potential for enhancing SLAM robustness in challenging environments\nZuo et al. (\n2019\n); Shan et al. (\n2020b\n); Zheng et al. (\n2025\n)\n. Light Detection and Ranging (LiDAR) sensors provide accurate and illumination-invariant geometric measurements, while emerging 4D Radar technology offers complementary advantages including weather robustness and direct velocity estimation\nZhang et al. (\n2023\n); Zhuang et al. (\n2023\n)\n. Fusing these ranging modalities with Inertial Measurement Units (IMUs) therefore offers a promising pathway to overcoming the inherent limitations of GNSS- and vision-based systems in agricultural UAV applications.\nDespite the theoretical advantages of multi-modal sensing, the development and evaluation of robust agricultural UAV SLAM systems remain constrained by the lack of specialized datasets that comprehensively capture the operational diversity of agricultural missions. Existing datasets predominantly focus on urban or indoor scenarios\nBurri et al. (\n2016\n); Majdik et al. (\n2017\n); Nguyen et al. (\n2022\n)\n, and only a few extending to semi-natural settings such as islands or rural towns\nLi et al. (\n2024\n)\n. However, these semi-structured environments still differ substantially from real farmland, where unstructured terrain, heterogeneous vegetation, and large-scale outdoor operations introduce fundamentally different sensing and localization challenges. Agricultural UAVs must operate across a wide range of tasks, from low-altitude crop inspection to high-speed field mapping, each characterized by distinct motion dynamics and complex environmental interactions that critically affect SLAM robustness. These limitations highlight the need for a dedicated agricultural UAV dataset that systematically represents the sensing, motion, and environmental diversity inherent to real-world farming operations.\nTo address this critical gap, this paper introduces a novel multi-modal dataset specifically developed for agricultural UAV SLAM research, incorporating LiDAR, 4D Radar, and IMU measurements collected using an agricultural UAV platform. Our dataset features high-precision ground truth trajectories obtained from a Fiber Optic Inertial Navigation System (FINS) module with a built-in Real-Time Kinematic (RTK) receiver (hereafter denoted as FINS_RTK), ensuring centimeter-level position accuracy and high-fidelity orientation references. This work aims to advance the development of robust SLAM systems tailored for agricultural autonomous operations. Our contributions are summarized as follows:\n1.\nAgriLiRa4D Dataset:\nWe introduce a large-scale agricultural UAV dataset covering diverse terrain types, motion dynamics, and flight altitudes. The dataset provides synchronized LiDAR, 4D Radar, and IMU measurements, complemented by centimeter-level position and high-precision orientation ground truth from a FINS_RTK system.\n2.\nSLAM Benchmark:\nWe perform comprehensive evaluations of representative multi-sensor fusion SLAM algorithms (LiDAR–Inertial, Radar–Inertial, and Radar–LiDAR–Inertial) under varied agricultural conditions, providing quantitative insights into their localization accuracy, robustness, and adaptability to complex outdoor environments.\n3.\nMulti-Modal Fusion Analysis:\nWe analyze multi-sensor fusion strategies in challenging agricultural scenarios, highlighting how Radar-LiDAR–IMU integration enhances pose estimation consistency and robustness across different crop types, terrain slopes, and UAV flight regimes.\nThe remainder of this paper is organized as follows. Section\n2\nprovides a detailed comparison between our dataset and existing benchmarks. Section\n3\ndescribes the UAV platform and sensing configuration used for data collection. Section\n4\npresents the characteristics of the dataset, highlighting the diverse agricultural scenarios and motion patterns. Section\n5\nreports experimental evaluations and analysis of three categories of state-of-the-art SLAM methods. Finally, Section\n6\nconcludes the paper and outlines potential directions for future work.\n2\nRelated Work\nTable 1\n:\nComparison of representative datasets grouped by platform type.\nSensors are abridged; limitations summarize typical constraints reported in the original papers.\nPlatform\nDataset (Venue)\nSensors\nGround Truth\nEnvironment\nPrimary Task\nMulti-Sensor UAV-included Datasets\nAriel\nEuRoC (IJRR)\nBurri et al. (\n2016\n)\nStereo (gray); IMU\nLaser; MoCap\nIndoor lab\nVisual-Inertial SLAM\nAriel\nZurich Urban MAV (IJRR)\nMajdik et al. (\n2017\n)\nMono (RGB); IMU\nPhotogrammetric reconstruction\nUrban\nSLAM; Reconstruction\nAriel\nUPenn Fast Flight (RAL)\nSun et al. (\n2018\n)\nStereo (gray); IMU\nGPS\nWarehouse; Woodland\nVisual-Inertial SLAM\nAriel\nUZH-FPV (ICRA)\nDelmerico et al. (\n2019\n)\nEvent; Mono (RGB); Stereo (gray); IMU\nLaser\nHanger; Grassland\nDrone racing\nAriel\nNTU-VIRAL (IJRR)\nNguyen et al. (\n2022\n)\nStereo (gray); LiDAR; IMU; UWB\nLaser\nBuildings\nMulti-sensor SLAM\nAriel&Ground\nGRACO (RAL)\nZhu et al. (\n2023\n)\nStereo (gray); LiDAR; IMU\nINS; GPS\nCampus\nCollaborative SLAM\nAriel\nMARS-LVIG (IJRR)\nLi et al. (\n2024\n)\nMono (RGB); LiDAR; IMU\nRTK; DJI L1\nIsland; Town; Valley; Airfield\nMulti-sensor SLAM\nAriel\nD\n2\nD^{2}\nSLAM (TRO)\nXu et al. (\n2024\n)\nQuad fisheye (RGB); RGB-D; IMU\nMoCap\nOffice\nCollaborative VI-SLAM\nAriel\nFIReStereo (RAL)\nDhrafani et al. (\n2025\n)\nThermal stereo; LiDAR; IMU\nSLAM algorithm\nWoodland; Parking Lot\nDepth; SLAM; Thermal perception\nAgricultural Datasets\nGround\nSugar Beets (IJRR)\nChebrolu et al. (\n2017\n)\nMulti-spectral; RGB-D; LiDAR; GNSS\nManually labelled\nSugar-beet Fields\nClassification; Mapping\nGround\nRosario (IJRR)\nPire et al. (\n2019\n)\nStereo (RGB); IMU; Wheel Odom.\nRTK\nSoybean Fields\nMulti-sensor SLAM\nGround\nMacadamia Orchard (AuRo)\nIslam et al. (\n2023\n)\nStereo (RGB)\n-\nOrchard\nVisual SLAM\nGround\nCitrusFarm (ISVC)\nTeng et al. (\n2023\n)\nMono (gray); Stereo (RGB); Thermal; NIR; IMU; LiDAR\nRTK\nOrchard\nCrop monitoring\nGround\nUnder-Canopy (IJRR)\nCuaran et al. (\n2024\n)\nStereo (RGB); IMU; Wheel Odom.\nRTK\nCorn and Soybean Fields\nMulti-sensor SLAM\nGround\nGREENBOT (Sensors)\nCañadas-Aránega et al. (\n2024\n)\nStereo (RGB); LiDAR; IMU; GNSS\n-\nGreenhouse\nSLAM\nGround\nMOLO-SLAM (Agriculture)\nLv et al. (\n2024\n)\nRGB-D; LiDAR; IMU\n-\nGrape and Tea Plantation\nSemantic SLAM\nAriel\nGrapeSLAM (Data Br.)\nWang et al. (\n2025a\n)\nMono (RGB); IMU\nRTK\nVineyard\nSLAM; SfM\nAriel\nWeedsGalore (WACV)\nCelikkan et al. (\n2025\n)\nMulti-spectral\nMannually labelled\nCotton Fields\nClassification; Detection; Segmentation\nAriel\nAgriLiRa4D\nLiDAR; 4D Radar; IMU\nFINS; RTK\nFlat, Hilly, and Terraced Fields\nMulti-sensor SLAM\nIn this section, we review the existing multi-sensor UAV-included datasets and agricultural datasets, examing their sensor configurations, collection environments, ground-truth acquisition, and typical downstream tasks, to position our proposed dataset in the broader research landscape. All the reviewed datasets are listed in Table\n1\n.\n2.1\nMulti-Sensor UAV-included Datasets\nIn recent years, multi-sensor fusion has become an increasingly popular paradigm for enhancing UAV autonomous navigation and localization. To address the challenges posed by complex and dynamic environments, modern pipelines have evolved from early visual–inertial frameworks to comprehensive multimodal configurations that integrate RGB, depth, stereo, LiDAR, event cameras, and GNSS/UWB signals. In line with this development, the research community has introduced a number of multi-sensor UAV datasets that serve as standard benchmarks for evaluating visual–inertial, LiDAR–visual, and other cross-modal navigation and localization systems.\nDepending on the operating environment, existing multi-sensor UAV datasets can be broadly divided into three categories.\nIndoor or controllable environments\n.\nRepresentative datasets in this category include EuRoC\nBurri et al. (\n2016\n)\n, UPenn Fast Flight\nSun et al. (\n2018\n)\n, and\nD\n2\nD^{2}\nSLAM\nXu et al. (\n2024\n)\n. These datasets are collected in laboratories, offices, and warehouses under controlled motion profiles, stable lighting conditions, and limited environmental disturbances. They have become widely adopted benchmarks for evaluating highly dynamic SLAM and aggressive flight navigation.\nUrban and campus environments\n.\nExamples include Zurich Urban MAV\nMajdik et al. (\n2017\n)\n, NTU-VIRAL\nNguyen et al. (\n2022\n)\n, and GRACO\nZhu et al. (\n2023\n)\n. These datasets capture flights through structured building complexes, university campuses, and road networks, offering rich 3D geometry and frequently incorporating GNSS-based ground truth. They serve as key benchmarks for multimodal SLAM, collaborative perception, and GNSS-assisted navigation in structured human-made environments.\nNatural or semi-natural environments\n.\nRepresentative datasets such as UZH-FPV\nDelmerico et al. (\n2019\n)\n, MARS-LVIG\nLi et al. (\n2024\n)\n, and FIReStereo\nDhrafani et al. (\n2025\n)\nfocus on natural terrains including islands, valleys, grasslands, and forests. These environments introduce challenges such as drastic illumination changes, motion blur, foliage occlusion, texture sparsity, and complex, unstructured geometries, making them valuable for stress-testing perception and navigation systems in outdoor and partially unstructured scenes.\nAlthough the aforementioned datasets have greatly advanced the development of multi-modal SLAM, they predominantly feature environments with clear man-made structures or natural geometric cues, offering stable feature points and semantic anchors for reliable perception and mapping. In contrast, agricultural scenes present fundamentally different and highly degraded conditions: dense and repetitive vegetation, weak textures, frequent occlusion and foliage motion, strong illumination and wind variations, and a lack of stable structural anchors. As a result, current datasets provide limited support for evaluating low-altitude UAV localization and mapping in real precision-agriculture environments.\n2.2\nAgricultural Datasets\nAgricultural datasets have historically emerged from the needs of crop growth monitoring and precision farming, where the primary objectives include plant phenotyping, vegetation segmentation, disease detection, and field-level condition assessment. Only in recent years has the community begun to introduce datasets specifically designed for agricultural SLAM and autonomous navigation. Compared with generic robotics datasets, agricultural datasets are uniquely shaped by crop structure, seasonal variations, and operational constraints, leading to diverse sensing setups and scene characteristics. Existing agricultural datasets can be grouped into four major categories according to platform type, scene structure, and intended task.\nPhenotyping and crop-monitoring datasets\n.\nEarly agricultural datasets, such as Sugar Beets\nChebrolu et al. (\n2017\n)\n, focus on multi-spectral, RGB-D, and LiDAR measurements for crop classification, inter-row localization, and biomass estimation. These datasets provide rich multimodal signals for static analysis and plant-level tasks, but they are not designed for evaluating UAV-based SLAM pipelines.\nGround robotic SLAM datasets in open farmland\n.\nDatasets including Rosario\nPire et al. (\n2019\n)\n, Under-Canopy\nCuaran et al. (\n2024\n)\n, and GREENBOT\nCañadas-Aránega et al. (\n2024\n)\ncapture near-ground navigation in soybean fields, corn rows, and greenhouses using stereo cameras, wheel odometry, IMU, LiDAR, and GNSS. These datasets rely heavily on structured row geometry and near-field texture cues, enabling reliable perception under constrained viewpoints but offering limited diversity in crop types and environmental conditions.\nGround datasets in orchards and plantations\n.\nDatasets such as Macadamia Orchard\nIslam et al. (\n2023\n)\n, CitrusFarm\nTeng et al. (\n2023\n)\n, and MOLO-SLAM\nLv et al. (\n2024\n)\nare collected in orchards and vineyards, where trees or vines form clear vertical structures and repeated spatial patterns. These environments introduce moderate natural variability while still providing stable geometric anchors (tree trunks, trellis systems), which simplify localization compared with open-field row crops.\nLimited UAV-based agricultural datasets\n.\nA small number of agricultural UAV datasets, such as GrapeSLAM\nWang et al. (\n2025a\n)\n, explore aerial views over vineyards but remain limited in crop type, spatial scale, and overall scene diversity. The WeedsGalore dataset\nCelikkan et al. (\n2025\n)\nbroadens sensing with multispectral and multi-temporal imagery, yet the absence of LiDAR, full 6-DoF ground truth, and richer geometric structure still restricts its suitability for benchmarking SLAM or reconstruction algorithms in more demanding agricultural settings.\nDespite their contributions, the aforementioned datasets predominantly focus on ground platforms, constrained viewpoints, and structured or semi-structured agricultural environments. Most datasets involve a single crop species, a single season, or a limited geographic region, with few providing long-term, multi-season, or multi-terrain coverage. Furthermore, existing UAV-based agricultural datasets remain scarce and are mostly collected in orchards, where stable vertical structures simplify perception. As a result, they fall short of capturing the dense foliage, highly repetitive textures, foliage-induced motion, strong illumination fluctuations, and other degraded conditions commonly encountered by UAVs operating at low altitude over open-field crops.\n3\nSystem Overview\n3.1\nSensor Setup\n(a)\nSensor setup on the TopXGun FP300E platform.\n(b)\nRelative positions and coordinate frames of all sensors.\nFigure 1\n:\nSensor configuration on the TopXGun FP300E.\nThe onboard setup integrates a RoboSense Airy LiDAR, a Mindcruise 4D Radar, and a FINS_RTK module for ground-truth reference (top), with the relative sensor positions and coordinate frames illustrated below (bottom).\nA TopXGun FP300E agricultural UAV\n1\n1\n1\nhttps://www.topxgunag.com/topxgun-fp300e-agricultural-drone\ncarries a customized SLAM payload consisting of a 3D LiDAR (with an integrated IMU) and a 4D Radar. The platform is equipped with a FINS_RTK navigation module, providing centimeter-level position accuracy and high-fidelity orientation ground truth. All sensor data are logged by an on-board ARM computer based on the RK3588 processor and running the Robot Operating System (ROS). The sensors interface with the computer over Gigabit Ethernet and are synchronized using the IEEE 1588 Precision Time Protocol (PTP). The overall hardware configuration is shown in Figure\n1(a)\n, and the specifications of each sensor are summarized below.\n(a)\nSide view\n(b)\nOblique view\nFigure 2\n:\nVisualization of the FoV configuration for the LiDAR and 4D Radar.\nThe two viewpoints illustrate their respective sensing coverages, with the LiDAR rendered in\nblue\nand the 4D Radar in\nyellow\n.\n1.\n3D LiDAR.\nWe employ the RoboSense Airy\n2\n2\n2\nhttps://www.robosense.ai/IncrementalComponents/Airy\nas a lightweight and cost-effective 3D LiDAR. As shown in Figure\n2\n, it provides a\n96\n96\n-beam configuration with a\n90\n​\n\\unit\n×\n360\n​\n\\unit\n90\\unit{}\\times 360\\unit{}\nFoV and a maximum range of\n\\qty\n60 (or\n\\qty\n30 at\n\\qty\n10 reflectivity in outdoor conditions). The sensor operates at\n\\qty\n10 and outputs point clouds with timestamps, ring indices, and reflectivity values. Its large vertical FoV is a key reason for selection, as narrower-coverage alternatives such as Velodyne VLP-16 (\n\\qty\n30) or Livox Mid-360 (\n\\qty\n59) may fail to capture sufficient structure in cluttered agricultural environments.\n2.\nIMU.\nThe LiDAR integrates an IIM-42652 IMU\n3\n3\n3\nhttps://invensense.tdk.com/products/smartindustrial/iim-42652/\n, providing angular velocity and linear acceleration at\n\\qty\n200. The gyroscope exhibits a bias instability below\n\\qty\n3.6\n\\per\nand a Temperature Coefficient of Offset (TCO) under\n\\qty\n0.02dps\n\\per\n. The accelerometer offers a TCO of\n\\qty\n0.15\n\\milli\n\\per\nand low spectral noise of\n\\qty\n70\n\\micro\n\\per\n. IMU measurements are transmitted via the same Ethernet interface as the LiDAR and logged synchronously in ROS.\n3.\n4D Radar.\nThe Mindcruise A1 4D Radar is mounted directly beneath the LiDAR to maximize FoV overlap and improve cross-sensor observability, as shown in Figure\n1(a)\nand\n2\n. It offers a\n60\n​\n\\unit\n×\n120\n​\n\\unit\n60\\unit{}\\times 120\\unit{}\nFoV and a detection range of up to\n\\qty\n100. In addition to range and angle, the sensor provides Doppler velocity measurements from\n\\qty\n-35\n\\per\nto\n\\qty\n20\n\\per\n, enabling the direct observation of dynamic targets. Operating at\n\\qty\n10, the Radar delivers reliable performance in dusty or foggy environments due to the longer wavelength of millimeter-wave signals, complementing the LiDAR in challenging agricultural scenarios.\n4.\nFINS_RTK.\nHigh-precision ground-truth poses are provided by the TJ-FINS70D FINS_RTK module, which fuses FINS inertial measurements with a built-in RTK receiver. The system achieves position accuracy better than\n\\qty\n2\n\\centi\n+\n+\n\\qty\n1ppm (50% CEP), and orientation accuracy of\n\\qty\n0.01 (\n\\qty\n1\nσ\n) in roll/pitch and\n\\qty\n0.05 (\n\\qty\n1\nσ\n) in yaw. Operating at\n\\qty\n100, this module supplies a reliable reference for quantitative evaluation of SLAM and odometry performance.\n3.2\nSensor Calibration\nScene\nSide View\nTop-down View\nGround\nHovering\nFigure 3\n:\nVisualization of the LiDAR and 4D Radar point clouds used to assess the extrinsic calibration.\nHeight-colored LiDAR points and white 4D Radar points are visualized in a common frame following extrinsic alignment, with side and top-down views illustrating the spatial consistency across scenarios.\nThe extrinsic parameters among the LiDAR, the 4D Radar, the RTK antenna phase centers, and the FINS_RTK module are derived directly from the UAV’s CAD design files and released together with the dataset (see Figure\n1(b)\n). Because the internal IMU is rigidly integrated within the LiDAR unit, its relative transform is factory-calibrated and remains constant during data collection.\nAccurate LiDAR–Radar extrinsic calibration is crucial for multi-sensor fusion and consistent cross-modal point cloud alignment. We formulate this calibration as a 3D–3D registration problem, aligning the coordinate frames of both sensors within a unified reference. The CAD-derived translation and rotation serve as initial priors, which are subsequently refined through a manual calibration procedure\nYan et al. (\n2022\n)\nusing multiple corner reflectors placed in the environment. This refinement significantly improves alignment precision compared to the raw CAD configuration, providing a reliable geometric basis for downstream multi-sensor SLAM.\nOnce calibration is completed, LiDAR and 4D Radar point clouds are transformed into the shared coordinate frame. Figure\n3\nshows the resulting alignment, demonstrating high spatial consistency across modalities. To further verify calibration accuracy, we visualize two representative scenarios—a static ground sequence and a UAV hovering case—displaying only the overlapping regions of the point clouds for clarity.\n3.3\nData Format\nTable 2\n:\nOverview of the sensors employed in the dataset.\nEach sensor is presented with its associated ROS topics and key specifications, providing a comprehensive reference for data acquisition and integration.\nSensor\nModule\nTopic Name\nMessage Type\nRate (Hz)\nLiDAR\nRobosense Airy\n/rslidar_points\nsensor_msgs/PointCloud2\n10\nIMU\nBuilt-in (LiDAR)\n/rslidar_imu_data\nsensor_msgs/Imu\n200\n4D Radar\nMindcruise A1\n/radar_points\nsensor_msgs/PointCloud2\n10\nFINS_RTK\nTJ-FINS70D\n/aircraft_pose_enu\ngeometry_msgs/PoseStamped\n100\n/aircraft_pose_flu\ngeometry_msgs/PoseStamped\n100\n/aircraft_position_llh\nsensor_msgs/NavSatFix\n100\nAll sensor data are stored in ROS bag files, a widely adopted standard for synchronized multi-sensor logging and seamless data playback in robotics. The corresponding ROS topics, message types, and update rates are summarized in Table\n2\n.\nThe LiDAR provides 3D point clouds containing\n(\nx\n,\ny\n,\nz\n)\n(x,y,z)\ncoordinates with per-point attributes including intensity, ring index, and precise timestamps. The IMU supplies raw inertial measurements consisting of linear acceleration and angular velocity. The 4D Radar generates point clouds enriched with range–angle information, Doppler velocity, Signal-to-Noise Ratio (SNR), and Radar Cross-Section (RCS) values, enabling both geometric and motion-related perception.\nFigure 4\n:\nGround-truth reference frames used in this work.\nThe UAV body frame (FRD) and the two frames relative to the take-off point, FLU and ENU, are shown for defining consistent trajectory coordinates.\nTo obtain high-precision and easily usable ground-truth trajectories, the FINS_RTK outputs are processed into three reference forms:\n(1) a pose reference expressed in the East–North–Up (ENU) frame relative to the take-off location;\n(2) a pose reference expressed in the Forward–Left–Up (FLU) body frame, also anchored at the take-off point; and\n(3) a geodetic reference containing latitude, longitude, and altitude in the global coordinate system.\nOrientation measurements follow the Front–Right–Down (FRD) aerospace convention. The relationships among these coordinate frames are visualized in Figure\n4\n. These unified ground-truth representations provide a consistent and accurate spatial foundation for benchmarking multi-sensor SLAM algorithms.\n4\nDataset Characteristics\nTable 3\n:\nDetailed configurations of flight paths and ROS bags for all sequences.\nThe ROS bags are organized into six sequence groups according to terrain category and scanning mode.\nScene\nSequence\nScanning Mode\nAltitude (m)\nSpeed (m/s)\nPath Length (m)\nFlat farmland\nNJFlatB01\nboundary\n5\n3\n434.77\nNJFlatB02\nboundary\n5\n8\n464.21\nNJFlatB03\nboundary\n10\n3\n456.32\nNJFlatB04\nboundary\n10\n8\n462.18\nNJFlatB05\nboundary\n15\n3\n465.89\nNJFlatB06\nboundary\n15\n8\n454.21\nNJFlatC01\ncoverage\n5\n8\n805.65\nNJFlatC02\ncoverage\n10\n3\n801.17\nNJFlatC03\ncoverage\n10\n8\n798.96\nNJFlatC04\ncoverage\n15\n3\n822.23\nHilly farmland\nNJHillB01\nboundary\n8\n3\n490.61\nNJHillB02\nboundary\n8\n8\n493.07\nNJHillB03\nboundary\n13\n3\n480.98\nNJHillB04\nboundary\n13\n8\n484.60\nNJHillB05\nboundary\n18\n3\n483.84\nNJHillB06\nboundary\n18\n8\n488.41\nNJHillC01\ncoverage\n8\n3\n776.47\nNJHillC02\ncoverage\n8\n8\n783.31\nNJHillC03\ncoverage\n13\n3\n761.55\nNJHillC04\ncoverage\n13\n8\n768.14\nNJHillC05\ncoverage\n18\n3\n756.07\nNJHillC06\ncoverage\n18\n8\n769.94\nTerraced farmland\nNJTerrB01\nboundary\n3\n3\n204.91\nNJTerrB02\nboundary\n6\n3\n207.21\nNJTerrB03\nboundary\n6\n6\n209.71\nNJTerrB04\nboundary\n9\n3\n211.95\nNJTerrB05\nboundary\n9\n6\n215.72\nNJTerrC01\ncoverage\n3\n3\n311.23\nNJTerrC02\ncoverage\n3\n6\n307.53\nNJTerrC03\ncoverage\n6\n3\n311.24\nNJTerrC04\ncoverage\n6\n6\n300.84\nNJTerrC05\ncoverage\n9\n3\n313.64\nNJTerrC06\ncoverage\n9\n6\n317.48\nFlat\nHilly\nTerraced\nScene\nScanning Path\nLiDAR\n4D Radar\nFigure 5\n:\nVisualization of the three representative farmland scenarios and their corresponding sensor data.\nEach column corresponds to a distinct terrain type: flat farmland, hilly farmland, and terraced farmland. From top to bottom, subfigures illustrate the real-world operation scenes, boundary (\nblue\n) and coverage (\nred\n) scanning paths, and the top-view height-colored LiDAR (Faster-LIO\nBai et al. (\n2022\n)\n) and 4D Radar (GaRLIO\nNoh et al. (\n2025\n)\n) maps, with zoomed-in regions highlighting local geometric details.\nTo ensure broad scenario diversity, data were collected across three representative farmland terrains—flat plains, hilly regions, and mountainous terraces—located in Nanjing, China. The dataset is organized into six sequence groups based on terrain type and scanning mode (\nboundary\nor\ncoverage\n), namely\nNJFlatB\n,\nNJFlatC\n,\nNJHillB\n,\nNJHillC\n,\nNJTerrB\n, and\nNJTerrC\n.\nFor all sequences except\nNJTerrB\nand\nNJTerrC\n, the UAV flew at a constant altitude with respect to the take-off point. In contrast, for the mountainous-terrain sequences\nNJTerrB\nand\nNJTerrC\n, the UAV maintained a fixed height Above Ground Level (AGL) to ensure flight safety and stable sensor coverage over rapidly varying elevation. Multiple combinations of flight altitudes and speeds were employed to introduce different levels of SLAM difficulty. Each sequence additionally begins with a short stationary or hovering segment to facilitate IMU initialization.\nTable\n3\nsummarizes the detailed flight configurations and ROS bags for all sequences. The real-world operational configurations, along with the corresponding UAV waypoint layouts, are illustrated in Figure\n5\n.\n4.1\nFlat Farmland\nThe\nNJFlatB\nand\nNJFlatC\nsequences were collected in flat agricultural fields consisting of nearly mature sorghum awaiting harvest (31.8921°N, 118.8548°E). As illustrated in Figure\n5\n, the area spans approximately\n\\qty\n250 by\n\\qty\n350, bordered in part by sparse trees and utility poles. The uniformly grown sorghum and largely featureless terrain yield minimal geometric variation, creating a challenging setting for feature extraction, data association, and stable state estimation. Two scanning modes were used, with flight altitudes relative to the take-off point at\n\\qty\n5,\n\\qty\n10, and\n\\qty\n15, and speeds of\n\\qty\n3\n\\per\nor\n\\qty\n8\n\\per\n, providing sequences with different motion dynamics and viewpoints.\n4.2\nHilly Farmland\nThe\nNJHillB\nand\nNJHillC\nsequences were captured over gently sloped farmland in hilly terrain (31.8348°N, 118.7813°E). The\n\\qty\n180 by\n\\qty\n280 area features moderate slopes of approximately\n\\qty\n15, with lower regions planted with mature sorghum and upper regions covered by short grass. A dense tree line forms one boundary of the site (see Figure\n5\n). The mixture of vegetation types and sloped geometry results in more structural cues than flat farmland, though still with limited distinctive features, offering a moderately challenging SLAM environment. Data were recorded in two scanning modes at constant heights from the take-off point (\n\\qty\n8,\n\\qty\n13,\n\\qty\n18) and at flight speeds of\n\\qty\n3\n\\per\nand\n\\qty\n8\n\\per\n.\n4.3\nTerraced Farmland\nThe\nNJTerrB\nand\nNJTerrC\nsequences were acquired in steep mountainous terrain consisting of terraced farmland primarily used for tea cultivation (31.7737°N, 118.6917°E). The area covers roughly\n\\qty\n100 by\n\\qty\n100, with slopes around\n\\qty\n45. Tea plants are grown along consistent contour lines, forming distinctive terrace patterns rich in geometric cues, as shown in Figure\n5\n. The combination of pronounced elevation changes and structured tea canopies makes this environment relatively favorable for SLAM. For safe operation in the steep terrain, the UAV took off from a road at the base of the terraces and maintained a constant height Above Ground Level (\n\\qty\n3,\n\\qty\n6,\n\\qty\n9) while flying at\n\\qty\n3\n\\per\nor\n\\qty\n6\n\\per\n.\n5\nExperiments\nTable 4\n:\nTranslation (meters) and rotation (degrees) RMSE of the ATE for the evaluated methods on the AgriLiRa4D dataset.\nFor each sequence, results are\nbold\nfor best, and\nunderlined\nfor second best. A dash (“–”) signifies the failure of the algorithm’s execution.\nScene\nSequence\nFAST-LIO2\nFaster-LIO\nEKF-RIO\nGaRLIO\nATE\nr\nATE\nt\nATE\nr\nATE\nt\nATE\nr\nATE\nt\nATE\nr\nATE\nt\nFlat farmland\nNJFlatB01\n6.67\n9.38\n4.33\n3.73\n-\n-\n4.46\n4.08\nNJFlatB02\n5.68\n4.60\n6.12\n3.19\n-\n-\n4.35\n2.75\nNJFlatB03\n4.06\n5.68\n3.20\n3.76\n-\n-\n4.87\n7.21\nNJFlatB04\n3.55\n3.73\n3.79\n4.44\n19.51\n29.47\n3.68\n2.91\nNJFlatB05\n4.57\n7.04\n2.83\n3.10\n-\n-\n3.02\n6.07\nNJFlatB06\n7.64\n8.69\n4.91\n4.74\n-\n-\n19.82\n44.65\nNJFlatC01\n20.48\n40.60\n6.12\n8.37\n15.93\n31.56\n16.80\n64.36\nNJFlatC02\n7.15\n15.05\n3.05\n4.33\n16.72\n82.66\n5.03\n7.72\nNJFlatC03\n4.92\n7.42\n3.08\n4.54\n22.67\n40.39\n3.18\n4.16\nNJFlatC04\n3.61\n5.57\n2.80\n2.58\n-\n-\n-\n-\nHilly farmland\nNJHillB01\n5.16\n8.05\n3.96\n4.55\n6.70\n11.64\n-\n-\nNJHillB02\n4.48\n4.50\n3.78\n3.54\n43.27\n53.06\n-\n-\nNJHillB03\n5.52\n6.48\n4.48\n4.24\n15.77\n20.33\n-\n-\nNJHillB04\n6.58\n7.85\n3.73\n4.28\n43.59\n56.02\n-\n-\nNJHillB05\n3.60\n46.15\n6.08\n63.22\n20.60\n28.18\n4.43\n5.84\nNJHillB06\n5.30\n3.80\n2.79\n3.10\n14.14\n28.64\n7.01\n25.05\nNJHillC01\n15.37\n47.85\n6.46\n12.06\n-\n-\n-\n-\nNJHillC02\n8.71\n18.97\n-\n-\n-\n-\n7.94\n11.69\nNJHillC03\n6.57\n41.13\n6.44\n20.46\n-\n-\n5.07\n10.12\nNJHillC04\n7.22\n24.60\n3.62\n5.15\n9.62\n23.18\n4.22\n4.36\nNJHillC05\n4.52\n26.57\n-\n-\n-\n-\n-\n-\nNJHillC06\n6.29\n20.08\n3.63\n26.15\n42.98\n64.17\n-\n-\nTerraced farmland\nNJTerrB01\n2.67\n0.70\n2.69\n0.67\n5.42\n5.03\n3.06\n1.67\nNJTerrB02\n2.61\n0.51\n2.64\n0.61\n17.59\n9.33\n21.54\n15.41\nNJTerrB03\n2.22\n0.51\n2.55\n0.50\n32.92\n15.91\n21.18\n17.62\nNJTerrB04\n2.63\n0.74\n2.77\n0.73\n3.43\n4.03\n38.13\n17.82\nNJTerrB05\n6.60\n3.60\n3.70\n1.50\n-\n-\n20.56\n14.50\nNJTerrC01\n3.02\n0.93\n3.07\n0.92\n-\n-\n8.35\n5.12\nNJTerrC02\n4.39\n2.27\n2.79\n0.76\n21.93\n13.49\n12.98\n7.66\nNJTerrC03\n2.63\n1.14\n3.56\n2.47\n12.20\n7.24\n7.16\n11.54\nNJTerrC04\n2.67\n1.21\n2.66\n1.66\n-\n-\n8.14\n10.46\nNJTerrC05\n2.32\n1.02\n2.22\n0.62\n6.14\n6.72\n2.27\n1.05\nNJTerrC06\n2.70\n1.89\n25.21\n21.58\n-\n-\n4.05\n2.82\n5.1\nBenchmark Methods and Evaluation\nTo thoroughly assess the proposed AgriLiRa4D dataset, we benchmark representative state-of-the-art multi-sensor SLAM algorithms covering three major sensing modalities: LiDAR–Inertial Odometry (LIO), Radar–Inertial Odometry (RIO), and Radar–LiDAR–Inertial Odometry (RLIO).\nLIO approaches such as FAST-LIO2\nXu et al. (\n2022\n)\n, Faster-LIO\nBai et al. (\n2022\n)\n, LIO-SAM\nShan et al. (\n2020a\n)\n, and LiLi-OM\nLi et al. (\n2021\n)\nfuse dense LiDAR scans with IMU integration and are widely adopted for precise odometry in structured and semi-structured environments.\nRIO methods, including EKF-RIO\nDoer and Trommer (\n2020\n)\n, DRIO\nChen et al. (\n2023\n)\n, and 4D-IRIOM\nZhuang et al. (\n2023\n)\n, leverage Radar’s robustness against visual and geometric degradation, while Doppler measurements provide additional motion cues when LiDAR becomes unreliable.\nRLIO frameworks such as DR-LRIO\nNissov et al. (\n2024\n)\n, AF-RLIO\nQian et al. (\n2025\n)\n, and GaRLIO\nNoh et al. (\n2025\n)\nfurther integrate the complementary characteristics of all three modalities to achieve enhanced consistency and robustness under challenging conditions.\nConsidering practical constraints such as open-source availability, implementation stability, and compatibility with our GNSS-free and vision-free evaluation protocol, we selected four algorithms for benchmarking: FAST-LIO2 and Faster-LIO for LIO, EKF-RIO for RIO, and GaRLIO for RLIO. Other representative methods discussed above were not included because many lack official open-source implementations, rely on sensing modalities that are not available in our dataset (such as visual, depth, or GNSS measurements), or require hardware interfaces and engineering adaptations that hinder reproducible large-scale evaluation. Overall, the selected four algorithms provide representative and reliable baselines that cover the LIO, RIO, and RLIO modality spectrum for benchmarking on AgriLiRa4D.\nPerformance is evaluated using the Root Mean Square Error (RMSE) of the Absolute Trajectory Error (ATE), computed after aligning each estimated trajectory with the high-precision FINS_RTK ground truth. All evaluations are performed using the evo toolkit\nGrupp (\n2017\n)\nto ensure consistency and reproducibility, with translational and rotational errors reported in meters and degrees, respectively.\nThe quantitative results across different farmland scenarios and flight configurations are summarized in Table\n4\n, providing a comprehensive comparison of the selected algorithms under varied environmental and operational conditions. A run is considered a failure if the translational ATE relative to the path length (Table\n3\n) exceeds 15%, or if the rotational ATE exceeds\n\\qty\n45.\n5.2\nOverall Benchmark Performance\nAcross all evaluated sequences, the LIO methods deliver the most stable and consistently accurate performance on the AgriLiRa4D dataset. Both FAST-LIO2 and Faster-LIO achieve the lowest ATE in the majority of sequences and maintain reliable operation across all farmland types, with only moderate degradation under higher altitudes or faster flight speeds. In contrast, the RIO baseline, EKF-RIO, exhibits the least stable behavior, failing on nearly all flat farmland sequences and showing large drift even in those hilly and terraced sequences where it successfully completes a run. The RLIO method GaRLIO demonstrates mixed performance: it frequently fails in hilly farmland, operates inconsistently in flat terrain, but achieves its most stable and competitive results in terraced farmland, where it occasionally matches or slightly outperforms the LIO baselines in rotation accuracy.\nOverall, the benchmark outcomes highlight a wide difficulty spectrum across sensing modalities and farmland types, demonstrating that AgriLiRa4D offers a comprehensive and discriminative testbed for evaluating multi-sensor SLAM robustness in real agricultural environments.\n5.3\nImpact of Sensing Modality\nThe performance differences observed among LIO, RIO, and RLIO methods can be largely attributed to the sensing characteristics and the original design intentions of each modality. LIO approaches, represented by FAST-LIO2 and Faster-LIO, inherently benefit from dense geometric constraints and tight LiDAR–IMU coupling. Both methods were originally validated on UAV platforms—including indoor and outdoor flight scenarios—making them naturally suited for the fast motion, wide-area scanning, and geometry-rich conditions typical of agricultural UAV operation. This strong geometric anchoring explains why LIO methods remain the most stable across all terrain types in our dataset.\nIn contrast, the RIO baseline EKF-RIO shows limited robustness in outdoor agricultural environments. Although EKF-RIO was originally evaluated on UAVs, its validation was restricted to indoor settings, where Radar returns are much denser and more structured. In large open agricultural fields, 4D Radar often produces sparse and noisy returns, especially over flat crops or grass surfaces, resulting in insufficient geometric constraints for stable odometry. Moreover, Radar measurements are highly sensitive to dynamic vegetation induced by UAV downwash, producing fluctuating point responses that challenge consistent data association. While Doppler velocity provides valuable motion cues, these cues become unreliable when the underlying spatial structure is weak, contributing to the significant drift and frequent failures observed in our evaluation.\nThe RLIO method GaRLIO exhibits mixed performance due to its sensing design and domain mismatch. GaRLIO was originally developed and tested primarily on Unmanned Ground Vehicle (UGV) platforms with near-ground viewpoints, balanced LiDAR–Radar overlap, and slower, more stable motion profiles. When applied to UAV flights, several mismatches arise: LiDAR–Radar common visibility decreases with altitude, Doppler signatures change under fast aerial motion, and Radar becomes more sensitive to vegetation-driven perturbations. Consequently, GaRLIO occasionally outperforms LIO in hilly farmland as shown in Figure\n6\nand Figure\n7\n, where the sloped terrain provides stronger and more distinguishable Radar returns that compensate for its reduced LiDAR support.\nAcross all modalities, a common trend is that positional drift predominantly accumulates along the vertical (Z) axis, as illustrated in Figure\n7\nfor the\nNJHillC04\nsequence. This behavior is consistent with the sensing geometry: airborne LiDAR primarily observes the environment from near-horizontal viewpoints, providing weaker constraints on altitude compared to horizontal motion, while 4D Radar exhibits coarser elevation resolution. As a result, Z-axis estimation relies more heavily on IMU integration, making it particularly susceptible to drift when geometric structure or Radar elevation cues are insufficient.\nFigure 6\n:\nTop-down view comparison of trajectories for the\nNJHillC04\nsequence.\nThe FINS_RTK ground truth trajectory is presented as the benchmark reference, together with four algorithm-estimated trajectories for performance comparison.\nFigure 7\n:\nPosition errors for the\nNJHillC04\nsequence.\nThe errors of four algorithms relative to the FINS_RTK ground truth are decomposed and displayed along the X, Y, and Z axes, respectively.\n5.4\nEffect of Terrain, Vegetation and Altitude\nThe varying performance across different farmland types in AgriLiRa4D can be explained by the combined effects of terrain geometry, vegetation characteristics, and flight parameters such as altitude and speed. As shown in Figure\n5\n, terraced farmland consistently yields the most favorable results for all methods due to its strong and repetitive geometric contours. The steep slopes and layered tea canopies create stable depth gradients that offer abundant geometric constraints for LiDAR-based matching and informative Doppler patterns for Radar processing. These structural advantages allow even the weaker sensing modalities, such as RIO, to complete trajectories reliably and also lead to GaRLIO’s best overall performance in this terrain category.\nIn contrast, flat farmland represents the most challenging scenario in the dataset. The nearly uniform sorghum fields provide minimal 3D geometric variation, resulting in sparse LiDAR features and weak Radar returns, illustrated in Figure\n5\n. The highly repetitive crop-row patterns further complicate data association, magnifying drift for all SLAM systems. Dynamic vegetation motion induced by UAV downwash contributes additional instability: sorghum stalks sway noticeably at the evaluated flight speeds of\n\\qty\n3\n\\per\nand\n\\qty\n8\n\\per\n, generating fluctuating LiDAR edges and volatile Radar reflections. These factors collectively explain the significant performance degradation observed in flat farmland, particularly in the high-speed (\nNJFlatB02/B04/B06\n) and long-path coverage sequences (\nNJFlatC01-C04\n), where the longest trajectories exceed\n\\qty\n800.\nHilly farmland presents intermediate difficulty. Although the terrain contains noticeable elevation changes, the local surface patches observed from typical UAV altitudes remain nearly planar for LiDAR, offering limited improvement for LIO compared with flat fields. In contrast, Radar is more sensitive to terrain gradients, primarily due to its substantially longer sensing range. This extended visibility enables RLIO to occasionally benefit from the sloped ground geometry. At the same time, grassy and shrub-covered surfaces in hilly regions remain vulnerable to wind-induced motion, introducing fluctuations in both LiDAR and Radar measurements.\nFlight altitude further modulates the effective sensing quality across all scenes. As altitude increases from\n\\qty\n3 to\n\\qty\n18 (see Table\n3\n), LiDAR beams intersect the ground at progressively shallower angles, reducing point density and diminishing geometric distinctiveness. Radar detections also become sparser and less reliable at higher elevations. These altitude-induced limitations also reduce the spatial overlap between LiDAR and Radar, weakening the cross-modal constraints required by RLIO systems. As a result, sequences flown at higher altitudes—such as\nNJFlatC04\nand\nNJHillC05\n—exhibit higher failure rates and larger trajectory errors across all modalities.\n6\nConclusion and Future Work\nIn this paper, we presented AgriLiRa4D, a multi-modal UAV dataset specifically designed to address the challenges of SLAM and localization in real agricultural environments. The dataset covers three representative farmland types and two flight modes, delivering diverse motion patterns and sensing conditions that are rarely included in existing benchmarks. AgriLiRa4D offers high-precision FINS_RTK ground truth, time-synchronized LiDAR, 4D Radar, and IMU measurements, as well as full calibration data, providing a reliable foundation for developing and evaluating multi-sensor fusion algorithms. Using this dataset, we benchmarked several state-of-the-art multi-sensor SLAM algorithms and highlighted the intrinsic difficulties posed by low-texture crops, repetitive planting structures, uneven terrain, and vegetation dynamics. These results underscore both the difficulty of the sequences and the importance of multi-modal fusion for achieving robust localization in agricultural settings.\nLooking ahead, we plan to extend AgriLiRa4D with additional crop types, seasonal variations, and more extreme operational conditions, such as night flights and adverse weather, to further support research on resilient agricultural autonomy.\nWe believe AgriLiRa4D will serve as a long-term resource for the robotics community, catalyzing progress in robust perception and navigation for agricultural UAVs. The challenges surfaced through this benchmark will guide our future work toward developing more resilient, adaptive, and generalizable SLAM frameworks capable of handling the complex, dynamic, and often ambiguous conditions intrinsic to real-world agricultural environments.\n{acks}\nThe authors would like to take this opportunity to thank RoboSense (Suteng Innovation Technology Co., Ltd.) for providing the Airy sensor, which was used to collect some of the data for our experiment and to conduct validations.\nReferences\nBai et al. (2022)\nBai C, Xiao T, Chen Y, Wang H, Zhang F and Gao X (2022) Faster-lio: Lightweight tightly coupled lidar-inertial odometry using parallel sparse incremental voxels.\nIEEE Robotics and Automation Letters\n7(2): 4861–4868.\n10.1109/LRA.2022.3152830\n.\nBurri et al. (2016)\nBurri M, Nikolic J, Gohl P, Schneider T, Rehder J, Omari S, Achtelik MW and Siegwart R (2016) The euroc micro aerial vehicle datasets.\nThe International Journal of Robotics Research\n35(10): 1157–1163.\n10.1177/0278364915620033\n.\nURL\nhttps://doi.org/10.1177/0278364915620033\n.\nCadena et al. (2017)\nCadena C, Carlone L, Carrillo H, Latif Y, Scaramuzza D, Neira J, Reid I and Leonard JJ (2017) Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age.\nIEEE Transactions on robotics\n32(6): 1309–1332.\nCañadas-Aránega et al. (2024)\nCañadas-Aránega F, Blanco-Claraco JL, Moreno JC and Rodriguez-Diaz F (2024) Multimodal mobile robotic dataset for a typical mediterranean greenhouse: The greenbot dataset.\nSensors\n24(6).\n10.3390/s24061874\n.\nURL\nhttps://www.mdpi.com/1424-8220/24/6/1874\n.\nCelikkan et al. (2025)\nCelikkan E, Kunzmann T, Yeskaliyev Y, Itzerott S, Klein N and Herold M (2025)  WeedsGalore: A Multispectral and Multitemporal UAV-Based Dataset for Crop and Weed Segmentation in Agricultural Maize Fields .\nIn:\n2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n. Los Alamitos, CA, USA: IEEE Computer Society, pp. 4767–4777.\n10.1109/WACV61041.2025.00467\n.\nURL\nhttps://doi.ieeecomputersociety.org/10.1109/WACV61041.2025.00467\n.\nChebrolu et al. (2017)\nChebrolu N, Lottes P, Schaefer A, Winterhalter W, Burgard W and Stachniss C (2017) Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields.\nThe International Journal of Robotics Research\n36(10): 1045–1052.\n10.1177/0278364917720510\n.\nURL\nhttps://doi.org/10.1177/0278364917720510\n.\nChen et al. (2023)\nChen H, Liu Y and Cheng Y (2023) Drio: Robust radar-inertial odometry in dynamic environments.\nIEEE Robotics and Automation Letters\n8(9): 5918–5925.\n10.1109/LRA.2023.3301290\n.\nCheng et al. (2023)\nCheng D, Yao Y, Liu R, Li X, Guan B and Yu F (2023) Precision agriculture management based on a surrogate model assisted multiobjective algorithmic framework.\nScientific Reports\n13(1): 1142.\nCuaran et al. (2024)\nCuaran J, Velasquez AEB, Gasparino MV, Uppalapati NK, Sivakumar AN, Wasserman J, Huzaifa M, Adve S and Chowdhary G (2024) Under-canopy dataset for advancing simultaneous localization and mapping in agricultural robotics.\nThe International Journal of Robotics Research\n43(6): 739–749.\n10.1177/02783649231215372\n.\nURL\nhttps://doi.org/10.1177/02783649231215372\n.\nDelmerico et al. (2019)\nDelmerico J, Cieslewski T, Rebecq H, Faessler M and Scaramuzza D (2019) Are we ready for autonomous drone racing? the uzh-fpv drone racing dataset.\nIn:\n2019 International Conference on Robotics and Automation (ICRA)\n. pp. 6713–6719.\n10.1109/ICRA.2019.8793887\n.\nDhrafani et al. (2025)\nDhrafani D, Liu Y, Jong A, Shin U, He Y, Harp T, Hu Y, Oh J and Scherer S (2025) Firestereo: Forest infrared stereo dataset for uas depth perception in visually degraded environments.\nIEEE Robotics and Automation Letters\n10(4): 3302–3309.\n10.1109/LRA.2025.3536278\n.\nDoer and Trommer (2020)\nDoer C and Trommer GF (2020) Radar inertial odometry with online calibration.\nIn:\n2020 European Navigation Conference (ENC)\n. pp. 1–10.\n10.23919/ENC48637.2020.9317343\n.\nGrupp (2017)\nGrupp M (2017) evo: Python package for the evaluation of odometry and slam.\nhttps://github.com/MichaelGrupp/evo\n.\nHe et al. (2025)\nHe J, Zhan Z, Tu Z, Zhu X and Yuan J (2025) A multi-sensor fusion approach for rapid orthoimage generation in large-scale uav mapping.\nIn:\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n. pp. 6808–6815.\n10.1109/IROS60139.2025.11247677\n.\nIslam et al. (2023)\nIslam R, Habibullah H and Hossain T (2023) Agri-slam: a real-time stereo visual slam for agricultural environment.\nAutonomous Robots\n47(6): 649–668.\n10.1007/s10514-023-10110-y\n.\nURL\nhttps://doi.org/10.1007/s10514-023-10110-y\n.\nLi et al. (2024)\nLi H, Zou Y, Chen N, Lin J, Liu X, Xu W, Zheng C, Li R, He D, Kong F, Cai Y, Liu Z, Zhou S, Xue K and Zhang F (2024) Mars-lvig dataset: A multi-sensor aerial robots slam dataset for lidar-visual-inertial-gnss fusion.\nThe International Journal of Robotics Research\n43(8): 1114–1127.\n10.1177/02783649241227968\n.\nURL\nhttps://doi.org/10.1177/02783649241227968\n.\nLi et al. (2021)\nLi K, Li M and Hanebeck UD (2021) Towards high-performance solid-state-lidar-inertial odometry and mapping.\nIEEE Robotics and Automation Letters\n6(3): 5167–5174.\n10.1109/LRA.2021.3070251\n.\nLv et al. (2024)\nLv J, Yao B, Guo H, Gao C, Wu W, Li J, Sun S and Luo Q (2024) Molo-slam: A semantic slam for accurate removal of dynamic objects in agricultural environments.\nAgriculture\n14(6).\n10.3390/agriculture14060819\n.\nURL\nhttps://www.mdpi.com/2077-0472/14/6/819\n.\nMajdik et al. (2017)\nMajdik AL, Till C and Scaramuzza D (2017) The zurich urban micro aerial vehicle dataset.\nThe International Journal of Robotics Research\n36(3): 269–273.\n10.1177/0278364917702237\n.\nURL\nhttps://doi.org/10.1177/0278364917702237\n.\nMur-Artal and Tardós (2017)\nMur-Artal R and Tardós JD (2017) Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras.\nIEEE transactions on robotics\n33(5): 1255–1262.\nNguyen et al. (2022)\nNguyen TM, Yuan S, Cao M, Lyu Y, Nguyen TH and Xie L (2022) Ntu viral: A visual-inertial-ranging-lidar dataset, from an aerial vehicle viewpoint.\nThe International Journal of Robotics Research\n41(3): 270–280.\n10.1177/02783649211052312\n.\nURL\nhttps://doi.org/10.1177/02783649211052312\n.\nNissov et al. (2024)\nNissov M, Khedekar N and Alexis K (2024) Degradation resilient lidar-radar-inertial odometry.\nIn:\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n. pp. 8587–8594.\n10.1109/ICRA57147.2024.10611444\n.\nNoh et al. (2025)\nNoh C, Yang W, Jung M, Jung S and Kim A (2025) Garlio: Gravity enhanced radar-lidar-inertial odometry.\nURL\nhttps://arxiv.org/abs/2502.07703\n.\nPini et al. (2020)\nPini M, Marucco G, Falco G, Nicola M and De Wilde W (2020) Experimental testbed and methodology for the assessment of rtk gnss receivers used in precision agriculture.\nIEEE access\n8: 14690–14703.\nPire et al. (2019)\nPire T, Mujica M, Civera J and Kofman E (2019) The rosario dataset: Multisensor data for localization and mapping in agricultural environments.\nThe International Journal of Robotics Research\n38(6): 633–641.\n10.1177/0278364919841437\n.\nURL\nhttps://doi.org/10.1177/0278364919841437\n.\nQian et al. (2025)\nQian C, Xu Y, Shi X, Chen J and Li L (2025) Af-rlio: Adaptive fusion of radar-lidar-inertial information for robust odometry in challenging environments.\nIn:\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n. pp. 1–7.\n10.1109/ICRA55743.2025.11128046\n.\nRadoglou-Grammatikis et al. (2020)\nRadoglou-Grammatikis P, Sarigiannidis P, Lagkas T and Moscholios I (2020) A compilation of uav applications for precision agriculture.\nComputer Networks\n172: 107148.\nShan et al. (2020a)\nShan T, Englot B, Meyers D, Wang W, Ratti C and Daniela R (2020a) Lio-sam: Tightly-coupled lidar inertial odometry via smoothing and mapping.\nIn:\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n. IEEE, pp. 5135–5142.\nShan et al. (2020b)\nShan T, Englot B, Meyers D, Wang W, Ratti C and Rus D (2020b) Lio-sam: Tightly-coupled lidar inertial odometry via smoothing and mapping.\nIn:\n2020 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n. IEEE, pp. 5135–5142.\nSun et al. (2018)\nSun K, Mohta K, Pfrommer B, Watterson M, Liu S, Mulgaonkar Y, Taylor CJ and Kumar V (2018) Robust stereo visual inertial odometry for fast autonomous flight.\nIEEE Robotics and Automation Letters\n3(2): 965–972.\n10.1109/LRA.2018.2793349\n.\nTeng et al. (2023)\nTeng H, Wang Y, Song X and Karydis K (2023) Multimodal dataset for localization, mapping and crop monitoring in citrus tree farms.\nIn: Bebis G, Ghiasi G, Fang Y, Sharf A, Dong Y, Weaver C, Leo Z, LaViola Jr JJ and Kohli L (eds.)\nAdvances in Visual Computing\n. Cham: Springer Nature Switzerland.\nISBN 978-3-031-47969-4, pp. 571–582.\nWang et al. (2025a)\nWang K, Vélez S, Kooistra L, Wang W and Valente J (2025a) GrapeSLAM: UAV-based monocular visual dataset for SLAM, SfM and 3d reconstruction with trajectories under challenging illumination conditions 60: 111495.\nhttps://doi.org/10.1016/j.dib.2025.111495\n.\nURL\nhttps://www.sciencedirect.com/science/article/pii/S2352340925002276\n.\nWang et al. (2025b)\nWang Q, Zhan Z, He J, Tu Z, Zhu X and Yuan J (2025b) High-quality spatial reconstruction and orthoimage generation using efficient 2d gaussian splatting.\nURL\nhttps://arxiv.org/abs/2503.19703\n.\nXu et al. (2024)\nXu H, Liu P, Chen X and Shen S (2024)\nd\n2\nd^{2}\nslam: Decentralized and distributed collaborative visual-inertial slam system for aerial swarm.\nIEEE Transactions on Robotics\n40: 3445–3464.\n10.1109/TRO.2024.3422003\n.\nXu et al. (2022)\nXu W, Cai Y, He D, Lin J and Zhang F (2022) Fast-lio2: Fast direct lidar-inertial odometry.\nIEEE Transactions on Robotics\n38(4): 2053–2073.\n10.1109/TRO.2022.3141876\n.\nYan et al. (2022)\nYan G, Liu Z, Wang C, Shi C, Wei P, Cai X, Ma T, Liu Z, Zhong Z, Liu Y, Zhao M, Ma Z and Li Y (2022) Opencalib: A multi-sensor calibration toolbox for autonomous driving.\narXiv preprint arXiv:2205.14087\n.\nZhang et al. (2023)\nZhang J, Zhuge H, Wu Z, Peng G, Wen M, Liu Y and Wang D (2023) 4dradarslam: A 4d imaging radar slam system for large-scale environments based on pose graph optimization.\nIn:\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n. IEEE, pp. 8333–8340.\nZheng et al. (2025)\nZheng C, Xu W, Zou Z, Hua T, Yuan C, He D, Zhou B, Liu Z, Lin J, Zhu F, Ren Y, Wang R, Meng F and Zhang F (2025) Fast-livo2: Fast, direct lidar–inertial–visual odometry.\nIEEE Transactions on Robotics\n41: 326–346.\n10.1109/TRO.2024.3502198\n.\nZhu et al. (2023)\nZhu Y, Kong Y, Jie Y, Xu S and Cheng H (2023) Graco: A multimodal dataset for ground and aerial cooperative localization and mapping.\nIEEE Robotics and Automation Letters\n8(2): 966–973.\n10.1109/LRA.2023.3234802\n.\nZhuang et al. (2023)\nZhuang Y, Wang B, Huai J and Li M (2023) 4d iriom: 4d imaging radar inertial odometry and mapping.\nIEEE Robotics and Automation Letters\n8(6): 3246–3253.\nZuo et al. (2019)\nZuo X, Geneva P, Lee W, Liu Y and Huang G (2019) Lic-fusion: Lidar-inertial-camera odometry.\nIn:\n2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n. IEEE, pp. 5848–5854.",
    "preview_text": "Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection. However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce. To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments. AgriLiRa4D spans three representative farmland types-flat, hilly, and terraced-and includes both boundary and coverage operation modes, resulting in six flight sequence groups. The dataset provides high-accuracy ground-truth trajectories from a Fiber Optic Inertial Navigation System with Real-Time Kinematic capability (FINS_RTK), along with synchronized measurements from a 3D LiDAR, a 4D Radar, and an Inertial Measurement Unit (IMU), accompanied by complete intrinsic and extrinsic calibrations. Leveraging its comprehensive sensor suite and diverse real-world scenarios, AgriLiRa4D supports diverse SLAM and localization studies and enables rigorous robustness evaluation against low-texture crops, repetitive patterns, dynamic vegetation, and other challenges of real agricultural environments. To further demonstrate its utility, we benchmark four state-of-the-art multi-sensor SLAM algorithms across different sensor combinations, highlighting the difficulty of the proposed sequences and the necessity of multi-modal approaches for reliable UAV localization. By filling a critical gap in agricultural SLAM datasets, AgriLiRa4D provides a valuable benchmark for the research community and contributes to advancing autonomous navigation technologies for agricultural UAVs. The dataset can be downloaded from: https://zhan994.github.io/AgriLiRa4D.\n\n\\corrauth\nYuhang Ming,\nSchool of Computer Science,\nHangzhou Dianzi University,\nHangzhou, 310018, China.\nJie Yuan,\nSchool of Electronic Science and Engineering,\nNanjing University,\nNanjing, 210023, China.\nAgriLiRa4D: A ",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "multi-sensor SLAM",
        "UAV dataset",
        "agricultural environments",
        "LiDAR",
        "Radar",
        "IMU",
        "robust localization",
        "autonomous navigation"
    ],
    "one_line_summary": "该论文介绍了一个用于农业无人机在挑战性环境中进行多传感器SLAM研究的数据集AgriLiRa4D，与给定的关键词（如视频扩散、多模态生成等）无关。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T14:56:56Z",
    "created_at": "2026-01-10T10:43:26.555214",
    "updated_at": "2026-01-10T10:43:26.555221",
    "flag": true
}