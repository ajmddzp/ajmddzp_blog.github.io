{
    "id": "2601.08713v1",
    "title": "Real-Time Localization Framework for Autonomous Basketball Robots",
    "authors": [
        "Naren Medarametla",
        "Sreejon Mondal"
    ],
    "abstract": "定位是自主机器人的一项基本能力，使其能够在动态环境中有效运行。在2025年机器人竞赛中，精确可靠的定位对于提升射击精度、避免与其他机器人碰撞以及高效规划赛场路径至关重要。本文提出一种混合定位算法，该方法融合经典技术与基于学习的方法，仅利用球场地面视觉数据实现篮球场上的自主定位。",
    "url": "https://arxiv.org/abs/2601.08713v1",
    "html_url": "https://arxiv.org/html/2601.08713v1",
    "html_content": "Real-Time Localization Framework for Autonomous Basketball Robots\nNaren Medarametla\nSchool of Computer Science Engineering\nVellore Institute of Technology\nChennai, India\nnaren.medarametla2023@vitstudent.ac.in\nSreejon Mondal\nSchool of Electrical and Electronics Engineering\nVellore Institute of Technology\nChennai, India\nsreejon.mondal2023@vitstudent.ac.in\nAbstract—\nLocalization is a fundamental capability for autonomous robots, enabling them\nto operate effectively in dynamic environments.\nIn Robocon 2025, accurate and\nreliable localization is crucial for improving shooting precision, avoiding\ncollisions with other robots, and navigating the competition field efficiently.\nIn this paper, we propose a hybrid localization algorithm that integrates classical\ntechniques with learning based methods that rely solely on visual data from the court’s\nfloor to achieve self-localization on the basketball field.\nKeywords—Robot Localization, Autonomous Navigation, Neural Networks, Robocon\nI. Introduction\nThe DD Robocon is the Indian national entry level competition to Asia Pacific Robot Contest (ABU Robocon).\nThe objective of the competition is for college teams to design, build, and operate robots to complete certain tasks.\nThe tasks vary each year and are usually based on cultural, historical, or technological themes related to the host country of the international contest.\nFor Robocon 2025, the theme was Robot Basketball, where two robots had to play the game of basketball following specific rules.\nThe arena consisted of a court of dimensions 15m\n×\n\\times\n8m and baskets at a height of 2.4m.\nDue to the nature of the game, the robots had to move quickly and unpredictably, and take shots from various positions on the court.\nThe robots had to have robust and real time self localization determine it’s distance from the basket.\nIn this paper we propose a vision based localization system based on the white field lines of the basketball court.\nSection II\nreviews existing methods and prior research related to this work.\nSection III\nprovides a detailed description of our proposed algorithm, approach, and model architecture.\nSection IV\nprovides the results obtained from our experiments.\nSection V\ndiscusses previous approaches which failed.\nSection VI\nevaluates the accuracy of our approach and discusses potential directions for future work.\nII. Related Work\nIn vision based self localization for dynamic field sports,\nresearchers have explored complementary lines of work that balance geometric modeling,\nprobabilistic estimation, and learned perception.\nLiu et al.\n[\n4\n]\nintroduced a\npose point matching method for omnidirectional cameras in which white field lines are\ndetected from brightness changes, corrected for lens effects, and matched against a\nfield model to recover position and heading in real time with errors reported on the\norder of tens of centimeters.\nRibeiro et al.\n[\n9\n]\naccelerated global\nrelocalization by precomputing a distance map of the pitch and then searching\nthe error surface between observed line points and this map under a known heading,\nwhich makes frequent global updates practical when calibration and line extraction\nare reliable.\nWatanabe et al.\n[\n11\n]\nframed model matching as a search\nproblem and optimized the robot pose with a genetic algorithm driven by omnidirectional\nwhite line features, which helps in scenes with sparse or ambiguous cues while requiring\ncareful parameterization to remain real time.\nLu et al.\n[\n5\n]\nreported a robust\nreal time pipeline based on omnidirectional vision that emphasizes efficient feature\nextraction and stable operation during matches, though performance naturally depends\non lighting and occlusions typical of indoor arenas.\nOn humanoid platforms, Tian et\nal.\n[\n10\n]\nadapted Monte Carlo Localization to fisheye optics and bipedal\ngait by designing a motion model for oscillatory head movement and a vision model\nfor field landmarks, together with a resampling strategy that stabilized estimates\non limited hardware.\nFusion based designs combine complementary sensing to limit drift\nand maintain responsiveness, Ismail et al.\n[\n2\n]\nfused encoder and gyro\nodometry with omnivision in a particle filter so that fast motion updates are anchored\nby drift free visual corrections even on symmetric fields.\nMore recent work uses inertial\nconstraints to stabilize vision before geometric reasoning, Nadiri et al.\n[\n8\n]\nfiltered IMU orientation and applied inverse perspective mapping to obtain a bird’s eye view\nfrom which field markers provide consistent distances on approximately planar surfaces.\nLearning based perception is widely used to localize robots relative to other agents\nrather than only to the field, Luo et al.\n[\n6\n]\ndetected robots with a convolutional\ndetector and estimated 3D positions through RGBD registration on embedded hardware, improving\nrobustness to appearance variation at the cost of additional calibration and power.\nTable I:\nComparison of Related Works\nAuthor & Year\nCamera Type\nMethodology\nPerformance\nLimitations\nLiu et al, 2009\nOmnidirectional\nExtracts white line feature points from brightness changes and matches them to model “pose points” for position & heading estimation.\nAchieves approx. 20 fps with approx. 20 cm mean error, robust to changes in goal color.\nRelies on good visibility of white lines, requires careful distortion correction.\nRibeiro et al, 2016\nOmnidirectional\nUses precomputed distance maps of field lines, matches observed line points to the least error grid cell given a known heading.\nAllows for very fast global localization and frequent updates.\nNeeds an accurate heading and precise field calibration, performance degrades with poor line detection.\nWatanabe et al, 2020\nOmnidirectional\nCombines white line detection with a Genetic Algorithm (GA) to optimize the pose model match.\nProvides robust performance even with sparse line cues and enables an effective global search.\nRequires tuning the GA for real-time performance, has a higher computational cost than analytic matchers.\nLuo et al, 2019/2020\nRGB + Kinect v2 Depth\nA two stage process where YOLOv2 detects robots in RGB, and then Kinect depth data is used to recover their 3D positions.\nHigh Mean Average Precision (mAP), robust against changes in jersey color.\nRequires an active depth sensor, cross sensor calibration, and has extra power demands.\nLu et al, 2013\nOmnidirectional\nEmploys a robust omnidirectional vision pipeline for feature extraction and localization.\nHas demonstrated real-time robustness in practical applications.\nSensitive to lighting conditions and occlusion.\nTian et al, 2010\nFisheye Lens\nImplements a particle filter (Monte Carlo Localization) with vision and motion models, handling distortion and motion oscillation.\nManages non Gaussian noise and multiple hypotheses, has been proven on hardware.\nIncurs a higher computational load, can suffer from pose aliasing in symmetric fields.\nIsmail et al, 2012\nOmnidirectional + Odometry\nFuses data from odometry (which is fast but drifts) and omnidirectional vision with MCL (which is drift free but slower).\nDelivers accurate and responsive localization with low latency (approx. 1.6 ms).\nRequires careful time synchronization, MCL remains compute heavy with sparse data.\nNadiri et al, 2025\nOmnidirectional + IMU\nUses an IMU stabilized camera with a Bird’s Eye View (BEV) transformation for marker detection.\nReduces mean error by approximately 9 cm, the BEV simplifies distance measurement.\nDemands reliable IMU calibration, sensitive to errors in camera setup.\nUnlike most of the reviewed approaches, which rely heavily on omnidirectional or fisheye optics to maximize field of view for white line detection,\nour method uses a regular monocular camera while still achieving reliable self localization.\nThis choice reduces the need for specialized distortion\ncorrection pipelines, and makes the approach more adaptable to standard camera modules already used in many robotics platforms.\nIII. Methodology\nOur approach is a two step process that begins with\nPreprocessing\nthe image,\nfollowed by passing it to the model for\nInference\n.\nA. Preprocessing\nThe input image having dimensions (640\n×\n\\times\n480\n×\n\\times\n3) is converted from the RGB color space to the HSV color space, then the white regions are masked out using two predefined HSV ranges.\nThe image is downsampled through a radial scan, flattened, and finally passed through the neural network.\nFigure\nI\nshows the preprocessing pipline and Algorithm 1 gives the implementation of the downsampling algorithm.\nFigure I:\nPreprocessing pipeline\nAlgorithm 1\nDownsampling\n1:\nInput:\nImage\n2:\nH\n←\nH\\leftarrow\nImage.height\n3:\nW\n←\nW\\leftarrow\nImage.width\n4:\nR\n←\nR\\leftarrow\nBlack Image\n5:\nfor\nangle\n←\n\\leftarrow\n0 to 180 step 2\ndo\n6:\nl\n​\na\n​\ns\n​\nt\n​\nP\n​\ni\n​\nx\n​\ne\n​\nl\n←\n0\nlastPixel\\leftarrow 0\n7:\nc\n​\nx\n←\nW / 2\ncx\\leftarrow\\text{W / 2}\n8:\nc\n​\ny\n←\nH\ncy\\leftarrow\\text{H}\n9:\nfor\nd\n←\n\\leftarrow\n0 to max(H, W)\ndo\n10:\nx\n←\nc\n​\nx\n+\nd\n×\ncos\n⁡\n(\na\n​\nn\n​\ng\n​\nl\n​\ne\n)\nx\\leftarrow cx+d\\times\\cos(angle)\n11:\ny\n←\nc\n​\ny\n−\nd\n×\nsin\n⁡\n(\na\n​\nn\n​\ng\n​\nl\n​\ne\n)\ny\\leftarrow cy-d\\times\\sin(angle)\n12:\nif\n0\n≤\nx\n<\nW\n​\nand\n​\n0\n≤\ny\n<\nH\n0\\leq x<\\text{W}\\ \\textbf{and}\\ 0\\leq y<\\text{H}\nthen\n13:\np\n​\ni\n​\nx\n​\ne\n​\nl\n←\nI\n​\nm\n​\na\n​\ng\n​\ne\n​\n[\ny\n]\n​\n[\nx\n]\npixel\\leftarrow Image[y][x]\n14:\nif\nlastPixel = 255 and pixel\n≠\n\\neq\n255\nthen\n15:\nR\n​\n[\ny\n]\n​\n[\nx\n]\n←\n255\nR[y][x]\\leftarrow 255\n16:\nend\nif\n17:\nl\n​\na\n​\ns\n​\nt\n​\nP\n​\ni\n​\nx\n​\ne\n​\nl\n←\np\n​\ni\n​\nx\n​\ne\n​\nl\nlastPixel\\leftarrow pixel\n18:\nend\nif\n19:\nend\nfor\n20:\nend\nfor\n21:\nReturn\nR\nB. Model Architecture\nThe proposed model is a feedforward neural network consisting of a flattening layer followed\nby four fully connected layers. The first 200 pixels from the top of the image are removed\nto reduce the input size, as they do not carry useful information and the pixel values are scaled\ndown to the range [0, 1].\nThe resulting image with dimensions (640\n×\n\\times\n280\n×\n\\times\n1) is flattened into a vector\nof size 1,79,200 and passed through the first linear layer, followed by a ReLU activation function.\nThe subsequent layers have sizes 1024, 256, and 64, each followed by a ReLU activation.\nThe final layer outputs a 2 dimensional vector representing the predicted\nx\nx\nand\ny\ny\npositions of the robot.\nThe rationale for using this relatively simple model lies in the simplicity of the input images\nand to reducing inference time.\nFigure II:\nFlow diagram\nFigure III:\nArchitecture diagram\nC. Dataset\nA digital twin of the robot was created and simulated in a replica of the Robocon 2025 arena\nusing the Gazebo\n[\n1\n]\nsimulator with the help of ROS2\n[\n7\n]\n.\nThe robot was driven through the arena capturing images and corresponding\nx\nx\n,\ny\ny\ncoordinates.\nA\nTimeSynchronizer was used to synchronize the frame header and the position header.\nA total of\n6283 images were captured and split into training and test datasets with a 0.9 to 0.1 ratio.\nCare was taken to include every part of the arena for an unbiased dataset.\nD. Simulation\nThe simulation environment was designed to closely replicate the physical conditions of the target field.\nA detailed world map of the area was created to serve as the operating environment, and a complete model of the robot was developed using the Unified Robot Description Format (URDF)\n[\n13\n]\n.\nThe simulation was implemented in Gazebo, where the robot’s onboard camera was modeled as a virtual sensor using Gazebo camera plugins to replicate image capture.\nThe robot was teleoperated within the environment using a joystick, leveraging the\njoy\npackage available in ROS 2 Humble, which enabled real-time manual control and data collection across different positions and orientations in the simulated space.\nThe images captured in Gazebo were sent back to ROS2 where the model would process them and predict the coordinates.\nFigure IV illustrates the block diagram\nFigure IV:\nBlcok Diagram\nFigure V:\nGazebo simulation\nE. Training\nThe model was trained for 15 epochs using an Adam optimizer\n[\n3\n]\nwith an initial learning rate of\n10\n−\n4\n10^{-4}\nand a Mean Squared Error (MSE) cost function in batches of size 8.\nIV. Results\nFigure\nVIII\ndepicts the loss at each epoch throughout the training process.\n8 independent images at different points of the court were again captured\nfrom the simulation and fed into the model.\nFigure\nX\nshows the plot between\nthe ground truth and the prediction made by the model for these 8 images.\nTable\nII\nillustrates the x and y losses for a corresponding number of test iterations.\nFigure\nVI\nand Figure\nVII\nshow the\nA Pearson correlation test was performed to determine if the prediction loss was dependent on the number of test iterations. For the x loss, the test yielded a p-value of 0.40. Since this value is well above the significance level of 0.05, we conclude that there is no statistically significant evidence of a relationship, indicating that the x loss is independent of the number of iterations. In contrast, the test for the y loss yielded a p-value of 0.01. This value is below 0.05, indicating a statistically significant negative correlation where the loss tends to decrease as iterations increase. A detailed breakdown of these calculations is provided below.\nThe code for the paper can be found in the following repository:\nhttps://github.com/NarenTheNumpkin/Basketball-robot-localization\nTable II:\nPrediction Loss\nS.No\nIterations\nx loss (m)\ny loss (m)\n1\n15\n0.1375\n0.1950\n2\n25\n0.1639\n0.1861\n3\n35\n0.1498\n0.1717\n4\n100\n0.1611\n0.1691\n5\n200\n0.1464\n0.1519\n6\n300\n0.1412\n0.1579\n7\n400\n0.1394\n0.1605\n8\n500\n0.1376\n0.1588\n9\n600\n0.1401\n0.1628\n0.1463\n0.1506\nFigure VI:\nx loss vs iterations\nFigure VII:\ny loss vs iterations\nFigure VIII:\nLoss curve\nFigure IX:\nGround truth vs prediction\n1. Iterations vs. x loss\nFor this test, we used 9 data pairs, obtained a Pearson correlation of\nr\n=\n−\n0.32\nr=-0.32\n, and computed the degrees of freedom as\nd\n​\nf\n=\n7\ndf=7\n(\nn\n−\n2\nn-2\n).\nThe t-statistic is calculated as\n t = -0.32 9-21 - (-0.32)\n2\n= -0.84670.9474 ≈-0.89 \nThe two tailed critical t-value for\nα\n=\n0.05\n\\alpha=0.05\nand df=7 is\n±\n2.365\n\\pm 2.365\n. Since our calculated t-statistic of -0.89 is greater than -2.365, we fail to reject the null hypothesis. The corresponding p value is 0.40. This confirms there is no statistically significant relationship.\nAt\nα\n=\n0.05\n\\alpha=0.05\nthere is insufficient evidence of a linear association between iterations and x loss. The observed correlation (r = -0.32) is small and likely due to sampling variability rather than a true effect.\n2. Iterations vs. y loss\nA corresponding test for iterations vs. y loss yielded\np\n=\n0.01\n<\n0.05\np=0.01<0.05\n. Therefore, we reject\nH\n0\nH_{0}\nand conclude there is a statistically significant negative linear relationship between iterations and y loss, indicating that y loss tends to decrease as the number of iterations increases.\n t = -0.67 9-21 - (-0.67)\n2\n= -1.77270.7424 ≈-2.39\nA. Comparision and Analysis\nTwo models were chosen as a baseline, EfficientNet-B\n[\n17\n]\nand MobileNetV2\n[\n16\n]\ndue to their ability to run on\nefficiently edge devices which is particularly suitable for faster inference time.\n1. MobilenetV2\nMobilenetV2 is built on depthwise separable convolutions and inverted residual blocks with linear bottlenecks. A standard\nk\n×\nk\nk{\\times}k\nconvolution with\nM\nM\ninput and\nN\nN\noutput channels over a\nD\nF\n×\nD\nF\nD_{F}{\\times}D_{F}\nmap costs\nCost\nstd\n=\nD\nK\n2\n⋅\nM\n⋅\nN\n⋅\nD\nF\n2\n\\mathrm{Cost}_{\\text{std}}=D_{K}^{2}\\cdot M\\cdot N\\cdot D_{F}^{2}\n(1)\nDepthwise separable convolutions reduce this to\nCost\ndw-sep\n=\nD\nK\n2\n⋅\nM\n⋅\nD\nF\n2\n+\nM\n⋅\nN\n⋅\nD\nF\n2\n\\mathrm{Cost}_{\\text{dw-sep}}=D_{K}^{2}\\cdot M\\cdot D_{F}^{2}\\;+\\;M\\cdot N\\cdot D_{F}^{2}\n(2)\nA depthwise separable block can be written as\ny\n=\nConv\n1\n×\n1\n​\n(\nσ\n​\n(\nBN\n​\n(\nConv\ndw\n​\n(\nx\n)\n)\n)\n)\ny=\\mathrm{Conv}_{1\\times 1}\\!\\Big(\\sigma\\!\\big(\\mathrm{BN}(\\mathrm{Conv}_{\\text{dw}}(x))\\big)\\Big)\n(3)\nwhere BN is batch normalization and\nσ\n\\sigma\nis a non linear operator,\nMobilenetV2 with inverted residual blocks and linear bottleneck has the operator flow\nz\n\\displaystyle z\n=\nϕ\n​\n(\nBN\n​\n(\nConv\n1\n×\n1\nexpand\n​\n(\nx\n)\n)\n)\n\\displaystyle=\\phi\\!\\big(\\mathrm{BN}(\\mathrm{Conv}^{\\text{expand}}_{1\\times 1}(x))\\big)\n(4)\nu\n\\displaystyle u\n=\nϕ\n​\n(\nBN\n​\n(\nConv\nk\n×\nk\ndw\n​\n(\nz\n)\n)\n)\n\\displaystyle=\\phi\\!\\big(\\mathrm{BN}(\\mathrm{Conv}^{\\text{dw}}_{k\\times k}(z))\\big)\n(5)\ny\n\\displaystyle y\n=\nBN\n​\n(\nConv\n1\n×\n1\nproj\n​\n(\nu\n)\n)\n\\displaystyle=\\mathrm{BN}(\\mathrm{Conv}^{\\text{proj}}_{1\\times 1}(u))\n(6)\n2. EfficientNet-B0\nEfficientNet is a family of lightweight CNN backbones that pair a simple convolutional stem with stacked MBConv stages and a compact head. The network downsamples spatial resolution via staged strides across MBConv blocks and applies squeeze and excitation to recalibrate channels, and adopts SiLU/Swish activations for stable training. A global average pooling layer and a small fully connected head finalize the architecture. Model variants use a compound policy to jointly scale depth, width, and input resolution to meet different latency/accuracy targets without changing the block design.\nThe backbone uses MBConv (inverted residual) blocks with squeeze-and-excitation (SE) and SiLU/Swish.\nA simplified MBConv with SE\nz\n\\displaystyle z\n=\nϕ\n​\n(\nBN\n​\n(\nConv\n1\n×\n1\nexpand\n​\n(\nx\n)\n)\n)\n,\n\\displaystyle=\\phi\\!\\big(\\mathrm{BN}(\\mathrm{Conv}^{\\text{expand}}_{1\\times 1}(x))\\big),\n(7)\nu\n\\displaystyle u\n=\nϕ\n​\n(\nBN\n​\n(\nConv\nk\n×\nk\n,\ns\ndw\n​\n(\nz\n)\n)\n)\n,\n\\displaystyle=\\phi\\!\\big(\\mathrm{BN}(\\mathrm{Conv}^{\\text{dw}}_{k\\times k,s}(z))\\big),\n(8)\ns\n\\displaystyle s\n=\nσ\n​\n(\nW\n2\n​\nδ\n​\n(\nW\n1\n​\nGAP\n​\n(\nu\n)\n)\n)\n⊙\nu\n,\n\\displaystyle=\\sigma\\!\\big(W_{2}\\,\\delta(W_{1}\\,\\mathrm{GAP}(u))\\big)\\odot u,\n(9)\ny\n\\displaystyle y\n=\nBN\n​\n(\nConv\n1\n×\n1\nproj\n​\n(\ns\n)\n)\n.\n\\displaystyle=\\mathrm{BN}(\\mathrm{Conv}^{\\text{proj}}_{1\\times 1}(s)).\n(10)\nSkip connection:\ny\n^\n=\n{\nx\n+\ny\n,\nif\n​\ns\n=\n1\n​\nand\n​\nC\nout\n=\nC\nin\n,\ny\n,\notherwise.\n\\hat{y}=\\begin{cases}x+y,&\\text{if }s=1\\text{ and }C_{\\text{out}}=C_{\\text{in}},\\\\\ny,&\\text{otherwise.}\\end{cases}\n(11)\nTable III:\nModel Comparison\nMetric\nFeedforward\nMobileNetV2\nEfficientNet-B0\nMSE\n0.040\n3.380\n3.778\nTime (ms)\n3.283\n4.213\n4.373\nRAM\n1.3 GB\n387 MB\n323 MB\nTemperature\n43.62°C\n43.69°C\n43.97°C\nThe above table shows the performance of the models on physical hardware on 100 samples. Our feedforward model achieves the lowest MSE and the fastest inference, primarily because its computation mainly involves a few dense matrix multiplications on a flattened preprocessed input. However, this design incurs higher memory usage, flattening a large spatial map into a 179,200-dimensional vector and feeding it into wide fully connected layers creates large weight matrices and activation tensors. In contrast, MobileNetV2 and EfficientNet-B0 are more memory-efficient due to depthwise separable convolutions and MBConv blocks, but they introduce slightly more latency on our setup.\nB. Hardware\nAll tests and measurements were performed on an NVIDIA Jetson Orin Nano. The module provides a 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores, a 6-core Arm Cortex-A78AE CPU, and 8 GB RAM. The reported latencies were obtained on this platform using batch size 1 and on-device acceleration.\nThe models were trained on an Apple MPS and were exported to an ONNX format for inference on the Jeton Nano Orin\nC. Explainable AI\nTo better understand the decision making process of our model, we employed the Integrated Gradients method\n[\n12\n]\nto attribute the model’s predictions to specific pixels in the input image.\nIntegrated Gradients is a gradient based attribution technique that satisfies desirable axioms such as sensitivity and implementation invariance.\nIt computes feature attributions by integrating the gradients of the model’s output with respect to the inputs, along a straight line path from a baseline to the actual input.\nThis allows us to highlight the regions of the input that most influenced the model’s prediction, thereby providing interpretability for both correct and incorrect decisions.\nMathematically, the attribution for the\ni\ni\n-th input feature is computed as:\nIntegratedGradients\ni\n​\n(\nx\n)\n=\n(\nx\ni\n−\nx\ni\n′\n)\n×\n1\nm\n​\n∑\nk\n=\n1\nm\n∂\nF\n​\n(\nx\n′\n+\nk\nm\n​\n(\nx\n−\nx\n′\n)\n)\n∂\nx\ni\n\\mathrm{IntegratedGradients}_{i}(x)=(x_{i}-x^{\\prime}_{i})\\times\\frac{1}{m}\\sum_{k=1}^{m}\\frac{\\partial F\\left(x^{\\prime}+\\frac{k}{m}(x-x^{\\prime})\\right)}{\\partial x_{i}}\nwhere\nx\nx\nis the input,\nx\n′\nx^{\\prime}\nis the baseline input, and\nF\nF\nis the model’s output function.\nThis technique helped us identify which parts of the scene were most important for the model’s localization decisions.\nIn our experiments, the attributions often concentrated on regions containing field lines and other spatial cues, suggesting that the model relies heavily on geometric features.\nFigure X:\nSaliency Maps\nV. Failed Approaches\nIn our earlier methodology, we employed an odometry-based approach for the self-localization of the robots.\nThis system integrated wheel encoders and an Inertial Measurement Unit (IMU), complemented by a TF-Luna LiDAR sensor to estimate the depth between the robot and the basket.\nHowever, the TF-Luna sensor demonstrated a limited effective range of approximately 3 to 4 meters and exhibited suboptimal accuracy within this range.\nAdditionally, a significant challenge arose due to the absence of a clearly distinguishable object or surface on the basket, which hindered reliable depth estimation.\nIn the subsequent approach, we employed an object detection model based on YOLOv8n\n[\n14\n]\n, trained on a custom dataset to detect the basket.\nThis model was utilized not only for basket detection but also for correcting the robot’s yaw orientation to ensure it consistently faced the target.\nTo estimate the horizontal distance from the basket, we defined discrete zones, with each zone representing a specific distance range.\nThe corresponding zone, selected manually by the pilot based on visual assessment, was then used in conjunction with the detected orientation to determine the appropriate RPM for the shooting mechanism.\nWhile this approach facilitated a simplified method for distance estimation, it was inherently dependent on manual selection and coarse approximations, which could lead to reduced accuracy and consistency.\nFigure XI:\nbasket detection\nWe improved upon the object detection approach by using the MiDaS\n[\n15\n]\n(Multiple Depth Estimation Accuracy with Single Network) deep learning model.\nMiDaS is a designed for monocular depth estimation, i.e., predicting depth from a single image.\nIt uses a ResNet based encoder decoder architecture where the encoder extracts features from the input image and the decoder upsamples these features to produce a depth map.\nThe approach combines the YOLOv8 model with the MiDaS depth estimation framework for relative depth prediction.\nThe pipeline begins by loading an input image, followed by inference using the YOLO model to localize the basket and identify its center.\nSimultaneously, MiDaS processes the same image to generate a dense relative depth map.\nOnce the basket is detected, the relative depth at the center of the bounding box is extracted from the depth map.\nAn interactive calibration step is used to relate MiDaS’s relative depth to real world distance.\nThe user is prompted to enter known ground truth distances for three distinct positions.\nThese pairs of relative depth and real distance are then used to fit a linear regression model, enabling real-time conversion of relative depth to metric distance.\nDuring tracking mode, the system periodically predicts the basket’s distance using the learned regression model and overlays the result on the original image alongside the visualized depth map.\nThe output is displayed in a side by side format showing both RGB and color mapped depth views for easier interpretation.\nFigure XII:\nbasket depth estimation\nA key limitation of this approach was its reliance on prior calibration, which required manually measuring the distance between the robot and the basket at least three times before accurate predictions could be made.\nIn response to these constraints, we developed the current method presented in this paper, building upon insights gained from the previous techniques.\nVI. Conclusion\nIn this work, we presented a hybrid localization framework for autonomous basketball robots competing\nin the Robocon 2025 arena.\nOur approach relies solely on floor images processed through a lightweight\nfeedforward neural network.\nBy preprocessing the images to highlight salient floor features and using a simple architecture,\nwe achieved an average prediction error of approximately 0.06 meters, demonstrating the potential of\nlearning-based localization methods.\nThis result shows that even with minimal sensor inputs and modest\nmodel complexity, it is possible to achieve sufficiently accurate localization.\nFuture improvements could\nfocus on integrating other sensors such as IMU, wheel odometry, or LIDAR with the vision-based system\nthrough filtering techniques like Kalman or particle filters to achieve more robust and reliable\nlocalization.\nAdditionally, exploring more neural network architectures may help to improve prediction\naccuracy.\nFinally, optimizing the model for deployment on embedded platforms with limited computational\nresources through techniques such as pruning, quantization, or hardware acceleration could be worked upon.\nReferences\n[1]\nN. Koenig and A. Howard, ”Design and use paradigms for Gazebo, an open-source\nmulti-robot simulator,” 2004 IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS) (IEEE Cat. No.04CH37566), Sendai, Japan, 2004, pp. 2149-2154 vol.3,\ndoi: 10.1109/IROS.2004.1389727.\n[2]\nM. Ismail\net al.\n, “Soccer robot localization based on sensor fusion from odometry and omnivision,” 2012.\n[3]\nDiederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization\n.\narXiv preprint arXiv:1412.6980, 2014.\n[4]\nQ. Liu\net al.\n, “A self-localization method through pose point matching for autonomous soccer robot based on omni-vision,” in\nProceedings of RoboCup Symposium\n, 2009.\n[5]\nH. Lu\net al.\n, “Robust and real-time self-localization based on omnidirectional vision for soccer robots,” 2013.\n[6]\nX. Luo\net al.\n, “Robot detection and localization based on deep learning,” 2020.\n[7]\nSteve Macenski, Tully Foote, Brian Gerkey, Chris Lalancette, and William Woodall.\nRobot Operating System 2: Design, architecture, and uses in the wild\n.\nScience Robotics, vol. 7, no. 66, 2022.\n[8]\nA. Nadiri\net al.\n, “IMU-stabilized bird’s-eye view localization for humanoid soccer robots,” 2025.\n[9]\nM. Ribeiro\net al.\n, “Fast computational processing for mobile robots self-localization,” 2016.\n[10]\nY. Tian\net al.\n, “Self-localization of humanoid robots with fish-eye lens in a soccer field,” 2010.\n[11]\nY. Watanabe\net al.\n, “Model-based self-localization with genetic algorithm for omnidirectional vision,” 2020.\n[12]\nM. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep networks,”\nin\nProceedings of the 34th International Conference on Machine Learning (ICML)\n,\nvol. 70, pp. 3319–3328, 2017.\n[13]\nD. Tola and P. Corke, “Understanding URDF: A dataset and analysis,”\nIEEE Robotics and Automation Letters\n, vol. 9, no. 5, pp. 4479–4486, 2024.\n[14]\nJ. Redmon, S. Divvala, R. Girshick, and A. Farhadi,\n“You only look once: Unified, real-time object detection,”\nin\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\n2016, pp. 779–788.\n[15]\nR. Ranftl, A. Boedt, and V. Koltun,\n“Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n,\nvol. 44, no. 3, pp. 1623–1637, 2020.\n[16]\nM. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n“MobileNetV2: Inverted residuals and linear bottlenecks,”\nin\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\n2018, pp. 4510–4520.\n[17]\nM. Tan and Q. V. Le,\n“EfficientNet: Rethinking model scaling for convolutional neural networks,”\nin\nProceedings of the 36th International Conference on Machine Learning (ICML)\n,\nPMLR, 2019, pp. 6105–6114.",
    "preview_text": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.\n\nReal-Time Localization Framework for Autonomous Basketball Robots\nNaren Medarametla\nSchool of Computer Science Engineering\nVellore Institute of Technology\nChennai, India\nnaren.medarametla2023@vitstudent.ac.in\nSreejon Mondal\nSchool of Electrical and Electronics Engineering\nVellore Institute of Technology\nChennai, India\nsreejon.mondal2023@vitstudent.ac.in\nAbstract—\nLocalization is a fundamental capability for autonomous robots, enabling them\nto operate effectively in dynamic environments.\nIn Robocon 2025, accurate and\nreliable localization is crucial for improving shooting precision, avoiding\ncollisions with other robots, and navigating the competition field efficiently.\nIn this paper, we propose a hybrid localization algorithm that integrates classical\ntechniques with learning based methods that rely solely on visual data from the court’s\nfloor to achieve self-localization on the basketball field.\nKeywords—Robot Localization, Autonomous Navigation, Neural Networks, Robocon\nI. Introduction\nThe DD Robocon is the Indian national entry level competition to Asia Pacific Robot Contest (ABU Robocon).\nThe objective of the competition is for college teams to design, build, and operate robots to complete certain tasks.\nThe tasks vary each year and are usually based on cultural, historical, or technological themes related to the host country of the international contest.\nFor Robocon 2025, the theme was Robot Basketball, where two robots had to",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "Robot Localization",
        "Autonomous Navigation",
        "Neural Networks"
    ],
    "one_line_summary": "这篇论文提出了一种用于自主篮球机器人的实时定位框架，结合经典技术和基于视觉的学习方法，但未涉及强化学习、VLA、扩散模型、Flow Matching、全身控制或VLM等关键词。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T16:40:18Z",
    "created_at": "2026-01-20T17:49:45.912995",
    "updated_at": "2026-01-20T17:49:45.913005"
}