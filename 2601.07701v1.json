{
    "id": "2601.07701v1",
    "title": "Deep Whole-body Parkour",
    "authors": [
        "Ziwen Zhuang",
        "Shaoting Zhu",
        "Mengjie Zhao",
        "Hang Zhao"
    ],
    "abstract": "å½“å‰äººå½¢æœºå™¨äººæ§åˆ¶æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤å¤§èŒƒå¼ï¼šæ„ŸçŸ¥å¼æ­¥æ€æ§åˆ¶ï¼Œè™½èƒ½è‰¯å¥½åº”å¯¹åœ°å½¢ä½†ä»…é™äºè¶³éƒ¨ç§»åŠ¨ï¼›ä»¥åŠé€šç”¨è¿åŠ¨è·Ÿè¸ªï¼Œè™½èƒ½å¤ç°å¤æ‚æŠ€èƒ½å´å¿½è§†ç¯å¢ƒé€‚åº”æ€§ã€‚æœ¬ç ”ç©¶èåˆè¿™ä¸¤ç§èŒƒå¼ï¼Œå®ç°äº†æ„ŸçŸ¥å¼é€šç”¨è¿åŠ¨æ§åˆ¶ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªå°†å¤–éƒ¨æ„ŸçŸ¥èå…¥å…¨èº«è¿åŠ¨è·Ÿè¸ªçš„æ¡†æ¶ï¼Œä½¿ä»¿äººæœºå™¨äººèƒ½å¤Ÿåœ¨ä¸å¹³å¦åœ°å½¢ä¸Šæ‰§è¡Œé«˜åº¦åŠ¨æ€çš„éç§»åŠ¨ä»»åŠ¡ã€‚é€šè¿‡è®­ç»ƒå•ä¸€ç­–ç•¥ä½¿å…¶åœ¨ä¸åŒåœ°å½¢ç‰¹å¾ä¸Šæ‰§è¡Œå¤šç§å·®å¼‚åŒ–åŠ¨ä½œï¼Œæˆ‘ä»¬è¯æ˜äº†å°†æ„ŸçŸ¥æ•´åˆè‡³æ§åˆ¶å›è·¯çš„é‡è¦ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šå®ç°ç¨³å¥ä¸”é«˜åº¦åŠ¨æ€çš„å¤šæ¥è§¦è¿åŠ¨ï¼Œå¦‚æ’‘è·³ç¿»æ»šä¸é±¼è·ƒç¿»æ»šï¼Œæ˜¾è‘—æ‹“å±•äº†æœºå™¨äººè¶…è¶Šç®€å•è¡Œèµ°æˆ–å¥”è·‘çš„è¶Šéšœèƒ½åŠ›ã€‚https://project-instinct.github.io/deep-whole-body-parkour",
    "url": "https://arxiv.org/abs/2601.07701v1",
    "html_url": "https://arxiv.org/html/2601.07701v1",
    "html_content": "Deep Whole-body Parkour\nZiwen Zhuang\n12\n,\nShaoting Zhu\n12\n,\nMengjie Zhao\n1\nand\nHang Zhao\n12\n2\nAbstract\nCurrent approaches to humanoid control generally fall into two paradigms: perceptive locomotion, which handles terrain well but is limited to pedal gaits, and general motion tracking, which reproduces complex skills but ignores environmental capabilities. This work unites these paradigms to achieve perceptive general motion control. We present a framework where exteroceptive sensing is integrated into whole-body motion tracking, permitting a humanoid to perform highly dynamic, non-locomotion tasks on uneven terrain. By training a single policy to perform multiple distinct motions across varied terrestrial features, we demonstrate the non-trivial benefit of integrating perception into the control loop. Our results show that this framework enables robust, highly dynamic multi-contact motionsâ€”such as vaulting and dive-rollingâ€”on unstructured terrain, significantly expanding the robotâ€™s traversability beyond simple walking or running.\nI\nIntroduction\nFigure 1:\nData-driven whole-body control framework. Real-world environment scans and human demonstrations are processed and aligned to generate feasible motion-terrain pairs. A policy is trained via large-scale reinforcement learning with exteroceptive observations, enabling the robot to replicate agile behaviors in the real world.\nDeep Reinforcement Learning (DRL) has fundamentally transformed the landscape of legged robotics, enabling systems to traverse complex environments with unprecedented robustness. By fusing proprioceptive data with exteroceptive observationsâ€”such as depth images or height mapsâ€”robots can now adapt their gait patterns to unstructured terrain in real-time\n[\n2\n,\n25\n,\n5\n]\n. This paradigm has rapidly evolved from low-speed navigation on static terrain to high-speed, agile maneuvers. Recent advancements have demonstrated that quadrupeds and humanoids are capable of dynamic parkour, autonomously leaping over gaps and climbing large obstacles\n[\n27\n,\n28\n,\n13\n]\n. However, despite these aggressive motions, this paradigm remains functionally limited to pedal locomotion. In these frameworks, the robot interacts with the environment exclusively through foot contacts, utilizing the upper body solely for balance rather than for contact-rich interaction. Consequently, while these robots can jump, they lack the capability to execute multi-contact skillsâ€”such as vaulting, scrambling, or hand-assisted climbingâ€”that are essential for human-level traversability in constrained spaces.\nConversely, the second paradigm focuses on general motion tracking, a field rooted in computer graphics that prioritizes kinematic over environmental adaptability. In simulation, previous works such as DeepMimic\n[\n18\n]\nand AMP\n[\n19\n]\ndemonstrated physically simulated avatars capable of mastering a rich repertoire of skillsâ€”including backflips, martial arts, and parkour vaultsâ€”by imitating reference motion data. Recently, this paradigm has successfully bridged the sim-to-real gap, with various studies deploying imitation-based policies on physical humanoids to reproduce expressive behaviors in the real world\n[\n12\n,\n26\n,\n22\n,\n4\n]\n. However, a critical limitation persists: these approaches are predominantly environment-agnostic. They rely solely on tracking a pre-recorded trajectory rather than reacting to the geometry of the world. Consider the specific challenge of vaulting onto a high box platform: a blind tracking policy might perfectly reproduce the kinematics of the vault in open space, but without visual feedback, it cannot adjust its jump timing or hand placement to the specific height and distance of the box. As a result, the robot is destined to either collide with the platform or miss the critical hand contacts entirely.\nIn this work, we propose to bridge these distinct paradigms by integrating exteroceptive depth perception directly into a whole-body motion tracking framework. We introduce a system where the robot learns not merely to mimic a reference animation but to adapt it based on the visual occupancy of the environment. Crucially, the integration of depth perception yields non-trivial robustness to initial conditions. Unlike blind tracking baselinesâ€”which require the robot to be placed at a precise, predefined distance from the obstacle to align the playback trajectoryâ€”our visually guided policy operates in a closed loop. This allows the system to tolerate significant variance in the setup: for example, if the robot is initialized at varying distances or angles relative to the platform, the policy leverages visual feedback to autonomously adjust its approach gaitâ€”shortening or lengthening its stepsâ€”to ensure accurate hand placement and successful vaulting. This capability effectively transforms fragile, fixed-trajectory tracking into robust, spatially-aware capabilities suitable for real-world deployment.\nII\nRelated Works\nDeep Reinforcement Learning for Legged Control\nDeep RL has established a robust standard for locomotion by leveraging proprioceptive history and privileged simulation states\n[\n21\n,\n9\n,\n10\n,\n23\n]\n. Through teacher-student frameworks or asymmetric actor-critics, policies learn to estimate terrain properties implicitly\n[\n8\n,\n10\n,\n9\n,\n17\n]\n. While highly stable on continuous irregularities, these â€blindâ€ agents are methodologically limited to reactive reflexes; without exteroception, they cannot anticipate or plan for discrete obstacles.\nPerceptive Locomotion\nIntegrating exteroception (e.g., depth or elevation maps) enables agents to modulate foot placement for upcoming terrain, facilitating dynamic parkour behaviors\n[\n16\n,\n1\n,\n27\n,\n28\n]\n. However, these systems typically rely on low-dimensional velocity commands (\nv\nx\n,\nv\ny\n,\nÏ‰\nz\nv_{x},v_{y},\\omega_{z}\n), leading to a\ntask under-specification\nproblem for humanoids. This low-bandwidth interface cannot disambiguate complex interaction modesâ€”such as vaulting versus jumpingâ€”thereby restricting the robot to simple pedal locomotion despite its expressive morphology.\nData-Driven Humanoid Motion Tracking\nMotion tracking approaches, such as DeepMimic\n[\n18\n]\nand AMP\n[\n19\n]\n, utilize reference motions as a dense task specification, successfully transferring agile skills to physical hardware\n[\n14\n,\n22\n,\n6\n,\n7\n,\n12\n]\n. However, these frameworks fundamentally operate under a\nplanar environment assumption\n. They treat locomotion purely as kinematic reproduction on flat ground, disregarding the necessity to perceive terrain-dependent information. Consequently, without geometric awareness, they lack the capability to traverse non-planar structures or adapt the reference motion to physical obstacles.\nIII\nMethod\nIII-A\nDataset Curation and Environment Generation\nMotivation and Capture\nWhile large-scale motion datasets such as AMASS\n[\n15\n]\nprovide extensive human kinematic data, and OMOMO\n[\n11\n]\nintroduces object manipulation, they generally lack dynamic, whole-body interactions with large-scale geometry. Agile parkour maneuversâ€”such as vaulting, climbing, and hurdlingâ€”rely critically on precise contacts between the agent and the terrain. To bridge this gap, we curate a custom dataset that strictly couples human dynamics with accurate environmental geometry.\nWe employ an optical motion capture system to record expert human actors performing parkour maneuvers on physical obstacles. To ensure high-fidelity spatial correspondence between the motion and the terrain, we simultaneously digitize the physical scene using a LiDAR-enabled iPad Pro (via the 3D Scanner App). This process yields a reconstructed mesh that is spatially aligned with the captured motion trajectories (Figure\n1\n).\nMotion Retargeting\nThe raw human motion is retargeted to the Unitree G1 humanoid robot using the GMR framework\n[\n3\n]\n. This process utilizes optimization-based kinematic filtering, followed by manual keyframe adjustment, to ensure the resulting trajectories are physically feasible for the robotâ€™s morphology. Special attention is paid to enforcing valid contact constraints and eliminating high-frequency capture noise.\nProcedural Environment Generation\nTo ensure the policy generalizes to diverse environments rather than overfitting to the specific spatial constraints of our capture laboratory, we post-process the scanned meshes for simulation. We isolate the functional geometry (e.g., obstacles, platforms, and rails) by segmenting out surrounding walls, ceilings, and extraneous laboratory context. This yields a set of canonical, context-agnostic obstacle meshes.\nFinally, we integrate these assets into NVIDIA Isaac Lab to create a massive-parallel training environment. We treat the retargeted motion and its corresponding obstacle mesh as a single\npaired instance\n. These pairs are procedurally instantiated across the simulation grid, creating an open-field training setup shown in Figure\n1\n. By stripping the scene of collision group artifacts (like room boundaries) and randomizing the placement of these motion-terrain pairs, we ensure the agent learns to condition its behavior strictly on local obstacle geometry.\nMassively Parallel Ray-Caster for Isolated Multi-Agent Training\nTo train a perceptive whole-body control policy within a unified framework, high-throughput depth simulation across thousands of parallel environments is essential. While IsaacLab provides GPU-accelerated simulation, clear limitations exist regarding sensor rendering: standard implementations struggle to simultaneously render complex moving articulations and static terrain while strictly isolating parallel environments. Specifically, distinct robots must not perceive â€ghostâ€ instances of other robots residing in different environments within the same physics scene.\nTo address this, we implement a custom, highly optimized ray-caster utilizing Nvidia Warp. To maximize memory efficiency, we employ mesh instancing; we collect a set of collision mesh prototypes aligned with critical articulations and maintain a global batch of transform matrices for all active instances. We introduce a collision grouping mechanism where static terrain is assigned a universal group ID (\nâˆ’\n1\n-1\n), which is visible to all agents. Conversely, each robot is assigned a unique collision group ID to ensure disjoint perception.\nAlgorithm 1\nMassively Parallel Grouped Ray-Casting\n1:\nRay inputs\nğ‘\n=\n{\n(\nğ¨\ni\n,\nğ\ni\n,\ng\ni\n)\n}\ni\n=\n1\nN\nr\nâ€‹\na\nâ€‹\ny\nâ€‹\ns\n\\mathbf{R}=\\{(\\mathbf{o}_{i},\\mathbf{d}_{i},g_{i})\\}_{i=1}^{N_{rays}}\n(origin, direction, group ID),\nMesh instances\nâ„³\n\\mathcal{M}\n,\nTransforms\nğ“\n\\mathbf{T}\n,\nMesh Group IDs\nğ’¢\nm\nâ€‹\ne\nâ€‹\ns\nâ€‹\nh\n\\mathcal{G}_{mesh}\n2:\nDepth map\nğƒ\nâˆˆ\nâ„\nN\nr\nâ€‹\na\nâ€‹\ny\nâ€‹\ns\n\\mathbf{D}\\in\\mathbb{R}^{N_{rays}}\n3:\nPhase 1: Acceleration Structure Construction (Pre-compute)\n4:\nInitialize hash map\nâ„‹\nm\nâ€‹\na\nâ€‹\np\n:\nGroup ID\nâ†’\nList of Mesh Indices\n\\mathcal{H}_{map}:\\text{Group ID}\\rightarrow\\text{List of Mesh Indices}\n5:\nfor\neach mesh instance\nm\nj\nâˆˆ\nâ„³\nm_{j}\\in\\mathcal{M}\ndo\n6:\ng\nâ€‹\ni\nâ€‹\nd\nâ†\nğ’¢\nm\nâ€‹\ne\nâ€‹\ns\nâ€‹\nh\nâ€‹\n[\nj\n]\ngid\\leftarrow\\mathcal{G}_{mesh}[j]\n7:\nAppend\nj\nj\nto\nâ„‹\nm\nâ€‹\na\nâ€‹\np\nâ€‹\n[\ng\nâ€‹\ni\nâ€‹\nd\n]\n\\mathcal{H}_{map}[gid]\n8:\nend\nfor\n9:\nDefine global static group\nG\ns\nâ€‹\nt\nâ€‹\na\nâ€‹\nt\nâ€‹\ni\nâ€‹\nc\nâ†\nâˆ’\n1\nG_{static}\\leftarrow-1\n10:\nPhase 2: Parallel Rendering (GPU Kernel)\n11:\nfor\neach ray\nr\ni\nâˆˆ\nğ‘\nr_{i}\\in\\mathbf{R}\nin parallel\ndo\n12:\nt\nm\nâ€‹\ni\nâ€‹\nn\nâ†\nâˆ\nt_{min}\\leftarrow\\infty\n13:\ng\na\nâ€‹\ng\nâ€‹\ne\nâ€‹\nn\nâ€‹\nt\nâ†\nr\ni\n.\ng\ni\ng_{agent}\\leftarrow r_{i}.g_{i}\n14:\nâŠ³\n\\triangleright\n1. Check Global Static Terrain\n15:\nfor\neach mesh index\ni\nâ€‹\nd\nâ€‹\nx\nidx\nin\nâ„‹\nm\nâ€‹\na\nâ€‹\np\nâ€‹\n[\nG\ns\nâ€‹\nt\nâ€‹\na\nâ€‹\nt\nâ€‹\ni\nâ€‹\nc\n]\n\\mathcal{H}_{map}[G_{static}]\ndo\n16:\nt\nâ†\nIntersect\nâ€‹\n(\nr\ni\n,\nâ„³\nâ€‹\n[\ni\nâ€‹\nd\nâ€‹\nx\n]\n,\nğ“\nâ€‹\n[\ni\nâ€‹\nd\nâ€‹\nx\n]\n)\nt\\leftarrow\\textsc{Intersect}(r_{i},\\mathcal{M}[idx],\\mathbf{T}[idx])\n17:\nif\nt\n<\nt\nm\nâ€‹\ni\nâ€‹\nn\nt<t_{min}\nthen\nt\nm\nâ€‹\ni\nâ€‹\nn\nâ†\nt\nt_{min}\\leftarrow t\n18:\nend\nif\n19:\nend\nfor\n20:\nâŠ³\n\\triangleright\n2. Check Agent-Specific Dynamic Objects (Skip others)\n21:\nfor\neach mesh index\ni\nâ€‹\nd\nâ€‹\nx\nidx\nin\nâ„‹\nm\nâ€‹\na\nâ€‹\np\nâ€‹\n[\ng\na\nâ€‹\ng\nâ€‹\ne\nâ€‹\nn\nâ€‹\nt\n]\n\\mathcal{H}_{map}[g_{agent}]\ndo\n22:\nt\nâ†\nIntersect\nâ€‹\n(\nr\ni\n,\nâ„³\nâ€‹\n[\ni\nâ€‹\nd\nâ€‹\nx\n]\n,\nğ“\nâ€‹\n[\ni\nâ€‹\nd\nâ€‹\nx\n]\n)\nt\\leftarrow\\textsc{Intersect}(r_{i},\\mathcal{M}[idx],\\mathbf{T}[idx])\n23:\nif\nt\n<\nt\nm\nâ€‹\ni\nâ€‹\nn\nt<t_{min}\nthen\nt\nm\nâ€‹\ni\nâ€‹\nn\nâ†\nt\nt_{min}\\leftarrow t\n24:\nend\nif\n25:\nend\nfor\n26:\nğƒ\nâ€‹\n[\ni\n]\nâ†\nt\nm\nâ€‹\ni\nâ€‹\nn\n\\mathbf{D}[i]\\leftarrow t_{min}\n27:\nend\nfor\n28:\nreturn\nğƒ\n\\mathbf{D}\nA naive implementation would require every cast ray to iterate through the global table of mesh group IDs to determine visibility, resulting in a complexity linear to the total number of meshes in the scene\nO\nâ€‹\n(\nN\n)\nO(N)\n. With tens of thousands of articulation parts in massively parallel training, this becomes a severe bottleneck. We propose a\nPrecomputed Grouped Ray-Casting\nstrategy to accelerate this process, as described in Algorithm\n1\n. We pre-compute a mapping from collision group indices to mesh instance IDs. Consequently, during the ray-marching phase, a ray associated with a specific agent iterates solely over the global static meshes and the specific subset of dynamic meshes belonging to that agentâ€™s group. This reduces the search space significantly, resulting in a\n10\nÃ—\n10\\times\nincrease in rendering speed compared to the naive baseline.\nIII-B\nTraining settings\nWe train the policy using both proprioception and exteroception, specifically through depth image. Following BeyondMimic\n[\n12\n]\nand VMP\n[\n20\n]\n, we design the reward terms by tracking the robot local poses in the relative frame and the global pose of the robot root link.\nRelative Frame\nBy defining the relative frame\nT\nrel\nT_{\\text{rel}}\n, we define the reference root transform\nT\nref\nT_{\\text{ref}}\nas the transform of the root link in the motion reference, and the robot actual root transform\nT\nrobot\nT_{\\text{robot}}\n. The relative frame\nT\nrel\nT_{\\text{rel}}\nis in between.\nT\nrel\nT_{\\text{rel}}\nhas the same x-y coordinate as\nT\nrobot\nT_{\\text{robot}}\nbut the same z coordinate as\nT\nref\nT_{\\text{ref}}\n.\nT\nrel\nT_{\\text{rel}}\nhas the same roll-pitch angle as\nT\nref\nT_{\\text{ref}}\nbut the same yaw angle as\nT\nrobot\nT_{\\text{robot}}\n.\nFigure 2:\nWe illustrate the basic concept of relative frame. It has the same x-y and yaw coordinates as the robot base frame and has the same z, roll and pitch coordinate in the reference frame.\nMathematically, we define\nT\nrel\nT_{\\text{rel}}\nas a\n4\nÃ—\n4\n4\\times 4\nmatrix where\nT\nrel\n=\n[\nR\nrel\nğ©\nrel\nğŸ\n1\n]\nT_{\\text{rel}}=\\begin{bmatrix}R_{\\text{rel}}&\\mathbf{p}_{\\text{rel}}\\\\\n\\mathbf{0}&1\\end{bmatrix}\nwhere the translation\nğ©\nrel\n\\mathbf{p}_{\\text{rel}}\ncombines the planar position of the robot with the vertical position of the reference,\nğ©\nrel\n=\n[\nx\nrobot\ny\nrobot\nz\nref\n]\n\\mathbf{p}_{\\text{rel}}=\\begin{bmatrix}x_{\\text{robot}}\\\\\ny_{\\text{robot}}\\\\\nz_{\\text{ref}}\\end{bmatrix}\nand the rotation\nR\nrel\nR_{\\text{rel}}\ncombines the heading (yaw) of the robot with the tilt (roll and pitch) of the reference\nR\nrel\n=\nR\nz\nâ€‹\n(\nÏˆ\nrobot\n)\nâ€‹\nR\ny\nâ€‹\n(\nÎ¸\nref\n)\nâ€‹\nR\nx\nâ€‹\n(\nÏ•\nref\n)\nR_{\\text{rel}}=R_{z}(\\psi_{\\text{robot}})\\,R_{y}(\\theta_{\\text{ref}})\\,R_{x}(\\phi_{\\text{ref}})\nMore intuitive illustration is shown in Figure\n2\n.\nRewards Definition\nWe adopt the policy training using asymmetric PPO. We feed the policy with the synchronized future joint position reference, future joint velocity reference, future root position reference (in reference transform) and future robot rotation reference (in actual robot root transform). Then we feed the policy with one frame of noised depth image and a historical proprioception observation with 8 frames. For critic network, we feed the network with the synchronized future joint position reference, future joint velocity reference, future root position reference (in the actual robot root transform), future robot rotation reference (in the actual robot root transform), actual robot link transforms (in robotâ€™s root transform), height-scan to the terrain, and a historical proprioception observation with 8 frames.\nFollowing BeyondMimic\n[\n12\n]\n, we train with simplified tracking rewards as follows:\nâ€¢\nGlobal root position reward based on distance to the reference position.\nâ€¢\nGlobal root rotation reward based on rotation difference to the reference rotation in axis angle.\nâ€¢\nLocal link position reward based on key links position differences in relative frame.\nâ€¢\nLocal link rotation reward based on key links rotation differences in relative frame in the magnitude of axis angle.\nâ€¢\nGlobal link linear velocity reward based on the key links linear velocity difference in the world frame.\nâ€¢\nGlobal link angular velocity reward based on the key links angular velocity difference in the world frame.\nâ€¢\nAction rate penalty.\nâ€¢\nJoint position limit penalty.\nâ€¢\nUndesired contact penalty.\nâ€¢\nApplied torque limits penalty.\nAdaptive Sampling\nAlgorithm 2\nAdaptive Sampling with Failure-Based Curriculum\n1:\nSet of reference motions\nâ„³\n=\n{\nm\n1\n,\nâ€¦\n,\nm\nK\n}\n\\mathcal{M}=\\{m_{1},\\dots,m_{K}\\}\n2:\nBin duration\nÎ”\nâ€‹\nt\nm\nâ€‹\na\nâ€‹\nx\n=\n1.0\nâ€‹\ns\n\\Delta t_{max}=1.0s\n3:\nSmoothing kernel\nğ’¦\n\\mathcal{K}\n(e.g., Exponential)\n4:\nInitialize:\nDiscretize each\nm\nk\nm_{k}\ninto bins\nB\nk\n=\n{\nb\nk\n,\n0\n,\nâ€¦\n,\nb\nk\n,\nT\nk\n}\nB_{k}=\\{b_{k,0},\\dots,b_{k,T_{k}}\\}\n5:\nInitialize:\nFailure counts\nF\nk\n,\nt\nâ†\n0\nF_{k,t}\\leftarrow 0\nand Sampling Weights\nW\nk\n,\nt\nâ†\nUniform\nW_{k,t}\\leftarrow\\text{Uniform}\n6:\nfor\neach training iteration\ndo\n7:\nâŠ³\n\\triangleright\n1. Adaptive Selection\n8:\nNormalize weights:\nP\nâ€‹\n(\nk\n,\nt\n)\nâ†\nW\nk\n,\nt\nâˆ‘\nk\nâ€²\n,\nt\nâ€²\nW\nk\nâ€²\n,\nt\nâ€²\nP(k,t)\\leftarrow\\frac{W_{k,t}}{\\sum_{k^{\\prime},t^{\\prime}}W_{k^{\\prime},t^{\\prime}}}\n9:\nSample motion index\nk\nk\nand start time\nt\ns\nâ€‹\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\nt\nâˆ¼\nP\nâ€‹\n(\nk\n,\nt\n)\nt_{start}\\sim P(k,t)\n10:\nâŠ³\n\\triangleright\n2. Policy Rollout\n11:\nInitialize robot state at\nS\nt\ns\nâ€‹\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\nt\nS_{t_{start}}\nfrom motion\nm\nk\nm_{k}\n12:\nExecute policy\nÏ€\n\\pi\nuntil completion or failure at\nt\nf\nâ€‹\na\nâ€‹\ni\nâ€‹\nl\nt_{fail}\n13:\nif\nFailure occurred at\nt\nf\nâ€‹\na\nâ€‹\ni\nâ€‹\nl\nt_{fail}\nthen\n14:\nIdentify discrete bin index\nt\n^\n\\hat{t}\ncorresponding to\nt\nf\nâ€‹\na\nâ€‹\ni\nâ€‹\nl\nt_{fail}\n15:\nF\nk\n,\nt\n^\nâ†\nF\nk\n,\nt\n^\n+\n1\nF_{k,\\hat{t}}\\leftarrow F_{k,\\hat{t}}+1\nâŠ³\n\\triangleright\nIncrement raw failure count\n16:\nâŠ³\n\\triangleright\n3. Update Sampling Distribution\n17:\nfor\neach bin\nj\nj\nin motion\nm\nk\nm_{k}\ndo\n18:\nApply smoothing:\nF\n~\nk\n,\nj\nâ†\nâˆ‘\ni\nF\nk\n,\ni\nâ‹…\nğ’¦\nâ€‹\n(\nj\n,\ni\n)\n\\tilde{F}_{k,j}\\leftarrow\\sum_{i}F_{k,i}\\cdot\\mathcal{K}(j,i)\n19:\nend\nfor\n20:\nUpdate weights:\nW\nk\n,\nt\nâ†\nF\n~\nk\n,\nt\n+\nÏµ\nW_{k,t}\\leftarrow\\tilde{F}_{k,t}+\\epsilon\nâŠ³\n\\triangleright\nAdd\nÏµ\n\\epsilon\nto ensure non-zero probability\n21:\nend\nif\n22:\nend\nfor\nWe formulate the training environment as a Markov Decision Process (MDP) and employ an adaptive sampling strategy to facilitate curriculum learning. To handle variable-length reference trajectories, we discretize each motion into a sequence of temporal bins with a maximum duration of\nt\nbin\n=\n1.0\nâ€‹\ns\nt_{\\text{bin}}=1.0s\n. This standardization allows us to treat distinct motion segments uniformly, regardless of the total trajectory length. During training rollouts, we track the agentâ€™s performance; if a termination or failure occurs, we increment the failure counter for the bin corresponding to the failure timestep. To maintain training stability and prevent abrupt distribution shifts, we apply temporal smoothing to these raw failure counts. This operation converts discrete integer counts into continuous failure scores, effectively diffusing the difficulty signal to adjacent timesteps. Finally, we utilize these smoothed scores to construct a probability distribution over all bins across all motions. The training reset state (which motion and what start time) is then sampled based on these weights, ensuring the agent prioritizes high-failure scenarios. The detailed implementation logic is shown in Algorithm\n2\n.\nEarly Timeout with Stuck Detection\nHowever, the randomization terms are not fully applicable to uniform randomization across the entire motion sequence. For example, the uniform randomization in sampling the robot state from the reference trajectory will place the robot in the middle of some terrain. This generates unsolvable situations for the policy to learn from. To resolve this issue, we truncate the trajectories that have caused the robot to get stuck over a given time limit at the start of the episode.\nNetwork inputs\nSince we design the training framework in one stage, the policy network does not have any odometry-related information. While based on the principle of asymmetric actor critic design, the critic network will have odometry-related information for more accurate value function prediction. Referring to\nZhuang and Zhao [\n26\n], Chen etÂ al. [\n4\n], Ze etÂ al. [\n24\n]\n, we train both policy network and critic network with 10 frames of future reference frame. Each frame has 0.1s interval with respect to the previous frame, which sums up to 1.0s future motion expectation.\nFor policy network, the inputs contain:\nâ€¢\nFuture joint position reference sequence.\nâ€¢\nFuture joint velocity reference sequence.\nâ€¢\nFuture base position reference sequence with respect to the base position of the reference frame.\nâ€¢\nFuture base rotation reference sequence with respect to the base rotation of the actual robot in simulator.\nâ€¢\nSingle frame of noised depth image from the robotâ€™s camera.\nâ€¢\n8 frames of historical proprioception.\nFor critic network, the inputs contain:\nâ€¢\nFuture joint position reference sequence.\nâ€¢\nFuture joint velocity reference sequence.\nâ€¢\nFuture base position reference sequence with respect to the base position of the actual robot in simulator.\nâ€¢\nActual key links position in robotâ€™s base frame.\nâ€¢\nActual key links rotation (tan-norm) in robotâ€™s base frame.\nâ€¢\nHeight scan data.\nâ€¢\n8 frames of historical proprioception.\nNetwork architecture\nSince the motion tracking task is only spread across a handful number of motions, we use a straight-forward design, with a CNN encoder encoding the depth image and feed to a 3 layer MLP network together with the proprioception history. The detailed parameters are listed in the appendix.\nIII-C\nBridging sim-to-real gap in depth perception\nFigure 3:\nTo bridge the sim-to-real gap in depth visualization, we apply several noise patterns in simulation and applied inpainting algorithm from GPU-based OpenCV implementation.\nIt should be noted that we train our policy using synchronized depth image observations. We deploy the policy using 50Hz depth image using RealSense. However, the builtin filtering algorithm from pyrealsense does not meet the 50Hz requirement on the Nvidia Jetsonâ€™s CPU. We use the inpainting algorithm from a GPU-based OpenCV implementation. Due to occasionally artifacts pattern in the real world because of reflection, motion blur or stereo algorithm error, we add gaussian noise, patched artifacts to the depth image in simulation similar to\nZhuang etÂ al. [\n27\n,\n28\n]\n. As shown in Figure\n3\n, the depth images fed into the network between simulation and real-world are quite similar in terms of pixel pattern.\nIII-D\nDeployment\nTo deploy the policy onboard with depth sensing running at 50Hz, we adopt ONNX as our neural network accelerator. Even though the motions are trained to a single depth-based neural network, we still need to choose the motion reference as input. Thus, we implemented a state machine mechanism to select the motion during deployment.\nAlso, to generate running logs during the real-world experiments while reducing the impact on the neural network process as low as possible, we run rosbag recording in a separate process. Also, we bind the CPUs separately for running the neural network and rosbag recording process to prevent further CPU scheduling.\nConsidering the policy is trained with root position reference in motion reference frame, this part of the observation does not depend on the odometry of the robot, neither in simulation nor in the real-world. Thus, we do not need any odometry system providing global position error between the robot and the motion reference. We provide the exact same joint position, joint velocity reference to the policy as it is trained. We provide the relative rotation difference based on the real-time IMU readings of the robot, before which the start heading of the motion reference was aligned to the robotâ€™s actual heading. After correcting the heading of the motion reference, as well as the direction of the reference position sequence, we provide the policy with only the position reference relative to the frame of reference position without any odometry information.\nIV\nExperiments\nIn this section, we aim to raise and answer some questions, addressing the pros and cons of adding exteroception to the highly agile motion tracking system. Also, to test the generalization across multiple terrains and multiple motions in the same terrain, we curate 4 motions across 3 different terrains. One is the triangular barrier formed by two road ramps. One is a big wooden box the size of\n0.5\nâ€‹\nm\nÃ—\n0.6\nâ€‹\nm\nÃ—\n0.4\nâ€‹\nm\n0.5m\\times 0.6m\\times 0.4m\n. The last terrain is only the flat terrain, as the complementary experiment to show that the system can still perform non-interactive motions in flat terrain. The 4 motions consist of â€˜kneel climbâ€™, â€˜roll vaultâ€™, â€˜dive rollâ€™, and â€˜jump sitâ€™.\nWe first run real-world experiments to the effectiveness of our pipeline that can be successfully deployed in the real robot, then we analyze the effect of adding depth vision to the system by proposing the following questions:\n1.\nWhat benefit does the additional depth vision provide to this agile whole-body control system that interacts with the unstructured terrain?\n2.\nAre there any drawbacks when depth vision is provided as additional information? Is it robust to unseen but trivial distractors in the scene?\n3.\nThroughout the entire training pipeline, what contributes the most to the successful sim-to-real deployment on the real robot without any odometry system?\nIV-A\nReal world experiment\nIn the real world experiment, we deploy our entire inference system using ROS2 on a Unitree G1 29-DOF, September 2025 version. The neural network is accelerated using ONNX (CPU version) onboard. We acquire the depth image using an Intel RealSense D435i installed on the head of the Unitree G1 in an individual process separate from the network inference process. We integrate a simple walking process to keep balance when the perceptive parkour policy is not activated. At each test, we control the robot to walk to a rough starting point with no accurate odometry or localization system. We then trigger the motion tracking policy with a specific motion reference trajectory. The system switches back to a walking policy when the motion reference trajectory is exhausted.\nWe perform real-world tests both indoors and outdoors. With indoor environments, we were able to collect running logs using a network cable from another computer. In outdoor environments, we only connect to the onboard compute without an additional logging system. Please notice that, in outdoor environments, the starting point of the motion is not determined. We only put the robot in front of the obstacle without any further calibration.\nIV-B\nNon-trivial benefit of depth vision\nTABLE I:\nComparison of training variants across four motions. Each motion is evaluated using two metrics (\nMPJPE\ng\n\\text{MPJPE}_{g}\nand\nMPJPE\nb\n\\text{MPJPE}_{b}\n).\nTraining Variant\nDive Roll\nKneel Climb\nRoll Vault\nJump Sit\nMPJPE\ng\n\\text{MPJPE}_{g}\nMPJPE\nb\n\\text{MPJPE}_{b}\nMPJPE\ng\n\\text{MPJPE}_{g}\nMPJPE\nb\n\\text{MPJPE}_{b}\nMPJPE\ng\n\\text{MPJPE}_{g}\nMPJPE\nb\n\\text{MPJPE}_{b}\nMPJPE\ng\n\\text{MPJPE}_{g}\nMPJPE\nb\n\\text{MPJPE}_{b}\nTesting with no position randomization\nw/o depth (BeyondMimic)\n0.1365\n0.0462\n0.1162\n0.0378\n0.1109\n0.0358\n0.1909\n0.0341\nw/o stuck detection\n0.1367\n0.0464\n0.1073\n0.0394\n0.1162\n0.0393\n0.1170\n0.0389\nlocal frame reward\n0.1213\n0.0242\n0.1145\n0.0383\n0.1131\n0.0332\n0.1358\n0.0299\nours\n0.1468\n0.0459\n0.1063\n0.0388\n0.1123\n0.0351\n0.1197\n0.0362\nTesting with position randomization\nw/o depth (BeyondMimic)\n0.2267\n0.0477\n0.1822\n0.0405\n0.1969\n0.0385\n0.2693\n0.0377\nw/o stuck detection\n0.1661\n0.0456\n0.1449\n0.0412\n0.1461\n0.0316\n0.1322\n0.0354\nlocal frame reward\n0.1892\n0.0389\n0.1477\n0.0402\n0.1453\n0.0311\n0.1312\n0.0301\nours\n0.1649\n0.0477\n0.1218\n0.0399\n0.1421\n0.0371\n0.1446\n0.0382\nWe investigate how exteroception contributes to the success and generalization of these agile whole-body motions. We uniformly spawn the robot around the initial starting position of the reference motion to test to what extent the policy is able to recover and successfully finish the motion. As shown in Figure\n6\nwe visualize the convergence of the robots in a batch, which start at different positions. As time goes on, all robots converge to the target obstacle and successfully finish the motion tracking task. Even though all trainings randomize the starting position at\n0.3\nâ€‹\nm\nÃ—\n0.3\nâ€‹\nm\n0.3m\\times 0.3m\nhorizontally around the reference motion. We test the motion convergence behavior with the range of\n0.5\nâ€‹\nm\nÃ—\n0.5\nâ€‹\nm\n0.5m\\times 0.5m\nhorizontally around the reference starting point.\nFigure 4:\nWe show the\nx\nâˆ’\ny\nx-y\nposition variance in a single batch of motion reference example to illustrate the emergence of positional correction ability when introducing depth vision to the end-to-end motion tracking system.\nWe then plot the position variance in x-y coordinate to further verify this observation in Figure\n4\n. The position variance drastically converges to a really low value. When the motion reaches its final stage, some of the position variances increase due to the common visual features from the depth camera.\nFigure 5:\nWe do a grid search around the starting position of the motion reference frame. We plot the headmap of the success rate of each motion at a\n1.2\nâ€‹\nm\nÃ—\n1.2\nâ€‹\nm\n1.2m\\times 1.2m\nspace. Red suggests the\n100\n%\n100\\%\nsuccess rate, while dark blue suggests the\n0\n%\n0\\%\nsuccess rate.\nTo further test the generalization ability of this visual-guided motion tracking pipeline, we perform a more aggressive out-of-distribution tests, even though the randomization during training is only from\nâˆ’\n0.15\nâ€‹\nm\n-0.15m\nto\n+\n0.15\nâ€‹\nm\n+0.15m\nin\nx\nx\nand\ny\ny\ncoordinate. We plot the success rate headmap from\nâˆ’\n0.6\nâ€‹\nm\n-0.6m\nto\n+\n0.6\nâ€‹\nm\n+0.6m\naround the initial motion reference starting point. This forms a\n1.2\nâ€‹\nm\nÃ—\n1.2\nâ€‹\nm\n1.2m\\times 1.2m\nrange of initialization. As shown in Figure\n5\n, the success rate of the motion tracking system is still\n100\n%\n100\\%\n. It only drops at the boundary of these experiments.\nFigure 6:\nWe show a series of direct example, illustrating the positional convergence when introducing depth vision in this end-to-end motion tracking system. As shown in this figure, the position converges automatically even before the scene interaction. Otherwise, for difficult motions like roll-vaulting and kneel-climbing, misplace at the scale of\n0.4\nâ€‹\nm\n0.4m\nwill lead to catastrophic failure of the scene-interaction task.\nIV-C\nVision robustness\nIn this section, we aim to study the robustness of this additional depth information in the motion tracking system. We build several scenes in the simulator that have never been used during training. They are the out-of-distribution cases for the trained motion tracking network. Considering the reference motions are still the same, the added entities in the scene should block the robotâ€™s dynamics as little as possible.\nFigure 7:\nHere we provide examples of adding distractors in the scene to test the robustness of adding unseen objects in the middle of the motion trajectory. We define â€œwide distractorâ€ (top-left), â€œplane distractorâ€ (bottom-left) and â€œwall distractorâ€ (right). They present different visual sensor information while not disrupting the original motion trajectory too much.\nAs shown in Figure\n7\n, we construct different objects in the scene as distractors and make sure the robot sees the additional objects. Some of the distractors do disrupt the dynamics of the entire motion, e.g, the bottom left example. We tested different variants of distractors and observed no drop in the success rate when running the same motion tracking system.\nTABLE II:\nMotion Tracking Evaluation: MPJPE-Global\nMPJPE\ng\n\\text{MPJPE}_{g}\nand MPJPE-Base\nMPJPE\nb\n\\text{MPJPE}_{b}\nacross different distractor conditions.\nDistractor Condition\nKneel Climb\nRoll Vault\nMPJPE\ng\n\\text{MPJPE}_{g}\nMPJPE\nb\n\\text{MPJPE}_{b}\nMPJPE\ng\n\\text{MPJPE}_{g}\nMPJPE\nb\n\\text{MPJPE}_{b}\nNo Distractor\n0.1022\n0.0390\n0.1159\n0.0349\nWide Distractor\n0.4848\n0.0405\n0.1543\n0.0373\nPlane Distractor\n0.4261\n0.0417\n0.5598\n0.0809\nWall Distractor\n0.1603\n0.0401\n0.1157\n0.0372\nWe focus on 2 motions that are suitable to add distractors while not significantly blocking the dynamics of the motion trajectory. As shown in Table\nII\n, we test the MPJPE in both the base frame and the world frame compared with the original terrain used in training. In total, there are 4 variants for both roll-vaulting and kneel-climbing. For each variant and the original training scene, we run a batch of 100 robots using the same policy network in the simulator at once. To make a fair comparison, we remove randomization when initializing the robot in the scene. As shown in Table\nII\n, only the plane distractor significantly increase the MPJPE metric, since the long platform changes the potential motion dynamics compared to the original reference motion. For the wide distractor, it also leads to larger MPJPE. By analyzing the motion, the wide beam disrupts the localization ability of the motion tracking policy. Thus, the MPJPE metric increases.\nIV-D\nAblation on the training recipe\nIn this section, we run ablation studies on several critical components of the entire training framework. There are several critical components in our training framework:\n1.\nTraining with depth input.\n2.\nRandomizing the initialization strategy with stuck detection, which prevents physically impossible rollouts.\n3.\nDefining local tracking rewards in the relative frame instead of the robotâ€™s base frame.\nAs shown in Table\nI\n, training motion tracking without exteroception is not robust to initial position perturbation. The starting position has to be exact, or it may face catastrophic failure during scene interaction. When trained only with local frame reward instead of the reward in the relative frame, the policy tends to track more accurate locally, but performs worse than our pipeline globally in interaction intensive cases. Also, we show the non-trivial benefit of applying stuck detection mechanism to prevent large portion of rollout steps becoming useless data samples. Training without stuck detection leads to slightly higher MPJPE error, while still successfully finished the entire motion tracking trajectory.\nV\nConclusion\nIn this work, we present a generalizable paradigm for humanoids to learn agile, physics-based interactions directly from human demonstrations and onboard sensing. We advance beyond standard motion tracking by unifying it with perceptive control into a\nsingle scalable training framework\n. Our analysis highlights the non-trivial benefits of depth vision for robustness against environmental distractors and validates the system through successful deployment in diverse indoor and outdoor real-world scenarios.\nFurthermore, by enabling fully end-to-end training with exteroception, this framework provides the critical infrastructure required for scaling up the training system of humanoid whole-body control. This end-to-end approach unlocks the potential to train extensive libraries of motion skills with intensive scene interaction. While challenges regarding data volume and autonomous skill selection remain, they outline a clear path for future research. Ultimately, this work establishes a baseline for developing general-purpose humanoid controllers capable of mastering contact-rich environments in a fully learned, end-to-end manner.\nAcknowledgments\nThis work is accomplished with the help of Xiangting Meng, Siqiao Huang, Yizhuo Gao, and Tairan He.\nReferences\nAgarwal etÂ al. [2022]\nAnanye Agarwal, Ashish Kumar, Jitendra Malik, and Deepak Pathak.\nLegged Locomotion in Challenging Terrains using Egocentric Vision.\nIn\n6th Annual Conference on Robot Learning\n, 2022.\nURL\nhttps://openreview.net/forum?id=Re3NjSwf0WF\n.\nAgarwal etÂ al. [2023]\nAnanye Agarwal, Ashish Kumar, Jitendra Malik, and Deepak Pathak.\nLegged Locomotion in Challenging Terrains using Egocentric Vision.\nIn Karen Liu, Dana Kulic, and Jeff Ichnowski, editors,\nProceedings of The 6th Conference on Robot Learning\n, volume 205 of\nProceedings of Machine Learning Research\n, pages 403â€“415. PMLR, December 2023.\nURL\nhttps://proceedings.mlr.press/v205/agarwal23a.html\n.\nAraujo etÂ al. [2025]\nJoaoÂ Pedro Araujo, Yanjie Ze, Pei Xu, Jiajun Wu, and C.Â Karen Liu.\nRetargeting Matters: General Motion Retargeting for Humanoid Motion Tracking.\narXiv preprint arXiv:2510.02252\n, 2025.\nChen etÂ al. [2025]\nZixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, XueÂ Bin Peng, and Xiaolong Wang.\nGMT: General Motion Tracking for Humanoid Whole-Body Control.\narXiv:2506.14770\n, 2025.\nDuan etÂ al. [2022]\nHelei Duan, Ashish Malik, Jeremy Dao, Aseem Saxena, Kevin Green, Jonah Siekmann, Alan Fern, and Jonathan Hurst.\nSim-to-Real Learning of Footstep-Constrained Bipedal Dynamic Walking, 2022.\nURL\nhttps://arxiv.org/abs/2203.07589\n.\n_eprint: 2203.07589.\nHe etÂ al. [2024a]\nTairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi.\nLearning human-to-humanoid real-time whole-body teleoperation.\narXiv preprint arXiv:2403.04436\n, 2024a.\nHe etÂ al. [2024b]\nTairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi Fan, and Yuke Zhu.\nHOVER: Versatile Neural Whole-Body Controller for Humanoid Robots.\narXiv preprint arXiv:2410.21229\n, 2024b.\nHwangbo etÂ al. [2019]\nJemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter.\nLearning agile and dynamic motor skills for legged robots.\nScience Robotics\n, 2019.\nKumar etÂ al. [2021]\nAshish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik.\nRma: Rapid motor adaptation for legged robots.\nIn\nRobotics: Science and Systems\n, 2021.\nLee etÂ al. [2020]\nJoonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter.\nLearning quadrupedal locomotion over challenging terrain.\nScience Robotics\n, 2020.\nLi etÂ al. [2023]\nJiaman Li, Jiajun Wu, and C.Â Karen Liu.\nObject Motion Guided Human Motion Synthesis.\nACM Trans. Graph.\n, 42(6), December 2023.\nISSN 0730-0301.\ndoi:\n10.1145/3618333\n.\nURL\nhttps://doi.org/10.1145/3618333\n.\nPlace: New York, NY, USA Publisher: Association for Computing Machinery.\nLiao etÂ al. [2025]\nQiayuan Liao, TakaraÂ E. Truong, Xiaoyu Huang, Yuman Gao, Guy Tevet, Koushil Sreenath, and C.Â Karen Liu.\nBeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion, 2025.\nURL\nhttps://arxiv.org/abs/2508.08241\n.\n_eprint: 2508.08241.\nLong etÂ al. [2024]\nJunfeng Long, Junli Ren, Moji Shi, Zirui Wang, Tao Huang, Ping Luo, and Jiangmiao Pang.\nLearning Humanoid Locomotion with Perceptive Internal Model, 2024.\nURL\nhttps://arxiv.org/abs/2411.14386\n.\n_eprint: 2411.14386.\nLuo etÂ al. [2023]\nZhengyi Luo, Jinkun Cao, AlexanderÂ W. Winkler, Kris Kitani, and Weipeng Xu.\nPerpetual Humanoid Control for Real-time Simulated Avatars.\nIn\nInternational Conference on Computer Vision (ICCV)\n, 2023.\nMahmood etÂ al. [2019]\nNaureen Mahmood, Nima Ghorbani, NikolausÂ F. Troje, Gerard Pons-Moll, and MichaelÂ J. Black.\nAMASS: Archive of Motion Capture as Surface Shapes.\nIn\nInternational Conference on Computer Vision\n, pages 5442â€“5451, October 2019.\nMiki etÂ al. [2022]\nTakahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter.\nLearning robust perceptive locomotion for quadrupedal robots in the wild.\nScience Robotics\n, 7(62):eabk2822, 2022.\ndoi:\n10.1126/scirobotics.abk2822\n.\nURL\nhttps://www.science.org/doi/abs/10.1126/scirobotics.abk2822\n.\n_eprint: https://www.science.org/doi/pdf/10.1126/scirobotics.abk2822.\nNahrendra etÂ al. [2023]\nI.Â MadeÂ Aswin Nahrendra, Byeongho Yu, and Hyun Myung.\nDreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning, March 2023.\nURL\nhttp://arxiv.org/abs/2301.10602\n.\narXiv:2301.10602 [cs].\nPeng etÂ al. [2018]\nXueÂ Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel vanÂ de Panne.\nDeepMimic: Example-guided Deep Reinforcement Learning of Physics-based Character Skills.\nACM Trans. Graph.\n, 37(4):143:1â€“143:14, July 2018.\nISSN 0730-0301.\ndoi:\n10.1145/3197517.3201311\n.\nURL\nhttp://doi.acm.org/10.1145/3197517.3201311\n.\nPlace: New York, NY, USA Publisher: ACM.\nPeng etÂ al. [2021]\nXueÂ Bin Peng, ZeÂ Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa.\nAMP: adversarial motion priors for stylized physics-based character control.\nACM Trans. Graph.\n, 40(4), July 2021.\nISSN 0730-0301.\ndoi:\n10.1145/3450626.3459670\n.\nURL\nhttps://doi.org/10.1145/3450626.3459670\n.\nPlace: New York, NY, USA Publisher: Association for Computing Machinery.\nSerifi etÂ al. [2024]\nAgon Serifi, Ruben Grandia, Espen Knoop, Markus Gross, and Moritz BÃ¤cher.\nVMP: Versatile Motion Priors for Robustly Tracking Motion on Physical Characters.\nIn\nProceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation\n, SCA â€™24, pages 1â€“11, Montreal, Quebec, Canada, 2024. Eurographics Association.\ndoi:\n10.1111/cgf.15175\n.\nISSN: 1467-8659.\nTan etÂ al. [2018]\nJie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke.\nSim-to-Real: Learning Agile Locomotion For Quadruped Robots.\n2018.\nURL\nhttps://arxiv.org/pdf/1804.10332.pdf\n.\nXue etÂ al. [2025]\nYufei Xue, Wentao Dong, Minghuan Liu, Weinan Zhang, and Jiangmiao Pang.\nA Unified and General Humanoid Whole-Body Controller for Fine-Grained Locomotion.\nIn\nRobotics: Science and Systems (RSS)\n, 2025.\nYang etÂ al. [2019]\nYuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, and Vikas Sindhwani.\nData Efficient Reinforcement Learning for Legged Robots.\narXiv:1907.03613 [cs]\n, October 2019.\nURL\nhttp://arxiv.org/abs/1907.03613\n.\narXiv: 1907.03613.\nZe etÂ al. [2025]\nYanjie Ze, Zixuan Chen, JoÃ£oÂ Pedro AraÃºjo, Zi-ang Cao, XueÂ Bin Peng, Jiajun Wu, and C.Â Karen Liu.\nTWIST: Teleoperated Whole-Body Imitation System, May 2025.\nURL\nhttp://arxiv.org/abs/2505.02833\n.\narXiv:2505.02833 [cs].\nZhang etÂ al. [2024]\nChong Zhang, Nikita Rudin, David Hoeller, and Marco Hutter.\nLearning Agile Locomotion on Risky Terrains.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 11864â€“11871, 2024.\ndoi:\n10.1109/IROS58592.2024.10801909\n.\nZhuang and Zhao [2025]\nZiwen Zhuang and Hang Zhao.\nEmbrace Contacts: humanoid shadowing with full body ground contacts.\nIn Joseph Lim, Shuran Song, and Hae-Won Park, editors,\nProceedings of The 9th Conference on Robot Learning\n, volume 305 of\nProceedings of Machine Learning Research\n, pages 3720â€“3735. PMLR, September 2025.\nURL\nhttps://proceedings.mlr.press/v305/zhuang25b.html\n.\nZhuang etÂ al. [2023]\nZiwen Zhuang, Zipeng Fu, Jianren Wang, ChristopherÂ G. Atkeson, SÃ¶ren Schwertfeger, Chelsea Finn, and Hang Zhao.\nRobot Parkour Learning.\nIn Jie Tan, Marc Toussaint, and Kourosh Darvish, editors,\nProceedings of The 7th Conference on Robot Learning\n, volume 229 of\nProceedings of Machine Learning Research\n, pages 73â€“92. PMLR, November 2023.\nURL\nhttps://proceedings.mlr.press/v229/zhuang23a.html\n.\nZhuang etÂ al. [2024]\nZiwen Zhuang, Shenzhe Yao, and Hang Zhao.\nHumanoid Parkour Learning.\nIn Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors,\nProceedings of The 8th Conference on Robot Learning\n, volume 270 of\nProceedings of Machine Learning Research\n, pages 1975â€“1991. PMLR, November 2024.\nURL\nhttps://proceedings.mlr.press/v270/zhuang25a.html\n.",
    "preview_text": "Current approaches to humanoid control generally fall into two paradigms: perceptive locomotion, which handles terrain well but is limited to pedal gaits, and general motion tracking, which reproduces complex skills but ignores environmental capabilities. This work unites these paradigms to achieve perceptive general motion control. We present a framework where exteroceptive sensing is integrated into whole-body motion tracking, permitting a humanoid to perform highly dynamic, non-locomotion tasks on uneven terrain. By training a single policy to perform multiple distinct motions across varied terrestrial features, we demonstrate the non-trivial benefit of integrating perception into the control loop. Our results show that this framework enables robust, highly dynamic multi-contact motions, such as vaulting and dive-rolling, on unstructured terrain, significantly expanding the robot's traversability beyond simple walking or running. https://project-instinct.github.io/deep-whole-body-parkour\n\nDeep Whole-body Parkour\nZiwen Zhuang\n12\n,\nShaoting Zhu\n12\n,\nMengjie Zhao\n1\nand\nHang Zhao\n12\n2\nAbstract\nCurrent approaches to humanoid control generally fall into two paradigms: perceptive locomotion, which handles terrain well but is limited to pedal gaits, and general motion tracking, which reproduces complex skills but ignores environmental capabilities. This work unites these paradigms to achieve perceptive general motion control. We present a framework where exteroceptive sensing is integrated into whole-body motion tracking, permitting a humanoid to perform highly dynamic, non-locomotion tasks on uneven terrain. By training a single policy to perform multiple distinct motions across varied terrestrial features, we demonstrate the non-trivial benefit of integrating perception into the control loop. Our results show that this framework enables robust, highly dynamic multi-contact motionsâ€”such as vaulting and dive-rollingâ€”on unstructured terrain, significantly expanding the ro",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ„ŸçŸ¥ä¸å…¨èº«è¿åŠ¨è·Ÿè¸ªçš„æ¡†æ¶ï¼Œä½¿ä»¿äººæœºå™¨äººèƒ½åœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šæ‰§è¡Œé«˜åº¦åŠ¨æ€çš„å¤šæ¥è§¦è¿åŠ¨ï¼Œå¦‚è·‘é…·åŠ¨ä½œã€‚",
    "detailed_summary": "## è®ºæ–‡ã€ŠDeep Whole-body Parkourã€‹è¯¦ç»†æ‘˜è¦\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nå½“å‰äººå½¢æœºå™¨äººæ§åˆ¶ä¸»è¦åˆ†ä¸ºä¸¤ç§èŒƒå¼ï¼š**æ„ŸçŸ¥å¼ç§»åŠ¨**ï¼ˆèƒ½é€‚åº”åœ°å½¢ä½†ä»…é™äºè¶³éƒ¨æ­¥æ€ï¼‰å’Œ**é€šç”¨è¿åŠ¨è·Ÿè¸ª**ï¼ˆèƒ½å¤ç°å¤æ‚æŠ€èƒ½ä½†å¿½ç•¥ç¯å¢ƒäº¤äº’ï¼‰ã€‚å‰è€…æ— æ³•æ‰§è¡Œéœ€è¦å…¨èº«å¤šæ¥è§¦ï¼ˆå¦‚æ”¯æ’‘è·³è·ƒã€ç¿»æ»šï¼‰çš„åŠ¨æ€æŠ€èƒ½ï¼Œè€Œåè€…å› ç¼ºä¹ç¯å¢ƒæ„ŸçŸ¥ï¼Œæ— æ³•æ ¹æ®å®é™…åœ°å½¢è°ƒæ•´åŠ¨ä½œï¼Œå¯¼è‡´åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­å¤±è´¥ã€‚æœ¬ç ”ç©¶æ—¨åœ¨**èåˆè¿™ä¸¤ç§èŒƒå¼**ï¼Œå®ç°æ—¢èƒ½æ„ŸçŸ¥ç¯å¢ƒåˆèƒ½æ‰§è¡Œå¤æ‚å…¨èº«åŠ¨ä½œçš„é€šç”¨æ§åˆ¶ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\n- **ç»Ÿä¸€æ¡†æ¶**ï¼šæå‡ºä¸€ä¸ªå°†**å¤–æ„ŸçŸ¥ï¼ˆæ·±åº¦è§†è§‰ï¼‰ç›´æ¥é›†æˆåˆ°å…¨èº«è¿åŠ¨è·Ÿè¸ª**ä¸­çš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚æœºå™¨äººä¸ä»…æ¨¡ä»¿å‚è€ƒåŠ¨ä½œï¼Œè¿˜èƒ½æ ¹æ®å®æ—¶æ·±åº¦å›¾åƒè°ƒæ•´åŠ¨ä½œä»¥é€‚åº”åœ°å½¢å‡ ä½•ã€‚\n- **æ•°æ®ä¸è®­ç»ƒ**ï¼š\n    - **å®šåˆ¶æ•°æ®é›†**ï¼šé€šè¿‡åŠ¨ä½œæ•æ‰å’Œæ¿€å…‰é›·è¾¾æ‰«æï¼Œæ„å»ºäº†**åŠ¨ä½œ-åœ°å½¢ç²¾ç¡®é…å¯¹**çš„è·‘é…·æ•°æ®é›†ï¼Œå¹¶é‡å®šå‘åˆ°Unitree G1æœºå™¨äººã€‚\n    - **é«˜æ•ˆæ„ŸçŸ¥æ¨¡æ‹Ÿ**ï¼šè®¾è®¡äº†**å¤§è§„æ¨¡å¹¶è¡Œåˆ†ç»„å…‰çº¿æŠ•å°„ç®—æ³•**ï¼Œé€šè¿‡é¢„è®¡ç®—ç¢°æ’ç»„æ˜ å°„ï¼Œå°†æ¸²æŸ“é€Ÿåº¦æå‡10å€ï¼Œæ”¯æŒæ•°åƒç¯å¢ƒå¹¶è¡Œè®­ç»ƒã€‚\n    - **ç›¸å¯¹å¸§å¥–åŠ±è®¾è®¡**ï¼šå®šä¹‰äº†ç»“åˆæœºå™¨äººå¹³é¢ä½ç½®/åèˆªè§’å’Œå‚è€ƒåŠ¨ä½œé«˜åº¦/å€¾æ–œè§’çš„**ç›¸å¯¹åæ ‡ç³»**ï¼Œç”¨äºè®¡ç®—è·Ÿè¸ªå¥–åŠ±ï¼Œæå‡äº†è¿åŠ¨ç¨³å®šæ€§ã€‚\n    - **è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ **ï¼šé‡‡ç”¨åŸºäºå¤±è´¥æ£€æµ‹çš„è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œè®©æ™ºèƒ½ä½“æ›´ä¸“æ³¨äºå­¦ä¹ å›°éš¾çš„åŠ¨ä½œç‰‡æ®µã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\n- **æ·±åº¦è§†è§‰çš„ç›Šå¤„**ï¼šå®éªŒè¡¨æ˜ï¼Œå¼•å…¥æ·±åº¦æ„ŸçŸ¥åï¼Œç­–ç•¥å¯¹åˆå§‹ä½ç½®æ‰°åŠ¨å…·æœ‰**æå¼ºçš„é²æ£’æ€§**ã€‚å³ä½¿èµ·å§‹ä½ç½®åœ¨è®­ç»ƒåˆ†å¸ƒå¤–ï¼ˆÂ±0.6ç±³ï¼‰ï¼ŒæˆåŠŸç‡ä»æ¥è¿‘100%ï¼Œè€Œâ€œç›²â€è·Ÿè¸ªåŸºçº¿åˆ™å®Œå…¨å¤±è´¥ã€‚\n- **è§†è§‰é²æ£’æ€§**ï¼šåœ¨è®­ç»ƒæœªè§è¿‡çš„è§†è§‰å¹²æ‰°ç‰©ï¼ˆå¦‚é¢å¤–éšœç¢ç‰©ï¼‰åœºæ™¯ä¸­æµ‹è¯•ï¼Œç­–ç•¥**æˆåŠŸç‡å’Œå…³é”®æŒ‡æ ‡ï¼ˆMPJPEï¼‰åŸºæœ¬ä¸å—å½±å“**ï¼Œè¯æ˜äº†å…¶å¯¹æ— å…³è§†è§‰ä¿¡æ¯çš„é²æ£’æ€§ã€‚\n- **ä»¿çœŸåˆ°ç°å®è¿ç§»**ï¼šé€šè¿‡å‘ä»¿çœŸæ·±åº¦å›¾åƒæ·»åŠ å™ªå£°å’Œä¼ªå½±ï¼Œå¹¶é‡‡ç”¨GPUåŠ é€Ÿä¿®å¤ç®—æ³•ï¼ŒæˆåŠŸå¼¥åˆäº†ä»¿çœŸä¸ç°å®æ„ŸçŸ¥çš„å·®è·ã€‚ç­–ç•¥æˆåŠŸéƒ¨ç½²åˆ°çœŸå®G1æœºå™¨äººä¸Šï¼Œåœ¨å®¤å†…å¤–å¤šç§éç»“æ„åŒ–åœ°å½¢ä¸Šå®Œæˆäº†**è·ªå§¿æ”€çˆ¬ã€ç¿»æ»šè·¨è¶Šã€é±¼è·ƒç¿»æ»š**ç­‰åŠ¨æ€å…¨èº«æŠ€èƒ½ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬å·¥ä½œé¦–æ¬¡æˆåŠŸå°†æ„ŸçŸ¥å¼ç§»åŠ¨ä¸å…¨èº«è¿åŠ¨è·Ÿè¸ªç»Ÿä¸€åˆ°ä¸€ä¸ª**å¯æ‰©å±•çš„ç«¯åˆ°ç«¯è®­ç»ƒæ¡†æ¶**ä¸­ã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **æŠ€æœ¯çªç ´**ï¼šä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œéœ€è¦**å…¨èº«å¤šæ¥è§¦**çš„ã€é«˜åº¦åŠ¨æ€çš„å¤æ‚æŠ€èƒ½ï¼Œå¹¶**é€‚åº”éç»“æ„åŒ–åœ°å½¢**ï¼Œæ˜¾è‘—æ‰©å±•äº†æœºå™¨äººçš„é€šè¿‡æ€§å’Œå®ç”¨æ€§ã€‚\n- **æ¡†æ¶è´¡çŒ®**ï¼šä¸ºæœªæ¥**è§„æ¨¡åŒ–è®­ç»ƒ**åŒ…å«å¤§é‡ç¯å¢ƒäº¤äº’æŠ€èƒ½çš„æœºå™¨äººæŠ€èƒ½åº“æä¾›äº†å…³é”®çš„åŸºç¡€è®¾æ–½å’Œå¯è¡Œè·¯å¾„ï¼Œæ¨åŠ¨äº†é€šç”¨äººå½¢æœºå™¨äººæ§åˆ¶å™¨çš„å‘å±•ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **æ ¸å¿ƒé—®é¢˜**\nå½“å‰äººå½¢æœºå™¨äººæ§åˆ¶å­˜åœ¨ä¸¤å¤§èŒƒå¼ï¼Œå„æœ‰å±€é™ï¼š\n- **æ„ŸçŸ¥å¼è¡Œèµ°**ï¼šèƒ½é€‚åº”å¤æ‚åœ°å½¢ï¼Œä½†ä»…é™äº**è¶³éƒ¨æ­¥æ€**ï¼Œæ— æ³•åˆ©ç”¨ä¸ŠåŠèº«è¿›è¡Œå¤šæ¥è§¦äº¤äº’ï¼ˆå¦‚æ’‘è·³ã€ç¿»æ»šï¼‰ã€‚\n- **é€šç”¨è¿åŠ¨è·Ÿè¸ª**ï¼šèƒ½æ¨¡ä»¿å¤æ‚çš„å…¨èº«æŠ€èƒ½ï¼ˆå¦‚åç©ºç¿»ï¼‰ï¼Œä½†**å¿½ç•¥ç¯å¢ƒæ„ŸçŸ¥**ï¼Œæ— æ³•æ ¹æ®åœ°å½¢å‡ ä½•è°ƒæ•´åŠ¨ä½œï¼Œå¯¼è‡´åœ¨çœŸå®ä¸–ç•Œä¸­æ‰§è¡Œæ¥è§¦å¯†é›†å‹ä»»åŠ¡æ—¶å¤±è´¥ã€‚\n\næœ¬æ–‡æ—¨åœ¨**èåˆè¿™ä¸¤ä¸ªèŒƒå¼**ï¼Œè§£å†³â€œ**å¦‚ä½•åœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šæ‰§è¡Œé«˜åº¦åŠ¨æ€ã€éœ€è¦å…¨èº«å¤šæ¥è§¦çš„æ•æ·æŠ€èƒ½**â€è¿™ä¸€æ ¸å¿ƒé—®é¢˜ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ª**æ„ŸçŸ¥å¼å…¨èº«è¿åŠ¨æ§åˆ¶æ¡†æ¶**ï¼Œå…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªå±‚é¢ï¼š\n\n#### 1. **èŒƒå¼èåˆï¼šå°†å¤–æ„ŸçŸ¥æ·±åº¦è§†è§‰é›†æˆåˆ°å…¨èº«è¿åŠ¨è·Ÿè¸ªä¸­**\n- **æŠ€æœ¯å®ç°**ï¼šè®­ç»ƒä¸€ä¸ª**å•ä¸€ç­–ç•¥**ï¼ŒåŒæ—¶æ¥æ”¶**æœªæ¥è¿åŠ¨å‚è€ƒåºåˆ—**ï¼ˆæ¥è‡ªäººç±»æ¼”ç¤ºï¼‰å’Œ**å®æ—¶æ·±åº¦å›¾åƒ**ä½œä¸ºè¾“å…¥ã€‚\n- **æ•ˆæœ**ï¼šæœºå™¨äººä¸å†ç›²ç›®å¤ç°é¢„è®¾è½¨è¿¹ï¼Œè€Œæ˜¯èƒ½æ ¹æ®å®æ—¶æ„ŸçŸ¥çš„éšœç¢ç‰©å‡ ä½•å½¢çŠ¶ï¼ˆå¦‚ç®±å­çš„é«˜åº¦ã€è·ç¦»ï¼‰ï¼ŒåŠ¨æ€è°ƒæ•´å…¶æ­¥æ€ã€èµ·è·³æ—¶æœºå’Œæ‰‹éƒ¨æ”¾ç½®ä½ç½®ï¼Œä»è€ŒæˆåŠŸå®Œæˆå¦‚æ’‘è·³ã€ç¿»æ»šç­‰éœ€è¦ç²¾ç¡®æ¥è§¦çš„ä»»åŠ¡ã€‚\n\n#### 2. **å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒåŸºç¡€è®¾æ–½**\nä¸ºäº†é«˜æ•ˆè®­ç»ƒè¿™ä¸ªæ„ŸçŸ¥-è¿åŠ¨èåˆçš„ç­–ç•¥ï¼Œè®ºæ–‡è®¾è®¡äº†ä¸¤é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼š\n- **å®šåˆ¶åŒ–å¤§è§„æ¨¡å¹¶è¡Œå…‰çº¿æŠ•å°„å™¨**ï¼š\n    - **é—®é¢˜**ï¼šæ ‡å‡†æ¨¡æ‹Ÿå™¨éš¾ä»¥åœ¨æ•°åƒä¸ªå¹¶è¡Œç¯å¢ƒä¸­é«˜æ•ˆã€éš”ç¦»åœ°æ¸²æŸ“æ·±åº¦å›¾åƒï¼ˆé˜²æ­¢æ™ºèƒ½ä½“çœ‹åˆ°â€œå¹½çµâ€éšœç¢ç‰©ï¼‰ã€‚\n    - **è§£å†³æ–¹æ¡ˆ**ï¼šå®ç°åŸºäºNvidia Warpçš„ä¼˜åŒ–å…‰çº¿æŠ•å°„å™¨ï¼Œé‡‡ç”¨**é¢„è®¡ç®—åˆ†ç»„æ˜ å°„**å’Œ**ç¢°æ’ç»„éš”ç¦»**æœºåˆ¶ã€‚\n    - **ä»·å€¼**ï¼šå°†æ¸²æŸ“é€Ÿåº¦æå‡**10å€**ï¼Œä½¿æµ·é‡å¹¶è¡Œè®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œæ˜¯æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–çš„åŸºç¡€ã€‚\n    ```python\n    # ç®—æ³•æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡åˆ†ç»„IDå¿«é€Ÿè¿‡æ»¤å¯è§ç½‘æ ¼\n    if ray.group_id == -1: # é™æ€åœ°å½¢ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“å¯è§\n        check_all_static_meshes()\n    else: # åŠ¨æ€ç‰©ä½“ï¼Œåªæ£€æŸ¥å±äºå½“å‰æ™ºèƒ½ä½“ç»„çš„ç½‘æ ¼\n        check_meshes_belonging_to_current_agent()\n    ```\n- **è‡ªé€‚åº”é‡‡æ ·ä¸è¯¾ç¨‹å­¦ä¹ **ï¼š\n    - **é—®é¢˜**ï¼šä¸åŒæŠ€èƒ½ã€ä¸åŒæ—¶é—´æ®µçš„éš¾åº¦ä¸åŒï¼Œå‡åŒ€é‡‡æ ·æ•ˆç‡ä½ä¸‹ã€‚\n    - **è§£å†³æ–¹æ¡ˆ**ï¼šå°†è¿åŠ¨è½¨è¿¹åˆ†æ®µï¼Œæ ¹æ®å¤±è´¥é¢‘ç‡åŠ¨æ€è°ƒæ•´é‡‡æ ·æƒé‡ï¼ˆAlgorithm 2ï¼‰ã€‚\n    - **ä»·å€¼**ï¼šè®©ç­–ç•¥è‡ªåŠ¨èšç„¦äºéš¾ä»¥æŒæ¡çš„è¿åŠ¨é˜¶æ®µï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œæå‡æœ€ç»ˆæ€§èƒ½ã€‚\n\n#### 3. **æ¨¡æ‹Ÿåˆ°ç°å®çš„ç¨³å¥éƒ¨ç½²æ–¹æ¡ˆ**\n- **æ·±åº¦æ„ŸçŸ¥åŸŸéšæœºåŒ–**ï¼šåœ¨æ¨¡æ‹Ÿä¸­å¯¹æ·±åº¦å›¾åƒæ·»åŠ é«˜æ–¯å™ªå£°ã€å—çŠ¶ä¼ªå½±ç­‰ï¼Œä»¥åŒ¹é…RealSenseç›¸æœºåœ¨ç°å®ä¸­çš„å™ªå£°æ¨¡å¼ï¼ˆåå°„ã€è¿åŠ¨æ¨¡ç³Šï¼‰ã€‚\n- **å…é‡Œç¨‹è®¡è®¾è®¡**ï¼š\n    - ç­–ç•¥è§‚æµ‹ä¸­ä½¿ç”¨**ç›¸å¯¹å¸§**å¥–åŠ±è®¾è®¡ï¼ˆç»“åˆæœºå™¨äººå¹³é¢ä½ç½®/åèˆªè§’å’Œå‚è€ƒè¿åŠ¨çš„å‚ç›´é«˜åº¦/å€¾æ–œè§’ï¼‰ã€‚\n    - **å®é™…ä»·å€¼**ï¼šéƒ¨ç½²æ—¶**å®Œå…¨ä¸éœ€è¦å…¨å±€å®šä½æˆ–é‡Œç¨‹è®¡ç³»ç»Ÿ**ã€‚æœºå™¨äººä»…éœ€å¯¹é½åˆå§‹æœå‘ï¼Œå³å¯ä¾é ç›¸å¯¹å‚è€ƒå’ŒIMUæ•°æ®å®Œæˆä»»åŠ¡ï¼Œæå¤§æå‡äº†åœ¨æˆ·å¤–ç­‰æ— å‡†å¤‡åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚\n\n### **è§£å†³æ–¹æ¡ˆæ€»ç»“**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ª**ç«¯åˆ°ç«¯çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶**è§£å†³äº†é—®é¢˜ï¼š\n1.  **æ•°æ®**ï¼šé‡‡é›†**äºº-åœ°å½¢é…å¯¹**çš„åŠ¨æ•æ•°æ®ï¼Œå¹¶é‡å®šå‘åˆ°æœºå™¨äººæ¨¡å‹ã€‚\n2.  **è®­ç»ƒ**ï¼šåœ¨ç¨‹åºåŒ–ç”Ÿæˆçš„å¤§é‡åœ°å½¢-è¿åŠ¨é…å¯¹åœºæ™¯ä¸­ï¼Œä½¿ç”¨ä¸Šè¿°å¤§è§„æ¨¡å¹¶è¡Œæ¨¡æ‹Ÿå™¨å’Œè‡ªé€‚åº”è¯¾ç¨‹ï¼Œè®­ç»ƒä¸€ä¸ªæ¥æ”¶**è¿åŠ¨å‚è€ƒ+æ·±åº¦å›¾åƒ**çš„ç­–ç•¥ã€‚\n3.  **éƒ¨ç½²**ï¼šé€šè¿‡åŸŸéšæœºåŒ–æŠ€æœ¯æ¡¥æ¥æ¨¡æ‹Ÿä¸ç°å®å·®è·ï¼Œå¹¶å°†è®­ç»ƒå¥½çš„ç­–ç•¥éƒ¨ç½²åˆ°çœŸå®æœºå™¨äººï¼ˆUnitree G1ï¼‰ä¸Šï¼Œå®ç°**å¼€ç¯è§¦å‘ã€æ— éœ€ç²¾ç¡®å®šä½çš„æ•æ·æŠ€èƒ½æ‰§è¡Œ**ã€‚\n\n### **å®é™…ä»·å€¼**\n- **æ‰©å±•äº†æœºå™¨äººçš„é€šè¿‡æ€§**ï¼šä½¿æœºå™¨äººä¸ä»…èƒ½èµ°ã€è·‘ã€è·³ï¼Œè¿˜èƒ½æ‰§è¡Œ**æ’‘è·³ã€è·ªå§¿æ”€çˆ¬ã€é±¼è·ƒç¿»æ»š**ç­‰å¤æ‚å¤šæ¥è§¦æŠ€èƒ½ï¼Œæ›´æ¥è¿‘äººç±»çš„ç§»åŠ¨èƒ½åŠ›ã€‚\n- **æå‡äº†ç³»ç»Ÿé²æ£’æ€§**ï¼šè§†è§‰åé¦ˆä½¿ç­–ç•¥èƒ½å®¹å¿åˆå§‹ä½ç½®çš„å¤§å¹…åå·®ï¼ˆè®ºæ–‡ä¸­æµ‹è¯•äº†1.2mÃ—1.2mèŒƒå›´å†…çš„ä»»æ„èµ·ç‚¹ï¼‰ï¼Œå¹¶å°†**è„†å¼±çš„å‰é¦ˆè½¨è¿¹æ’­æ”¾è½¬å˜ä¸ºç¨³å¥çš„é—­ç¯ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›**ã€‚\n- **ä¸ºè§„æ¨¡åŒ–å­¦ä¹ å¥ å®šåŸºç¡€**ï¼šè¯¥æ¡†æ¶ä¸ºæœªæ¥è®­ç»ƒåŒ…å«å¤§é‡äº¤äº’æŠ€èƒ½çš„â€œæŠ€èƒ½åº“â€æä¾›äº†å¯æ‰©å±•çš„æŠ€æœ¯åŸºç¡€ï¼Œæ˜¯è¿ˆå‘é€šç”¨äººå½¢æœºå™¨äººçš„é‡è¦ä¸€æ­¥ã€‚",
            "timestamp": "2026-01-21T12:10:18.282880",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠDeep Whole-body Parkourã€‹æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘æ¢³ç†\n\nåŸºäºè®ºæ–‡å†…å®¹åŠå…¶å¼•ç”¨çš„å‰ä½œï¼Œè¯¥ç ”ç©¶æ–¹å‘çš„å‘å±•è„‰ç»œå¯ä»¥æ¸…æ™°åœ°åˆ’åˆ†ä¸ºå‡ ä¸ªå…³é”®é˜¶æ®µå’Œä¸åŒçš„æŠ€æœ¯å‘å±•æ–¹å‘ã€‚å…¶æ ¸å¿ƒæ¼”è¿›é€»è¾‘æ˜¯**ä»å•ä¸€èŒƒå¼ï¼ˆæ„ŸçŸ¥æˆ–æ¨¡ä»¿ï¼‰èµ°å‘èåˆï¼Œä»ç®€å•ä»»åŠ¡ï¼ˆè¶³å¼ç§»åŠ¨ï¼‰èµ°å‘å¤æ‚ä»»åŠ¡ï¼ˆå…¨èº«æ¥è§¦äº¤äº’ï¼‰ï¼Œä»ä»¿çœŸèµ°å‘ç°å®**ã€‚\n\n### å‘å±•è„‰ç»œæ€»è§ˆ\n\nè¯¥é¢†åŸŸä¸»è¦å›´ç»•ä¸¤å¤§æŠ€æœ¯èŒƒå¼å±•å¼€ç«äº‰ä¸èåˆï¼š\n1.  **æ„ŸçŸ¥å¼ç§»åŠ¨**ï¼šå…³æ³¨ç¯å¢ƒé€‚åº”ï¼Œä½†ä»…é™äºè¶³å¼æ­¥æ€ã€‚\n2.  **å…¨èº«è¿åŠ¨è·Ÿè¸ª**ï¼šå…³æ³¨å¤æ‚æŠ€èƒ½å¤ç°ï¼Œä½†å¿½ç•¥ç¯å¢ƒæ„ŸçŸ¥ã€‚\n\nã€ŠDeep Whole-body Parkourã€‹çš„å·¥ä½œæ˜¯è¿™ä¸¤å¤§èŒƒå¼èåˆçš„é‡Œç¨‹ç¢‘ã€‚\n\n### åˆ†é˜¶æ®µæŠ€æœ¯æ¼”è¿›ä¸æ”¹è¿›\n\n#### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€èŒƒå¼ç¡®ç«‹ (2018-2021)\næ­¤é˜¶æ®µä¸¤å¤§èŒƒå¼å„è‡ªç‹¬ç«‹å‘å±•ï¼Œå¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚\n\n*   **å…¨èº«è¿åŠ¨è·Ÿè¸ªèŒƒå¼ (ä»¥DeepMimic, AMPä¸ºä»£è¡¨)**\n    *   **å‰ä½œ**ï¼šPeng et al. (2018) çš„ **DeepMimic** å’Œ (2021) çš„ **AMP**ã€‚\n    *   **æ ¸å¿ƒè´¡çŒ®**ï¼šåœ¨ä»¿çœŸä¸­ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ ï¼ˆå¼ºåŒ–å­¦ä¹ +å‚è€ƒè¿åŠ¨æ•°æ®ï¼‰ä½¿è™šæ‹Ÿè§’è‰²æŒæ¡ä¸°å¯Œçš„æŠ€èƒ½ï¼ˆå¦‚åç©ºç¿»ã€æ­¦æœ¯ã€è·‘é…·ï¼‰ã€‚\n    *   **æ”¹è¿›ä¸å±€é™**ï¼š\n        *   **æ”¹è¿›**ï¼šç›¸æ¯”ä¹‹å‰çš„ç‰©ç†æ§åˆ¶æ–¹æ³•ï¼Œèƒ½ç”Ÿæˆæ›´è‡ªç„¶ã€æ›´å¤æ‚çš„å…¨èº«è¿åŠ¨ã€‚\n        *   **å±€é™**ï¼š**ç¯å¢ƒæ— å…³**ã€‚ç­–ç•¥æ˜¯â€œç›²ç›®çš„â€ï¼Œä»…è·Ÿè¸ªé¢„å®šä¹‰è½¨è¿¹ï¼Œæ— æ³•æ ¹æ®å®é™…ç¯å¢ƒå‡ ä½•ï¼ˆå¦‚éšœç¢ç‰©é«˜åº¦ã€è·ç¦»ï¼‰è°ƒæ•´åŠ¨ä½œã€‚è¿™å¯¼è‡´å…¶æ— æ³•åœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šå¯é æ‰§è¡Œéœ€è¦ç²¾ç¡®æ¥è§¦çš„ä»»åŠ¡ï¼ˆå¦‚æ’‘è·³ï¼‰ã€‚\n\n*   **æ„ŸçŸ¥å¼ç§»åŠ¨èŒƒå¼ (ä»¥RMAã€ç›²èµ°ç­–ç•¥ä¸ºä»£è¡¨)**\n    *   **å‰ä½œ**ï¼šHwangbo et al. (2019), Lee et al. (2020), Kumar et al. (2021) çš„ **RMA** ç­‰ã€‚\n    *   **æ ¸å¿ƒè´¡çŒ®**ï¼šå°†æœ¬ä½“æ„ŸçŸ¥ä¸å¤–éƒ¨æ„ŸçŸ¥ï¼ˆå¦‚æ·±åº¦å›¾ï¼‰ç»“åˆï¼Œä½¿è¶³å¼æœºå™¨äººèƒ½åœ¨å¤æ‚åœ°å½¢ä¸Šå®ç°é²æ£’çš„è¡Œèµ°ã€å¥”è·‘ç”šè‡³è·³è·ƒã€‚\n    *   **æ”¹è¿›ä¸å±€é™**ï¼š\n        *   **æ”¹è¿›**ï¼šä»ä½é€Ÿé™æ€åœ°å½¢å¯¼èˆªå‘å±•åˆ°é«˜é€Ÿæ•æ·æœºåŠ¨ï¼Œå®ç°äº†å¯¹åœ°å½¢çš„å®æ—¶é€‚åº”ã€‚\n        *   **å±€é™**ï¼š**åŠŸèƒ½å—é™**ã€‚äº¤äº’ä»…é™äºè¶³éƒ¨æ¥è§¦ï¼Œä¸ŠåŠèº«ä»…ç”¨äºå¹³è¡¡ã€‚æ— æ³•æ‰§è¡Œéœ€è¦æ‰‹éƒ¨æ¥è§¦çš„å¤æ‚æŠ€èƒ½ï¼ˆå¦‚æ’‘è·³ã€æ”€çˆ¬ï¼‰ï¼Œå³â€œä»»åŠ¡æ¬ è§„èŒƒâ€é—®é¢˜â€”â€”ä½ç»´é€Ÿåº¦æŒ‡ä»¤æ— æ³•åŒºåˆ†â€œè·³è·ƒâ€å’Œâ€œæ’‘è·³â€ç­‰ä¸åŒäº¤äº’æ¨¡å¼ã€‚\n\n#### ç¬¬äºŒé˜¶æ®µï¼šèŒƒå¼æ·±åŒ–ä¸ä»¿çœŸåˆ°ç°å®çš„è¿ç§» (2022-2024)\næ­¤é˜¶æ®µä¸¤å¤§èŒƒå¼å‡åœ¨å„è‡ªæ–¹å‘ä¸Šæ·±åŒ–ï¼Œå¹¶å¼€å§‹å°è¯•å‘çœŸå®æœºå™¨äººéƒ¨ç½²ã€‚\n\n*   **å…¨èº«è¿åŠ¨è·Ÿè¸ªçš„â€œç°å®åŒ–â€**\n    *   **å‰ä½œ**ï¼šLuo et al. (2023), Chen et al. (2025), Liao et al. (2025) çš„ **BeyondMimic** ç­‰ã€‚\n    *   **æ ¸å¿ƒè´¡çŒ®**ï¼šæˆåŠŸå°†æ¨¡ä»¿å­¦ä¹ ç­–ç•¥è¿ç§»åˆ°å®ä½“äººå½¢æœºå™¨äººä¸Šï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­å¤ç°äº†å¯Œæœ‰è¡¨ç°åŠ›çš„è¡Œä¸ºã€‚\n    *   **æ”¹è¿›**ï¼š**è·¨è¶Šäº†ä»¿çœŸåˆ°ç°å®çš„é¸¿æ²Ÿ**ã€‚é€šè¿‡æ”¹è¿›è®­ç»ƒæ¡†æ¶ã€çŠ¶æ€è¡¨ç¤ºå’Œç³»ç»Ÿè¾¨è¯†ï¼Œä½¿ä»¿çœŸä¸­è®­ç»ƒçš„ç­–ç•¥èƒ½ç›´æ¥æˆ–ç»å¾®è°ƒåéƒ¨ç½²äºç¡¬ä»¶ã€‚\n    *   **é—ç•™é—®é¢˜**ï¼š**ç¯å¢ƒæ„ŸçŸ¥é—®é¢˜ä»æœªè§£å†³**ã€‚è¿™äº›å·¥ä½œå¤§å¤šåŸºäºâ€œå¹³é¢ç¯å¢ƒå‡è®¾â€ï¼Œåœ¨éƒ¨ç½²æ—¶ä»è¦æ±‚æœºå™¨äººèµ·å§‹ä½ç½®ä¸å‚è€ƒè½¨è¿¹ä¸¥æ ¼å¯¹é½ï¼Œç¼ºä¹å¯¹éå¹³é¢åœ°å½¢çš„é€‚åº”èƒ½åŠ›ã€‚\n\n*   **æ„ŸçŸ¥å¼ç§»åŠ¨çš„â€œå¤æ‚åŒ–â€**\n    *   **å‰ä½œ**ï¼šZhuang et al. (2023, 2024) çš„ **Robot/Humanoid Parkour Learning**, Agarwal et al. (2022, 2023), Miki et al. (2022)ã€‚\n    *   **æ ¸å¿ƒè´¡çŒ®**ï¼šä½¿å››è¶³å’Œäººå½¢æœºå™¨äººèƒ½å¤Ÿæ‰§è¡ŒåŠ¨æ€è·‘é…·ï¼Œå¦‚è‡ªä¸»è·¨è¶Šæ²Ÿå£‘ã€æ”€çˆ¬å¤§å‹éšœç¢ã€‚\n    *   **æ”¹è¿›**ï¼šå°†æ„ŸçŸ¥èƒ½åŠ›åº”ç”¨äºæ›´åŠ¨æ€ã€æ›´å…·æŒ‘æˆ˜æ€§çš„è¶³å¼ç§»åŠ¨ä»»åŠ¡ã€‚\n    *   **é—ç•™é—®é¢˜**ï¼š**äº¤äº’æ¨¡å¼ä¾ç„¶å•ä¸€**ã€‚å°½ç®¡åŠ¨ä½œæ›´æ¿€è¿›ï¼Œä½†æœ¬è´¨ä¸Šä»æ˜¯â€œè¸æ¿å¼ç§»åŠ¨â€ï¼Œæœªåˆ©ç”¨ä¸ŠåŠèº«è¿›è¡Œæ¥è§¦å¼äº¤äº’ã€‚\n\n#### ç¬¬ä¸‰é˜¶æ®µï¼šèŒƒå¼èåˆä¸ç³»ç»Ÿåˆ›æ–° (2025 - æœ¬å·¥ä½œ)\nã€ŠDeep Whole-body Parkourã€‹ä½œä¸ºä½œè€…å›¢é˜Ÿï¼ˆZhuang, Zhaoç­‰ï¼‰å·¥ä½œçš„å»¶ç»­ï¼Œå®ç°äº†è´¨çš„é£è·ƒã€‚\n\n*   **å‰ä½œåŸºç¡€**ï¼šç›´æ¥å»ºç«‹åœ¨ **Humanoid Parkour Learning** (Zhuang et al., 2024) çš„æ„ŸçŸ¥ç§»åŠ¨ï¼Œä»¥åŠ **BeyondMimic** (Liao et al., 2025) å’Œ **Embrace Contacts** (Zhuang & Zhao, 2025) çš„è¿åŠ¨è·Ÿè¸ªæŠ€æœ¯ä¹‹ä¸Šã€‚\n*   **æœ¬å·¥ä½œçš„æ ¸å¿ƒæ”¹è¿›ä¸åˆ›æ–°**ï¼š\n    1.  **èŒƒå¼ç»Ÿä¸€**ï¼šé¦–æ¬¡å°†**å¤–éƒ¨æ„ŸçŸ¥æ·±åº¦é›†æˆåˆ°å…¨èº«è¿åŠ¨è·Ÿè¸ªæ¡†æ¶**ä¸­ã€‚ç­–ç•¥ä¸ä»…æ¨¡ä»¿åŠ¨ç”»ï¼Œè¿˜èƒ½æ ¹æ®ç¯å¢ƒçš„è§†è§‰å æ®ä¿¡æ¯è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚\n    2.  **ä»»åŠ¡å‡çº§**ï¼šä»â€œè¶³å¼è·‘é…·â€å‡çº§ä¸ºâ€œ**å…¨èº«æ¥è§¦å¼è·‘é…·**â€ã€‚æœºå™¨äººå¯ä»¥æ‰§è¡Œæ’‘è·³ã€æ»šç¿»ç­‰å¤šæ¥è§¦æŠ€èƒ½ï¼Œæå¤§æ‰©å±•äº†åœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šçš„é€šè¿‡æ€§ã€‚\n    3.  **é²æ£’æ€§çªç ´**ï¼šè§£å†³äº†ç›²è·Ÿè¸ªç­–ç•¥å¯¹åˆå§‹ä½ç½®æåº¦æ•æ„Ÿçš„é—®é¢˜ã€‚é€šè¿‡è§†è§‰åé¦ˆå½¢æˆé—­ç¯æ§åˆ¶ï¼Œèƒ½å®¹å¿èµ·å§‹ä½ç½®å’Œè§’åº¦çš„å¤§èŒƒå›´å˜åŒ–ï¼Œè‡ªåŠ¨è°ƒæ•´æ­¥æ€ä»¥ç¡®ä¿æˆåŠŸäº¤äº’ã€‚\n    4.  **å·¥ç¨‹åŒ–åˆ›æ–°**ï¼š\n        *   **æ•°æ®**ï¼šåˆ›å»ºäº†**è¿åŠ¨-åœ°å½¢é…å¯¹æ•°æ®é›†**ï¼Œé€šè¿‡åŠ¨ä½œæ•æ‰å’ŒLiDARæ‰«æç¡®ä¿äººä½“åŠ¨åŠ›å­¦ä¸ç¯å¢ƒå‡ ä½•çš„é«˜ä¿çœŸå¯¹é½ã€‚\n        *   **ä»¿çœŸ**ï¼šè®¾è®¡äº†**å¤§è§„æ¨¡å¹¶è¡Œåˆ†ç»„å…‰çº¿æŠ•å°„å™¨**ï¼ˆAlgorithm 1ï¼‰ï¼Œè§£å†³äº†æ•°åƒä¸ªå¹¶è¡Œç¯å¢ƒä¸­GPUæ·±åº¦æ¨¡æ‹Ÿçš„éš”ç¦»ä¸æ•ˆç‡ç“¶é¢ˆï¼Œæ¸²æŸ“é€Ÿåº¦æå‡10å€ã€‚\n        *   **è®­ç»ƒ**ï¼šå¼•å…¥äº†**åŸºäºå¤±è´¥çš„è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ **ï¼ˆAlgorithm 2ï¼‰å’Œ**å¡æ­»æ£€æµ‹**ï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡å’Œæ ·æœ¬è´¨é‡ã€‚\n        *   **éƒ¨ç½²**ï¼šé‡‡ç”¨**ç›¸å¯¹å¸§å¥–åŠ±è®¾è®¡**ï¼Œä½¿ç­–ç•¥ä¸ä¾èµ–äºå…¨å±€é‡Œç¨‹è®¡ï¼Œä»…éœ€IMUå’Œå…³èŠ‚å‚è€ƒå³å¯åœ¨çœŸå®ä¸–ç•Œè¿è¡Œï¼Œç®€åŒ–äº†éƒ¨ç½²ã€‚\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªæ–¹å‘\n\n1.  **æ„ŸçŸ¥ä¸æ§åˆ¶çš„èåˆæ–¹å‘**ï¼šè¿™æ˜¯æœ€æ ¸å¿ƒçš„ä¸»çº¿ã€‚ä»â€œæ— æ„ŸçŸ¥çš„æ¨¡ä»¿â€å’Œâ€œä»…è¶³éƒ¨æ„ŸçŸ¥çš„ç§»åŠ¨â€ï¼Œå‘å±•åˆ°â€œ**å…¨èº«æ„ŸçŸ¥æ¨¡ä»¿**â€ã€‚æœ¬å·¥ä½œæ˜¯è¿™ä¸€æ–¹å‘çš„é›†å¤§æˆè€…ï¼Œè¯æ˜äº†èåˆå¸¦æ¥çš„éå¹³å‡¡æ”¶ç›Šï¼ˆå¯¹åˆå§‹æ¡ä»¶çš„é²æ£’æ€§ï¼‰ã€‚\n2.  **æŠ€èƒ½å¤æ‚åŒ–ä¸äº¤äº’å¤šå…ƒåŒ–æ–¹å‘**ï¼šä»â€œè¡Œèµ°/å¥”è·‘â€åˆ°â€œåŠ¨æ€è·³è·ƒâ€ï¼Œå†åˆ°â€œ**å¤šæ¥è§¦å…¨èº«æŠ€èƒ½**â€ï¼ˆæ’‘è·³ã€è·ªå§¿æ”€çˆ¬ã€æ»šç¿»ï¼‰ã€‚äº¤äº’ç‚¹ä»è„šæ‰©å±•åˆ°æ‰‹ã€è†ã€èº¯å¹²ï¼Œæ›´æ¥è¿‘äººç±»çš„ç§»åŠ¨èƒ½åŠ›ã€‚\n3.  **ä»¿çœŸåˆ°ç°å®è¿ç§»çš„é²æ£’æ€§æ–¹å‘**ï¼šä»ä¾èµ–ç²¾ç¡®çš„ä»¿çœŸæ¨¡å‹å’Œç³»ç»Ÿè¾¨è¯†ï¼Œå‘å±•åˆ°é€šè¿‡**æ„ŸçŸ¥è¾“å…¥å¯¹é½**ï¼ˆåœ¨ä»¿çœŸä¸­å¯¹æ·±åº¦å›¾åƒæ·»åŠ çœŸå®å™ªå£°å’Œä¿®å¤ï¼‰å’Œ**ç­–ç•¥ç»“æ„è®¾è®¡**ï¼ˆç›¸å¯¹å¸§ã€æ— é‡Œç¨‹è®¡ä¾èµ–ï¼‰æ¥ç›´æ¥è·¨è¶Šè™šå®é¸¿æ²Ÿã€‚\n4.  **å¤§è§„æ¨¡è®­ç»ƒçš„ç³»ç»Ÿå·¥ç¨‹æ–¹å‘**ï¼šä¸ºäº†è§£å†³å­¦ä¹ å¤æ‚æŠ€èƒ½æ‰€éœ€çš„æµ·é‡æ•°æ®ï¼Œç ”ç©¶ä»å•ç¯å¢ƒè®­ç»ƒå‘å±•åˆ°**å¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸ**ã€‚æœ¬å·¥ä½œä¸­çš„å®šåˆ¶åŒ–å…‰çº¿æŠ•å°„å™¨å’Œè¯¾ç¨‹å­¦ä¹ æœºåˆ¶ï¼Œæ˜¯æ”¯æ’‘å¤æ‚ä»»åŠ¡è®­ç»ƒå¾—ä»¥å®ç°çš„å…³é”®åŸºç¡€è®¾æ–½åˆ›æ–°ã€‚\n\n**æ€»ç»“**ï¼šè¯¥æ–¹å‘çš„å‘å±•æ˜¯ä¸€ä¸ªä»**åˆ†æ²»**åˆ°**ç»Ÿä¸€**ã€ä»**ç®€å•**åˆ°**å¤æ‚**ã€ä»**è™šæ‹Ÿ**åˆ°**ç°å®**çš„èºæ—‹ä¸Šå‡è¿‡ç¨‹ã€‚ã€ŠDeep Whole-body Parkourã€‹æ ‡å¿—ç€äººå½¢æœºå™¨äººæ§åˆ¶è¿›å…¥äº†ä¸€ä¸ªæ–°é˜¶æ®µï¼Œå³é€šè¿‡ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œè·å¾—åœ¨å¤æ‚ç¯å¢ƒä¸­æ‰§è¡Œæ¥è§¦å¼ã€å…¨èº«æ€§æ•æ·æŠ€èƒ½çš„èƒ½åŠ›ï¼Œä¸ºé€šç”¨äººå½¢æ§åˆ¶å™¨çš„å‘å±•å¥ å®šäº†åšå®åŸºç¡€ã€‚",
            "timestamp": "2026-01-21T12:11:15.790358",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯ç¬¬ä¸€ä½œè€… Ziwen Zhuangï¼‰åœ¨**æ„ŸçŸ¥å‹äººå½¢æœºå™¨äººè·‘é…·**é¢†åŸŸæœ‰ä¸€ç³»åˆ—å¥ åŸºæ€§å·¥ä½œã€‚ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒå‰ä½œçš„è¯¦ç»†æ¢³ç†ï¼Œå¹¶ä»¥è¡¨æ ¼å½¢å¼å¯¹æ¯”æœ¬æ–‡ä¸å®ƒä»¬çš„æ ¸å¿ƒåŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n### å‰ä½œæ¦‚è§ˆè¡¨æ ¼\n\n| è®ºæ–‡æ ‡é¢˜ | å‘è¡¨å¹´ä»½/ä¼šè®® | æ ¸å¿ƒè´¡çŒ® | æœ¬æ–‡ä¸å‰ä½œçš„ä¸»è¦åŒºåˆ«ä¸æ”¹è¿› |\n| :--- | :--- | :--- | :--- |\n| **Robot Parkour Learning** | 2023 (CoRL) | **å››è¶³æœºå™¨äººè·‘é…·**ï¼šé¦–æ¬¡å°†æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä¸**å¤–æ„Ÿå—æ„ŸçŸ¥**ï¼ˆæ·±åº¦å›¾ï¼‰ç»“åˆï¼Œä½¿å››è¶³æœºå™¨äººï¼ˆå¦‚Unitree Go1ï¼‰èƒ½åœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šå®Œæˆ**åŠ¨æ€è·‘é…·**ï¼ˆå¦‚è·³è·ƒé—´éš™ã€æ”€çˆ¬é«˜å°ï¼‰ã€‚**æ ¸å¿ƒèŒƒå¼**ï¼šæ„ŸçŸ¥å¼è¶³å¼è¿åŠ¨ã€‚ | **1. ç ”ç©¶å¯¹è±¡ä¸åŒ**ï¼šå‰ä½œèšç„¦**å››è¶³æœºå™¨äºº**ï¼Œæœ¬æ–‡èšç„¦**äººå½¢æœºå™¨äºº**ã€‚<br>**2. äº¤äº’æ¨¡å¼ä¸åŒ**ï¼šå‰ä½œæ˜¯**çº¯è¶³å¼äº¤äº’**ï¼ˆpedal locomotionï¼‰ï¼Œæœºå™¨äººä»…é€šè¿‡è„šä¸ç¯å¢ƒæ¥è§¦ã€‚æœ¬æ–‡æ˜¯**å…¨èº«æ¥è§¦äº¤äº’**ï¼ˆwhole-body multi-contactï¼‰ï¼Œæ¶‰åŠæ‰‹ã€èº¯å¹²ç­‰å¤šéƒ¨ä½æ¥è§¦ã€‚<br>**3. ä»»åŠ¡å¤æ‚æ€§ä¸åŒ**ï¼šå‰ä½œä»»åŠ¡æ˜¯â€œè·³è¿‡å»/çˆ¬ä¸Šå»â€ï¼Œæœ¬æ–‡ä»»åŠ¡æ˜¯å¤æ‚çš„å…¨èº«æŠ€èƒ½ï¼ˆå¦‚æ’‘è·³ã€ç¿»æ»šï¼‰ï¼Œéœ€è¦ç²¾ç¡®çš„æ—¶åºå’Œæ¥è§¦ç‚¹è§„åˆ’ã€‚ |\n| **Humanoid Parkour Learning** | 2024 (CoRL) | **äººå½¢æœºå™¨äººè·‘é…·**ï¼šå°†æ„ŸçŸ¥å¼è¿åŠ¨èŒƒå¼æ‰©å±•åˆ°äººå½¢æœºå™¨äººï¼ˆUnitree H1ï¼‰ï¼Œå®ç°äº†äººå½¢æœºå™¨äººåœ¨å¤æ‚åœ°å½¢ä¸Šçš„**åŠ¨æ€è¶³å¼è·‘é…·**ï¼ˆå¦‚è·³è¿œã€è·³é«˜ï¼‰ã€‚ä½†ä»å±äº**è¶³å¼è¿åŠ¨èŒƒå¼**ï¼Œä¸ŠåŠèº«ä¸»è¦ç”¨äºå¹³è¡¡ï¼Œè€Œéä¸»åŠ¨ç¯å¢ƒäº¤äº’ã€‚ | **1. æ ¸å¿ƒèŒƒå¼èåˆ**ï¼šå‰ä½œæ˜¯**æ„ŸçŸ¥å¼è¿åŠ¨**èŒƒå¼çš„å»¶ä¼¸ï¼Œä»å±â€œè¶³å¼è·‘é…·â€ã€‚æœ¬æ–‡æ˜¯**èŒƒå¼é©å‘½**ï¼Œé¦–æ¬¡å°†**æ„ŸçŸ¥å¼è¿åŠ¨**ä¸**å…¨èº«è¿åŠ¨è·Ÿè¸ª**ä¸¤å¤§èŒƒå¼ç»Ÿä¸€ã€‚<br>**2. æŠ€èƒ½åº“æ‰©å±•**ï¼šå‰ä½œæŠ€èƒ½åŸºäºé€Ÿåº¦å‘½ä»¤ï¼ˆvx, vy, Ï‰zï¼‰ï¼Œæœ¬è´¨æ˜¯é«˜çº§æ­¥æ€ã€‚æœ¬æ–‡æŠ€èƒ½åŸºäº**äººç±»æ¼”ç¤ºçš„å…¨èº«è¿åŠ¨æ•°æ®**ï¼Œèƒ½å®ç°éæ­¥æ€çš„ã€æ¥è§¦ä¸°å¯Œçš„æŠ€èƒ½ï¼ˆvault, rollï¼‰ã€‚<br>**3. æ§åˆ¶æ¥å£**ï¼šå‰ä½œæ˜¯**ä½ç»´å‘½ä»¤**ï¼ˆä»»åŠ¡æ¬ æŒ‡å®šï¼‰ï¼Œæœ¬æ–‡æ˜¯**é«˜ç»´è¿åŠ¨å‚è€ƒè½¨è¿¹**ï¼ˆä»»åŠ¡å…¨æŒ‡å®šï¼‰ã€‚ |\n| **Embrace Contacts** | 2025 (CoRL) | **äººå½¢æœºå™¨äººè¿åŠ¨è·Ÿè¸ª**ï¼šä¸“æ³¨äºä»äººç±»æ¼”ç¤ºä¸­è¿›è¡Œ**å…¨èº«è¿åŠ¨æ¨¡ä»¿**ï¼ˆå¦‚åç©ºç¿»ã€æ­¦æœ¯ï¼‰ï¼Œå¹¶å¼ºè°ƒäº†**å…¨èº«æ¥è§¦**åœ¨è¿åŠ¨ç‰©ç†çœŸå®æ€§ä¸­çš„é‡è¦æ€§ã€‚å±äº**è¿åŠ¨è·Ÿè¸ªèŒƒå¼**ï¼Œä½†**æœªé›†æˆç¯å¢ƒæ„ŸçŸ¥**ã€‚ | **1. å…³é”®è¦ç´ å¢åŠ **ï¼šå‰ä½œæ˜¯â€œç›²â€è·Ÿè¸ªï¼Œ**æ— ç¯å¢ƒæ„ŸçŸ¥**ã€‚æœ¬æ–‡æœ€å¤§åˆ›æ–°æ˜¯**å°†æ·±åº¦æ„ŸçŸ¥é›†æˆåˆ°è¿åŠ¨è·Ÿè¸ªé—­ç¯ä¸­**ï¼Œä½¿æœºå™¨äººèƒ½æ ¹æ®å®é™…åœ°å½¢å‡ ä½•è°ƒæ•´åŠ¨ä½œã€‚<br>**2. ç¯å¢ƒé€‚åº”æ€§**ï¼šå‰ä½œå‡è®¾å¹³é¢ç¯å¢ƒæˆ–å›ºå®šéšœç¢ç‰©ä½ç½®ã€‚æœ¬æ–‡é€šè¿‡æ„ŸçŸ¥ï¼Œèƒ½å¤„ç†**åˆå§‹ä½ç½®éšæœºåŒ–**ã€**åœ°å½¢å˜åŒ–**ï¼Œå®ç°äº†**ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›**ã€‚<br>**3. é²æ£’æ€§æå‡**ï¼šå‰ä½œéœ€è¦ç²¾ç¡®çš„åˆå§‹å¯¹é½ã€‚æœ¬æ–‡ç­–ç•¥èƒ½å®¹å¿è¾ƒå¤§çš„åˆå§‹ä½å§¿è¯¯å·®ï¼Œè‡ªä¸»è°ƒæ•´æ­¥æ€å’Œæ¥è§¦æ—¶æœºã€‚ |\n\n### æœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„å…·ä½“æ”¹è¿›ä¸åˆ›æ–°\n\næœ¬æ–‡ã€ŠDeep Whole-body Parkourã€‹å¹¶éç®€å•å»¶ç»­ï¼Œè€Œæ˜¯åœ¨å‰ä½œåŸºç¡€ä¸Šçš„ä¸€æ¬¡**ç³»ç»Ÿæ€§æ•´åˆä¸è·¨è¶Š**ï¼Œä¸»è¦æ”¹å–„ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªå±‚é¢ï¼š\n\n1.  **èŒƒå¼ç»Ÿä¸€ä¸é—®é¢˜è§£å†³**ï¼š\n    -   **è§£å†³äº†â€œæ„ŸçŸ¥è¿åŠ¨â€èŒƒå¼çš„å±€é™æ€§**ï¼šå‰ä½œã€ŠRobot/Humanoid Parkour Learningã€‹è™½ç„¶è®©æœºå™¨äººâ€œçœ‹å¾—è§â€åœ°å½¢ï¼Œä½†æ§åˆ¶æŒ‡ä»¤ï¼ˆé€Ÿåº¦å‘½ä»¤ï¼‰è¿‡äºç®€å•ï¼Œæ— æ³•è¡¨è¾¾å¤æ‚çš„å…¨èº«ååŒåŠ¨ä½œï¼ˆå¦‚ç”¨æ‰‹æ’‘è·³ï¼‰ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥**å…¨èº«è¿åŠ¨å‚è€ƒè½¨è¿¹**ä½œä¸ºé«˜å±‚ä»»åŠ¡æè¿°ï¼Œè§£å†³äº†â€œä»»åŠ¡æ¬ æŒ‡å®šâ€é—®é¢˜ã€‚\n    -   **è§£å†³äº†â€œè¿åŠ¨è·Ÿè¸ªâ€èŒƒå¼çš„å±€é™æ€§**ï¼šå‰ä½œã€ŠEmbrace Contactsã€‹åŠDeepMimic/AMPç­‰æ–¹æ³•èƒ½ç”Ÿæˆå¤æ‚åŠ¨ä½œï¼Œä½†åƒâ€œç›²äººâ€ä¸€æ ·æ‰§è¡Œå›ºå®šè½¨è¿¹ï¼Œæ— æ³•æ ¹æ®å®é™…ç®±å­é«˜åº¦è°ƒæ•´æ’‘è·³åŠ›åº¦ã€‚æœ¬æ–‡é€šè¿‡**æ³¨å…¥æ·±åº¦è§†è§‰è§‚æµ‹**ï¼Œèµ‹äºˆäº†è¿åŠ¨è·Ÿè¸ªç­–ç•¥**å®æ—¶ç¯å¢ƒé€‚åº”èƒ½åŠ›**ã€‚\n\n2.  **æŠ€æœ¯åˆ›æ–°ä¸å·¥ç¨‹å®ç°**ï¼š\n    -   **æ•°æ®åˆ¶å¤‡**ï¼šåˆ›å»ºäº†**è¿åŠ¨-åœ°å½¢é…å¯¹æ•°æ®é›†**ã€‚ä¸ä»…ç”¨åŠ¨æ•è®°å½•äººç±»è·‘é…·åŠ¨ä½œï¼Œè¿˜åŒæ­¥ç”¨LiDARæ‰«æçœŸå®éšœç¢ç‰©å‡ ä½•ï¼Œç¡®ä¿äº†æ¼”ç¤ºä¸ç¯å¢ƒçš„ä¸¥æ ¼ç©ºé—´å¯¹åº”ã€‚è¿™æ¯”ä½¿ç”¨AMASSç­‰é€šç”¨æ•°æ®é›†æˆ–å‡è®¾å¹³é¢ç¯å¢ƒå‰è¿›äº†ä¸€å¤§æ­¥ã€‚\n    -   **é«˜æ•ˆæ„ŸçŸ¥æ¨¡æ‹Ÿ**ï¼šé’ˆå¯¹å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒä¸­ï¼Œæ•°åƒä¸ªç¯å¢ƒåŒæ—¶æ¸²æŸ“æ·±åº¦å›¾ä¸”éœ€éš”ç¦»æ™ºèƒ½ä½“æ„ŸçŸ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†**é¢„è®¡ç®—åˆ†ç»„å…‰çº¿æŠ•å°„ç®—æ³•**ï¼ˆAlgorithm 1ï¼‰ã€‚é€šè¿‡ç¢°æ’åˆ†ç»„å’Œé¢„è®¡ç®—æ˜ å°„ï¼Œå°†æ¸²æŸ“å¤æ‚åº¦ä»O(N)é™ä½ï¼Œå®ç°äº†**10å€çš„é€Ÿåº¦æå‡**ï¼Œè¿™æ˜¯æ”¯æ’‘å¤§è§„æ¨¡è®­ç»ƒçš„å…³é”®å·¥ç¨‹è´¡çŒ®ã€‚\n    -   **è®­ç»ƒæ¡†æ¶è®¾è®¡**ï¼š\n        -   **ç›¸å¯¹å¸§å¥–åŠ±**ï¼šå®šä¹‰äº†ç»“åˆæœºå™¨äººå¹³é¢ä½ç½®/åèˆªè§’å’Œå‚è€ƒè¿åŠ¨å‚ç›´ä½ç½®/å€¾æ–œè§’çš„â€œç›¸å¯¹å¸§â€ï¼Œåœ¨æ­¤å¸§ä¸‹è®¡ç®—å±€éƒ¨é“¾æ¥è·Ÿè¸ªå¥–åŠ±ã€‚è¿™æ¯”çº¯ç²¹åŸºäºæœºå™¨äººæœ¬ä½“å¸§çš„å¥–åŠ±æ›´èƒ½ä¿ƒè¿›å…¨å±€è¿åŠ¨çš„æ­£ç¡®æ‰§è¡Œï¼ˆè§è¡¨I ablationï¼‰ã€‚\n        -   **è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ **ï¼šé‡‡ç”¨åŸºäºå¤±è´¥çš„è¯¾ç¨‹å­¦ä¹ ï¼ˆAlgorithm 2ï¼‰ï¼Œå¯¹é•¿è¿åŠ¨åºåˆ—åˆ†æ®µå¹¶ç»Ÿè®¡å¤±è´¥æ—¶é—´ï¼ŒåŠ¨æ€è°ƒæ•´é‡‡æ ·åˆ†å¸ƒï¼Œè®©æ™ºèƒ½ä½“æ›´ä¸“æ³¨äºéš¾ä»¥æŒæ¡çš„è¿åŠ¨é˜¶æ®µã€‚\n        -   **å¡æ­»æ£€æµ‹**ï¼šåœ¨è®­ç»ƒåˆæœŸï¼Œéšæœºåˆå§‹åŒ–å¯èƒ½å°†æœºå™¨äººç½®äºåœ°å½¢ä¸­é—´ç­‰æ— è§£çŠ¶æ€ã€‚æœ¬æ–‡å¼•å…¥æ—©æœŸè¶…æ—¶ä¸å¡æ­»æ£€æµ‹æœºåˆ¶ï¼Œé¿å…äº†æ— æ•ˆæ ·æœ¬ï¼Œæå‡äº†è®­ç»ƒæ•ˆç‡ã€‚\n\n3.  **å®é™…ä»·å€¼ä¸æ³›åŒ–èƒ½åŠ›æå‡**ï¼š\n    -   **é›¶é‡Œç¨‹è®¡éƒ¨ç½²**ï¼šç­–ç•¥è®¾è®¡ä½¿**ç½‘ç»œè¾“å…¥ä¸ä¾èµ–äºå…¨å±€é‡Œç¨‹è®¡**ã€‚å‚è€ƒä½ç½®æ˜¯åŸºäºè¿åŠ¨å‚è€ƒç³»çš„ï¼Œæ—‹è½¬å‚è€ƒä»…éœ€ä¸æœºå™¨äººå®æ—¶IMUå¯¹é½ã€‚è¿™ç®€åŒ–äº†çœŸå®ä¸–ç•Œéƒ¨ç½²ï¼Œé¿å…äº†ç´¯ç§¯å®šä½è¯¯å·®çš„å½±å“ã€‚\n    -   **å¼ºå¤§çš„åˆå§‹ä½ç½®å®¹é”™**ï¼šå®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨è®­ç»ƒåˆ†å¸ƒå¤–ï¼ˆåˆå§‹åŒ–èŒƒå›´ä»Â±0.15mæ‰©å¤§åˆ°Â±0.6mï¼‰ï¼Œç­–ç•¥ä»èƒ½é€šè¿‡è§†è§‰åé¦ˆè‡ªåŠ¨è°ƒæ•´æ­¥æ€ï¼Œæ”¶æ•›åˆ°ç›®æ ‡ç‚¹ï¼ŒæˆåŠŸç‡è¾¾100%ï¼ˆè§å›¾4,5,6ï¼‰ã€‚è¿™æ˜¯â€œç›²â€è·Ÿè¸ªåŸºçº¿æ— æ³•åšåˆ°çš„ã€‚\n    -   **å¯¹è§†è§‰å¹²æ‰°çš„é²æ£’æ€§**ï¼šåœ¨ä»¿çœŸä¸­æµ‹è¯•äº†æœªè®­ç»ƒè¿‡çš„åœºæ™¯å¹²æ‰°ç‰©ï¼ˆå®½æ¢ã€å¹³æ¿ã€å¢™ï¼‰ï¼Œç­–ç•¥åœ¨å¤šæ•°æƒ…å†µä¸‹æ€§èƒ½ä¸‹é™å¾ˆå°ï¼ˆè§è¡¨IIï¼‰ï¼Œè¯æ˜äº†å…¶ä¸“æ³¨äºä»»åŠ¡ç›¸å…³å‡ ä½•ç‰¹å¾çš„èƒ½åŠ›ã€‚\n\n**æ€»ç»“**ï¼šæœ¬æ–‡ä½œè€…çš„å‰ä½œåˆ†åˆ«æ·±è€•äº**æ„ŸçŸ¥å¼è¶³å¼è¿åŠ¨**å’Œ**æ— æ„ŸçŸ¥å…¨èº«è¿åŠ¨è·Ÿè¸ª**ä¸¤ä¸ªæ–¹å‘ã€‚æœ¬æ–‡çš„**æ ¸å¿ƒçªç ´**åœ¨äºå°†äºŒè€…æœ‰æœºç»“åˆï¼Œåˆ›é€ äº†ä¸€ä¸ª**æ„ŸçŸ¥å¼å…¨èº«è¿åŠ¨æ§åˆ¶**çš„æ–°èŒƒå¼ã€‚å…·ä½“æ”¹è¿›æ˜¯**é€šè¿‡ä¸€å¥—åˆ›æ–°çš„æ•°æ®ã€ç®—æ³•å’Œè®­ç»ƒæ¡†æ¶**ï¼Œä½¿æœºå™¨äººä¸ä»…èƒ½æ¨¡ä»¿äººç±»çš„å¤æ‚åŠ¨æ€æŠ€èƒ½ï¼Œè¿˜èƒ½æ ¹æ®å®æ—¶çœ‹åˆ°çš„3Dç¯å¢ƒç²¾ç¡®è°ƒæ•´è¿™äº›æŠ€èƒ½ï¼Œä»è€Œåœ¨çœŸå®éç»“æ„åŒ–åœ°å½¢ä¸­å®ç°é²æ£’çš„ã€æ¥è§¦ä¸°å¯Œçš„è·‘é…·è¡Œä¸ºã€‚è¿™ä¸ºäººå½¢æœºå™¨äººè¿ˆå‘æ›´é€šç”¨ã€æ›´è‡ªä¸»çš„ç¯å¢ƒäº¤äº’èƒ½åŠ›å¥ å®šäº†å…³é”®åŸºç¡€ã€‚",
            "timestamp": "2026-01-21T12:12:09.986542",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä»”ç»†åˆ†æè¿™ç¯‡é¢˜ä¸ºã€ŠDeep Whole-body Parkourã€‹çš„è®ºæ–‡ï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ä¸ä»·å€¼\n\næœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äº**å°†æ„ŸçŸ¥å¼è¿åŠ¨ï¼ˆPerceptive Locomotionï¼‰ä¸å…¨èº«è¿åŠ¨è·Ÿè¸ªï¼ˆWhole-body Motion Trackingï¼‰ä¸¤å¤§èŒƒå¼ç»Ÿä¸€**ï¼Œæå‡ºäº†ä¸€ä¸ª**æ„ŸçŸ¥å¼å…¨èº«è¿åŠ¨æ§åˆ¶**æ¡†æ¶ã€‚è¿™ä½¿å¾—äººå½¢æœºå™¨äººèƒ½å¤Ÿåœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šæ‰§è¡Œé«˜åº¦åŠ¨æ€ã€å¤šæ¥è§¦çš„è·‘é…·åŠ¨ä½œï¼ˆå¦‚æ’‘è·³ã€ç¿»æ»šï¼‰ï¼Œæ˜¾è‘—æ‰©å±•äº†æœºå™¨äººçš„é€šè¿‡èƒ½åŠ›ã€‚\n\n## å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€æ•°å­¦åŸç†åŠå…·ä½“å‚æ•°ã€‚\n\n| æŠ€æœ¯æ–¹æ³• | æ ¸å¿ƒç›®æ ‡ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. æ•°æ®é‡‡é›†ä¸é…å¯¹æ•°æ®é›†æ„å»º** | è·å–ä¸åœ°å½¢ç²¾ç¡®å¯¹é½çš„åŠ¨æ€ã€å…¨èº«äº¤äº’è¿åŠ¨æ•°æ®ï¼Œè§£å†³ç°æœ‰æ•°æ®é›†ï¼ˆå¦‚AMASSï¼‰ç¼ºä¹æ­¤ç±»æ•°æ®çš„é—®é¢˜ã€‚ | **åŸç†**ï¼šä¸ºç¡®ä¿è¿åŠ¨ä¸åœ°å½¢çš„ç©ºé—´å¯¹åº”æ€§ï¼Œé‡‡ç”¨åŒæ­¥é‡‡é›†ç­–ç•¥ï¼š<br>1. **è¿åŠ¨æ•æ‰**ï¼šä½¿ç”¨å…‰å­¦åŠ¨æ•ç³»ç»Ÿè®°å½•äººç±»ä¸“å®¶åœ¨çœŸå®éšœç¢ç‰©ä¸Šçš„è·‘é…·åŠ¨ä½œã€‚<br>2. **åœ°å½¢æ‰«æ**ï¼šä½¿ç”¨å¸¦LiDARçš„iPad Proï¼ˆ3D Scanner Appï¼‰åŒæ­¥æ‰«æç‰©ç†åœºæ™¯ï¼Œç”Ÿæˆä¸åŠ¨æ•è½¨è¿¹ç©ºé—´å¯¹é½çš„ç½‘æ ¼æ¨¡å‹ã€‚<br>3. **è¿åŠ¨é‡å®šå‘**ï¼šä½¿ç”¨GMRæ¡†æ¶å°†äººä½“è¿åŠ¨é‡å®šå‘åˆ°Unitree G1æœºå™¨äººæ¨¡å‹ã€‚é€šè¿‡ä¼˜åŒ–å’Œæ‰‹åŠ¨å…³é”®å¸§è°ƒæ•´ï¼Œç¡®ä¿è¿åŠ¨åœ¨æœºå™¨äººå½¢æ€å­¦ä¸Šç‰©ç†å¯è¡Œï¼Œå¹¶æ»¡è¶³æ¥è§¦çº¦æŸã€‚ | - **è®¾å¤‡**ï¼šå…‰å­¦åŠ¨æ•ç³»ç»Ÿã€iPad Proï¼ˆLiDARï¼‰ã€‚<br>- **æœºå™¨äººå¹³å°**ï¼šUnitree G1ï¼ˆ29è‡ªç”±åº¦ï¼‰ã€‚<br>- **é‡å®šå‘å·¥å…·**ï¼šGMRæ¡†æ¶ã€‚ |\n| **2. å¤§è§„æ¨¡å¹¶è¡Œåˆ†ç»„å…‰çº¿æŠ•å°„ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰** | åœ¨æ•°åƒä¸ªå¹¶è¡Œä»¿çœŸç¯å¢ƒä¸­ï¼Œå®ç°é«˜æ•ˆã€éš”ç¦»çš„æ·±åº¦å›¾åƒæ¨¡æ‹Ÿï¼Œé¿å…ä¸åŒç¯å¢ƒä¸­çš„æœºå™¨äººç›¸äº’â€œçœ‹è§â€ï¼ˆâ€œghostâ€ instancesï¼‰ã€‚ | **æ•°å­¦åŸç†ä¸ç®—æ³•**ï¼š<br>- **é—®é¢˜**ï¼šæ ‡å‡†æ¸²æŸ“å™¨éš¾ä»¥åœ¨å¹¶è¡Œç¯å¢ƒä¸­éš”ç¦»æ¸²æŸ“åŠ¨æ€å…³èŠ‚å’Œé™æ€åœ°å½¢ã€‚<br>- **è§£å†³æ–¹æ¡ˆ**ï¼šè‡ªå®šä¹‰åŸºäºNvidia Warpçš„å…‰çº¿æŠ•å°„å™¨ï¼Œå¼•å…¥**ç¢°æ’åˆ†ç»„æœºåˆ¶**ã€‚<br>  - é™æ€åœ°å½¢åˆ†é…å…¨å±€ç»„ID `-1`ï¼Œå¯¹æ‰€æœ‰æ™ºèƒ½ä½“å¯è§ã€‚<br>  - æ¯ä¸ªæœºå™¨äººåˆ†é…å”¯ä¸€ç»„IDï¼Œç¡®ä¿æ„ŸçŸ¥éš”ç¦»ã€‚<br>- **åŠ é€Ÿç­–ç•¥ï¼ˆç®—æ³•1ï¼‰**ï¼š<br>  1. **é¢„è®¡ç®—åŠ é€Ÿç»“æ„**ï¼šæ„å»ºå“ˆå¸Œæ˜ å°„ `â„‹_map`ï¼Œå°†ç»„IDæ˜ å°„åˆ°ç½‘æ ¼å®ä¾‹ç´¢å¼•åˆ—è¡¨ã€‚<br>  2. **å¹¶è¡Œæ¸²æŸ“**ï¼šå¯¹äºæ¯æ¡å…‰çº¿ï¼ˆåŒ…å«åŸç‚¹ `o_i`ã€æ–¹å‘ `d_i`ã€ç»„ID `g_i`ï¼‰ï¼š<br>     a. é¦–å…ˆæ£€æŸ¥å…¨å±€é™æ€åœ°å½¢ï¼ˆç»„IDä¸º`-1`çš„ç½‘æ ¼ï¼‰ã€‚<br>     b. ç„¶åä»…æ£€æŸ¥å±äºè¯¥å…‰çº¿ç»„ID `g_i` çš„åŠ¨æ€ç½‘æ ¼ã€‚<br>  - **å¤æ‚åº¦**ï¼šä»æœ´ç´ çš„O(N)é™ä½åˆ°O(1)ï¼ˆé™æ€ï¼‰+ O(N_agent)ï¼Œå…¶ä¸­N_agentæ˜¯å•ä¸ªæœºå™¨äººçš„ç½‘æ ¼æ•°ã€‚ | - **å®ç°å¹³å°**ï¼šNVIDIA Isaac Lab, Nvidia Warpã€‚<br>- **æ€§èƒ½æå‡**ï¼šç›¸æ¯”æœ´ç´ å®ç°ï¼Œæ¸²æŸ“é€Ÿåº¦æå‡ **10å€**ã€‚<br>- **å…³é”®å‚æ•°**ï¼šé™æ€ç»„ID = `-1`ã€‚ |\n| **3. ç›¸å¯¹å¸§å¥–åŠ±è®¾è®¡** | è®¾è®¡ä¸€ç§å¥–åŠ±è®¡ç®—æ¡†æ¶ï¼Œä½¿ç­–ç•¥æ—¢èƒ½è·Ÿè¸ªå‚è€ƒè¿åŠ¨çš„æ•´ä½“å§¿æ€ï¼Œåˆèƒ½é€‚åº”æœºå™¨äººåœ¨åœ°å½¢ä¸Šçš„å®é™…ä½ç½®å’Œæœå‘ã€‚ | **æ•°å­¦æ¨å¯¼**ï¼š<br>å®šä¹‰å‚è€ƒæ ¹å˜æ¢ `T_ref`ã€æœºå™¨äººå®é™…æ ¹å˜æ¢ `T_robot`ã€‚ç›¸å¯¹å¸§å˜æ¢ `T_rel` æ˜¯ä¸€ä¸ª4x4çŸ©é˜µï¼š<br>`T_rel = [ [R_rel, p_rel], [0, 1] ]`<br><br>**å¹³ç§» `p_rel`**ï¼šç»“åˆæœºå™¨äººçš„å¹³é¢ä½ç½®å’Œå‚è€ƒçš„å‚ç›´ä½ç½®ã€‚<br>`p_rel = [x_robot, y_robot, z_ref]^T`<br><br>**æ—‹è½¬ `R_rel`**ï¼šç»“åˆæœºå™¨äººçš„åèˆªè§’ï¼ˆYawï¼‰å’Œå‚è€ƒçš„æ¨ªæ»šè§’ï¼ˆRollï¼‰ã€ä¿¯ä»°è§’ï¼ˆPitchï¼‰ã€‚<br>`R_rel = R_z(Ïˆ_robot) * R_y(Î¸_ref) * R_x(Ï†_ref)`<br>å…¶ä¸­ `R_z, R_y, R_x` åˆ†åˆ«ä¸ºç»•Zã€Yã€Xè½´çš„åŸºæœ¬æ—‹è½¬çŸ©é˜µã€‚<br><br>**å¥–åŠ±è®¡ç®—**ï¼šåœ¨æ­¤ç›¸å¯¹å¸§ä¸‹è®¡ç®—å±€éƒ¨å…³èŠ‚ä½ç½®å’Œæ—‹è½¬çš„è·Ÿè¸ªè¯¯å·®ï¼ˆå¦‚è½´è§’å·®ï¼‰ï¼Œè€Œéç›´æ¥åœ¨æœºå™¨äººåŸºåº§æ ‡ç³»æˆ–ä¸–ç•Œåæ ‡ç³»ä¸‹è®¡ç®—ã€‚è¿™ä½¿å¾—ç­–ç•¥å­¦ä¹ åœ¨æ°´å¹³é¢ä¸Šâ€œè·Ÿéšâ€æœºå™¨äººè‡ªå·±çš„ä½ç½®ï¼Œåœ¨å‚ç›´å’Œå€¾æ–œé¢ä¸Šâ€œè·Ÿéšâ€å‚è€ƒè¿åŠ¨çš„å§¿æ€ã€‚ | - **å‚è€ƒå¸§é•¿åº¦**ï¼šç­–ç•¥ç½‘ç»œè¾“å…¥æœªæ¥ **1.0ç§’** çš„å‚è€ƒä¿¡æ¯ï¼Œä»¥ **0.1ç§’** ä¸ºé—´éš”ï¼Œå…± **10å¸§**ã€‚<br>- **å†å²ä¿¡æ¯**ï¼šç­–ç•¥ç½‘ç»œè¾“å…¥ **8å¸§** å†å²æœ¬ä½“æ„ŸçŸ¥ä¿¡æ¯ã€‚ |\n| **4. è‡ªé€‚åº”é‡‡æ ·ä¸è¯¾ç¨‹å­¦ä¹ ** | é€šè¿‡åŸºäºå¤±è´¥çš„è¯¾ç¨‹å­¦ä¹ ï¼Œè‡ªåŠ¨å°†è®­ç»ƒé‡ç‚¹é›†ä¸­åœ¨éš¾ä»¥æŒæ¡çš„è¿åŠ¨ç‰‡æ®µä¸Šï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚ | **ç®—æ³•åŸç†ï¼ˆç®—æ³•2ï¼‰**ï¼š<br>1. **ç¦»æ•£åŒ–**ï¼šå°†æ¯ä¸ªå‚è€ƒè¿åŠ¨ `m_k` æŒ‰æœ€å¤§æ—¶é•¿ `Î”t_max` ç¦»æ•£åŒ–ä¸ºæ—¶é—´ä»“ `b_{k,t}`ã€‚<br>2. **å¤±è´¥ç»Ÿè®¡**ï¼šå½“ç­–ç•¥åœ¨æ—¶é—´ `t_fail` å¤±è´¥æ—¶ï¼Œæ‰¾åˆ°å¯¹åº”çš„ä»“ç´¢å¼• `t_hat`ï¼Œå¢åŠ å…¶å¤±è´¥è®¡æ•° `F_{k, t_hat}`ã€‚<br>3. **æ—¶é—´å¹³æ»‘**ï¼šå¯¹åŸå§‹å¤±è´¥è®¡æ•°åº”ç”¨å¹³æ»‘æ ¸ `ğ’¦`ï¼ˆå¦‚æŒ‡æ•°æ ¸ï¼‰ï¼Œå¾—åˆ°å¹³æ»‘åçš„å¤±è´¥åˆ†æ•° `FÌƒ_{k, j}`ï¼Œå°†å›°éš¾ä¿¡å·æ‰©æ•£åˆ°ç›¸é‚»æ—¶é—´æ­¥ã€‚<br>4. **é‡‡æ ·åˆ†å¸ƒæ›´æ–°**ï¼šæ ¹æ®å¹³æ»‘åçš„å¤±è´¥åˆ†æ•°æ›´æ–°æ¯ä¸ªä»“çš„é‡‡æ ·æƒé‡ `W_{k,t} = FÌƒ_{k,t} + Ïµ`ï¼ˆ`Ïµ` ç¡®ä¿éé›¶æ¦‚ç‡ï¼‰ã€‚<br>5. **é‡ç½®çŠ¶æ€é‡‡æ ·**ï¼šæ¯æ¬¡è®­ç»ƒè¿­ä»£ï¼Œæ ¹æ®å½’ä¸€åŒ–çš„æƒé‡åˆ†å¸ƒ `P(k,t)` é‡‡æ ·èµ·å§‹è¿åŠ¨ `k` å’Œèµ·å§‹æ—¶é—´ `t_start`ã€‚ | - **ä»“æœ€å¤§æ—¶é•¿**ï¼š`Î”t_max = 1.0ç§’`ã€‚<br>- **é˜²å‘†æ£€æµ‹**ï¼šåœ¨æƒ…èŠ‚å¼€å§‹æ—¶ï¼Œè‹¥æœºå™¨äººå› éšæœºåˆå§‹åŒ–å¡åœ¨éšœç¢ç‰©ä¸­è¶…è¿‡æ—¶é—´é™åˆ¶ï¼Œåˆ™æå‰ç»ˆæ­¢è¯¥å›åˆï¼Œé¿å…æ— æ•ˆæ•°æ®ã€‚ |\n| **5. æ¨¡æ‹Ÿåˆ°çœŸå®çš„æ·±åº¦æ„ŸçŸ¥è¿ç§»** | é€šè¿‡åœ¨ä»¿çœŸä¸­æ¨¡æ‹ŸçœŸå®æ·±åº¦ç›¸æœºçš„å™ªå£°å’Œä¼ªå½±ï¼Œç¼©å°æ·±åº¦æ„ŸçŸ¥çš„æ¨¡æ‹Ÿ-ç°å®å·®è·ã€‚ | **åŸç†**ï¼šä¸ºäº†ä½¿ç­–ç•¥å¯¹çœŸå®ä¸–ç•Œçš„æ·±åº¦å›¾åƒå™ªå£°é²æ£’ï¼Œåœ¨ä»¿çœŸè®­ç»ƒæ—¶å¯¹æ·±åº¦å›¾åƒæ·»åŠ å¤šç§å™ªå£°æ¨¡å¼ï¼š<br>1. **é«˜æ–¯å™ªå£°**ã€‚<br>2. **å—çŠ¶ä¼ªå½±**ï¼ˆæ¨¡æ‹Ÿç«‹ä½“åŒ¹é…ç®—æ³•é”™è¯¯ï¼‰ã€‚<br>3. **è¿åŠ¨æ¨¡ç³Š**æ•ˆåº”ã€‚<br><br>**éƒ¨ç½²æ—¶å¤„ç†**ï¼šåœ¨çœŸå®æœºå™¨äººï¼ˆä½¿ç”¨Intel RealSense D435iï¼‰ä¸Šï¼Œä¸ºæ»¡è¶³50Hzå¤„ç†éœ€æ±‚ï¼Œä½¿ç”¨**åŸºäºGPUçš„OpenCVä¿®å¤ç®—æ³•**è¿›è¡Œæ·±åº¦å›¾ä¿®å¤ï¼Œè€ŒéCPUç«¯çš„pyrealsenseå†…ç½®æ»¤æ³¢å™¨ã€‚ | - **ç›¸æœº**ï¼šIntel RealSense D435iã€‚<br>- **å¤„ç†é¢‘ç‡**ï¼š50Hzã€‚<br>- **éƒ¨ç½²åŠ é€Ÿ**ï¼šä½¿ç”¨ONNX Runtimeï¼ˆCPUç‰ˆï¼‰åŠ é€Ÿç½‘ç»œæ¨ç†ã€‚<br>- **å™ªå£°æ¨¡æ‹Ÿ**ï¼šå‚è€ƒäº†ä½œè€…ä¹‹å‰çš„å·¥ä½œï¼ˆZhuang et al. 2023, 2024ï¼‰ã€‚ |\n| **6. æ— é‡Œç¨‹è®¡éƒ¨ç½²æ–¹æ¡ˆ** | æ¶ˆé™¤å¯¹å…¨å±€å®šä½æˆ–ç²¾ç¡®é‡Œç¨‹è®¡ç³»ç»Ÿçš„ä¾èµ–ï¼Œä½¿ç³»ç»Ÿéƒ¨ç½²æ›´ç®€å•ã€æ›´é²æ£’ã€‚ | **æ•°å­¦ä¸å·¥ç¨‹åŸç†**ï¼š<br>- **å…³é”®è§‚å¯Ÿ**ï¼šç­–ç•¥è®­ç»ƒæ—¶ï¼Œè¾“å…¥çš„**æ ¹ä½ç½®å‚è€ƒæ˜¯åœ¨è¿åŠ¨å‚è€ƒç³»ä¸­**ï¼Œä¸ä¾èµ–äºæœºå™¨äººçš„é‡Œç¨‹è®¡ã€‚<br>- **éƒ¨ç½²æµç¨‹**ï¼š<br>  1. **å¯¹é½èˆªå‘**ï¼šåœ¨åŠ¨ä½œå¼€å§‹å‰ï¼Œå°†å‚è€ƒè¿åŠ¨çš„åˆå§‹èˆªå‘ä¸æœºå™¨äººIMUè¯»å–çš„å®é™…èˆªå‘å¯¹é½ã€‚<br>  2. **ä¿®æ­£å‚è€ƒ**ï¼šæ ¹æ®å¯¹é½åçš„èˆªå‘ï¼Œæ—‹è½¬æ•´ä¸ªå‚è€ƒä½ç½®åºåˆ—çš„æ–¹å‘ã€‚<br>  3. **æä¾›è§‚æµ‹**ï¼šå‘ç­–ç•¥æä¾›ä¿®æ­£åçš„å…³èŠ‚ä½ç½®/é€Ÿåº¦å‚è€ƒï¼Œä»¥åŠåŸºäºå®æ—¶IMUè¯»æ•°è®¡ç®—çš„**ç›¸å¯¹æ—‹è½¬å·®**ï¼ˆä½œä¸ºæœªæ¥åŸºåº§æ—‹è½¬å‚è€ƒï¼‰ã€‚<br>  ç­–ç•¥ä»…æ¥æ”¶**ç›¸å¯¹äºå‚è€ƒä½ç½®å¸§çš„ä½ç½®å‚è€ƒ**ï¼Œæ— éœ€ä»»ä½•å…¨å±€ä½ç½®åé¦ˆã€‚ | - **çŠ¶æ€æœº**ï¼šéƒ¨ç½²æ—¶ä½¿ç”¨çŠ¶æ€æœºæœºåˆ¶æ¥é€‰æ‹©è¦æ‰§è¡Œçš„åŠ¨ä½œå‚è€ƒã€‚<br>- **è¿›ç¨‹éš”ç¦»**ï¼šå°†ç¥ç»ç½‘ç»œæ¨ç†è¿›ç¨‹ä¸rosbagæ—¥å¿—è®°å½•è¿›ç¨‹ç»‘å®šåˆ°ä¸åŒçš„CPUæ ¸å¿ƒï¼Œé¿å…è°ƒåº¦å¹²æ‰°ã€‚<br>- **æ— å…¨å±€å®šä½**ï¼šæ•´ä¸ªç³»ç»Ÿä¸ä¾èµ–SLAMæˆ–è§†è§‰é‡Œç¨‹è®¡æä¾›å…¨å±€ä½ç½®ã€‚ |\n\n## æ€»ç»“\n\næœ¬æ–‡çš„æŠ€æœ¯åˆ›æ–°ä½“ç³»ç¯ç¯ç›¸æ‰£ï¼šä»**æ•°æ®å±‚é¢çš„ç²¾å‡†å¯¹é½**ï¼Œåˆ°**ä»¿çœŸå±‚é¢çš„å¤§è§„æ¨¡é«˜æ•ˆæ„ŸçŸ¥æ¨¡æ‹Ÿ**ï¼Œå†åˆ°**ç®—æ³•å±‚é¢çš„ç›¸å¯¹å¸§å¥–åŠ±å’Œè‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ **ï¼Œæœ€åé€šè¿‡**ç²¾å¿ƒè®¾è®¡çš„å™ªå£°æ¨¡æ‹Ÿå’Œæ— é‡Œç¨‹è®¡éƒ¨ç½²æ–¹æ¡ˆ**å®ç° sim-to-realã€‚è¿™å¥—æ–¹æ³•æˆåŠŸåœ°å°†è§†è§‰æ„ŸçŸ¥é—­ç¯",
            "timestamp": "2026-01-21T12:13:16.122030",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡åˆ†æï¼šè¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ„ŸçŸ¥ä¸å…¨èº«è¿åŠ¨è·Ÿè¸ªç›¸ç»“åˆçš„äººå½¢æœºå™¨äººæ§åˆ¶æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåœ¨äºä½¿ç”¨**è‡ªå®šä¹‰é‡‡é›†çš„æ•°æ®é›†**å’Œ**ç‰¹å®šçš„è¯„ä»·æŒ‡æ ‡**æ¥è®­ç»ƒå’ŒéªŒè¯æ¨¡å‹ã€‚\n\n### ä¸€ã€ æ•°æ®é›†ç»†èŠ‚\n\næœ¬æ–‡æ²¡æœ‰ä½¿ç”¨ç°æœ‰çš„é€šç”¨å¤§è§„æ¨¡è¿åŠ¨æ•°æ®é›†ï¼ˆå¦‚AMASSï¼‰ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹ä¸å¤§å‹åœ°å½¢å‡ ä½•ç»“æ„çš„åŠ¨æ€å…¨èº«äº¤äº’æ•°æ®ã€‚ä½œè€…**ä¸“é—¨ä¸ºç ”ç©¶å®šåˆ¶äº†ä¸€ä¸ªæ•°æ®é›†**ã€‚\n\n| æ•°æ®é¡¹ | å…·ä½“ç»†èŠ‚ä¸æè¿° |\n| :--- | :--- |\n| **æ•°æ®å†…å®¹** | **äººç±»è·‘é…·åŠ¨ä½œ**ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šæ’‘è·³ã€ç¿»æ»šã€è·ªå§¿æ”€çˆ¬ã€è·³åç­‰éœ€è¦ä¸éšœç¢ç‰©è¿›è¡Œå¤šæ¥è§¦çš„æ•æ·åŠ¨ä½œã€‚ |\n| **é‡‡é›†è®¾å¤‡** | 1. **å…‰å­¦åŠ¨ä½œæ•æ‰ç³»ç»Ÿ**ï¼šç”¨äºè®°å½•ä¸“ä¸šæ¼”å‘˜çš„åŠ¨ä½œè½¨è¿¹ã€‚<br>2. **å¸¦LiDARçš„iPad Pro**ï¼ˆé€šè¿‡3D Scanner Appï¼‰ï¼šç”¨äºåŒæ­¥æ‰«æå’Œæ•°å­—åŒ–æ¼”å‘˜è¡¨æ¼”æ—¶æ‰€å¤„çš„**çœŸå®ç‰©ç†éšœç¢ç‰©åœºæ™¯**ã€‚ |\n| **æ ¸å¿ƒç‰¹ç‚¹** | **ä¸¥æ ¼çš„åŠ¨ä½œ-åœ°å½¢é…å¯¹**ï¼šç¡®ä¿æ¯ä¸ªåŠ¨ä½œæ¼”ç¤ºéƒ½ä¸å…¶å‘ç”Ÿæ—¶çš„ç²¾ç¡®ç¯å¢ƒå‡ ä½•ç»“æ„ï¼ˆç½‘æ ¼æ¨¡å‹ï¼‰åœ¨ç©ºé—´ä¸Šå¯¹é½ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ã€‚è¿™æ˜¯å®ç°æ„ŸçŸ¥è¿åŠ¨æ§åˆ¶çš„åŸºç¡€ã€‚ |\n| **åå¤„ç†** | 1. **è¿åŠ¨é‡å®šå‘**ï¼šä½¿ç”¨GMRæ¡†æ¶å°†äººä½“åŠ¨ä½œé‡å®šå‘åˆ°Unitree G1äººå½¢æœºå™¨äººï¼Œé€šè¿‡ä¼˜åŒ–å’Œæ‰‹åŠ¨è°ƒæ•´ç¡®ä¿ç‰©ç†å¯è¡Œæ€§ã€‚<br>2. **ç¯å¢ƒç”Ÿæˆ**ï¼šä»æ‰«æçš„ç½‘æ ¼ä¸­åˆ†å‰²å‡ºåŠŸèƒ½æ€§éšœç¢ç‰©ï¼ˆå¦‚ç®±å­ã€æ æ†ï¼‰ï¼Œå»é™¤å®éªŒå®¤èƒŒæ™¯ï¼Œç”Ÿæˆ**é€šç”¨çš„ã€ä¸ä¸Šä¸‹æ–‡æ— å…³çš„éšœç¢ç‰©èµ„äº§**ã€‚ |\n| **è®­ç»ƒä½¿ç”¨** | å°†é‡å®šå‘åçš„åŠ¨ä½œåºåˆ—ä¸å…¶å¯¹åº”çš„éšœç¢ç‰©ç½‘æ ¼ä½œä¸ºâ€œé…å¯¹å®ä¾‹â€ï¼Œåœ¨NVIDIA Isaac Labä¸­**ç¨‹åºåŒ–åœ°ã€å¤§è§„æ¨¡å¹¶è¡Œåœ°å®ä¾‹åŒ–**ï¼Œåˆ›å»ºå¤šæ ·åŒ–çš„è®­ç»ƒç¯å¢ƒï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šåœºæ™¯ã€‚ |\n\n### äºŒã€ è¯„ä»·æŒ‡æ ‡ç»†èŠ‚\n\nè®ºæ–‡ä¸»è¦é€šè¿‡ä»¿çœŸå®éªŒå’ŒæŒ‡æ ‡å¯¹æ¯”æ¥é‡åŒ–æ–¹æ³•çš„æ€§èƒ½ï¼Œä¸»è¦è¯„ä»·æŒ‡æ ‡å¦‚ä¸‹ï¼š\n\n| è¯„ä»·æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ä¸ç‰©ç†æ„ä¹‰ | ç”¨é€”ä¸è§£è¯» |\n| :--- | :--- | :--- |\n| **MPJPEâ‚‰ (å…¨å±€å¹³å‡å…³èŠ‚ä½ç½®è¯¯å·®)** | **Mean Per Joint Position Error (Global)**ã€‚è®¡ç®—æ‰€æœ‰å…³é”®å…³èŠ‚åœ¨**ä¸–ç•Œåæ ‡ç³»**ä¸‹çš„ä½ç½®ä¸å‚è€ƒè¿åŠ¨å¯¹åº”å…³èŠ‚ä½ç½®ä¹‹é—´çš„å¹³å‡æ¬§æ°è·ç¦»ã€‚ | è¡¡é‡**æ•´ä½“è¿åŠ¨è·Ÿè¸ªçš„å…¨å±€ç²¾åº¦**ã€‚å€¼è¶Šå°ï¼Œè¯´æ˜æœºå™¨äººå®é™…å®Œæˆçš„åŠ¨ä½œåœ¨ä¸‰ç»´ç©ºé—´é‡Œä¸ç›®æ ‡åŠ¨ä½œè¶Šæ¥è¿‘ã€‚è¿™å¯¹äºè¯„ä¼°ä¸éšœç¢ç‰©äº¤äº’ï¼ˆå¦‚æ‰‹æ˜¯å¦å‡†ç¡®æ’‘åˆ°ç®±å­ï¼‰è‡³å…³é‡è¦ã€‚ |\n| **MPJPEáµ¦ (åŸºåº§åæ ‡ç³»å¹³å‡å…³èŠ‚ä½ç½®è¯¯å·®)** | **Mean Per Joint Position Error (Base)**ã€‚è®¡ç®—æ‰€æœ‰å…³é”®å…³èŠ‚åœ¨**æœºå™¨äººåŸºåº§åæ ‡ç³»**ä¸‹çš„ä½ç½®ä¸å‚è€ƒè¿åŠ¨å¯¹åº”å…³èŠ‚ä½ç½®ä¹‹é—´çš„å¹³å‡æ¬§æ°è·ç¦»ã€‚ | è¡¡é‡**å±€éƒ¨å§¿æ€æ¨¡ä»¿çš„ç²¾åº¦**ï¼Œæ’é™¤äº†å…¨å±€ä½ç½®å’Œåèˆªè§’çš„å½±å“ã€‚å€¼å°è¯´æ˜æœºå™¨äººçš„èº«ä½“å§¿æ€ã€å››è‚¢ç›¸å¯¹ä½ç½®æ¨¡ä»¿å¾—å¥½ã€‚ |\n| **æˆåŠŸç‡** | åœ¨ç‰¹å®šåˆå§‹ä½ç½®èŒƒå›´å†…ï¼Œç­–ç•¥æˆåŠŸå®Œæˆæ•´ä¸ªåŠ¨ä½œåºåˆ—ï¼ˆå¦‚æˆåŠŸæ’‘è·³è¶Šè¿‡éšœç¢ï¼‰çš„æ¯”ç‡ã€‚ | **ç»¼åˆæ€§èƒ½çš„ç›´è§‚ä½“ç°**ã€‚è®ºæ–‡é€šè¿‡ç½‘æ ¼æœç´¢ç»˜åˆ¶çƒ­åŠ›å›¾ï¼ˆå›¾5ï¼‰ï¼Œå±•ç¤ºäº†æ–¹æ³•åœ¨è¿œè¶…è®­ç»ƒåˆ†å¸ƒçš„å¤§èŒƒå›´åˆå§‹ä½ç½®ä¸‹ä»èƒ½ä¿æŒé«˜æˆåŠŸç‡ï¼Œè¯æ˜äº†å…¶**é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›**ã€‚ |\n| **ä½ç½®æ”¶æ•›æ–¹å·®** | åœ¨ä»¿çœŸä¸­ï¼Œä»ä¸åŒåˆå§‹ä½ç½®å¼€å§‹çš„ä¸€æ‰¹æœºå™¨äººï¼Œå…¶`(x, y)`æ°´å¹³ä½ç½®éšæ—¶é—´å˜åŒ–çš„æ–¹å·®ã€‚ | ç›´è§‚å±•ç¤º**æ·±åº¦è§†è§‰å¼•å…¥çš„â€œä½ç½®è‡ªåŠ¨æ ¡æ­£â€èƒ½åŠ›**ã€‚å¦‚å›¾4æ‰€ç¤ºï¼Œæ–¹å·®è¿…é€Ÿå‡å°ï¼Œè¡¨æ˜ç­–ç•¥èƒ½åˆ©ç”¨è§†è§‰ä¿¡æ¯ä¸»åŠ¨è°ƒæ•´æ­¥æ€ï¼Œä½¿æ‰€æœ‰ä¸ªä½“æ”¶æ•›åˆ°æ­£ç¡®çš„äº¤äº’ç‚¹ï¼Œè€Œâ€œç›²è·Ÿè¸ªâ€ç­–ç•¥æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚ |\n| **å¯¹å¹²æ‰°ç‰©çš„é²æ£’æ€§** | åœ¨ä»¿çœŸåœºæ™¯ä¸­æ·»åŠ è®­ç»ƒæ—¶æœªè§è¿‡çš„å¹²æ‰°ç‰©ï¼ˆå¦‚å®½æ¢ã€å¹³æ¿ã€å¢™ï¼Œè§å›¾7ï¼‰ï¼Œä½¿ç”¨ä¸Šè¿°MPJPEæŒ‡æ ‡è¯„ä¼°ç­–ç•¥æ€§èƒ½æ˜¯å¦ä¸‹é™ã€‚ | æµ‹è¯•**æ„ŸçŸ¥ç³»ç»Ÿçš„ç¨³å¥æ€§**ã€‚å¦‚è¡¨IIæ‰€ç¤ºï¼Œå¤§éƒ¨åˆ†å¹²æ‰°ç‰©æœªå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¯´æ˜ç­–ç•¥å…³æ³¨çš„æ˜¯ä¸ä»»åŠ¡ç›¸å…³çš„å…³é”®åœ°å½¢ç‰¹å¾ï¼Œè€Œéè¢«æ— å…³è§†è§‰ä¿¡æ¯è¯¯å¯¼ã€‚ |\n\n### ä¸‰ã€ æŒ‡æ ‡ä¸æ•°æ®é›†çš„æ ¸å¿ƒå…³è”\n\n1.  **æ•°æ®é©±åŠ¨è®­ç»ƒ**ï¼š**å®šåˆ¶åŒ–çš„åŠ¨ä½œ-åœ°å½¢é…å¯¹æ•°æ®é›†**æ˜¯è®­ç»ƒâ€œæ„ŸçŸ¥è¿åŠ¨è·Ÿè¸ªâ€ç­–ç•¥çš„åŸºçŸ³ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿå­¦ä¹ åŠ¨ä½œå¦‚ä½•æ ¹æ®åœ°å½¢å‡ ä½•è¿›è¡Œè°ƒæ•´ã€‚\n2.  **æŒ‡æ ‡éªŒè¯ä»·å€¼**ï¼š\n    - **MPJPEâ‚‰ å’Œ MPJPEáµ¦** å…±åŒæ­ç¤ºäº†ç­–ç•¥çš„è·Ÿè¸ªè´¨é‡ã€‚è®ºæ–‡çš„æ¶ˆèå®éªŒï¼ˆè¡¨Iï¼‰è¡¨æ˜ï¼Œ**æ²¡æœ‰æ·±åº¦è§†è§‰**çš„ç­–ç•¥åœ¨åˆå§‹ä½ç½®éšæœºåŒ–æ—¶MPJPEâ‚‰æ€¥å‰§ä¸Šå‡ï¼Œè¯æ˜äº†æ„ŸçŸ¥å¯¹äºåœ¨éç»“æ„åŒ–åœ°å½¢ä¸Šä¿æŒå…¨å±€ç²¾åº¦çš„â€œéå¹³å‡¡ç›Šå¤„â€ã€‚\n    - **æˆåŠŸç‡ä¸æ”¶æ•›æ€§** æŒ‡æ ‡ç›´æ¥éªŒè¯äº†è®ºæ–‡çš„æ ¸å¿ƒä¸»å¼ ï¼š**èåˆæ„ŸçŸ¥å®ç°äº†ä»è„†å¼±çš„è½¨è¿¹å›æ”¾åˆ°é²æ£’çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›çš„è½¬å˜**ï¼Œä½¿äººå½¢æœºå™¨äººèƒ½åœ¨çœŸå®ä¸–ç•Œä¸ç¡®å®šçš„åˆå§‹æ¡ä»¶ä¸‹æˆåŠŸå®Œæˆå¤æ‚ä»»åŠ¡ã€‚\n\n**æ€»ç»“**ï¼šæœ¬æ–‡é€šè¿‡ç²¾å¿ƒæ„å»ºçš„**é…å¯¹æ•°æ®é›†**è§£å†³äº†è®­ç»ƒæ•°æ®ç¼ºå¤±çš„é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†ä¸€å¥—**å¤šè§’åº¦çš„è¯„ä»·æŒ‡æ ‡ä½“ç³»**ï¼ˆç²¾åº¦ã€é²æ£’æ€§ã€æ³›åŒ–æ€§ï¼‰ï¼Œç³»ç»Ÿæ€§åœ°è¯æ˜äº†å°†å¤–æ„ŸçŸ¥æ·±åº¦ä¿¡æ¯é›†æˆåˆ°å…¨èº«è¿åŠ¨è·Ÿè¸ªæ¡†æ¶ä¸­çš„æœ‰æ•ˆæ€§å’Œå·¨å¤§ä¼˜åŠ¿ã€‚",
            "timestamp": "2026-01-21T12:13:56.324719",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡æ„å»ºå¹¶ä½¿ç”¨äº†**è‡ªå®šä¹‰çš„ã€ä¸¥æ ¼è€¦åˆè¿åŠ¨ä¸ç¯å¢ƒçš„é…å¯¹æ•°æ®é›†**ï¼Œè€Œéç›´æ¥ä½¿ç”¨ç°æœ‰å…¬å¼€æ•°æ®é›†ã€‚å…·ä½“æ„å»ºæµç¨‹å¦‚ä¸‹ï¼š\n\n- **æ•°æ®é‡‡é›†**ï¼š\n    - **è¿åŠ¨æ•°æ®**ï¼šä½¿ç”¨å…‰å­¦åŠ¨ä½œæ•æ‰ç³»ç»Ÿè®°å½•ä¸“ä¸šè·‘é…·è¿åŠ¨å‘˜åœ¨çœŸå®éšœç¢ç‰©ä¸Šæ‰§è¡ŒåŠ¨æ€æŠ€èƒ½ï¼ˆå¦‚æ’‘è·³ã€ç¿»æ»šã€è·ªå§¿æ”€çˆ¬ï¼‰ã€‚\n    - **ç¯å¢ƒæ•°æ®**ï¼šä½¿ç”¨æ­è½½LiDARçš„iPad Proï¼ˆé€šè¿‡3D Scanner Appï¼‰åŒæ­¥æ‰«æç‰©ç†åœºæ™¯ï¼Œç”Ÿæˆä¸è¿åŠ¨è½¨è¿¹ç©ºé—´å¯¹é½çš„é«˜ç²¾åº¦ä¸‰ç»´ç½‘æ ¼ã€‚\n- **æ•°æ®å¤„ç†**ï¼š\n    - **è¿åŠ¨é‡å®šå‘**ï¼šä½¿ç”¨GMRæ¡†æ¶å°†äººä½“è¿åŠ¨æ•°æ®ä¼˜åŒ–å¹¶é‡å®šå‘è‡³Unitree G1äººå½¢æœºå™¨äººï¼Œç¡®ä¿è¿åŠ¨å­¦å¯è¡Œæ€§ã€‚\n    - **ç¯å¢ƒæ³›åŒ–**ï¼šä»æ‰«æç½‘æ ¼ä¸­åˆ†å‰²å‡ºæ ¸å¿ƒéšœç¢ç‰©åŠŸèƒ½å‡ ä½•ä½“ï¼Œå»é™¤å®éªŒå®¤èƒŒæ™¯ï¼Œç”Ÿæˆä¸€ç»„**ä¸Šä¸‹æ–‡æ— å…³çš„è§„èŒƒéšœç¢ç‰©ç½‘æ ¼**ã€‚\n- **è®­ç»ƒç¯å¢ƒç”Ÿæˆ**ï¼šå°†**è¿åŠ¨-åœ°å½¢é…å¯¹å®ä¾‹**ç¨‹åºåŒ–åœ°å®ä¾‹åŒ–åˆ°NVIDIA Isaac Labçš„å¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸç¯å¢ƒä¸­ï¼Œé€šè¿‡éšæœºåŒ–ä½ç½®æ¥ç¡®ä¿ç­–ç•¥å­¦ä¹ ä¾èµ–äºå±€éƒ¨å‡ ä½•è€Œéç‰¹å®šå¸ƒå±€ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨äº†ä¸¤ç±»æ ¸å¿ƒæŒ‡æ ‡æ¥è¯„ä¼°è¿åŠ¨è·Ÿè¸ªçš„ç²¾åº¦å’Œé²æ£’æ€§ï¼š\n\n1.  **MPJPE-Global (`MPJPE_g`)**ï¼š\n    - **å®šä¹‰**ï¼š**å…¨å±€è¿åŠ¨è½¨è¿¹è¯¯å·®**ã€‚è®¡ç®—æœºå™¨äººå…³é”®å…³èŠ‚ä½ç½®ä¸å‚è€ƒè¿åŠ¨åœ¨**ä¸–ç•Œåæ ‡ç³»**ä¸‹çš„ä½ç½®å·®å¼‚ã€‚\n    - **æ„ä¹‰**ï¼šè¡¡é‡**æ•´ä½“è¿åŠ¨è·Ÿè¸ªçš„ç»å¯¹ç²¾åº¦**ï¼Œå°¤å…¶å…³æ³¨ä¸éšœç¢ç‰©äº¤äº’ï¼ˆå¦‚æ‰‹éƒ¨æ”¾ç½®ã€èº«ä½“è·¨è¶Šï¼‰çš„å…¨å±€å‡†ç¡®æ€§ã€‚è¯¯å·®å¢å¤§ä¼šç›´æ¥å¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚\n\n2.  **MPJPE-Base (`MPJPE_b`)**ï¼š\n    - **å®šä¹‰**ï¼š**åŸºåº§ï¼ˆèº¯å¹²ï¼‰ç›¸å¯¹è¿åŠ¨è¯¯å·®**ã€‚è®¡ç®—æœºå™¨äººåŸºåº§ï¼ˆéª¨ç›†ï¼‰çš„ä½å§¿ä¸å‚è€ƒè¿åŠ¨åœ¨**æœºå™¨äººè‡ªèº«åŸºåº§åæ ‡ç³»**ä¸‹çš„å·®å¼‚ã€‚\n    - **æ„ä¹‰**ï¼šè¡¡é‡**å±€éƒ¨å§¿æ€æ¨¡ä»¿çš„ä¿çœŸåº¦**ï¼Œåæ˜ ç­–ç•¥åœ¨æ— ç¯å¢ƒäº¤äº’æˆ–å¹³å¦åœ°å½¢ä¸Šè·Ÿè¸ªå‚è€ƒè¿åŠ¨å½¢æ€çš„èƒ½åŠ›ã€‚\n\n### ä¸‰ã€ æ€§èƒ½æå‡ä¸å…³é”®å‘ç°\nè®ºæ–‡é€šè¿‡æ¶ˆèå®éªŒå’Œå¯¹æ¯”æµ‹è¯•ï¼ŒéªŒè¯äº†å…¶æ¡†æ¶ï¼ˆé›†æˆæ·±åº¦æ„ŸçŸ¥çš„å…¨èº«è¿åŠ¨è·Ÿè¸ªï¼‰å¸¦æ¥çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼š\n\n#### 1. **æ ¸å¿ƒæ€§èƒ½æå‡ï¼šå¯¹åˆå§‹ä½ç½®æ‰°åŠ¨çš„é²æ£’æ€§**\n- **é—®é¢˜**ï¼šä¼ ç»Ÿâ€œç›²è·Ÿè¸ªâ€æ–¹æ³•ï¼ˆå¦‚BeyondMimicåŸºçº¿ï¼‰ä¸¥é‡ä¾èµ–ç²¾ç¡®çš„åˆå§‹ä½å§¿ã€‚ä¸€æ—¦æœºå™¨äººçš„èµ·å§‹ä½ç½®ä¸å‚è€ƒè½¨è¿¹é¢„è®¾ä½ç½®å­˜åœ¨åå·®ï¼Œææ˜“å¯¼è‡´äº¤äº’å¤±è´¥ï¼ˆå¦‚æ’ä¸Šæˆ–é”™è¿‡éšœç¢ç‰©ï¼‰ã€‚\n- **æœ¬æ–‡æ–¹æ³•æå‡**ï¼šé€šè¿‡å¼•å…¥æ·±åº¦è§†è§‰å½¢æˆ**é—­ç¯æ§åˆ¶**ï¼Œç­–ç•¥èƒ½ä¸»åŠ¨è°ƒæ•´æ­¥æ€å’Œæ—¶æœºä»¥é€‚åº”å®é™…è·ç¦»ã€‚\n- **æ•°æ®ä½è¯**ï¼ˆå‚è§è¡¨IåŠå›¾4,5,6ï¼‰ï¼š\n    - **è®­ç»ƒå¤–æ³›åŒ–**ï¼šå°½ç®¡è®­ç»ƒæ—¶åªåœ¨`Â±0.15m`èŒƒå›´å†…éšæœºåŒ–èµ·å§‹ä½ç½®ï¼Œä½†æµ‹è¯•æ—¶åœ¨**`Â±0.6m`ï¼ˆæ€»è®¡1.2mÃ—1.2mï¼‰çš„å¹¿é˜”èŒƒå›´å†…**ï¼ŒæˆåŠŸç‡ä»æ¥è¿‘100%ã€‚\n    - **è½¨è¿¹æ”¶æ•›**ï¼šä»ä¸åŒèµ·å§‹ç‚¹å‡ºå‘çš„æœºå™¨äººï¼Œå…¶`(x, y)`ä½ç½®æ–¹å·®ä¼šéšç€æ—¶é—´è¿…é€Ÿæ”¶æ•›ï¼Œç¡®ä¿åœ¨å…³é”®äº¤äº’æ—¶åˆ»å‡†ç¡®å¯¹é½éšœç¢ç‰©ã€‚\n    - **æŒ‡æ ‡å¯¹æ¯”**ï¼šåœ¨â€œå¸¦ä½ç½®éšæœºåŒ–â€çš„æµ‹è¯•ä¸­ï¼Œæœ¬æ–‡æ–¹æ³•ï¼ˆ`ours`ï¼‰çš„å…¨å±€è¯¯å·®`MPJPE_g`æ˜¾è‘—ä½äºæ— æ·±åº¦æ„ŸçŸ¥çš„åŸºçº¿ï¼ˆ`w/o depth`ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨â€œDive Rollâ€åŠ¨ä½œä¸­ï¼Œè¯¯å·®ä»`0.2267`é™è‡³`0.1649`ã€‚\n\n#### 2. **æ·±åº¦æ„ŸçŸ¥çš„é²æ£’æ€§éªŒè¯**\n- **æµ‹è¯•åœºæ™¯**ï¼šåœ¨ä»¿çœŸä¸­å¼•å…¥è®­ç»ƒæœªè§è¿‡çš„è§†è§‰å¹²æ‰°ç‰©ï¼ˆå¦‚å®½æ¢ã€å¹³æ¿ã€å¢™ï¼‰ï¼Œè¿™äº›ç‰©ä½“æä¾›æ–°çš„æ·±åº¦ä¿¡æ¯ä½†åŸºæœ¬ä¸é˜»ç¢è¿åŠ¨åŠ¨åŠ›å­¦ã€‚\n- **å‘ç°**ï¼ˆå‚è§è¡¨IIï¼‰ï¼š\n    - å¯¹äºå¤šæ•°å¹²æ‰°ç‰©ï¼ˆå¦‚å®½æ¢ã€å¢™ï¼‰ï¼Œç­–ç•¥çš„`MPJPE_b`ï¼ˆå±€éƒ¨ä¿çœŸåº¦ï¼‰ä¿æŒç¨³å®šï¼Œè¯´æ˜ç­–ç•¥**æœªå¯¹å¹²æ‰°ç‰©äº§ç”Ÿè¿‡æ¿€ååº”**ï¼Œä¿æŒäº†è¿åŠ¨è·Ÿè¸ªçš„æ ¸å¿ƒèƒ½åŠ›ã€‚\n    - ä»…å½“å¹²æ‰°ç‰©æ˜¾è‘—æ”¹å˜åœºæ™¯åŠ¨åŠ›å­¦ï¼ˆå¦‚â€œå¹³æ¿å¹²æ‰°ç‰©â€æ¨ªç©¿è·¯å¾„ï¼‰æ—¶ï¼Œ`MPJPE_g`ä¼šä¸Šå‡ï¼Œè¿™å±äºåˆç†é¢„æœŸã€‚\n\n#### 3. **è®­ç»ƒæ¡†æ¶å…³é”®ç»„ä»¶çš„è´¡çŒ®**\næ¶ˆèç ”ç©¶ï¼ˆè¡¨Iï¼‰æ˜ç¡®äº†å„ç»„ä»¶çš„é‡è¦æ€§ï¼š\n- **æ·±åº¦è¾“å…¥ (`w/o depth`)**ï¼šç§»é™¤åï¼Œåœ¨ä½ç½®éšæœºåŒ–ä¸‹çš„æ€§èƒ½**æ€¥å‰§ä¸‹é™**ï¼Œè¯æ˜äº†æ„ŸçŸ¥å¯¹äºé€‚åº”ç¯å¢ƒå‡ ä½•æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚\n- **å¡æ­»æ£€æµ‹ (`w/o stuck detection`)**ï¼šç§»é™¤åï¼Œ`MPJPE`è¯¯å·®æ™®éè½»å¾®ä¸Šå‡ã€‚è¯¥æœºåˆ¶é€šè¿‡å‰”é™¤ç‰©ç†ä¸Šä¸å¯èƒ½çš„åˆå§‹çŠ¶æ€ï¼Œ**æå‡äº†è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•ˆç‡**ã€‚\n- **ç›¸å¯¹å¸§å¥–åŠ± (`local frame reward`)**ï¼šä½¿ç”¨æœºå™¨äººåŸºåº§åæ ‡ç³»ä¸‹çš„å¥–åŠ±ä»£æ›¿æœ¬æ–‡çš„â€œç›¸å¯¹å¸§â€å¥–åŠ±ï¼Œä¼šå¯¼è‡´**å±€éƒ¨è·Ÿè¸ªæ›´å‡†ä½†å…¨å±€äº¤äº’æ€§èƒ½ä¸‹é™**ï¼ŒéªŒè¯äº†ç›¸å¯¹å¸§è®¾è®¡åœ¨è§£è€¦å…¨å±€è·¯å¾„è°ƒæ•´ä¸å±€éƒ¨å§¿æ€æ¨¡ä»¿æ–¹é¢çš„ä¼˜åŠ¿ã€‚\n\n### æ€»ç»“\næœ¬æ–‡é€šè¿‡**è‡ªå®šä¹‰çš„ã€è¿åŠ¨-ç¯å¢ƒé…å¯¹æ•°æ®é›†**è¿›è¡Œå®éªŒï¼Œä½¿ç”¨**`MPJPE_g`ï¼ˆå…¨å±€ç²¾åº¦ï¼‰å’Œ`MPJPE_b`ï¼ˆå±€éƒ¨ä¿çœŸåº¦ï¼‰** ä½œä¸ºæ ¸å¿ƒæŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œå…¶æå‡ºçš„**æ„ŸçŸ¥å…¨èº«è¿åŠ¨è·Ÿè¸ªæ¡†æ¶**å¸¦æ¥äº†**éå¹³å‡¡çš„ï¼ˆnon-trivialï¼‰æ€§èƒ½æå‡**ï¼š\n- **æ ¸å¿ƒä»·å€¼**ï¼šå°†è„†å¼±ã€ä¾èµ–ç²¾ç¡®åˆå§‹åŒ–çš„å¼€ç¯è¿åŠ¨è·Ÿè¸ªï¼Œè½¬å˜ä¸º**å¯¹èµ·å§‹ä½ç½®å…·æœ‰é«˜åº¦é²æ£’æ€§ï¼ˆ>1ç±³èŒƒå›´ï¼‰çš„é—­ç¯è‡ªé€‚åº”ç³»ç»Ÿ**ã€‚\n- **å®é™…æ„ä¹‰**ï¼šè¿™ä½¿å¾—åœ¨**æ— éœ€ç²¾ç¡®å®šä½å’Œé‡Œç¨‹è®¡**çš„æˆ·å¤–éç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œéƒ¨ç½²åŠ¨æ€ã€æ¥è§¦ä¸°å¯Œçš„å…¨èº«æŠ€èƒ½ï¼ˆå¦‚è·‘é…·ï¼‰æˆä¸ºå¯èƒ½ï¼Œæå¤§æ‹“å±•äº†äººå½¢æœºå™¨äººçš„å®é™…é€šè¿‡æ€§å’Œå®ç”¨æ€§ã€‚",
            "timestamp": "2026-01-21T12:14:39.247197",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼Œæœ¬æ–‡æå‡ºçš„â€œæ„ŸçŸ¥å…¨èº«è·‘é…·â€æ¡†æ¶è™½ç„¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **æ•°æ®ä¸æŠ€èƒ½åº“çš„è§„æ¨¡é™åˆ¶**ï¼šè®ºæ–‡è®­ç»ƒçš„ç­–ç•¥ä»…é’ˆå¯¹**4ç§ç‰¹å®šåŠ¨ä½œ**ï¼ˆè·ªå§¿æ”€çˆ¬ã€ç¿»æ»šè·¨è¶Šã€é±¼è·ƒç¿»æ»šã€è·³è·ƒåä¸‹ï¼‰åœ¨3ç§åœ°å½¢ä¸Šè¿›è¡Œã€‚è¿™è·ç¦»ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†ä»»æ„åœ°å½¢ã€æ‰§è¡Œä»»æ„æ¥è§¦å¼æŠ€èƒ½çš„â€œé€šç”¨â€æ§åˆ¶å™¨è¿˜æœ‰å¾ˆå¤§å·®è·ã€‚æ•°æ®æ”¶é›†ï¼ˆåŠ¨ä½œæ•æ‰ä¸åœºæ™¯æ‰«æï¼‰è¿‡ç¨‹æˆæœ¬é«˜æ˜‚ä¸”å¤æ‚ï¼Œé™åˆ¶äº†æŠ€èƒ½åº“çš„å¿«é€Ÿæ‰©å±•ã€‚\n\n- **åŠ¨ä½œé€‰æ‹©ä¸åºåˆ—åŒ–çš„æŒ‘æˆ˜**ï¼šåœ¨éƒ¨ç½²æ—¶ï¼Œç³»ç»Ÿéœ€è¦ä¸€ä¸ª**çŠ¶æ€æœºæœºåˆ¶**æ¥æ‰‹åŠ¨é€‰æ‹©è¾“å…¥çš„åŠ¨ä½œå‚è€ƒè½¨è¿¹ã€‚è®ºæ–‡æœªå®ç°**è‡ªä¸»çš„åŠ¨ä½œæŠ€èƒ½é€‰æ‹©ä¸åºåˆ—åŒ–**ã€‚åœ¨å¤æ‚ç¯å¢ƒä¸­ï¼Œæœºå™¨äººéœ€è¦æ ¹æ®æ„ŸçŸ¥ä¿¡æ¯è‡ªåŠ¨åˆ¤æ–­â€œä½•æ—¶ã€ä½¿ç”¨ä½•ç§æŠ€èƒ½â€æ¥è·¨è¶Šéšœç¢ï¼Œè¿™æ˜¯å®ç°å®Œå…¨è‡ªä¸»å¯¼èˆªçš„å…³é”®ç“¶é¢ˆã€‚\n\n- **æ„ŸçŸ¥æ¨¡æ€ä¸ç†è§£çš„å±€é™**ï¼š\n    - ä»…ä½¿ç”¨äº†**å•ç›®æ·±åº¦å›¾åƒ**ä½œä¸ºå¤–éƒ¨æ„ŸçŸ¥è¾“å…¥ï¼Œç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚æè´¨ã€å¯æŠ“æ¡åŒºåŸŸè¯†åˆ«ï¼‰ã€‚\n    - ç³»ç»Ÿå¯¹â€œå¹²æ‰°ç‰©â€çš„é²æ£’æ€§æµ‹è¯•ï¼ˆIV-CèŠ‚ï¼‰è¡¨æ˜ï¼Œè™½ç„¶å¯¹ä¸é˜»ç¢åŠ¨åŠ›å­¦çš„ç‰©ä½“é²æ£’ï¼Œä½†åƒâ€œå¹³å°å¹²æ‰°ç‰©â€è¿™ç§æ”¹å˜åœ°å½¢ç»“æ„çš„ç‰©ä½“ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™è¯´æ˜å…¶æ„ŸçŸ¥æ›´å¤šæ˜¯å‡ ä½•é©±åŠ¨ï¼Œè€ŒéåŸºäºç‰©ç†äº¤äº’çš„æ·±å±‚ç†è§£ã€‚\n    - æœªæ¶‰åŠåŠ¨æ€æˆ–ç§»åŠ¨éšœç¢ç‰©ã€‚\n\n- **ä»¿çœŸåˆ°ç°å®çš„å·®è·è™½è¢«ç¼“è§£ä½†æœªæ ¹é™¤**ï¼šå°½ç®¡é€šè¿‡æ·»åŠ å™ªå£°å’Œä¿®å¤ç®—æ³•ï¼ˆIII-CèŠ‚ï¼‰æ¥å¼¥åˆæ·±åº¦æ„ŸçŸ¥çš„sim-to-realå·®è·ï¼Œä½†è¿™ç§æ–¹æ³•ä¾èµ–äºå¯¹çœŸå®å™ªå£°æ¨¡å¼çš„å»ºæ¨¡ã€‚å¯¹äºæ›´å¤æ‚çš„ä¼ æ„Ÿå™¨æ•…éšœï¼ˆå¦‚å¤§é¢ç§¯ç¼ºå¤±ã€å¼ºçƒˆåå°„ï¼‰æˆ–æœªå»ºæ¨¡çš„ç‰©ç†ç‰¹æ€§ï¼ˆå¦‚åœ°é¢æ‘©æ“¦ç³»æ•°çªå˜ï¼‰ï¼Œç­–ç•¥çš„é²æ£’æ€§ä»æœ‰å¾…éªŒè¯ã€‚\n\n- **è®¡ç®—æ•ˆç‡ä¸å®æ—¶æ€§**ï¼šç­–ç•¥éƒ¨ç½²ä¾èµ–äº50Hzçš„æ·±åº¦å›¾åƒå’ŒONNXåŠ é€Ÿï¼Œä½†åœ¨èµ„æºå—é™çš„åµŒå…¥å¼å¹³å°ï¼ˆå¦‚Jetsonï¼‰ä¸ŠåŒæ—¶è¿è¡Œæ„ŸçŸ¥ã€ç½‘ç»œæ¨ç†å’Œåº•å±‚æ§åˆ¶ï¼Œå¯¹ç³»ç»Ÿçš„å®æ—¶æ€§å’Œå¯é æ€§æå‡ºäº†æŒç»­æŒ‘æˆ˜ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘å±•å¼€ï¼š\n\n- **æ‰©å±•æŠ€èƒ½åº“ä¸è‡ªåŠ¨åŒ–æ•°æ®ç”Ÿæˆ**ï¼š\n    - åˆ©ç”¨**å¤§è§„æ¨¡äººç±»è§†é¢‘æ•°æ®**ï¼ˆå¦‚äº’è”ç½‘è·‘é…·è§†é¢‘ï¼‰ç»“åˆ**3Dåœºæ™¯é‡å»º**æŠ€æœ¯ï¼Œä»¥åŠè‡ªåŠ¨æˆ–è‡ªåŠ¨åŒ–çš„æ–¹å¼ç”Ÿæˆâ€œåŠ¨ä½œ-åœ°å½¢â€é…å¯¹æ•°æ®ï¼Œé™ä½æ•°æ®æ”¶é›†æˆæœ¬ã€‚\n    - æ¢ç´¢**ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰** æ¥è‡ªåŠ¨ç”Ÿæˆå¤šæ ·åŒ–çš„ã€ç‰©ç†å¯è¡Œçš„å‚è€ƒåŠ¨ä½œï¼Œæˆ–å¯¹ç°æœ‰åŠ¨ä½œè¿›è¡Œé€‚é…æ€§å˜å½¢ã€‚\n\n- **å®ç°é«˜å±‚æŠ€èƒ½è§„åˆ’ä¸è‡ªä¸»åºåˆ—åŒ–**ï¼š\n    - åœ¨ç°æœ‰åº•å±‚ç­–ç•¥ä¹‹ä¸Šï¼Œæ„å»ºä¸€ä¸ª**é«˜å±‚è§„åˆ’å™¨**ã€‚è¯¥è§„åˆ’å™¨åŸºäºæ›´ä¸°å¯Œçš„ç¯å¢ƒæ„ŸçŸ¥ï¼ˆå¯èƒ½åŒ…å«è¯­ä¹‰åˆ†å‰²ã€æ‹“æ‰‘åˆ†æï¼‰æ¥å®æ—¶é€‰æ‹©å¹¶åºåˆ—åŒ–åº•å±‚æŠ€èƒ½ï¼Œä»¥å®Œæˆå¤æ‚çš„ç©¿è¶Šä»»åŠ¡ã€‚\n    - ç ”ç©¶**åŸºäºLLM/VLMçš„è¯­ä¹‰åœºæ™¯ç†è§£ä¸ä»»åŠ¡åˆ†è§£**ï¼Œå°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤è½¬åŒ–ä¸ºä¸€ç³»åˆ—å¯æ‰§è¡Œçš„æŠ€èƒ½ã€‚\n\n- **å¢å¼ºæ„ŸçŸ¥ä¸ç†è§£èƒ½åŠ›**ï¼š\n    - èåˆ**å¤šæ¨¡æ€æ„ŸçŸ¥**ï¼Œå¦‚RGBå›¾åƒï¼ˆç”¨äºè¯­ä¹‰è¯†åˆ«ï¼‰ã€è§¦è§‰ä¼ æ„Ÿï¼ˆç”¨äºæ¥è§¦åŠ›ä¼˜åŒ–ï¼‰ã€æƒ¯æ€§ä¿¡æ¯ç­‰ï¼Œä½¿æœºå™¨äººèƒ½ç†è§£åœ°å½¢çš„åŠŸèƒ½å±æ€§ï¼ˆå¦‚â€œå¯æ”¯æ’‘â€ã€â€œå¯æŠ“æ¡â€ï¼‰ã€‚\n    - å¼•å…¥**ä¸–ç•Œæ¨¡å‹**æˆ–**å‰ç»æ€§é¢„æµ‹**ï¼Œè®©æœºå™¨äººèƒ½é¢„æµ‹åŠ¨ä½œçš„æ½œåœ¨åæœï¼Œä»è€Œåœ¨æ¥è§¦å‰è¿›è¡Œæ›´ç²¾ç»†çš„è°ƒæ•´ã€‚\n\n- **æå‡ä»¿çœŸçœŸå®æ€§ä¸è®­ç»ƒæ•ˆç‡**ï¼š\n    - é‡‡ç”¨**åŸŸéšæœºåŒ–**å’Œ**ç³»ç»Ÿè¾¨è¯†**çš„æ›´é«˜çº§å½¢å¼ï¼Œè®©ç­–ç•¥èƒ½é€‚åº”æ›´å¹¿æ³›çš„æœªå»ºæ¨¡ç‰©ç†åŠ¨æ€ã€‚\n    - æ¢ç´¢**ç¦»çº¿å¼ºåŒ–å­¦ä¹ **ä¸**å¤§è§„æ¨¡ä»¿çœŸé¢„è®­ç»ƒ**ç»“åˆï¼Œåˆ©ç”¨æµ·é‡æ— æ ‡ç­¾ä»¿çœŸæ•°æ®å…ˆå­¦ä¹ åŸºç¡€è¿åŠ¨èƒ½åŠ›ï¼Œå†é€šè¿‡é«˜æ•ˆåœ¨çº¿è°ƒä¼˜é€‚é…å…·ä½“æŠ€èƒ½ã€‚\n\n- **ç«¯åˆ°ç«¯æ¶æ„çš„æ·±åŒ–**ï¼š\n    - æ¢ç´¢**å®Œå…¨ç«¯åˆ°ç«¯**çš„æ¶æ„ï¼Œå°†æ„ŸçŸ¥ã€è§„åˆ’ã€æ§åˆ¶æ•´åˆè¿›ä¸€ä¸ªç»Ÿä¸€çš„ã€å¯å¾®åˆ†çš„å­¦ä¹ æ¡†æ¶ä¸­ï¼Œå‡å°‘æ¨¡å—åŒ–è®¾è®¡å¸¦æ¥çš„ä¿¡æ¯æŸå¤±å’Œè¯¯å·®ç´¯ç§¯ã€‚\n    - ç ”ç©¶**ç»ˆèº«å­¦ä¹ **æœºåˆ¶ï¼Œä½¿æœºå™¨äººèƒ½åœ¨å®é™…éƒ¨ç½²ä¸­æŒç»­ä»æˆåŠŸå’Œå¤±è´¥çš„ç»éªŒä¸­å­¦ä¹ å¹¶æ”¹è¿›æŠ€èƒ½ã€‚\n\n**æ€»ç»“è€Œè¨€ï¼Œæœ¬æ–‡çš„æ ¸å¿ƒä»·å€¼åœ¨äºæˆåŠŸåœ°å°†æ„ŸçŸ¥ä¸å…¨èº«è¿åŠ¨è·Ÿè¸ªç›¸ç»“åˆï¼Œä¸ºé€šç”¨äººå½¢æœºå™¨äººæ§åˆ¶å¥ å®šäº†å…³é”®çš„æŠ€æœ¯åŸºç¡€ã€‚æœªæ¥çš„å·¥ä½œå°†å›´ç»•â€œæ‰©å±•æ€§â€ã€â€œè‡ªä¸»æ€§â€å’Œâ€œé€šç”¨æ€§â€å±•å¼€ï¼Œç›®æ ‡æ˜¯å®ç°ä¸€ä¸ªèƒ½åƒäººç±»ä¸€æ ·åœ¨å¤æ‚ã€æœªçŸ¥ç¯å¢ƒä¸­è‡ªå¦‚è¿ç”¨å…¨èº«è¿›è¡Œç§»åŠ¨å’Œæ“ä½œçš„æ™ºèƒ½ä½“ã€‚**",
            "timestamp": "2026-01-21T12:15:14.086614",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T16:33:16Z",
    "created_at": "2026-01-21T12:09:11.278035",
    "updated_at": "2026-01-21T12:09:11.278046"
}