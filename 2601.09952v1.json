{
    "id": "2601.09952v1",
    "title": "OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport",
    "authors": [
        "Zhihua Zhao",
        "Guoqiang Li",
        "Chen Min",
        "Kangping Lu"
    ],
    "abstract": "åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­å®ç°å¯é çš„å¯é€šè¡ŒåŒºåŸŸåˆ†å‰²ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶çš„è§„åˆ’ä¸å†³ç­–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨åˆ†å¸ƒå¤–åœºæ™¯ä¸­å¸¸å‡ºç°åˆ†å‰²æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¿›è€Œå½±å“ä¸‹æ¸¸é©¾é©¶ä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºOT-Driveâ€”â€”ä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“çš„å¤šæ¨¡æ€èåˆæ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†RGBå›¾åƒä¸è¡¨é¢æ³•çº¿èåˆé—®é¢˜æ„å»ºä¸ºåˆ†å¸ƒä¼ è¾“ä»»åŠ¡ï¼šé¦–å…ˆè®¾è®¡æ–°å‹åœºæ™¯é”šç‚¹ç”Ÿæˆå™¨ï¼Œå°†åœºæ™¯ä¿¡æ¯è§£è€¦ä¸ºå¤©æ°”ã€æ—¶æ®µä¸é“è·¯ç±»å‹çš„è”åˆåˆ†å¸ƒï¼Œæ„å»ºå¯æ³›åŒ–è‡³æœªçŸ¥åœºæ™¯çš„è¯­ä¹‰é”šç‚¹ï¼›è¿›è€Œåˆ›æ–°æ€§åœ°æå‡ºåŸºäºæœ€ä¼˜ä¼ è¾“çš„å¤šæ¨¡æ€èåˆæ¨¡å—ï¼Œå°†RGBä¸è¡¨é¢æ³•çº¿ç‰¹å¾æ˜ å°„è‡³è¯­ä¹‰é”šç‚¹å®šä¹‰çš„æµå½¢ç©ºé—´ï¼Œå®ç°åˆ†å¸ƒå¤–åœºæ™¯ä¸‹é²æ£’çš„å¯é€šè¡ŒåŒºåŸŸåˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼Œæœ¬æ–¹æ³•åœ¨ORFDåˆ†å¸ƒå¤–åœºæ™¯è¾¾åˆ°95.16%çš„å¹³å‡äº¤å¹¶æ¯”ï¼Œè¾ƒç°æœ‰æ–¹æ³•æå‡6.35%ï¼›åœ¨è·¨æ•°æ®é›†è¿ç§»ä»»åŠ¡ä¸­è·å¾—89.79%çš„å¹³å‡äº¤å¹¶æ¯”ï¼Œè¶…è¶ŠåŸºçº¿æ–¹æ³•13.99%ã€‚è¿™äº›ç»“æœè¯æ˜ï¼Œæ‰€ææ¨¡å‹ä»…éœ€æœ‰é™è®­ç»ƒæ•°æ®å³å¯å®ç°å¼ºåˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å®é™…éƒ¨ç½²çš„å®ç”¨æ€§ä¸æ•ˆç‡ã€‚",
    "url": "https://arxiv.org/abs/2601.09952v1",
    "html_url": "https://arxiv.org/html/2601.09952v1",
    "html_content": "OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport\nZhihuaÂ Zhao,Â GuoqiangÂ Li\nâˆ—\n,Â ChenÂ Min,Â andÂ KangpingÂ Lu\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nâˆ—\nCorresponding author: Guoqiang Li (guoqiangli@bit.edu.cn).Zhihua Zhao and Guoqiang Li are with the School of Mechanical Engineering, Beijing Institute of Technology, Beijing 100081, China (e-mail: zhihuazhao@bit.edu.cn; guoqiangli@bit.edu.cn).Chen Min is with the Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China (e-mail: mincheng@ict.ac.cn).Kangping Lu is with Shandong Pengxiang Automobile Co., Ltd (e-mail: pengxiang_lu@163.com).\nAbstract\nReliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving.\nHowever, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks.\nTo address this issue, we propose OT-Drive, an Optimal Transportâ€“driven multi-modal fusion framework.\nThe proposed method formulates RGB and surface normal fusion as a distribution transport problem.\nSpecifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios.\nSubsequently, we design an innovative Optimal Transport-based\nmulti-modal fusion module (OT Fusion) to transport RGB and\nsurface normal features onto the manifold defined by the\nsemantic anchors, enabling robust traversable area segmentation\nunder OOD scenarios.\nExperimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.\nThese results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.\nIndex Terms:\ntraversable area segmentation, out-of-distribution, multi-modal fusion, freespace detection\nI\nIntroduction\nAutonomous driving technology has achieved remarkable success in structured urban environments.\nHowever, deploying autonomous driving systems in unstructured environmentsâ€”such as farm roads\n[\n28\n]\n, rural areas\n[\n39\n]\n, and mining zones\n[\n45\n]\nâ€”remains a significant challenge.\nUnlike urban roads, unstructured scenes lack clear boundaries, standardized traffic signs, and well-defined lane markings\n[\n25\n]\n.\nIn such scenarios, reliable traversable area segmentation serves as a fundamental prerequisite, providing safety-critical boundary constraints for motion planning and control\n[\n2\n]\n.\nCapitalizing on the complementary strengths of visual textures and geometric structures, multi-modal semantic segmentation has become the prevailing paradigm for traversable area segmentation\n[\n8\n,\n42\n,\n23\n,\n16\n,\n19\n]\n.\nUnder this paradigm, heterogeneous modalitiesâ€”typically RGB image and surface normalâ€”are jointly processed to predict a dense binary label for each pixel, indicating whether it is traversable or non-traversable.\nHowever, existing multi-modal fusion approaches often overfit to specific scene characteristics and implicitly assume that heterogeneous modalities can be directly aligned in a shared feature space.\nAs a result, these methods may perform well in in-distribution (ID) scenarios that closely resemble the training data but suffer from increased false detections under OOD scenarios, as illustrated in Fig.\n1\n.\nSuch failures can lead to catastrophic consequences, including planning trajectories into hazardous terrain (e.g., ditches or obstacles) or triggering abrupt emergency stops\n[\n9\n]\n.\nFigure 1\n:\nExisting segmentation methods exhibit poor OOD generalization. Red and green regions indicate false and true traversable areas, respectively.\nTo address these challenges, we propose OT-Drive, an Optimal Transport\n[\n15\n]\n-driven multi-modal fusion framework for improving the generalization of traversable area segmentation in unstructured environments.\nOur core insight is to formulate RGB and surface normal fusion as a scene-conditioned distribution transport problem.\nSpecifically, we first present a Scene Anchor Generator (SAG) that decomposes complex scene variations into disentangled environmental attributesâ€”namely weather, time-of-day, and road type.\nBased on this explicit disentanglement, SAG constructs scene anchors that can generalize to novel, unseen combinations of environmental conditions.\nFurthermore, we propose an Optimal Transport-based Fusion (OT Fusion) module that, rather than forcing heterogeneous modalities into a shared feature space, seeks an optimal transport plan to align image features and surface normal features onto the manifold defined by the scene anchors.\nThis mechanism achieves distribution-level optimal fusion that enables robust cross-modal alignment. Consequently, OT-Drive ensures reliable traversable area segmentation in OOD scenarios.\nThe\nmain contributions\nof this work are summarized as follows:\nâ€¢\nWe propose OT-Drive, a novel multi-modal fusion framework that models RGB and surface normal fusion via optimal transport, achieving robust OOD generalization for traversable area segmentation. To the best of the authorsâ€™ knowledge, this is the first work to apply optimal transport to enhance the robustness of heterogeneous modality fusion under distribution shift.\nâ€¢\nWe design a novel Scene Anchor Generator (SAG) that disentangles scene variations into independent environmental attributes, constructing scene anchors that generalize to unseen environmental combinations.\nâ€¢\nWe develop an innovative Optimal Transport-based Fusion (OT Fusion) module that aligns heterogeneous modality features onto the manifold defined by scene anchors via optimal transport.\nBy minimizing the global transportation cost, this module achieves distribution-level optimal fusion that is highly robust to OOD scenarios.\nâ€¢\nExtensive experiments on multiple off-road datasets and the challenging cross-dataset transfer benchmarks validate the proposed framework, demonstrating state-of-the-art OOD generalization and confirming its practicality for real-world deployment.\nII\nRelated Work\nII-A\nSingle-Modal Traversable Area Segmentation\nEarly off-road traversable area segmentation primarily relied on geometric modeling from point clouds. Pioneering works employed explicit geometric constraints, such as Markov Random Fields (MRF) for ground height estimation\n[\n14\n]\nand concentric zone-based strategies for robust ground fitting, as seen in the Patchwork series\n[\n20\n,\n18\n]\n.\nTransitioning to data-driven approaches, end-to-end neural networks have been widely adopted to directly predict traversability from point clouds\n[\n31\n,\n30\n]\n.\nMore recently, hybrid architectures that project 3D point clouds onto 2D planes have enabled real-time modeling\n[\n38\n]\n, while self-supervised contrastive learning has been explored to resolve label ambiguity\n[\n40\n]\n.\nWhile point cloud-based methods provide strong geometric priors, they often lack the semantic understanding to differentiate structurally similar terrains.\nConversely, vision-based methods utilize grouped attention mechanisms to analyze terrain difficulty and texture\n[\n10\n,\n34\n]\n.\nTo overcome the limited field of view (FOV) of monocular cameras, recent approaches have extended perception capabilities via multi-view inputs and cross-view coordinate mappings\n[\n22\n,\n3\n]\n.\nIn summary, single-modal approaches inherently face trade-offs between geometric accuracy and semantic richness, limiting their reliability across diverse environmental conditions.\nII-B\nMulti-Modal Fusion for Traversable Area Segmentation\nTo capitalize on the complementary strengths of geometry and semantics, multi-modal fusion methods integrate heterogeneous sensor data, such as camera-LiDAR\n[\n16\n]\nor hyperspectral-radar combinations\n[\n28\n]\n.\nGiven the sparsity and irregularity of 3D point clouds, a prevalent strategy involves projecting them into camera-view surface normal maps to extract dense geometric features\n[\n5\n,\n37\n]\n.\nBuilding on this representation, researchers have integrated surface normal with RGB image using advanced architecturesâ€”ranging from Vision Transformers\n[\n23\n]\nto Swin Transformers\n[\n41\n]\nâ€”to enhance feature extraction and alignment.\nOther approaches focus on refining boundary details by employing edge decoding modules\n[\n42\n]\nor duplex transformers\n[\n19\n]\n.\nFurthermore, to mitigate the impact of sensor noise and calibration errors, recent works have developed adaptive weighting and noise-aware intermediate fusion mechanisms\n[\n21\n,\n8\n]\n.\nHowever, most existing methods rely on pixel-level or token-level alignment, which often fails to capture distribution shifts in OOD scenarios.\nII-C\nDomain Generalization for Semantic Segmentation\nDomain Generalization (DG) aims to mitigate performance degradation caused by distribution discrepancies between training and test domains.\nEarly research primarily addressed this challenge through data augmentation and domain-invariant representation learning, exploiting texture statistics from ImageNet pre-training to enhance generalization\n[\n17\n,\n11\n]\n.\nRecently, leveraging textual semantics from Vision-Language Models (VLMs) to guide visual feature alignment has become a standard practice for enhancing generalization.\nOne line of methods constructs diverse pseudo-target domains through text prompts or generative models to enhance robustness against environmental variations\n[\n4\n,\n26\n]\n.\nAnother line directly incorporates text embeddings into segmentation frameworks, such as serving as decoder queries\n[\n27\n]\nor semantic prototypes\n[\n44\n]\n, significantly improving generalization under extreme styles or OOD scenarios.\nFurthermore, recent studies explore generating OOD prompts with varying semantic distances via WordNet\n[\n33\n]\n, optimizing cross-modal alignment\n[\n43\n]\n, and employing domain-aware prompt learning\n[\n12\n]\nto alleviate semantic misalignment.\nDespite significant progress in single-modal DG, existing works rarely address the distribution shifts inherent in heterogeneous multi-sensor fusion frameworks. This remains an open challenge in complex perception systems such as autonomous driving.\nIII\nMethod\nFigure 2\n:\nOverall architecture of the proposed OT-Drive.\nThe framework is composed of two modules designed for OOD generalization in traversable area segmentation:\n1) The Scene Anchor Generator (SAG) constructs scene-specific semantic anchors from the input image.\n2) The OT Fusion Module leverages optimal transport to align RGB and surface normal features onto the manifold spanned by the semantic anchors, achieving distribution-level feature fusion.\nIII-A\nProblem Formulation\nLet\nğ’®\n\\mathcal{S}\ndenote the space of all real-world scene attribute combinations, such as weather (atmospheric conditions), time-of-day (temporal conditions), and road type (terrain properties). However, the training set typically covers only a sparse subset of this space:\nğ’®\ntrain\nâŠ‚\nğ’®\n,\n|\nğ’®\ntrain\n|\nâ‰ª\n|\nğ’®\n|\n\\mathcal{S}_{\\text{train}}\\subset\\mathcal{S},\\quad|\\mathcal{S}_{\\text{train}}|\\ll|\\mathcal{S}|\n(1)\nThe core challenge of OOD generalization arises because the test set contains novel attribute combinations absent from training:\nğ’®\ntest\nâŠˆ\nğ’®\ntrain\n,\ni.e.,\nâ€‹\nâˆƒ\ns\nâˆˆ\nğ’®\ntest\n:\ns\nâˆ‰\nğ’®\ntrain\n\\mathcal{S}_{\\text{test}}\\not\\subseteq\\mathcal{S}_{\\text{train}},\\quad\\text{i.e., }\\exists\\,s\\in\\mathcal{S}_{\\text{test}}:s\\notin\\mathcal{S}_{\\text{train}}\n(2)\nTo address this challenge, let\nâ„\n\\mathcal{I}\nand\nğ’©\n\\mathcal{N}\ndenote the input spaces for RGB image and surface normal, respectively, and let\nğ’´\n\\mathcal{Y}\nrepresent the traversability label space. Our objective is to learn a robust mapping\nf\n:\n(\nâ„\n,\nğ’©\n)\nâ†’\nğ’´\nf:(\\mathcal{I},\\mathcal{N})\\rightarrow\\mathcal{Y}\nthat generalizes to OOD scenes:\nmin\nf\nâ¡\nğ”¼\n(\nx\n,\ny\n)\nâˆ¼\nğ’Ÿ\ntest\nâ€‹\n[\nâ„’\nseg\nâ€‹\n(\nf\nâ€‹\n(\nx\n)\n,\ny\n)\n]\n,\nâˆ€\nğ’®\ntest\nâŠˆ\nğ’®\ntrain\n\\min_{f}\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{\\text{test}}}\\left[\\mathcal{L}_{\\text{seg}}(f(x),y)\\right],\\quad\\forall\\,\\mathcal{S}_{\\text{test}}\\not\\subseteq\\mathcal{S}_{\\text{train}}\n(3)\nwhere\nx\n=\n(\nI\n,\nN\n)\nx=(I,N)\ndenotes the multi-modal input and\nâ„’\nseg\n\\mathcal{L}_{\\text{seg}}\nis the segmentation loss.\nIII-B\nScene Anchor Generator\nRecent Vision-Language Models (VLMs)\n[\n29\n,\n13\n,\n6\n]\nhave shown that language semantics are inherently domain-invariant: while visual features vary significantly across domains, the language semantics remain consistent.\nThis motivates us to leverage VLMs to construct domain-invariant scene anchors.\nFigure 3\n:\nScene Anchor Generator (SAG).\nTop: Scene attributes (weather, time-of-day, road type) are classified to derive the scene distribution.\nBottom: Scene prototypes are weighted by the distribution to form the scene anchor.\nIII-B\n1\nScene Marginal Probability Modeling\nOur key insight is that rather than modeling\nP\nâ€‹\n(\nğ’®\n)\nP(\\mathcal{S})\nholistically, we can approximate the scene distribution using marginal probabilities of key attributes for real-time efficiency.\nIn this paper, we select weather (atmospheric conditions), time-of-day (temporal conditions), and road type (terrain properties) as the key attributes, which align with the annotation schema of major off-road datasets\n[\n23\n,\n24\n]\n.\nWe factorize the scene distribution into marginal distributions:\nP\nâ€‹\n(\nğ’®\n|\nI\n)\nâ‰ˆ\nP\nâ€‹\n(\nW\n|\nI\n)\nâŠ—\nP\nâ€‹\n(\nD\n|\nI\n)\nâŠ—\nP\nâ€‹\n(\nR\n|\nI\n)\n,\nP(\\mathcal{S}|I)\\approx P(W|I)\\otimes P(D|I)\\otimes P(R|I),\n(4)\nwhere\nâŠ—\n\\otimes\ndenotes the tensor product,\nW\nW\ndenotes the weather,\nD\nD\ndenotes the time-of-day, and\nR\nR\ndenotes the road type.\nLeveraging the image-text contrastive capability of VLMs, we build three lightweight classifiers using the image\n[CLS]\ntoken. The probability for each attribute\na\nâˆˆ\n{\nW\n,\nL\n,\nR\n}\na\\in\\{W,L,R\\}\nis computed as:\np\nâ€‹\n(\na\ni\n|\nI\n)\n=\nexp\nâ¡\n(\nâŸ¨\nCLS\nimg\n,\nT\na\ni\nâŸ©\n/\nÏ„\n)\nâˆ‘\nj\nexp\nâ¡\n(\nâŸ¨\nCLS\nimg\n,\nT\na\nj\nâŸ©\n/\nÏ„\n)\n,\np(a_{i}|I)=\\frac{\\exp(\\langle\\text{CLS}_{\\text{img}},T_{a_{i}}\\rangle/\\tau)}{\\sum_{j}\\exp(\\langle\\text{CLS}_{\\text{img}},T_{a_{j}}\\rangle/\\tau)},\n(5)\nwhere\nT\na\ni\nT_{a_{i}}\ndenotes the text embedding for the\ni\ni\n-th category of attribute\na\na\n, and\nÏ„\n\\tau\nis a learnable temperature parameter.\nIII-B\n2\nScene Anchor Generation\nAs illustrated in the lower part of Fig.\n3\n, we construct a static Scene Prototype\nS\nâ€‹\nP\nSP\nby enumerating the combinatorial space of disjoint attributes (\nW\nÃ—\nD\nÃ—\nR\nW\\times D\\times R\n). Additionally, we introduce two learnable Meta-Traversability embeddings\nM\nâ€‹\ne\nâ€‹\nt\nâ€‹\na\nâˆˆ\nâ„\n2\nMeta\\in\\mathbb{R}^{2}\nto encode the latent semantics of traversable and non-traversable regions:\nS\nâ€‹\nP\n=\nE\nT\nâ€‹\n(\nğ’®\n,\nM\nâ€‹\ne\nâ€‹\nt\nâ€‹\na\n)\n,\nSP=E_{T}(\\mathcal{S},Meta),\n(6)\nwhere\nğ’®\n\\mathcal{S}\ndenotes the set of all possible scene combinations.\nThe Scene Anchor\nT\nÂ¯\nS\n\\bar{T}_{S}\nfor the current environment is then synthesized by aggregating the prototypes, weighted by their posterior probabilities inferred from the input image:\nT\nÂ¯\nS\n=\nâˆ‘\ns\nâˆˆ\nğ’®\nP\nâ€‹\n(\ns\n|\nI\n)\nâ‹…\nE\nT\nâ€‹\n(\ns\n,\nM\nâ€‹\ne\nâ€‹\nt\nâ€‹\na\n)\n,\n\\bar{T}_{S}=\\sum_{s\\in\\mathcal{S}}P(s|I)\\cdot E_{T}(s,Meta),\n(7)\nwhere\nğ’®\n\\mathcal{S}\ndenotes the set of all possible scene combinations, and\nE\nT\nâ€‹\n(\nâ‹…\n)\nE_{T}(\\cdot)\nis the frozen text encoder.\nIII-B\n3\nInference Efficiency\nA key advantage of this design is its efficiency: the text encoder is only used during training, and at deployment we simply weight the cached prototypes\nS\nâ€‹\nP\nSP\nwith inferred posterior probabilities to derive scene anchors.\nThis achieves zero-overhead semantic guidance by eliminating the text encoder at inference,\nimproving speed by 25.6% (from 2.97 to 3.73 FPS).\nIII-C\nMulti-modal Fusion via Optimal Transport\nAccording to the manifold hypothesis\n[\n36\n]\n, features from different modalities lie on distinct low-dimensional manifolds.\nConventional pixel-level or token-level alignment assumes rigid point-wise correspondence between these manifolds, which breaks down under OOD scenarios.\nOur key insight is that by transporting visual features from different modalities (RGB and surface normal) onto the scene anchor manifold, we can perform feature fusion on a domain-invariant representation space.\nFigure 4\n:\nOT Fusion Module.\nThe image and normal features are then transported to the scene anchor manifold via optimal transport for fusion.\nIII-C\n1\nSource Distribution and Target Distribution\nAs illustrated in Fig.\n4\n, we treat the extracted feature maps\nF\nâ„\n,\nF\nğ’©\nâˆˆ\nâ„\nH\nâ€²\nÃ—\nW\nâ€²\nÃ—\nC\nF_{\\mathcal{I}},F_{\\mathcal{N}}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times C}\nas uniform source distributions over the pixel grid:\nÎ¼\nâ„\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nÎ´\nf\ni\nâ„\n,\nÎ¼\nğ’©\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nÎ´\nf\ni\nğ’©\n,\n\\mu_{\\mathcal{I}}=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{f_{i}^{\\mathcal{I}}},\\quad\\mu_{\\mathcal{N}}=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{f_{i}^{\\mathcal{N}}},\n(8)\nwhere\nN\n=\nH\nâ€²\nÃ—\nW\nâ€²\nN=H^{\\prime}\\times W^{\\prime}\ndenotes the number of spatial locations and\nÎ´\n\\delta\nis the Dirac delta function.\nThe target distribution\nÎ½\n\\nu\nis derived by fusing the pre-segmentation probability maps from both modalities via element-wise max pooling:\nÎ½\n=\nâˆ‘\nk\n=\n1\nK\nm\nk\nâ€‹\nÎ´\nT\nk\n,\nm\nk\n=\nmax\nâ¡\n(\nP\nâ„\n(\nk\n)\n,\nP\nğ’©\n(\nk\n)\n)\nâˆ‘\nj\n=\n1\nK\nmax\nâ¡\n(\nP\nâ„\n(\nj\n)\n,\nP\nğ’©\n(\nj\n)\n)\n,\n\\nu=\\sum_{k=1}^{K}m_{k}\\delta_{T_{k}},\\quad m_{k}=\\frac{\\max(P_{\\mathcal{I}}^{(k)},P_{\\mathcal{N}}^{(k)})}{\\sum_{j=1}^{K}\\max(P_{\\mathcal{I}}^{(j)},P_{\\mathcal{N}}^{(j)})},\n(9)\nwhere\nT\nk\nT_{k}\ndenotes the\nk\nk\n-th semantic anchor and\nP\nâ„\n(\nk\n)\n,\nP\nğ’©\n(\nk\n)\nP_{\\mathcal{I}}^{(k)},P_{\\mathcal{N}}^{(k)}\nare the pre-segmentation probabilities for class\nk\nk\nfrom the image and surface normal branches, respectively. Max pooling selects the most confident prediction from either view, ensuring robust target mass estimation.\nIII-C\n2\nOptimal Transport Plan\nGiven source distributions\nÎ¼\nâ„\n\\mu_{\\mathcal{I}}\nand\nÎ¼\nğ’©\n\\mu_{\\mathcal{N}}\nfrom the image and surface normal branches, we solve two parallel entropy-regularized OT problems.\nTaking the image branch as an example, we first define the cost matrix\nC\nC\nusing cosine distance:\nC\ni\nâ€‹\nk\n=\n1\nâˆ’\nf\ni\nâ‹…\nT\nk\nâ€–\nf\ni\nâ€–\n2\nâ€‹\nâ€–\nT\nk\nâ€–\n2\n.\nC_{ik}=1-\\frac{f_{i}\\cdot T_{k}}{\\|f_{i}\\|_{2}\\|T_{k}\\|_{2}}.\n(10)\nThe optimal transport plan is then obtained by minimizing:\nÏ€\nâ„\nâˆ—\n=\narg\nâ¡\nmin\nÏ€\nâˆˆ\nÎ \nâ€‹\n(\nÎ¼\nâ„\n,\nÎ½\n)\nâŸ¨\nC\nâ„\n,\nÏ€\nâŸ©\nâˆ’\nÎµ\nâ€‹\nH\nâ€‹\n(\nÏ€\n)\n,\n\\pi^{*}_{\\mathcal{I}}=\\mathop{\\arg\\min}_{\\pi\\in\\Pi(\\mu_{\\mathcal{I}},\\nu)}\\langle C_{\\mathcal{I}},\\pi\\rangle-\\varepsilon H(\\pi),\n(11)\nwhere\nÎ \nâ€‹\n(\nÎ¼\nâ„\n,\nÎ½\n)\n\\Pi(\\mu_{\\mathcal{I}},\\nu)\nis the set of joint couplings with marginals\nÎ¼\nâ„\n\\mu_{\\mathcal{I}}\nand\nÎ½\n\\nu\n,\nH\nâ€‹\n(\nÏ€\n)\nH(\\pi)\nis the Shannon entropy, and\nÎµ\n\\varepsilon\nis the regularization parameter.\nThe surface normal branch\nÏ€\nğ’©\nâˆ—\n\\pi^{*}_{\\mathcal{N}}\nis computed analogously.\nBoth optimal solutions are obtained efficiently via Sinkhorn-Knopp iterations\n[\n32\n]\n, which are fully differentiable, enabling end-to-end training.\nIII-C\n3\nBarycentric Projection and Fusion\nWe then project both modalities onto the scene anchor\nT\nÂ¯\nS\n\\bar{T}_{S}\nvia barycentric mapping and fuse them by averaging:\nF\n~\nâ„\n\\displaystyle\\widetilde{F}_{\\mathcal{I}}\n=\nÏ€\nâ„\nâˆ—\nâ‹…\nT\nÂ¯\nS\n,\nF\n~\nğ’©\n=\nÏ€\nğ’©\nâˆ—\nâ‹…\nT\nÂ¯\nS\n,\n\\displaystyle=\\pi^{*}_{\\mathcal{I}}\\cdot\\bar{T}_{S},\\quad\\widetilde{F}_{\\mathcal{N}}=\\pi^{*}_{\\mathcal{N}}\\cdot\\bar{T}_{S},\n(12)\nF\nfusion\n\\displaystyle F_{\\text{fusion}}\n=\nÎ»\nâ€‹\nF\n~\nâ„\n+\n(\n1\nâˆ’\nÎ»\n)\nâ€‹\nF\n~\nğ’©\n,\n\\displaystyle=\\lambda\\widetilde{F}_{\\mathcal{I}}+(1-\\lambda)\\widetilde{F}_{\\mathcal{N}},\n(13)\nwhere\nÎ»\n\\lambda\nis the fusion weight. After projection, both modalities reside in the same semantic space, enabling straightforward cross-modal fusion.\nIII-D\nMasked Decoder\nThe fused features\nF\nfusion\nF_{\\text{fusion}}\nare decoded into mask features\nF\nmask\nF_{\\text{mask}}\nand multi-scale features\nF\nmulti\nF_{\\text{multi}}\n. Crucially, we initialize the traversability queries using the Scene Anchor\nT\nÂ¯\nS\n\\bar{T}_{S}\n, which explicitly conditions the segmentation process on scene priors:\nQ\nS\n(\n0\n)\n\\displaystyle Q_{S}^{(0)}\n=\nT\nÂ¯\nS\n+\nQ\npos\n,\n\\displaystyle=\\bar{T}_{S}+Q_{\\text{pos}},\n(14)\nQ\nS\n(\nl\n)\n\\displaystyle Q_{S}^{(l)}\n=\nDecoderLayer\nâ€‹\n(\nQ\nS\n(\nl\nâˆ’\n1\n)\n,\nF\nmulti\n,\nM\n(\nl\nâˆ’\n1\n)\n)\n,\n\\displaystyle=\\mathrm{DecoderLayer}\\left(Q_{S}^{(l-1)},F_{\\text{multi}},M^{(l-1)}\\right),\n(15)\nwhere\nQ\npos\nQ_{\\text{pos}}\nis the learnable positional encoding and\nM\n(\nl\nâˆ’\n1\n)\nM^{(l-1)}\nis the mask from the previous layer.\nThe final segmentation mask is produced by computing cosine similarity between refined queries and mask features:\nM\n(\nL\n)\n=\nÏƒ\nâ€‹\n(\nQ\nS\n(\nL\n)\nâ‹…\nF\nmask\nâ€–\nQ\nS\n(\nL\n)\nâ€–\n2\nâ€‹\nâ€–\nF\nmask\nâ€–\n2\n+\nÏµ\n)\n,\nM^{(L)}=\\sigma\\left(\\frac{Q_{S}^{(L)}\\cdot F_{\\text{mask}}}{\\|Q_{S}^{(L)}\\|_{2}\\|F_{\\text{mask}}\\|_{2}+\\epsilon}\\right),\n(16)\nwhere\nÏƒ\n\\sigma\ndenotes the Sigmoid function. The output\nM\n(\nL\n)\nM^{(L)}\nserves as the final traversability prediction.\nIII-E\nLoss Function\nThe total training objective combines four components:\nSegmentation Loss.\nFollowing Mask2Former\n[\n1\n]\n, we apply auxiliary supervision at each decoder layer using classification, binary cross-entropy, and Dice losses:\nâ„’\nseg\n=\nâˆ‘\nl\n=\n1\nL\n(\nÎ»\ncls\nâ€‹\nâ„’\ncls\nl\n+\nÎ»\nbce\nâ€‹\nâ„’\nbce\nl\n+\nÎ»\ndice\nâ€‹\nâ„’\ndice\nl\n)\n,\n\\mathcal{L}_{\\text{seg}}=\\sum_{l=1}^{L}\\left(\\lambda_{\\text{cls}}\\mathcal{L}_{\\text{cls}}^{l}+\\lambda_{\\text{bce}}\\mathcal{L}_{\\text{bce}}^{l}+\\lambda_{\\text{dice}}\\mathcal{L}_{\\text{dice}}^{l}\\right),\n(17)\nwhere\nÎ»\ncls\n,\nÎ»\nbce\n,\nÎ»\ndice\n\\lambda_{\\text{cls}},\\lambda_{\\text{bce}},\\lambda_{\\text{dice}}\nare the loss weights for each term.\nVision-Language Regularization.\nTo preserve the pre-trained alignment, we regularize both visual and textual representations:\nâ„’\nreg\n=\nâ€–\nCLS\nimg\nâˆ’\nCLS\nimg\nfrozen\nâ€–\n2\n2\nâŸ\nV2V\n+\nâ„’\nCE\nâ€‹\n(\nT\nÂ¯\nS\n,\nT\nÂ¯\nfrozen\n)\nâŸ\nL2L\n,\n\\mathcal{L}_{\\text{reg}}=\\underbrace{\\|\\text{CLS}_{\\text{img}}-\\text{CLS}_{\\text{img}}^{\\text{frozen}}\\|_{2}^{2}}_{\\text{V2V}}+\\underbrace{\\mathcal{L}_{\\text{CE}}(\\bar{T}_{S},\\bar{T}_{\\text{frozen}})}_{\\text{L2L}},\n(18)\nwhere\nCLS\nimg\nfrozen\n\\text{CLS}_{\\text{img}}^{\\text{frozen}}\nand\nT\nÂ¯\nfrozen\n\\bar{T}_{\\text{frozen}}\ndenote the feature embeddings extracted by the fixed pre-trained encoders.\nScene Classification Loss.\nTo supervise the SAG, we impose a cross-entropy classification loss\nâ„’\ncls\nscene\n\\mathcal{L}_{\\text{cls}}^{\\text{scene}}\non the predicted factor distributions.\nTotal Objective.\nâ„’\ntotal\n=\nÎ»\n1\nâ€‹\nâ„’\nseg\n+\nÎ»\n2\nâ€‹\nâ„’\nreg\n+\nÎ»\n3\nâ€‹\nâ„’\ncls\nscene\n.\n\\mathcal{L}_{\\text{total}}=\\lambda_{\\text{1}}\\mathcal{L}_{\\text{seg}}+\\lambda_{\\text{2}}\\mathcal{L}_{\\text{reg}}+\\lambda_{\\text{3}}\\mathcal{L}_{\\text{cls}}^{\\text{scene}}.\n(19)\nwhere\nÎ»\n1\n,\nÎ»\n2\n,\nÎ»\n3\n\\lambda_{\\text{1}},\\lambda_{\\text{2}},\\lambda_{\\text{3}}\nare the loss weights for each term.\nIV\nExperiments\nIV-A\nDataset Information\nTo evaluate the modelâ€™s generalization capability, we conduct comprehensive experiments on two off-road datasets: ORFD\n[\n23\n]\nand ORAD-3D\n[\n24\n]\n.\nBoth datasets provide high-resolution images and LiDAR data across diverse weather, time-of-day, and road conditions. The distribution of these attributes is detailed in Fig.\n5\n.\nFigure 5\n:\nLong-tailed distribution analysis of the ORFD dataset.\nFollowing Sec.\nIII-A\n, we partition both datasets into Known (ID) and Unknown (OOD) splits based on scene attribute combinations, where OOD contains only combinations absent from training (see Fig.\n6\n).\nFigure 6\n:\nComparison of Known and Unknown scenes. Red text indicates attribute differences from the training distribution.\nDetailed statistics of this splitting protocol are summarized in Table\nI\n.\nTABLE I\n:\nComparison of statistical information for the ORFD and ORAD-3D datasets\nDataset\nTotal Samples\nScenes\nSplit Ratio\nTrain Set\nVal. Set\nTest Set\nKnown Scenes\nUnknown Scenes\nORFD\n[\n23\n]\n12K\n30\n7:1:2\n8.3K (21)\n1.6K (4)\n1.4K (3)\n0.7K(2)\nORAD-3D\n[\n24\n]\n57K\n142\n7:1:2\n40K (102)\n5.7K (15)\n9.2K (22)\n1.9K(4)\nORFD-to-ORAD3D\n11.1K\n26\n-\n-\n-\n9.3K (22)\n1.8K(4)\nTABLE II\n:\nPerformance comparison on the ORFD test set\nModel\nOverall Metrics (%)\nKnown Scenes (%)\nUnknown Scenes (%)\nÎ”\n\\Delta\n(Unknown - Known) (%)\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nOFF-Net\n[\n23\n]\n89.07\n93.65\n90.87\n83.48\n95.53\n96.39\n95.95\n92.25\n78.08\n89.47\n80.62\n68.42\nâ†“\n\\downarrow\n17.45\nâ†“\n\\downarrow\n6.92\nâ†“\n\\downarrow\n15.33\nâ†“\n\\downarrow\n23.83\nM2F2-Net\n[\n42\n]\n88.80\n93.92\n90.73\n83.23\n94.58\n96.47\n95.43\n91.31\n78.44\n89.88\n81.05\n69.01\nâ†“\n\\downarrow\n16.14\nâ†“\n\\downarrow\n6.59\nâ†“\n\\downarrow\n14.38\nâ†“\n\\downarrow\n22.30\nROD\n[\n35\n]\n97.14\n95.02\n96.01\n92.40\n97.43\n96.13\n96.75\n93.73\n96.51\n91.83\n93.94\n88.81\nâ†“\n\\downarrow\n0.92\nâ†“\n\\downarrow\n4.30\nâ†“\n\\downarrow\n2.81\nâ†“\n\\downarrow\n4.92\nOT-Drive (Ours)\n97.70\n97.57\n97.63\n95.40\n97.78\n97.53\n97.65\n95.42\n97.34\n97.66\n97.50\n95.16\nâ†“\n\\downarrow\n0.44\nâ†‘\n\\uparrow\n0.13\nâ†“\n\\downarrow\n0.15\nâ†“\n\\downarrow\n0.26\nTABLE III\n:\nPerformance comparison on the ORAD-3D test set\nModel\nOverall Metrics (%)\nKnown Scenes (%)\nUnknown Scenes (%)\nÎ”\n\\Delta\n(Unknown - Known) (%)\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nOFF-Net\n[\n23\n]\n93.92\n93.07\n93.48\n87.86\n94.12\n93.35\n93.72\n88.29\n92.94\n91.67\n92.27\n85.83\nâ†“\n\\downarrow\n1.18\nâ†“\n\\downarrow\n1.68\nâ†“\n\\downarrow\n1.45\nâ†“\n\\downarrow\n2.46\nM2F2-Net\n[\n42\n]\n94.58\n93.80\n94.18\n89.09\n94.92\n94.05\n94.47\n89.60\n92.94\n92.61\n92.77\n86.67\nâ†“\n\\downarrow\n1.98\nâ†“\n\\downarrow\n1.44\nâ†“\n\\downarrow\n1.70\nâ†“\n\\downarrow\n2.93\nROD\n[\n35\n]\n91.71\n83.08\n85.92\n75.92\n92.29\n84.11\n86.86\n77.29\n88.80\n78.00\n81.14\n69.42\nâ†“\n\\downarrow\n3.49\nâ†“\n\\downarrow\n6.11\nâ†“\n\\downarrow\n5.72\nâ†“\n\\downarrow\n7.87\nOT-Drive (Ours)\n95.32\n94.92\n95.12\n90.75\n95.41\n95.03\n95.22\n90.93\n94.91\n94.35\n94.63\n89.89\nâ†“\n\\downarrow\n0.50\nâ†“\n\\downarrow\n0.68\nâ†“\n\\downarrow\n0.59\nâ†“\n\\downarrow\n1.04\nIV-B\nExperimental Setup\nWe employ EVA02-CLIP-B-16\n[\n6\n]\nas the visual backbone at\n512\nÃ—\n512\n512\\times 512\nresolution. Surface normals are estimated using D2NT\n[\n7\n]\n. The decoder follows a two-stage design: a Pixel Decoder\n[\n27\n]\nwith multi-scale deformable attention. The model is trained for 20,000 iterations with a batch size of 16 using\nAdamW optimizer (learning rate\n1\nâ€‹\ne\nâˆ’\n5\n1\\text{e}{-5}\n, weight decay\n1\nâ€‹\ne\nâˆ’\n4\n1\\text{e}{-4}\n).\nThe fusion weight\nÎ»\n\\lambda\nin Eq.Â (12) is set to 0.5.\nWe report standard semantic segmentation metrics: mean Accuracy (mAcc), mean Recall (mRec), mean F1-score (mF1), and mean Intersection over Union (mIoU).\nIV-C\nBaselines\nâ€¢\nOFF-Net\n[\n23\n]\n: A Transformer-based fusion network combining image and surface normal.\nâ€¢\nM2F2-Net\n[\n42\n]\n: Features multi-modal cross-fusion and edge-aware decoding for improved boundary accuracy.\nâ€¢\nROD\n[\n35\n]\n: A single-modal RGB method using pretrained ViT for efficient free-space detection.\nIV-D\nQuantitative Analysis\nTo evaluate the generalization capability of OT-Drive, we conduct extensive comparative evaluations against three baselines under two distinct settings:\nIV-D\n1\nWithin-Dataset Performance\nTable\nII\n, Table\nIII\n, and Fig.\n7\npresent quantitative results on ORFD and ORAD-3D.\n1.\nGeneralization to OOD Scenarios:\nOur method achieves minimal performance degradation in OOD scenarios (\nÎ”\nâ€‹\nm\nâ€‹\nI\nâ€‹\no\nâ€‹\nU\n<\n1.1\n%\n\\Delta mIoU<1.1\\%\n), substantially outperforming baselines (see Figure\n7\n).\nExisting methods rely on memorized dataset-specific patterns that fail to generalize to novel scenes. Our SAG addresses this by constructing domain-invariant scene anchors from language semantics, effectively enhancing OOD generalization.\n2.\nRobustness of OT Fusion:\nOur method achieves the best performance on both datasets, consistently outperforming existing methods in both ID and OOD scenarios (see Tables\nII\nâ€“\nIII\n).\nUnlike conventional methods that rely on pixel/token-level alignment sensitive to domain variations, our OT Fusion aligns features at the distribution level via transport constraints, yielding robust predictions across all scenarios.\nFigure 7\n:\nRelative performance degradation (\nÎ”\n\\Delta\nmIoU, %) from known to unknown scenes. Lower values indicate stronger generalization capability.\nIV-D\n2\nCross-Dataset Generalization\nAs presented in Table\nIV\n, we perform a cross-dataset transfer evaluation by training on ORFD (8.3K) and testing directly on ORAD-3D (11.1K) without fine-tuning.\nOur method demonstrates strong data efficiency: even when tested on a larger dataset (11.1K) than training (8.3K), it still achieves the best OOD generalization (outperforming baselines by 13.99%) with minimal performance degradation (\nÎ”\nâ€‹\nm\nâ€‹\nI\nâ€‹\no\nâ€‹\nU\n=\n+\n0.07\n%\n\\Delta mIoU=+0.07\\%\n).\nThis is because we do not directly model the entire scene distribution, but instead factorize it into marginal probabilities over independent attributes, enabling compositional generalization by recombining scene factors learned from known scenarios.\nTABLE IV\n:\nCross-dataset generalization: trained on ORFD, tested on ORAD-3D test set (26 scenes)\nModel\nOverall Metrics (%)\nKnown Scenes (%)\nUnknown Scenes (%)\nÎ”\n\\Delta\n(Unknown - Known) (%)\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nmAcc\nmRecall\nmF1\nmIoU\nOFF-Net\n[\n23\n]\n92.23\n92.05\n92.13\n85.57\n93.98\n93.43\n93.70\n88.24\n83.18\n84.70\n83.88\n72.85\nâ†“\n\\downarrow\n10.80\nâ†“\n\\downarrow\n8.73\nâ†“\n\\downarrow\n9.82\nâ†“\n\\downarrow\n15.39\nM2F2-Net\n[\n42\n]\n91.44\n93.45\n92.33\n85.87\n93.00\n94.24\n93.58\n88.02\n84.23\n89.69\n86.00\n75.80\nâ†“\n\\downarrow\n8.77\nâ†“\n\\downarrow\n4.55\nâ†“\n\\downarrow\n7.58\nâ†“\n\\downarrow\n12.22\nROD\n[\n35\n]\n92.28\n86.42\n88.64\n79.97\n92.33\n87.14\n89.14\n80.73\n92.32\n82.21\n85.62\n75.65\nâ†“\n\\downarrow\n0.01\nâ†“\n\\downarrow\n4.93\nâ†“\n\\downarrow\n3.52\nâ†“\n\\downarrow\n5.08\nOT-Drive (Ours)\n95.03\n94.10\n94.55\n89.74\n95.01\n94.11\n94.54\n89.72\n95.14\n94.02\n94.56\n89.79\nâ†‘\n\\uparrow\n0.13\nâ†“\n\\downarrow\n0.09\nâ†‘\n\\uparrow\n0.02\nâ†‘\n\\uparrow\n0.07\nIV-E\nAblation Study\nTo validate the contributions of each proposed component, we conduct progressive ablation experiments, with results summarized in Table\nV\n.\nTABLE V\n:\nAblation study on ORFD and ORAD-3D datasets.\nNo.\nComponents\nORFD (mIoU %)\nORAD-3D (mIoU %)\nScene Anchor\nOT Fusion\nMarginal Prob.\nOverall\nKnown\nUnknown\nÎ”\n\\Delta\nOverall\nKnown\nUnknown\nÎ”\n\\Delta\n1\n85.44\n92.14\n73.06\nâ†“\n\\downarrow\n19.08\n87.96\n88.89\n83.53\nâ†“\n\\downarrow\n5.35\n2\nâœ“\n94.35\n94.36\n94.09\nâ†“\n\\downarrow\n0.27\n90.57\n90.85\n89.26\nâ†“\n\\downarrow\n1.58\n3\nâœ“\nâœ“\n95.48\n95.98\n94.15\nâ†“\n\\downarrow\n1.83\n90.55\n90.73\n89.65\nâ†“\n\\downarrow\n1.08\nOurs\nâœ“\nâœ“\nâœ“\n95.40\n95.42\n95.16\nâ†“\n\\downarrow\n0.26\n90.75\n90.93\n89.89\nâ†“\n\\downarrow\n1.04\n1.\nEffectiveness of Scene Anchor.\nComparing Model #1 with Model #2 demonstrates substantial OOD generalization improvements on both ORFD and ORAD-3D, with mIoU gains of 21.03% and 5.73%, respectively. This validates that scene anchors effectively enhance the modelâ€™s generalization capability.\n2.\nEffectiveness of OT Fusion.\nComparing Model #2 with Model #3 reveals consistent performance gains in OOD scenarios, validating that distribution-level feature fusion via optimal transport can achieve robust generalization when encountering OOD features in unseen environments.\n3.\nEffectiveness of Marginal Probability Modeling.\nComparing Model #3 with the full model demonstrates that marginal probability modeling is essential for generalizing to unseen scene combinations, as it enables factor recombination and significantly narrows the generalization gap on both datasets.\nIV-F\nQualitative Analysis\nFigure 8\n:\nQualitative comparison of segmentation results. From left to right: RGB input, OFF-Net\n[\n23\n]\n, M2F2-Net\n[\n42\n]\n, ROD\n[\n35\n]\n, and OT-Drive (Ours).\nWe present qualitative comparisons between our method and baseline models in Fig.\n8\n, specifically targeting unknown scene combinations.\nExisting methods show inconsistent performance: multi-modal methods excel at night but struggle on snow, while RGB-only models exhibit the opposite behavior.\nHowever, our method exhibits consistent superiority across both conditions, validating the effectiveness of our distribution-level OT Fusion.\nIn gravel and dirt road scenarios, existing methods frequently misclassify geometrically flat regions.\nThis occurs because their traversability semantics are biased by flat areas in the training set.\nOur SAG addresses this by compositionally combining marginal probability distributions to construct domain-invariant scene anchors that generalize to unseen scenarios.\nIV-G\nEfficiency Analysis\nTo assess practical deployment potential, we benchmark all methods on the NVIDIA Tesla T4â€”a widely adopted edge computing GPU.\nFor multi-modal methods, we include GPU-accelerated surface normal estimation in the total pipeline time.\nAs shown in Table\nVI\n, by simply applying standard inference optimizations (PyTorch 2.0 graph compilation and FP16 mixed-precision), OT-Drive achieves 21.11 FPSâ€”a 5.66\nÃ—\n\\times\nspeedup over the base configuration.\nThese results demonstrate the strong real-time deployment potential of OT-Drive on resource-constrained edge devices.\nTABLE VI\n:\nComparison of inference speed (FPS) on Tesla T4.\nâ€ \n\\dagger\nincludes surface normal estimation latency.\nMethod\nOFF-Net\nâ€ \nM2F2-Net\nâ€ \nROD\nOT-Drive\nâ€ \n(Ours)\nBase\nOptimized\nFPS\n4.49\n4.23\n3.58\n3.73\n21.11\nV\nConclusion\nIn this paper, we presented OT-Drive, a novel multi-modal fusion framework designed to address the challenge of OOD generalization in off-road traversable area segmentation.\nThe Scene Anchor Generator models marginal probabilities of scene attributes and constructs generalizable scene anchors by composing environmental attributes.\nThe OT Fusion module then aligns RGB and surface normal features onto the semantic manifold defined by these anchors, achieving distribution-level fusion via Optimal Transport.\nExtensive experiments demonstrate that OT-Drive significantly outperforms existing baselines, particularly in OOD scenarios, and achieves a substantial 13.99% mIoU gain in cross-dataset transfer tasks.\nMoreover, OT-Drive attains a real-time inference speed of 21.11 FPS, ensuring its suitability for practical deployment.\nWe believe this work represents a meaningful exploration of leveraging Vision-Language Models (VLMs) for traversable area segmentation.\nDespite these advancements, our method relies on a predefined closed-set scene space to ensure real-time efficiency.\nIn the future, we aim to extend this framework by exploring the integration of the complex reasoning capabilities and open-world knowledge of Multimodal Large Language Models (MLLMs), enabling more fine-grained and context-aware traversable area segmentation in complex environments.\nReferences\n[1]\nB. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar\n(2022)\nMasked-attention mask transformer for universal image segmentation\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 1290â€“1299\n.\nCited by:\nÂ§\nIII-E\n.\n[2]\nG. Cheng and J. Y. Zheng\n(2022-12)\nSequential semantic segmentation of road profiles for path and speed planning\n.\nIEEE Transactions on Intelligent Transportation Systems\n23\n(\n12\n),\npp.Â 23869â€“23882\n(\nen\n).\nExternal Links:\nISSN 1558-0016\n,\nDocument\nCited by:\nÂ§I\n.\n[3]\nC. Chung, G. Georgakis, P. Spieler, C. Padgett, A. Agha, and S. Khattak\n(2024)\nPixel to elevation: learning to predict elevation maps at long range using images for autonomous offroad navigation\n.\nIEEE Robotics and Automation Letters\n9\n(\n7\n),\npp.Â 6170â€“6177\n.\nCited by:\nÂ§\nII-A\n.\n[4]\nM. Fahes, T. Vu, A. Bursuc, P. PÃ©rez, and R. De Charette\n(2024-06)\nA simple recipe for language-guided domain generalized segmentation\n.\nIn\n2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nSeattle, WA, USA\n,\npp.Â 23428â€“23437\n(\nen\n).\nExternal Links:\nDocument\nCited by:\nÂ§\nII-C\n.\n[5]\nR. Fan, H. Wang, P. Cai, and M. Liu\n(2020)\nSne-roadseg: incorporating surface normal information into semantic segmentation for accurate freespace detection\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 340â€“356\n.\nCited by:\nÂ§\nII-B\n.\n[6]\nY. Fang, Q. Sun, X. Wang, T. Huang, X. Wang, and Y. Cao\n(2024-09)\nEVA-02: A Visual Representation for Neon Genesis\n.\nImage and Vision Computing\n149\n,\npp.Â 105171\n.\nExternal Links:\n2303.11331\n,\nISSN 02628856\n,\nDocument\nCited by:\nÂ§\nIII-B\n,\nÂ§\nIV-B\n.\n[7]\nY. Feng, B. Xue, M. Liu, Q. Chen, and R. Fan\n(2023)\nD2NT: a high-performing depth-to-normal translator\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 12360â€“12366\n.\nCited by:\nÂ§\nIV-B\n.\n[8]\nZ. Feng, Y. Feng, Y. Guo, and Y. Sun\n(2023)\nAdaptive-mask fusion network for segmentation of drivable road and negative obstacle with untrustworthy features\n.\nIn\n2023 IEEE Intelligent Vehicles Symposium (IV)\n,\npp.Â 1â€“6\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[9]\nZ. Feng, Y. Guo, Q. Liang, M. U. M. Bhutta, H. Wang, M. Liu, and Y. Sun\n(2022)\nMAFNet: segmentation of road potholes with multimodal attention fusion network for autonomous vehicles\n.\nIEEE Transactions on Instrumentation and Measurement\n71\n,\npp.Â 1â€“12\n(\nen\n).\nExternal Links:\nISSN 1557-9662\n,\nDocument\nCited by:\nÂ§I\n.\n[10]\nT. Guan, D. Kothandaraman, R. Chandra, A. J. Sathyamoorthy, K. Weerakoon, and D. Manocha\n(2022)\nGa-nav: Efficient terrain segmentation for robot navigation in unstructured outdoor environments\n.\nIEEE Robotics and Automation Letters\n7\n(\n3\n),\npp.Â 8138â€“8145\n(\nen\n).\nCited by:\nÂ§\nII-A\n.\n[11]\nJ. Hwang, D. Kim, H. Yoon, D. Kim, and S. Seo\n(2025)\nHow to relieve distribution shifts in semantic segmentation for off-road environments\n.\nIEEE Robotics and Automation Letters\n10\n(\n5\n),\npp.Â 4500â€“4507\n(\nen\n).\nExternal Links:\nISSN 2377-3766\n,\nDocument\nCited by:\nÂ§\nII-C\n.\n[12]\nS. Jeon, K. Hong, and H. Byun\n(2025)\nExploiting domain properties in language-driven domain generalization for semantic segmentation\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 20791â€“20801\n.\nCited by:\nÂ§\nII-C\n.\n[13]\nC. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. Le, Y. Sung, Z. Li, and T. Duerig\n(2021)\nScaling up visual and vision-language representation learning with noisy text supervision\n.\nIn\nInternational conference on machine learning\n,\npp.Â 4904â€“4916\n.\nCited by:\nÂ§\nIII-B\n.\n[14]\nV. JimÃ©nez, J. Godoy, A. ArtuÃ±edo, and J. Villagra\n(2021)\nGround segmentation algorithm for sloped terrain and sparse LiDAR point cloud\n.\nIEEE Access\n9\n,\npp.Â 132914â€“132927\n.\nCited by:\nÂ§\nII-A\n.\n[15]\nL. Kantorovitch\n(1958)\nOn the Translocation of Masses\n.\nManagement Science\n5\n(\n1\n),\npp.Â 1â€“4\n(\nen\n).\nExternal Links:\n2626967\nCited by:\nÂ§I\n.\n[16]\nO. Kim, J. Seo, S. Ahn, and C. H. Kim\n(2024)\nUfo: uncertainty-aware lidar-image fusion for off-road semantic terrain map estimation\n.\nIn\n2024 IEEE Intelligent Vehicles Symposium (IV)\n,\npp.Â 192â€“199\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[17]\nS. Kim, D. Kim, and H. Kim\n(2023)\nTexture learning domain randomization for domain generalized segmentation\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 677â€“687\n.\nCited by:\nÂ§\nII-C\n.\n[18]\nS. Lee, H. Lim, and H. Myung\n(2022)\nPatchwork++: fast and robust ground segmentation solving partial under-segmentation using 3d point cloud\n.\nIn\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 13276â€“13283\n.\nCited by:\nÂ§\nII-A\n.\n[19]\nJ. Li, Y. Zhan, P. Yun, G. Zhou, Q. Chen, and R. Fan\n(2024)\nRoadFormer: Duplex transformer for RGB-normal semantic road scene parsing\n.\nIEEE Transactions on Intelligent Vehicles\n(\nen\n).\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[20]\nH. Lim, M. Oh, and H. Myung\n(2021)\nPatchwork: concentric zone-based region-wise ground segmentation with ground likelihood estimation using a 3d LiDAR sensor\n.\nIEEE Robotics and Automation Letters\n6\n(\n4\n),\npp.Â 6458â€“6465\n(\nen\n).\nExternal Links:\nDocument\nCited by:\nÂ§\nII-A\n.\n[21]\nY. Lv, Z. Liu, G. Li, and X. Chang\n(2024)\nNoise-aware intermediary fusion network for off-road freespace detection\n.\nIEEE Transactions on Intelligent Vehicles\n,\npp.Â 1â€“11\n(\nen\n).\nExternal Links:\nISSN 2379-8904\n,\nDocument\nCited by:\nÂ§\nII-B\n.\n[22]\nX. Meng, N. Hatch, A. Lambert, A. Li, N. Wagener, M. Schmittle, J. Lee, W. Yuan, Z. Chen, S. Deng,\net al.\n(2023)\nTerrainnet: visual modeling of complex terrain for high-speed, off-road navigation\n.\narXiv preprint arXiv:2303.15771\n.\nCited by:\nÂ§\nII-A\n.\n[23]\nC. Min, W. Jiang, D. Zhao, J. Xu, L. Xiao, Y. Nie, and B. Dai\n(2022)\nOrfd: a dataset and benchmark for off-road freespace detection\n.\nIn\n2022 international conference on robotics and automation (ICRA)\n,\npp.Â 2532â€“2538\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIII-B\n1\n,\nFigure 8\n,\nFigure 8\n,\n1st item\n,\nÂ§\nIV-A\n,\nTABLE I\n,\nTABLE II\n,\nTABLE III\n,\nTABLE IV\n.\n[24]\nC. Min, J. Mei, H. Zhai, S. Wang, T. Sun, F. Kong, H. Li, F. Mao, F. Liu, S. Wang,\net al.\n(2025)\nAdvancing off-road autonomous driving: the large-scale orad-3d dataset and comprehensive benchmarks\n.\narXiv preprint arXiv:2510.16500\n.\nCited by:\nÂ§\nIII-B\n1\n,\nÂ§\nIV-A\n,\nTABLE I\n.\n[25]\nC. Min, S. Si, X. Wang, H. Xue, W. Jiang, Z. Chen, Q. Zhu, J. Mei, T. Sun, H. Zhai, E. Shang, L. Shang, K. Zhao, H. Fu, Z. Xiao, C. Li, B. Dai, D. Zhao, L. Xiao, Y. Nie, and Y. Hu\n(2025)\nAutonomous ground robots in unstructured environments: how far have we come?\n.\nJournal of Field Robotics\n.\nCited by:\nÂ§I\n.\n[26]\nJ. Niemeijer, M. Schwonberg, J. TermÃ¶hlen, N. M. Schmidt, and T. Fingscheidt\n(2024)\nGeneralization by adaptation: diffusion-based domain extension for domain-generalized semantic segmentation\n.\nIn\nProceedings of the IEEE/CVF winter conference on applications of computer vision\n,\npp.Â 2830â€“2840\n.\nCited by:\nÂ§\nII-C\n.\n[27]\nB. Pak, B. Woo, S. Kim, D. Kim, and H. Kim\n(2024)\nTextual query-driven mask transformer for domain generalized segmentation\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 37â€“54\n.\nCited by:\nÂ§\nII-C\n,\nÂ§\nIV-B\n.\n[28]\nF. Philippe, J. Laconte, P. Lapray, M. Spisser, and J. Lauffenburger\n(2025)\nCollision-aware traversability analysis for autonomous vehicles in the context of agricultural robotics\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 13138â€“13145\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[29]\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\net al.\n(2021)\nLearning transferable visual models from natural language supervision\n.\nIn\nInternational conference on machine learning\n,\npp.Â 8748â€“8763\n.\nCited by:\nÂ§\nIII-B\n.\n[30]\nJ. Seo, T. Kim, K. Kwak, J. Min, and I. Shim\n(2023)\nScate: a scalable framework for self-supervised traversability estimation in unstructured environments\n.\nIEEE Robotics and Automation Letters\n8\n(\n2\n),\npp.Â 888â€“895\n(\nen\n).\nCited by:\nÂ§\nII-A\n.\n[31]\nA. Shaban, X. Meng, J. Lee, B. Boots, and D. Fox\n(2022)\nSemantic terrain classification for off-road autonomous driving\n.\nIn\nConference on Robot Learning\n,\npp.Â 619â€“629\n.\nCited by:\nÂ§\nII-A\n.\n[32]\nR. Sinkhorn and P. Knopp\n(1967)\nConcerning nonnegative matrices and doubly stochastic matrices\n.\nPacific Journal of Mathematics\n21\n(\n2\n),\npp.Â 343â€“348\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-C\n2\n.\n[33]\nS. Song and J. Lee\n(2025)\nLeveraging text-driven semantic variation for robust ood segmentation\n.\nIn\n2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 9909â€“9916\n.\nCited by:\nÂ§\nII-C\n.\n[34]\nC. Sun, P. Ge, T. Zhang, and Q. Xiang\n(2023)\nPassable area detection in off-road environments based on improved pspnet\n.\nIn\n2023 4th International Conference on Computer Vision, Image and Deep Learning (CVIDL)\n,\npp.Â 687â€“692\n.\nCited by:\nÂ§\nII-A\n.\n[35]\nT. Sun, H. Ye, J. Mei, L. Chen, F. Zhao, L. Zong, and Y. Hu\n(2025)\nROD: rgb-only fast and efficient off-road freespace detection\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 9787â€“9793\n.\nCited by:\nFigure 8\n,\nFigure 8\n,\n3rd item\n,\nTABLE II\n,\nTABLE III\n,\nTABLE IV\n.\n[36]\nJ. B. Tenenbaum, V. d. Silva, and J. C. Langford\n(2000)\nA global geometric framework for nonlinear dimensionality reduction\n.\nscience\n290\n(\n5500\n),\npp.Â 2319â€“2323\n.\nCited by:\nÂ§\nIII-C\n.\n[37]\nH. Wang, R. Fan, P. Cai, and M. Liu\n(2021-09)\nSNE-RoadSeg+: Rethinking Depth-Normal Translation and Deep Supervision for Freespace Detection\n.\nIn\n2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nPrague, Czech Republic\n,\npp.Â 1140â€“1145\n(\nen\n).\nExternal Links:\nDocument\nCited by:\nÂ§\nII-B\n.\n[38]\nZ. Wang, Z. Liao, B. Zhou, G. Yu, and W. Luo\n(2024)\nSwinURNet: hybrid transformer-CNN architecture for real-time unstructured road segmentation\n.\nIEEE Transactions on Instrumentation and Measurement\n73\n,\npp.Â 1â€“16\n(\nen\n).\nExternal Links:\nISSN 1557-9662\n,\nDocument\nCited by:\nÂ§\nII-A\n.\n[39]\nM. Xu, Y. Wang, X. Zhang, D. Mao, C. Wang, R. Song, and Y. Li\n(2024)\nTransformer-based traversability analysis for autonomous navigation in outdoor environments with water hazard\n.\nIEEE Transactions on Intelligent Vehicles\n,\npp.Â 1â€“14\n(\nen\n).\nExternal Links:\nISSN 2379-8904\n,\nDocument\nCited by:\nÂ§I\n.\n[40]\nH. Xue, L. Xiao, X. Hu, R. Xie, H. Fu, Y. Nie, and B. Dai\n(2025)\nContrastive label disambiguation for self-supervised terrain traversability learning in off-road environments\n.\nIEEE Transactions on Intelligent Transportation Systems\n.\nCited by:\nÂ§\nII-A\n.\n[41]\nT. Yan, Y. Wang, H. Lv, H. Sun, D. Zhang, and Y. Yang\n(2024)\nFsn-swin: a network for freespace detection in unstructured environments\n.\nIEEE Access\n12\n,\npp.Â 12308â€“12322\n(\nen\n).\nExternal Links:\nISSN 2169-3536\n,\nDocument\nCited by:\nÂ§\nII-B\n.\n[42]\nH. Ye, J. Mei, and Y. Hu\n(2023)\nM2f2-net: multi-modal feature fusion for unstructured off-road freespace detection\n.\nIn\n2023 IEEE Intelligent Vehicles Symposium (IV)\n,\npp.Â 1â€“7\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nFigure 8\n,\nFigure 8\n,\n2nd item\n,\nTABLE II\n,\nTABLE III\n,\nTABLE IV\n.\n[43]\nW. Yue, Z. Zhou, Y. Cao, and Liuman\n(2024-10)\nCross-modal domain generalization semantic segmentation based on fusion features\n.\nKnowledge-Based Systems\n302\n,\npp.Â 112356\n(\nen\n).\nExternal Links:\nISSN 0950-7051\n,\nDocument\nCited by:\nÂ§\nII-C\n.\n[44]\nX. Zhang and R. T. Tan\n(2025)\nMamba as a bridge: where vision foundation models meet vision language models for domain-generalized semantic segmentation\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 14527â€“14537\n.\nCited by:\nÂ§\nII-C\n.\n[45]\nC. Zheng, L. Liu, Y. Meng, M. Wang, and X. Jiang\n(2024-03)\nPassable area segmentation for open-pit mine road from vehicle perspective\n.\nEngineering Applications of Artificial Intelligence\n129\n,\npp.Â 107610\n(\nen\n).\nExternal Links:\nISSN 0952-1976\n,\nDocument\nCited by:\nÂ§I\n.",
    "preview_text": "Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.\n\nOT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport\nZhihuaÂ Zhao,Â GuoqiangÂ Li\nâˆ—\n,Â ChenÂ Min,Â andÂ KangpingÂ Lu\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\nâˆ—\nCorresponding author: Guoqiang Li (guoqiangli@bit.edu.cn).Zhihua Zhao and Guoqiang Li are with the School of Mechanical Engineering, Beijing Institute of Technology, Beijing 100081, China (e-mail: zhihuazhao@bit.edu.cn; guoqiangli@bit.edu.cn).Chen Min i",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "Optimal Transport",
        "multi-modal fusion",
        "traversable area segmentation",
        "out-of-distribution",
        "autonomous driving",
        "off-road"
    ],
    "one_line_summary": "OT-Drive æ˜¯ä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“çš„å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œç”¨äºåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­å®ç°é²æ£’çš„å¯é€šè¡ŒåŒºåŸŸåˆ†å‰²ï¼Œä»¥åº”å¯¹åˆ†å¸ƒå¤–åœºæ™¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-15T00:23:45Z",
    "created_at": "2026-01-20T17:49:52.775872",
    "updated_at": "2026-01-20T17:49:52.775880",
    "recommend": 0
}