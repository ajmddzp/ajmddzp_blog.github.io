{
    "id": "2601.15025v1",
    "title": "ExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data",
    "authors": [
        "Marian Renz",
        "Martin Günther",
        "Felix Igelbrink",
        "Oscar Lima",
        "Martin Atzmueller"
    ],
    "abstract": "尽管深度学习已显著推动了机器人目标识别技术的发展，但纯数据驱动的方法往往缺乏语义一致性，且未能充分利用环境中已有的宝贵先验知识。本报告介绍了ExPrIS项目，该项目通过研究如何利用知识层面的预期来优化传感器数据的目标解析，以应对这一挑战。我们的方法基于增量式构建三维语义场景图（3DSSG），整合了来自两个维度的预期信息：一是基于历史观测的上下文先验，二是来自外部知识图谱（如ConceptNet）的语义知识。这些信息通过异构图神经网络（GNN）进行嵌入，形成具有预期偏置的推理机制。该方法突破了传统逐帧静态分析的局限，提升了场景理解在时序上的鲁棒性与一致性。报告详细阐述了该架构的设计原理、评估结果，并规划了其在移动机器人平台上的集成方案。",
    "url": "https://arxiv.org/abs/2601.15025v1",
    "html_url": "https://arxiv.org/html/2601.15025v1",
    "html_content": "[1,2]\n\\fnm\nMarian\n\\sur\nRenz\n1]\n\\orgname\nGerman Research Center for Artificial Intelligence,\n\\orgdiv\nResearch Department Cooperative and Autonomous Systems,\n\\orgaddress\n\\city\nOsnabrück,\n\\country\nGermany\n2]\n\\orgname\nOsnabrück University, Institute of Computer Science,\n\\orgdiv\nSemantic Information Systems Group,\n\\orgaddress\n\\city\nOsnabrück,\n\\country\nGermany\nExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data\nmarian.renz@dfki.de\n\\fnm\nMartin\n\\sur\nGünther\nmartin.guenther@dfki.de\n\\fnm\nFelix\n\\sur\nIgelbrink\nfelix.igelbrink@dfki.de\n\\fnm\nOscar\n\\sur\nLima\noscar.lima@dfki.de\n\\fnm\nMartin\n\\sur\nAtzmueller\nmartin.atzmueller@uos.de\n[\n[\nAbstract\nWhile deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-level expectations can serve as priors to improve object interpretation from sensor data. Our approach is based on the incremental construction of a\n3D Semantic Scene Graph (3DSSG)\n. We integrate expectations from two sources: contextual priors from past observations and semantic knowledge from external graphs like ConceptNet. These are embedded into a heterogeneous\nGraph Neural Network (GNN)\nto create an expectation-biased inference process. This method moves beyond static, frame-by-frame analysis to enhance the robustness and consistency of scene understanding over time. The report details this architecture, its evaluation, and outlines its planned integration on a mobile robotic platform.\nkeywords:\n3D Semantic Scene Graphs, Knowledge Integration, Robotic Perception, Graph Neural Networks\n1\nIntroduction\nObject recognition and semantic classification in sensor data like RGB or RGB-D have seen considerably advances in recent years—specifically due to significant methodological developments and improvements in machine-learning-based approaches. In particular, this is related to deep learning methods, which rely on purely data-driven approaches.\nHowever, while such advanced object recognition approaches handle rich online sensor data well, they fail to exploit another important source of information: knowledge-level context about the environment. In robotics, for instance, such information is typically available to any autonomous system, especially those with a plan-based control layer. Specifically, this includes knowledge about environmental areas that are currently occluded or have not been recently observed.\nIn parallel, as a way to structure and reason about such environmental knowledge,\n3D Semantic Scene Graphs (3DSSGs)\n[\nArmeni2019-ip\n]\nhave emerged in the past years as a symbolic abstraction of complex environments. Originally from the field of 2D image understanding\n[\nJohnson2015-mw\n]\n, their application to 3D data for scene graph prediction\n[\nWald2020-yj\n]\nand semantic mapping\n[\nHughes2022-fg\n,\nRosinol2020-wi\n]\nhas led to significant advances in the field of robotics. More recently, the combination of\n3DSSGs\nwith open set prediction models such as\nContrastive Language-Image Pretraining (CLIP)\n[\nRadford2021-mj\n]\nhas produced even more powerful representations\n[\nGu2024-bq\n,\nMaggio2024-ca\n]\n.\nHowever, the popular approaches to construct such an open set scene graph\n[\nGu2024-bq\n,\nMaggio2024-ca\n]\nstill only rely on the power of the underlying deep neural networks, without considering prior observations from a sensor data stream or the knowledge level.\nHence, a symbolic representation of persistent objects, such as\n3DSSGs\n, together with expectations about where to be able to perceive them and powerful object detection functionality in sensor data should enable an integrated approach combining knowledge-driven as well as data-driven methods. This is the approach that we sketch in this paper – connected to the ExPrIS project.\n1\n1\n1\nhttps://www.dfki.de/en/web/research/projects-and-publications/project/expris\nHere, the main research question is:\nWhat would be a fitting approach to processing expectations, derived from a knowledge level, as priors in modern scene graph generation methods?\n2\nSemantic Mapping with 3D Semantic Scene Graphs\nAs a foundational contribution of this project, we conducted a comprehensive survey on the state of the art in online knowledge integration for 3D semantic mapping\n[\nIgelbrink2024-gg\n]\n.\nThe survey identifies a clear trend towards structured, symbolic environment representations, with\n3DSSGs\nemerging as a particularly powerful and flexible paradigm.\nGuided by this analysis, the ExPrIS project adopts the\n3DSSG\nas its core data structure to systematically investigate the integration of knowledge-level expectations.\nFigure 1:\nThe ExPrIS/LIEREx semantic mapping architecture. This paper focuses on the ExPrIS and semantic map parts.\nThe ExPrIS project is complemented by its sister project, LIEREx (Language-Image Embeddings for Robotic Exploration); see Fig.\n1\n. While ExPrIS focuses on integrating structured knowledge to generate expectations, LIEREx extends this approach by incorporating modern vision-language models. This enables open-vocabulary queries and a more flexible, language-driven interaction with the environment. Both projects share the\n3DSSG\nas their foundational representation and are designed to be interoperable, combining symbolic knowledge (ExPrIS) with multimodal open-set perception (LIEREx).\nWe have based our semantic mapping framework on a dynamic\n3DSSG\n, which serves as the foundational environment representation for both ExPrIS and LIEREx. The general architecture of the\n3DSSG\nis inspired by the work of\nRosinol2020-wi\nas well as the Hydra framework\n[\nHughes2022-fg\n]\n.\nThe core component of the\n3DSSG\nis a heterogeneous graph organized in a hierarchy of multiple layers with different edge and node types. Each layer represents a different type of semantic concept ranging from low-level concepts (e. g., individual objects) to higher-level concepts (e. g., rooms and buildings). Besides per-layer edges to represent relationships between concepts of the same abstraction level, the\n3DSSG\nalso allows for edges between nodes in different layers to represent meta-relations, e. g., memberships. In Hydra and other preexisting works, the structure of layers is fixed to a rigid hierarchy of objects, places, rooms and buildings/floors which is suitable for mapping primarily simple organized indoor environments while allowing for scaling to larger environments consisting of many rooms in potentially multiple buildings as well\n[\nStrader2024-cv\n]\n. However, this rigid structure limits adaptability to new domains and prevents the integration of dynamically inferred higher-order semantic concepts required to fully utilize existing domain-specific knowledge, which usually possesses a different underlying ontology as well as fully utilizing additional implicit knowledge sources such as\nCLIP\nfeatures. Another method is creating the underlying graph structure based on distance heuristics, e. g., objects within a 0.5 m radius of each other’s bounding box center are connected by an edge in the scene graph\n[\nWald2020-yj\n]\n. This generalizes well and is easy to implement, but can create too densely connected subgraphs for scenes with many small objects in proximity to each other or fail to connect larger objects, if their size exceeds the distance threshold (0.5 m in this example).\nFor our approach, we introduce a more modular and extensible layer design that separates spatial from semantic relations and supports dynamic inference of composite concepts. This flexibility allows the graph to evolve beyond static categories and incorporate domain-specific or task-driven abstractions. Our\n3DSSG\nis designed to be constructed incrementally from sensor data, with layers being updated in a bottom-up fashion. The scene graph structure is derived by detecting physical contact between objects and building a hierarchical graph structure based on support relationships (see Fig.\n2\n).\nInstead of utilizing truncated-signed-distance-function (TSDF)-based meshes to represent geometry, we currently employ a combination of bounding volumes and sparse voxel grids to represent low-level object instances and higher-level entities in the\n3DSSG\n. While TSDF grids provide high-quality surface reconstruction of geometry from 3D sensor data, they incur significant computational and memory overhead, limiting their scalability. Additionally, TSDF structures do not integrate well with existing deep learning frameworks without requiring additional modules and workarounds. Since our focus is on semantic reasoning rather than dense reconstruction, our choice allows for efficient storage and manipulation of 3D shapes and additional fast GPU-accelerated processing through integration into PyTorch while maintaining sufficient fidelity to support downstream tasks such as object recognition, spatial inference, and view planning.\nThe\n3DSSG\nimplementation is designed to be modular and extensible, supporting future enhancements such as temporal reasoning, dynamic object tracking, and probabilistic inference as well as integration with other established frameworks such as the Hydra\n3DSSG\nmentioned earlier.\nIn future work, the currently separate heterogeneous\n3DSSG\ngeneration model described in Sec.\n4\nwill be fully integrated into this architecture, managing one or multiple layers of the\n3DSSG\nand enabling it to benefit from higher-order concepts and the knowledge encoded within them. Furthermore, in the LIEREx project, we plan on extending the hierarchical structure by introducing meta-nodes, which group semantically related objects. These meta-nodes can then be used as\nexpectation\n(see Sec.\n3\n) to assist scene graph prediction.\nFigure 2:\nHierarchical scene graph based on contact and support relationships.\n3\nKnowledge-Based Expectation Generation\nWhile mapping- or SLAM-based\n3DSSGs\ncan be used as a structured scene representation, they lack semantic relationships between objects. Such relationships (e. g.,\nattached to\n,\nstanding on\n,\nunder\n) can be modeled as edges between objects and deliver important semantic information for high-level planning. For this, we introduce an approach to incremental scene graph prediction, which utilizes\nexpectations\nduring both training and inference.\nWe define an expectation to be any kind of additional information that is available prior to an inference step and can influence the outcome of the respective inference. For this work, we differentiate between two kinds of expectations:\n1.\nContext: information about the environment in which the prediction is taking place, such as results from prior predictions, e. g., the scene graph constructed by predictions from previous frames\n2.\nKnowledge: static, general information about the domain, e. g., formulated as a knowledge graph\nFigure 3:\n3DSSG\nmodel for expectation integration\nFig.\n3\nshows the presented concept of expectation integration for\n3DSSG\nprediction from RGB-D sensor data. The goal is to incrementally predict a global\n3DSSG\nby accumulating local scene graphs predicted from single, segmented RGB-D frames. Nodes in this graph represent the segmented objects, and edges represent the semantic or spatial relationships between these objects. The presence of a potential edge is estimated using a simple distance threshold between object segments in the projected depth frame as heuristic (see Sec.\n2\n). Node and edge classes are then predicted using a\nGraph Neural Network (GNN)\n. In the current state, this global\n3DSSG\nis constructed separately for\nGNN\ntraining and not by using the mapping framework described in Sec.\n2\nbut will be integrated as described in Sec.\n5\n.\nThe current state of this accumulated, global\n3DSSG\nserves as the first type of expectation to be integrated in each local graph prediction. It can be thought of as a context or prior observations, assisting the prediction. The integration occurs by matching segments from the local frame to already observed geometric instances and connecting matching nodes with an edge (see Fig.\n2\n). This way, information from the global graph can traverse into the local graph by the\nGNN\n’s message passing process.\nUsing the same principle, the global nodes can be connected to a static knowledge graph using the predicted node classes. This knowledge layer can be interpreted as external prior knowledge about the environment, the second type of expectations.\nTo this end, we chose the common-sense knowledge graph ConceptNet\n[\nSpeer2017-os\n]\nas a source for prior knowledge. Common indoor 3D datasets usually consist of everyday objects and scenes, making common-sense knowledge graphs a suitable source for additional knowledge. However, ConceptNet contains a lot of noise by e. g., numerous nodes and relations from linguistic contexts (e. g., metaphors or abstract concepts) which are not useful for physical scene understanding and\n3DSSG\ngeneration. This and the sheer size of ConceptNet requires the extraction of subgraphs based on the desired nodes and relationships.\nThe pipeline for extracting subgraphs from ConceptNet consists of a breadth-first search based on a set of node classes and specified relationship types for n hops. Furthermore, various graph embedding algorithms have been implemented to generate additional feature vectors for the knowledge graph required for the message passing process, providing alternatives to the pre-computed Numberbatch\n[\nSpeer2017-os\n]\ngraph embeddings already provided by ConceptNet.\n4\nExpectation-Biased Machine Learning\nBuilding on the concept of expectations introduced in Sec.\n3\n, we embed prior observations and semantic knowledge directly into the learning process for incremental 3D scene graph prediction. The objective is to move beyond static, frame-by-frame perception and enable context-aware, expectation-biased inference that improves robustness and semantic consistency over time.\nFor incremental prediction, the model processes individual RGB-D frames sequentially, constructing partial local graphs from individual observations in the frame and integrating them into a growing global graph. This setup reflects real-world robotic perception tasks, where complete scene reconstructions are rarely available upfront.\nWhile Sec.\n3\nintroduced the principle of linking local observations to a global context, this section focuses on how this linkage is exploited during learning. Instead of treating the global graph as a passive memory, we integrate it as an active component in the message-passing process of a\nGNN\n. The global layer aggregates historical observations and semantic priors, while the local layer represents the current frame. Cross-layer edges allow for information flow between the layers, enabling the model to refine predictions for partially observed objects and relationships by leveraging accumulated context. This design transforms the global graph from a static expectation source into a dynamic biasing mechanism during inference.\nGlobal nodes are enriched with prior-based features, such as previously predicted class labels or\nCLIP\nembeddings\n[\nRadford2021-mj\n]\n. Unlike earlier approaches that encode global context as fixed vectors, our method integrates these priors directly into the heterogeneous graph structure, ensuring that expectations adaptively influence both node and edge classification.\nThe architecture employs a heterogeneous\nGNN\n(GraphSAGE\n[\nHamilton2017-nc\n]\nor HGT\n[\nHu2020-gs\n]\n) capable of handling multiple node and edge types. Node features combine PointNet-based\n[\nQi2017-wl\n]\ngeometric feature vectors with additional handcrafted descriptors, while edge features capture relative spatial configurations. The training objective is a composite loss over node and edge predictions in the local graph, scaled to account for class imbalance. Importantly, only local predictions contribute to the loss, ensuring that the global graph serves as a bias rather than a direct supervision signal. A more detailed overview of the presented architecture and evaluation can be found in our paper\n[\nRenz2025-uk\n]\n; code is available on GitHub.\n2\n2\n2\nhttps://github.com/m4renz/incremental-scene-graph-prediction\n5\nPlanned Integration and Evaluation on a Robot Platform\nTo bridge the gap between offline dataset analysis and real-world application, the final stage of the ExPrIS project is dedicated to integrating and evaluating the full pipeline on an embodied agent. This step is essential in order to validate the robustness and practical benefits of our expectation-biased framework in a dynamic environment subject to real-world sensor noise and unmodeled complexities.\nThe planned experiments will be conducted on the Mobipick\n[\nLima2023icaps\n]\nmobile manipulator (Fig.\n4\n). This platform combines a MiR100 mobile base with a UR5 robotic arm and a Robotiq gripper, providing capabilities for both navigation and object manipulation. Its end-effector-mounted RGB-D camera will serve as the primary sensor, delivering the rich data streams required for our perception pipeline.\nFigure 4:\nThe Mobipick robot\nThe Mobipick platform will be equipped with the complete software architecture presented in this paper. The system’s primary objective will be to incrementally build a\n3DSSG\n(Sec.\n2\n) of its environment in real time. The core of the evaluation will focus on the performance of the expectation-biased\nGNN\nmodel (Sec.\n4\n), which will process live sensor data, leveraging both contextual and semantic priors (Sec.\n3\n) to continuously refine its scene interpretation.\nTo test the system’s capabilities, we will employ a “tidy-up” scenario. In this task, the robot will first explore a lab environment to build a semantic map. Subsequently, it will be tasked with identifying and relocating objects that are “out of place”, requiring it to reason about both the current and the correct semantic context of objects. This scenario is designed to rigorously test the core value of ExPrIS: improving semantic consistency and robustness in perception over longer-term robot operation.\n6\nDiscussion and Outlook\nThrough the real-world experiments proposed in the previous section, we hypothesize that the expectation-biased approach will demonstrate clear advantages over context-free, frame-by-frame perception models. We expect to observe a significant improvement in semantic consistency, where the system’s understanding of objects remains stable and coherent over time, even in the face of partial observations or perceptual ambiguities.\nThe “tidy-up” scenario is specifically designed to test whether the robot can resolve such ambiguities by leveraging its memory of past observations—a key feature enabled by the global graph structure.\nLooking beyond this immediate validation, future work will extend the incremental scene graph prediction architecture in several key directions. We plan to more deeply incorporate external knowledge graphs (as introduced in Sec.\n3\n), moving beyond their use as simple priors towards enabling more complex, symbolic knowledge processing. This will be critical for our long-term goals of large-scale semantic mapping and\nExplainable AI (XAI)\n. In parallel to the core prediction task, we have already begun investigating methods for graph\nXAI\nto assess the effect and extent of the knowledge graph’s influence on local scene graph predictions. A comprehensive evaluation of the embodied system, including the full integration of the open-vocabulary query capabilities from the sister project LIEREx, is currently underway, and the results will be the subject of a forthcoming publication.\n\\bmhead\nAcknowledgements\nThis work is supported by the ExPrIS project through a grant from the German Federal Ministry of Research, Technology and Space (BMFTR) with Grant Number 16IW23001.\nThe DFKI Niedersachsen (DFKI NI) is sponsored by the Ministry of Science and Culture of Lower Saxony and the Volkswagen Stiftung.\nDeclarations\nThe authors have no competing interests to declare that are relevant to the content of this article.\nReferences",
    "preview_text": "While deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-level expectations can serve as to improve object interpretation from sensor data. Our approach is based on the incremental construction of a 3D Semantic Scene Graph (3DSSG). We integrate expectations from two sources: contextual priors from past observations and semantic knowledge from external graphs like ConceptNet. These are embedded into a heterogeneous Graph Neural Network (GNN) to create an expectation-biased inference process. This method moves beyond static, frame-by-frame analysis to enhance the robustness and consistency of scene understanding over time. The report details this architecture, its evaluation, and outlines its planned integration on a mobile robotic platform.\n\n[1,2]\n\\fnm\nMarian\n\\sur\nRenz\n1]\n\\orgname\nGerman Research Center for Artificial Intelligence,\n\\orgdiv\nResearch Department Cooperative and Autonomous Systems,\n\\orgaddress\n\\city\nOsnabrück,\n\\country\nGermany\n2]\n\\orgname\nOsnabrück University, Institute of Computer Science,\n\\orgdiv\nSemantic Information Systems Group,\n\\orgaddress\n\\city\nOsnabrück,\n\\country\nGermany\nExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data\nmarian.renz@dfki.de\n\\fnm\nMartin\n\\sur\nGünther\nmartin.guenther@dfki.de\n\\fnm\nFelix\n\\sur\nIgelbrink\nfelix.igelbrink@dfki.de\n\\fnm\nOscar\n\\sur\nLima\noscar.lima@dfki.de\n\\fnm\nMartin\n\\sur\nAtzmueller\nmartin.atzmueller@uos.de\n[\n[\nAbstract\nWhile deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "robotic object recognition",
        "3D Semantic Scene Graph",
        "Graph Neural Network",
        "semantic consistency",
        "knowledge-level expectations"
    ],
    "one_line_summary": "该论文提出ExPrIS项目，通过整合上下文先验和外部语义知识到图神经网络中，以知识级期望作为先验来改进机器人从传感器数据中解释对象的方法。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T14:27:38Z",
    "created_at": "2026-01-27T15:53:21.652528",
    "updated_at": "2026-01-27T15:53:21.652534"
}