{
  "id": "2601.10208v1",
  "title": "Terrain-Adaptive Mobile 3D Printing with Hierarchical Control",
  "authors": [
    "Shuangshan Nors Li",
    "J. Nathan Kutz"
  ],
  "abstract": "Mobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.",
  "url": "https://arxiv.org/abs/2601.10208v1",
  "html_url": "https://arxiv.org/html/2601.10208v1",
  "html_content": "Terrain-Adaptive Mobile 3D Printing with Hierarchical Control\nShuangshan Nors Li\n1\nand J. Nathan Kutz\n1,2\nAbstract\nMobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.\nKeywords -\nMobile 3D printing; AI-hardware integration; Hierarchical control; Terrain adaptation; Sensor fusion\n1\nIntroduction\nAutomated construction has long been a human aspiration\n[\n14\n,\n2\n]\n. It promises not only to accelerate labor-intensive tasks, but also to protect workers from danger by enabling construction in hazardous environments. Such capability is critical for establishing infrastructure in places that are otherwise inaccessible, including disaster recovery sites, remote locations, and challenging terrains. Existing robotic systems have begun to address parts of this challenge. Gantry-based systems, such as contour crafting and large-scale 3D printing\n[\n11\n,\n6\n]\n, have demonstrated the ability to fabricate buildings with high geometric precision. However, significant challenges remain in material properties, geometric conformity, and process robustness\n[\n3\n]\n. Similarly, fixed robotic arms deployed on-site\n[\n5\n]\nprovide flexible control and accurate material deposition. However, both approaches face fundamental limitations: gantry systems are immobile and require extensive setup, while fixed arms are restricted by their limited workspace. Both methods are therefore infrastructure-dependent and face challenges when applied to rapid deployment in unstructured environments.\nMobile robotic platforms offer a promising alternative by combining locomotion with manipulation capabilities\n[\n9\n,\n1\n]\n. However, integrating high-fidelity construction with platform mobility remains a significant challenge\n[\n4\n]\n: ground irregularities induce vibrations and pose disturbances that compromise print quality, while the dynamic nature of mobile bases conflicts with the precision requirements of material deposition.\nHere we present a hierarchical control framework that addresses these challenges by integrating data-driven learning with precision hardware control (Figure\n1\n). The core idea is to leverage predictive models to learn environmental dynamics from sensor data and embed the learned knowledge into the hardware control loop, enabling intelligent real-time responses. Specifically, we developed a disturbance prediction module that learns terrain-to-perturbation mappings from multi-source sensors, predicting terrain-induced disturbances ahead of time to enable proactive compensation rather than reactive correction.\nThe main contributions of this work are:\nâ€¢\nA three-layer hierarchical control architecture that separates path planning, predictive control, and hardware execution across appropriate time scales;\nâ€¢\nIntegration of data-driven disturbance prediction that enables proactive compensation for terrain-induced perturbations;\nâ€¢\nExperimental validation demonstrating sub-centimeter printing accuracy on uneven terrain with full platform mobility.\nFigure 1\n:\nOverview of terrain-adaptive mobile 3D printing. The mobile robot adapts to uneven ground and prints the initial layer (Step 1), adjusts layer height to compensate for terrain undulation (Step 2), levels the base layer to ensure structural stability (Step 3), and achieves high-speed printing with real-time terrain adaptation (Step 4).\n2\nSystem framework\nBy integrating data-driven models with real-time sensor feedback and precision hardware control, we developed a terrain-adaptive printing framework that achieves both mobility and accuracy simultaneously (Figure\n2\n). The core philosophy is to enable the system to continuously learn environmental dynamics from sensor data and embed the learned predictive capabilities into the hardware control loop.\nFigure 2\n:\nHierarchical control framework for terrain-adaptive mobile 3D printing. The system integrates adaptive path planning (left) with a perception-control loop (bottom). Sensor feedback from IMUs and vision-based tracking is processed through the disturbance prediction module, which enables feature selection for proactive compensation. The control policy generates real-time strategies for robotic actuation, enabling the mobile platform to maintain printing accuracy on unstructured terrain (right).\n2.1\nData-driven disturbance prediction\nThe disturbance prediction module is the intelligent core of the framework. It learns terrain-to-perturbation mappings from multi-source sensors including IMU for chassis pose, RGB camera for end-effector tracking, and depth camera for terrain profiling. The module predicts 6-DOF disturbances\nğ’…\n^\nt\n+\nÎ”\nâ€‹\nt\nâˆˆ\nâ„\n6\n\\hat{\\bm{d}}_{t+\\Delta t}\\in\\mathbb{R}^{6}\n(position and orientation perturbations) over a prediction horizon\nÎ”\nâ€‹\nt\n=\n0.5\n\\Delta t=0.5\ns:\nğ’…\n^\nt\n+\nÎ”\nâ€‹\nt\n=\ng\nâ€‹\n(\nğ’›\nt\n;\nğœ½\n)\n\\hat{\\bm{d}}_{t+\\Delta t}=g(\\bm{z}_{t};\\bm{\\theta})\n(1)\nwhere\nğ’›\nt\nâˆˆ\nâ„\nd\n\\bm{z}_{t}\\in\\mathbb{R}^{d}\nrepresents fused sensor features (\nd\nd\nranges from 12 to 20 depending on sensor configuration) and\nğœ½\n\\bm{\\theta}\ndenotes learned model parameters. The input vector aggregates multi-modal information including IMU-derived linear acceleration and angular velocity, estimated base orientation, depth-derived terrain descriptors (local height variation and slope estimates), and end-effector pose relative to the mobile base. Raw images are not used directly; instead, compact geometric descriptors are extracted to enable real-time inference.\nSparse regression techniques\n[\n7\n]\nare applied for feature selection, identifying the most predictive sensor channels for disturbance estimation. A lightweight feedforward network (two hidden layers with 128 and 64 units) processes the fused features and achieves inference latency of approximately 12 ms, enabling real-time proactive compensation rather than reactive correction\n[\n12\n]\n.\nThe prediction module was trained on approximately 7 hours of operation data (4 hours from simulation, 3 hours from outdoor experiments) collected across four terrain types: flat concrete, grass-covered soil, loose gravel, and mixed terrain with slopes. Data from both sources were combined directly for training; the modelâ€™s ability to generalize across these domains was validated through the outdoor experimental results. The dataset comprises approximately 1,400 trajectory segments totaling over 200 meters of traversed path, sampled at 50 Hz. Training and validation sets were split 80/20 at the trajectory level to avoid temporal correlation. Cross-time generalization was validated by resuming a printing task after a 24-hour interruption; the model maintained consistent prediction performance, demonstrating robustness to temporal discontinuities encountered in practical construction scenarios.\n2.2\nThree-layer hierarchical architecture\nThis prediction module is deeply integrated with the hierarchical hardware control architecture, forming a complete perception-learning-actuation closed loop:\nUpper layer (0.1 Hz)\nhandles adaptive path segmentation and velocity planning, transforming time-based printing trajectories into path-parameterized representations for speed-independent precision control. The path parameterization maps the desired trajectory\nğ’‘\nâ€‹\n(\nt\n)\n\\bm{p}(t)\nto a parameter-based form:\nğ’‘\nâ€‹\n(\ns\n)\n=\n(\n1\nâˆ’\ns\n)\nâ€‹\nğ’‘\n0\n+\ns\nâ€‹\nğ’‘\nf\n,\ns\nâˆˆ\n[\n0\n,\n1\n]\n\\bm{p}(s)=(1-s)\\bm{p}_{0}+s\\bm{p}_{f},\\quad s\\in[0,1]\n(2)\nwhere\nğ’‘\n0\n\\bm{p}_{0}\nand\nğ’‘\nf\n\\bm{p}_{f}\ndenote initial and final positions, enabling velocity-independent trajectory tracking. Complex trajectories are decomposed into piecewise-linear segments at this layer. This layer dynamically adjusts path complexity based on geometric features and terrain conditions, ensuring smooth transitions between printing segments.\nMiddle layer (10 Hz)\nimplements preview model predictive control (MPC), receiving disturbance predictions and coordinating chassis-manipulator motion\n[\n8\n,\n13\n]\n. At each control cycle, the MPC solves:\nmin\nğ’–\nâ€‹\nâˆ‘\nk\n=\n0\nN\n(\nâ€–\nğ’†\nk\nâ€–\nğ‘¸\n2\n+\nâ€–\nÎ”\nâ€‹\nğ’–\nk\nâ€–\nğ‘¹\n2\n)\n\\min_{\\bm{u}}\\sum_{k=0}^{N}\\left(\\|\\bm{e}_{k}\\|^{2}_{\\bm{Q}}+\\|\\Delta\\bm{u}_{k}\\|^{2}_{\\bm{R}}\\right)\n(3)\nsubject to system dynamics\nğ’™\nk\n+\n1\n=\nf\nâ€‹\n(\nğ’™\nk\n,\nğ’–\nk\n,\nğ’…\n^\nk\n)\n\\bm{x}_{k+1}=f(\\bm{x}_{k},\\bm{u}_{k},\\hat{\\bm{d}}_{k})\nand operational constraints (joint limits, velocity bounds), where\nğ’†\nk\n\\bm{e}_{k}\nis tracking error,\nğ’–\nk\n\\bm{u}_{k}\nis control input, and\nğ’…\n^\nk\n\\hat{\\bm{d}}_{k}\nis the predicted disturbance from Eq.Â (\n1\n). The key innovation is a frequency-decomposition coordination strategy: the chassis handles low-frequency (\n<\n<\n1 Hz), large-amplitude motions for terrain following, while the manipulator provides high-frequency (\n>\n>\n5 Hz) precision corrections for accurate deposition.\nLower layer (100 Hz)\nprovides precision hardware execution, controlling chassis steering, wheel speeds, and joint torques. High-frequency operation ensures real-time execution of MPC commands, while sensor data is fed back to the prediction module for continuous learning and model updates.\nThis hierarchical architecture enables the system to develop adaptive learning capabilities: as the robot operates across different terrains, the prediction module continuously accumulates experience and improves prediction accuracy.\n3\nValidation\n3.1\nSimulation validation\nTo validate the hierarchical control framework before physical deployment, we conducted dynamics simulations in MATLAB/Simulink. The simulation environment modeled a 10-meter trajectory comprising a 5-meter flat section followed by a 5-meter slope at 5Â° inclination (Figure\n3\n). To emulate terrain-induced disturbances, Gaussian noise with\nÂ±\n\\pm\n5 mm amplitude was injected into the wheel transformation matrices (4\nÃ—\n\\times\n4 homogeneous transforms containing orientation and position). While real terrain disturbances exhibit spatially correlated, non-Gaussian characteristics, the simulation provides a controlled environment to verify the MPCâ€™s disturbance rejection capability; the outdoor experiments subsequently confirm generalization to realistic disturbance patterns.\nFigure 3\n:\nSimulink simulation environment showing the mobile robot traversing a slope terrain. The trajectory includes a 5-meter flat section followed by a 5-meter slope at 5Â° inclination.\nThe MPC controller operated with a prediction horizon of 10 steps and control horizon of 5 steps, running at the middle control layer (10 Hz). Simulations were executed with a fixed-step solver at 1 ms resolution over a 30-second trajectory, representing continuous mobile printing operation.\nFigure 4\n:\nSimulated end-effector position error over a 30-second trajectory. Despite continuous terrain disturbances from slope transition (dashed line) and injected noise, the MPC-based compensation maintains position errors below 5 mm in all three axes.\nFigure\n4\nshows the end-effector position error throughout the simulated trajectory. Despite continuous terrain disturbances from both the slope transition and injected noise, the MPC-based compensation maintained position errors below 5 mm in all three axes. The error remained bounded without accumulation, validating the effectiveness of the predictive control strategy before field deployment.\n3.2\nExperimental setup\nTo further validate the terrain-adaptive capabilities on physical hardware, we conducted outdoor experiments on diverse terrain conditions. The experiments evaluated the systemâ€™s ability to maintain printing precision despite ground irregularities.\nThe experimental platform consisted of a four-wheel drive mobile base (1.2 m\nÃ—\n\\times\n0.8 m, 150 kg payload capacity) equipped with a 6-DOF robotic arm (1.5 m reach, 10 kg payload), as shown in Figure\n5\n. Tests were conducted on terrain featuring slopes, surface roughness, and mixed surface types including grass, gravel, and concrete with irregularities. These conditions represent realistic challenges encountered in disaster recovery sites and undeveloped construction areas.\nFigure 5\n:\nExperimental mobile 3D printing platform. Left: initial prototype with material hopper and extrusion system. Right: integrated system with 6-DOF robotic arm for terrain-adaptive printing.\nSensing systems include: IMU (100 Hz) for chassis state estimation, joint encoders (1000 Hz) and force/torque sensors (500 Hz) for manipulator control, stereo vision (30 Hz) for end-effector tracking, and RGB-D camera (30 Hz) for terrain mapping.\n3.3\nPerformance results\nIn the primary validation experiment, the mobile robotic platform constructed a concrete foundation structure on uneven ground. Although limited in physical size, this structure represents a fundamental construction primitive that can be repeatedly deployed to form larger foundations or continuous wall segmentsâ€“in practice, large-scale construction can be decomposed into such locally adaptive printing units. Despite challenging ground conditions, the hierarchical control framework maintained fabrication accuracy within sub-centimeter level across the entire build, as confirmed by LiDAR scanning.\nThe disturbance prediction module proved critical for maintaining accuracy. By learning terrain-to-perturbation mappings, the system achieved effective disturbance rejection across different terrain types. Table\n1\nsummarizes end-effector tracking accuracy across different terrain conditions.\nTable 1\n:\nEnd-effector tracking accuracy during printing on different terrain types\nTerrain\nHeight Dev.\n(mm)\nMisalign.\n(\nâˆ˜\n)\nMax Dev.\n(mm)\nFlat concrete\n2.1\nÂ±\n\\pm\n0.4\n0.08\nÂ±\n\\pm\n0.02\n3.8\nGrass (soft)\n4.3\nÂ±\n\\pm\n1.2\n0.21\nÂ±\n\\pm\n0.08\n8.7\nGravel\n5.1\nÂ±\n\\pm\n1.5\n0.25\nÂ±\n\\pm\n0.09\n9.2\nSlope (5\nâˆ˜\n)\n3.8\nÂ±\n\\pm\n0.9\n0.18\nÂ±\n\\pm\n0.06\n6.5\nMixed\n4.7\nÂ±\n\\pm\n1.3\n0.22\nÂ±\n\\pm\n0.07\n8.9\nTable\n2\npresents additional system performance metrics, demonstrating real-time capability and rapid disturbance recovery.\nTable 2\n:\nSystem performance metrics\nMetric\nValue\nMPC computation time\n8.3\nÂ±\n\\pm\n2.1 ms\nPrediction latency\n12.5\nÂ±\n\\pm\n3.2 ms\nSettling time\n0.35\nÂ±\n\\pm\n0.12 s\nLayer height consistency\nÂ±\n\\pm\n0.8 mm\nThe final structure exhibited consistent layer stacking and mechanical stability. Across all terrain types, the system maintained sub-centimeter accuracy (mean position error\n<\n<\n5.1 mm), validating that the proposed framework enables mobile platforms to achieve precision approaching stationary gantry systems while operating on challenging terrain.\n4\nDiscussion and conclusion\nThis study presents a hierarchical control framework that addresses the fundamental challenge of maintaining deposition precision on mobile platforms operating over unstructured terrain. Unlike existing approaches that either sacrifice mobility for accuracy or accuracy for mobility, our three-layer architecture achieves both through temporal separation of control concerns and predictive disturbance compensation.\nThe key technical contributions include: (1) a three-layer hierarchical control architecture that separates path planning, predictive control, and hardware execution across appropriate time scales; (2) integration of data-driven disturbance prediction that enables proactive compensation for terrain-induced perturbations; and (3) dual-layer chassis-manipulator coordination through frequency decomposition.\n4.1\nFailure modes and mitigation\nMobile construction systems operating on unstructured terrain face several characteristic failure modes. Understanding these failure modes and how the proposed framework addresses them is essential for assessing deployment readiness.\nTerrain perception degradation\noccurs when environmental conditions compromise sensor accuracy. Varying lighting affects RGB-based end-effector tracking, while surface reflectivity and dust interfere with depth measurements. The multi-modal sensor fusion approach mitigates this by combining IMU, vision, and depth data, allowing the prediction module to maintain reasonable estimates even when individual sensors degrade. The learned terrain-to-perturbation mappings provide robustness by encoding statistical regularities rather than relying on instantaneous measurements alone.\nUnlike laboratory robotic systems where disturbances are isolated events, construction sites exhibit persistent environmental uncertainty: airborne dust from excavation activities, rapidly changing shadows from moving equipment, and surface glare from wet concrete or reflective materials. These conditions demand sensor fusion strategies that degrade gracefully rather than fail catastrophically. The hierarchical architecture addresses this by allowing the prediction module to operate with reduced feature sets when specific sensors become unreliable, falling back to IMU-dominated estimation during severe visual degradation while maintaining acceptableâ€”though reducedâ€”compensation performance.\nWheel slip and chassis compliance\nintroduce discrepancies between commanded and actual platform motion, particularly on soft or loose surfaces. The frequency-decomposition coordination strategy addresses this by assigning terrain-following to the low-frequency chassis control loop while reserving the high-frequency manipulator for precision corrections. This separation ensures that chassis-level disturbances do not directly propagate to the deposition point.\nLong-horizon drift\nposes a risk for extended construction tasks where small errors could accumulate over time. The experimental results provide direct evidence against this concern: over a 5-hour continuous printing session comprising approximately 140 layers, no systematic error accumulation was observed. This stability arises from the closed-loop architecture where sensor feedback continuously updates the prediction module, preventing drift from compounding across layers.\nMaterial rheology and extrusion dynamics can further influence deposition quality; however, this work focuses on terrain-induced kinematic disturbances, and material-process interactions will be addressed in future studies.\n4.2\nScalability implications\nThe dominant barriers to scaling mobile construction systems are typically long-duration stability and site-to-site variability, rather than geometric size alone. The proposed framework addresses both concerns through its hierarchical design.\nFor duration scaling, the demonstrated 5-hour operation without error accumulation suggests that the architecture handles the primary failure modes that typically prevent mobile systems from sustaining construction-time-scale operations. The locally adaptive nature of the control frameworkâ€”where each printing segment is treated as a path-parameterized unit with real-time disturbance compensationâ€”means that scaling to larger structures involves repeating proven primitives rather than extrapolating untested behaviors.\nFor multi-robot scaling, the modular architecture provides a natural extension path. Each robot can operate its own perception-prediction-control loop while coordinating at the path-planning level. Recent work on aerial multi-robot construction\n[\n15\n]\nand collaborative mobile printing\n[\n10\n]\ndemonstrates the feasibility of such coordination, and the hierarchical separation in our framework aligns well with distributed control paradigms.\nFrom a practical deployment perspective, the system requires a minimum sensor configuration of one IMU and either stereo vision or an RGB-D camera for basic operation; the full sensor suite enhances performance but is not strictly necessary for all terrain types. The framework assumes locally continuous terrain variations with spatial frequencies below approximately 0.5 cycles per meterâ€”corresponding to undulations with wavelengths greater than 2 meters. Extremely discontinuous obstacles such as steps, curbs, or debris piles exceeding the manipulatorâ€™s compensation range (approximately 15 cm vertical displacement) remain outside the current operational envelope and require path replanning at the upper control layer.\nThe construction workflow integration follows a modular primitive approach: rather than attempting continuous printing of entire structures, the system treats each foundation segment, wall section, or structural element as an independent printing unit with well-defined start and end conditions. This decomposition aligns naturally with construction sequencing practices, where curing time, material logistics, and inspection checkpoints create natural task boundaries. The demonstrated 5-hour operation capability corresponds to typical work shifts, suggesting compatibility with existing site management practices.\nSeveral limitations remain before widespread deployment. The current system depends on accurate terrain mapping for optimal disturbance prediction, which may be challenging in rapidly changing environments such as active construction sites with concurrent human activity. Additionally, extending the framework to different construction materials will require characterizing material-specific disturbance responses. A systematic comparison with reactive-only control would further quantify the predictive moduleâ€™s contribution.\nLooking ahead, the practical implications of this work extend to disaster recovery, remote infrastructure construction, and eventually to extraterrestrial construction where autonomous operation on irregular surfaces is essential. By demonstrating that precision and mobility can coexist in construction robotics, this framework establishes a foundation for autonomous building in environments previously inaccessible to conventional methods.\nReferences\n[1]\nA. Alhijaily, A. Alqarni, Z. M. Kilic, and P. Bartolo\n(2025)\nDevelopment of a mobile 3d printer and comparative evaluation against traditional gantry systems\n.\nJournal of Intelligent Manufacturing\n36\n(\n6\n),\npp.Â 3783â€“3800\n.\nCited by:\nÂ§1\n.\n[2]\nR. Bogue\n(2017)\nWhat are the prospects for robots in the construction industry?\n.\nIndustrial Robot: An International Journal\n45\n(\n1\n),\npp.Â 1â€“6\n.\nCited by:\nÂ§1\n.\n[3]\nR. A. Buswell, W. L. De Silva, S. Z. Jones, and J. Dirrenberger\n(2018)\n3D printing using concrete extrusion: a roadmap for research\n.\nCement and concrete research\n112\n,\npp.Â 37â€“49\n.\nCited by:\nÂ§1\n.\n[4]\nK. DÃ¶rfler, G. Dielemans, S. Leutenegger, S. E. Jenny, J. Pankert, J. Sustarevas, L. Lachmayer, A. Raatz, and D. Lowke\n(2024)\nAdvancing construction in existing contexts: prospects and barriers of 3d printing with mobile robots for building maintenance and repair\n.\nCement and Concrete Research\n186\n,\npp.Â 107656\n.\nCited by:\nÂ§1\n.\n[5]\nM. Giftthaler, T. Sandy, K. DÃ¶rfler, I. Brooks, M. Buckingham, G. Rey, M. Kohler, F. Gramazio, and J. Buchli\n(2017)\nMobile robotic fabrication at 1: 1 scale: the in situ fabricator: system, experiences and current developments\n.\nConstruction Robotics\n1\n(\n1\n),\npp.Â 3â€“14\n.\nCited by:\nÂ§1\n.\n[6]\nC. Gosselin, R. Duballet, P. Roux, N. GaudilliÃ¨re, J. Dirrenberger, and P. Morel\n(2016)\nLarge-scale 3d printing of ultra-high performance concreteâ€“a new processing route for architects and builders\n.\nMaterials & Design\n100\n,\npp.Â 102â€“109\n.\nCited by:\nÂ§1\n.\n[7]\nE. Kaiser, J. N. Kutz, and S. L. Brunton\n(2018)\nSparse identification of nonlinear dynamics for model predictive control in the low-data limit\n.\nProceedings of the Royal Society A\n474\n(\n2219\n),\npp.Â 20180335\n.\nCited by:\nÂ§2.1\n.\n[8]\nS. Katayama, M. Murooka, and Y. Tazaki\n(2023)\nModel predictive control of legged and humanoid robots: models and algorithms\n.\nAdvanced Robotics\n37\n(\n5\n),\npp.Â 298â€“315\n.\nCited by:\nÂ§2.2\n.\n[9]\nS. J. Keating, J. C. Leland, L. Cai, and N. Oxman\n(2017)\nToward site-specific and self-sufficient robotic fabrication on architectural scales\n.\nScience robotics\n2\n(\n5\n),\npp.Â eaam8986\n.\nCited by:\nÂ§1\n.\n[10]\nS. Li, T. Lan, H. Nguyen, and P. Tran\n(2025)\nFrontiers in construction 3d printing: self-monitoring, multi-robot, drone-assisted processes\n.\nProgress in Additive Manufacturing\n10\n(\n4\n),\npp.Â 2001â€“2030\n.\nCited by:\nÂ§4.2\n.\n[11]\nS. Lim, R. A. Buswell, T. T. Le, S. A. Austin, A. G. Gibb, and T. Thorpe\n(2012)\nDevelopments in construction-scale additive manufacturing processes\n.\nAutomation in construction\n21\n,\npp.Â 262â€“268\n.\nCited by:\nÂ§1\n.\n[12]\nJ. M. Silva, G. Wagner, R. Silva, A. Morais, J. Ribeiro, S. Mould, B. Figueiredo, J. M. NÃ³brega, and P. J. Cruz\n(2024)\nReal-time precision in 3d concrete printing: controlling layer morphology via machine vision and learning algorithms\n.\nInventions\n9\n(\n4\n),\npp.Â 80\n.\nCited by:\nÂ§2.1\n.\n[13]\nM. Stamatopoulos, J. HaluÅ¡ka, E. Small, J. Marroush, A. Banerjee, and G. Nikolakopoulos\n(2025)\nFully autonomous chunk-based aerial additive manufacturing with offset-free predictive control\n.\nAutomation in Construction\n178\n,\npp.Â 106361\n.\nCited by:\nÂ§2.2\n.\n[14]\nK. You, C. Zhou, L. Ding, and Y. Wang\n(2025)\nConstruction robotics in extreme environments: from earth to space\n.\nEngineering\n.\nCited by:\nÂ§1\n.\n[15]\nK. Zhang, P. Chermprayong, F. Xiao, D. Tzoumanikas, B. Dams, S. Kay, B. B. Kocer, A. Burns, L. Orr, T. Alhinai,\net al.\n(2022)\nAerial additive manufacturing with multiple autonomous robots\n.\nNature\n609\n(\n7928\n),\npp.Â 709â€“717\n.\nCited by:\nÂ§4.2\n.",
  "preview_text": "Mobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.\n\nTerrain-Adaptive Mobile 3D Printing with Hierarchical Control\nShuangshan Nors Li\n1\nand J. Nathan Kutz\n1,2\nAbstract\nMobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and preci",
  "is_relevant": true,
  "relevance_score": 4.0,
  "extracted_keywords": [
    "locomotion",
    "whole body control"
  ],
  "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆAIé©±åŠ¨æ‰°åŠ¨é¢„æµ‹ä¸åˆ†å±‚ç¡¬ä»¶æ§åˆ¶çš„ç§»åŠ¨3Dæ‰“å°æ¡†æ¶ï¼Œç”¨äºåœ¨ä¸è§„åˆ™åœ°å½¢ä¸Šå®ç°è‡ªé€‚åº”æ‰“å°ï¼Œä¸»è¦æ¶‰åŠè¿åŠ¨æ§åˆ¶å’Œå…¨èº«åè°ƒæŠ€æœ¯ã€‚",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-15T09:15:06Z",
  "created_at": "2026-01-20T17:49:54.000037",
  "updated_at": "2026-01-20T17:49:54.000044"
}