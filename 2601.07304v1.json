{
    "id": "2601.07304v1",
    "title": "Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts",
    "authors": [
        "Yun Chen",
        "Bowei Huang",
        "Fan Guo",
        "Kang Song"
    ],
    "abstract": "åœ¨éç»“æ„åŒ–ä»“åº“ä¸­è¿›è¡Œè‡ªä¸»ç§»åŠ¨æ“ä½œï¼Œéœ€è¦åœ¨é«˜æ•ˆçš„å¤§èŒƒå›´å¯¼èˆªä¸é«˜ç²¾åº¦ç‰©ä½“äº¤äº’ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¼ ç»Ÿçš„ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹è¿™ä¸¤ä¸ªä¸åŒé˜¶æ®µçš„å†²çªæ€§éœ€æ±‚ï¼šå¯¼èˆªä¾èµ–äºå¤§ç©ºé—´å†…çš„é²æ£’å†³ç­–ï¼Œè€Œæ“ä½œåˆ™éœ€è¦å¯¹å±€éƒ¨ç²¾ç»†ç»†èŠ‚ä¿æŒé«˜åº¦æ•æ„Ÿã€‚å¼ºåˆ¶è®©å•ä¸€ç½‘ç»œåŒæ—¶å­¦ä¹ è¿™äº›ä¸åŒç›®æ ‡å¸¸å¯¼è‡´ä¼˜åŒ–å¹²æ‰°ï¼Œå³æå‡ä¸€ä¸ªä»»åŠ¡çš„æ€§èƒ½ä¼šæŸå®³å¦ä¸€ä¸ªä»»åŠ¡ã€‚ä¸ºçªç ´è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“ä¸ºè‡ªä¸»å‰è½¦è®¾è®¡çš„å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†é•¿æ—¶ç¨‹ä»»åŠ¡åˆ†è§£ä¸ºç”±è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨æ§åˆ¶çš„ä¸“ä¸šåŒ–å­ç­–ç•¥ï¼Œå®ç°äº†å®è§‚å¯¼èˆªä¸å¾®è§‚æ“ä½œçš„è§£è€¦ï¼Œä½¿æ¯ä¸ªä¸“å®¶èƒ½ä¸“æ³¨äºå…¶ç‰¹å®šåŠ¨ä½œç©ºé—´è€Œäº’ä¸å¹²æ‰°ã€‚è§„åˆ’å™¨åè°ƒè¿™äº›ä¸“å®¶çš„é¡ºåºæ‰§è¡Œï¼Œå¼¥åˆäº†ä»»åŠ¡è§„åˆ’ä¸è¿ç»­æ§åˆ¶ä¹‹é—´çš„é¸¿æ²Ÿã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³ç¨€ç–æ¢ç´¢é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆæ¨¡ä»¿-å¼ºåŒ–è®­ç»ƒç­–ç•¥ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¸“å®¶æ¼”ç¤ºåˆå§‹åŒ–ç­–ç•¥ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚åœ¨Gazeboä»¿çœŸç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼ŒHMERæ¡†æ¶æ˜¾è‘—ä¼˜äºåºåˆ—åŒ–åŸºçº¿æ–¹æ³•ä¸ç«¯åˆ°ç«¯åŸºçº¿æ–¹æ³•ï¼šä»»åŠ¡æˆåŠŸç‡æå‡è‡³94.2%ï¼ˆåŸºçº¿æ–¹æ³•ä¸º62.5%ï¼‰ï¼Œæ“ä½œæ—¶é—´å‡å°‘21.4%ï¼Œæ”¾ç½®è¯¯å·®ä¿æŒåœ¨1.5å˜ç±³ä»¥å†…ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ç²¾å¯†ç‰©æ–™æ¬è¿ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.07304v1",
    "html_url": "https://arxiv.org/html/2601.07304v1",
    "html_content": "\\useunder\n\\ul\nHeterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts\nYun Chen, Bowei Huang, Fan Guo, Kang Song\nThis work was supported in part by the National Key Research and Development Program of China under Grant 2022YFE0100100, the Tianjin Municipal Science and Technology Program under Grant 19ZXZNGX00050, the State Key Laborator of Engines at Tianjin University under Grant K2022-13 and the Research and Innovation Project for Postgraduates in Tianjin under Grant 2022BKYZ041. (Corresponding author: Kang Song.)\nYun Chen, Kang Song, Fan Guo and Bowei Huang are with the State Key Laboratory of Engines, Tianjin University, Tianjin 300072, China. (e-mail: cyun@tju.edu.cn; songkangtju@tju.edu.cn).\nAbstract\nAutonomous mobile manipulation in unstructured warehouses requires a balance between efficient large-scale navigation and high-precision object interaction. Traditional end-to-end learning approaches often struggle to handle the conflicting demands of these distinct phases. Navigation relies on robust decision-making over large spaces, while manipulation needs high sensitivity to fine local details. Forcing a single network to learn these different objectives simultaneously often causes optimization interference, where improving one task degrades the other. To address these limitations, we propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts. HMER decomposes long-horizon tasks into specialized sub-policies controlled by a Semantic Task Planner. This structure separates macro-level navigation from micro-level manipulation, allowing each expert to focus on its specific action space without interference. The planner coordinates the sequential execution of these experts, bridging the gap between task planning and continuous control. Furthermore, to solve the problem of sparse exploration, we introduce a Hybrid Imitation-Reinforcement Training Strategy. This method uses expert demonstrations to initialize the policy and Reinforcement Learning for fine-tuning. Experiments in Gazebo simulations show that HMER significantly outperforms sequential and end-to-end baselines. Our method achieves a task success rate of 94.2% (compared to 62.5% for baselines), reduces operation time by 21.4%, and maintains placement error within 1.5 cm, validating its efficacy for precise material handling.\nI\nIntroduction\nIn the rapidly evolving landscape of smart logistics, the demand for autonomous mobile manipulationâ€”particularly involving heavy-duty machinery like forkliftsâ€”is increasing rapidly. Unlike Automatic Guided Vehicles (AGVs) that adhere to rigid, pre-defined paths, autonomous forklifts in modern warehouses must operate in unstructured environments and execute complex long-horizon tasks. These missions typically involve a continuous cycle: navigating through dynamic obstacles, visually searching for cargo, picking it up, and stacking it with centimeter-level precision. This dual requirement poses a significant robotic control challenge: the system must simultaneously master large-scale spatial navigation and fine-grained object interaction, integrating sequential decision-making with complex vehicle dynamics.\nHowever, developing a robust control policy for such tasks remains an open problem due to three fundamental limitations in existing paradigms. First, traditional End-to-End Reinforcement Learning (RL) methods often succumb to optimization interference. Compelling a single network to simultaneously encode geometric navigation features and semantic manipulation cues results in gradient conflict, where the optimization of large-scale mobility dominates the learning of fine-grained interaction. Second, while sequential modular approaches mitigate this complexity by executing sub-tasks in series, they suffer from error propagation and irreversibility. Since these frameworks typically execute navigation and manipulation phases in isolation, minor state deviations accumulated during transit inevitably cascade into the manipulation phase. Lacking a high-level mechanism to dynamically revert states or re-plan, these systems cannot recover from such propagated errors, leading to task failure in unstructured environments. Third, although Imitation Learning (IL) offers a stable policy initialization, it is fundamentally restricted by the performance ceiling of the demonstrator. Pure supervised learning is confined to the distribution of the training data and precludes the discovery of superior control strategies through environmental interaction, thereby failing to achieve the sub-centimeter precision mandated by industrial operations.\nTo address these limitations, we propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts. Our core insight is to structurally decouple the complex long-horizon task into a hierarchy of specialized experts, thereby resolving the optimization interference problem. Specifically, we design a Navigation Expert that focuses on macro-level geometric robustness for path planning, and Manipulation Experts that focus on micro-level semantic details for precise interaction. These specialized skills are orchestrated by a high-level Semantic Task Planner, which acts as a closed-loop decision maker to bridge discrete task logic with continuous motor control.\nFurthermore, to solve the trade-off between exploration efficiency and control precision, we introduce a unified Hybrid Imitation-Reinforcement Training Strategy. Training long-horizon tasks from scratch is computationally inefficient due to sparse rewards. We first utilize expert demonstrations to warm-start all expert policies via Behavioral Cloning (BC), establishing a baseline competency. Subsequently, we employ Reinforcement Learning (RL) to fine-tune these policies end-to-end. This refinement phase is critical: it enables the manipulation agents to learn subtle, non-intuitive control maneuvers that surpass the heuristic expert, achieving the high precision required for real-world deployment.\nThe main contributions of this paper are summarized as follows:\n1. A Heterogeneous Multi-Expert Reinforcement Learning Framework (HMER): Distinct from traditional end-to-end learning paradigms, we propose a hierarchical framework that fundamentally addresses the optimization interference problem in autonomous mobile manipulation. By explicitly decoupling macro-level navigation from micro-level manipulation, our approach allows each expert to specialize in its specific action space without conflicting gradients, ensuring robust performance.\n2. Hierarchical Semantic Task Planning: We design a high-level Semantic Task Planner to orchestrate the sequential execution of heterogeneous experts. Unlike rigid sequential methods, this state-aware mechanism enables the autonomous forklift to adaptively switch between navigation and stacking phases and autonomously recover from propagated errors via closed-loop decision making.\n3. Unified Hybrid Imitation-Reinforcement Training Strategy: To overcome the exploration sparsity of Reinforcement Learning (RL) and the precision bottlenecks of Imitation Learning (IL), we introduce a hybrid training paradigm. This strategy leverages expert demonstrations to warm-start policy networks and subsequently employs RL for residual refinement. This enables the system to achieve 1.5 cm placement precision, significantly surpassing the capabilities of heuristic baselines and meeting strict industrial safety standards.\nThe remainder of this paper is organized as follows: Section II reviews related work in autonomous mobile manipulation and hierarchical reinforcement learning. Section III formally formulates the long-horizon multi-goal task as a Markov Decision Process (MDP). Section IV details the proposed HMER framework, elaborating on the Semantic Task Planner and the decoupled expert policies, followed by the hybrid training strategy. Section V describes the high-fidelity simulation environment and implementation details. Section VI presents extensive experimental results, comparative analyses against baselines, and ablation studies. Finally, Section VII concludes the paper and outlines future directions.\nII\nRelated work\nResearch on autonomous mobile manipulation has evolved significantly in recent years, broadly branching into three primary paradigms: classical planning, data-driven monolithic learning, and hierarchical approaches. In this section, we review the literature in these domains and position our work within the context of unstructured warehouse logistics.\nII-A\nTask and Motion Planning (TAMP)\nEarly attempts to solve mobile manipulation predominantly relied on classical Task and Motion Planning (TAMP). These approaches view navigation and manipulation as distinct geometric problems. TAMP methods\n[\n8\n,\n4\n]\nintegrate these by searching for a logically consistent sequence of symbolic actions coupled with continuous motion trajectories. To handle complex kinematic constraints, researchers have developed extensible interface layers\n[\n16\n]\nand optimization-based logic-geometric programming\n[\n19\n]\nto bridge symbolic planners with continuous solvers.\nWhile theoretically sound in structured settings, TAMP methods suffer from a critical limitation: reliance on\nâ€œprivileged information.â€\nThey typically assume access to pre-built high-definition maps\n[\n18\n]\nor precise 3D object models\n[\n15\n]\nfor collision checking. In the context of autonomous forklifts, this rigidity is problematic as warehouse layouts change dynamically with moving inventory. Unlike TAMP frameworks that require exact belief space estimation\n[\n9\n]\n, our approach relies purely on onboard sensors (LiDAR and RGB), allowing the agent to adapt to unstructured environments without perfect state estimation.\nII-B\nEnd-to-End and Imitation Learning\nTo overcome the dependency on manual modeling, researchers have turned to data-driven approaches. End-to-End Reinforcement Learning (RL) attempts to map raw sensor observations directly to control actions\n[\n11\n]\n. While successful in stationary manipulation\n[\n10\n]\n, these methods struggle when applied to long-horizon mobile manipulation\n[\n21\n]\n. A primary challenge is\noptimization interference\n: optimizing a single monolithic network to simultaneously handle LiDAR-based navigation and Vision-based manipulation often leads to conflicting gradients\n[\n22\n]\n, causing convergence failure or sub-optimal policies.\nAlternatively, Imitation Learning (IL) bypasses the exploration difficulty by mimicking expert demonstrations. Algorithms ranging from classical Behavior Cloning to Generative Adversarial Imitation Learning (GAIL)\n[\n5\n]\nand the recent Diffusion Policy\n[\n3\n]\nhave demonstrated impressive capabilities. However, IL suffers from two fundamental limitations in industrial settings: (1)\nCovariate Shift\n: slight deviations during execution lead to state distributions not covered in the training data, causing compounding errors\n[\n14\n]\n; and (2)\nPrecision Bottleneck\n: the policy is strictly upper-bounded by the quality of the dataset\n[\n12\n]\nand lacks an online correction mechanism to achieve the sub-centimeter accuracy required for stacking. To address these, our work employs a hybrid strategy: we use IL for stable initialization and RL for fine-tuning, enabling the agent to exceed expert precision via residual learning.\nII-C\nHierarchical Reinforcement Learning (HRL)\nTo address the temporal abstraction challenge, HRL decomposes complex decision-making into high-level planning and low-level skills\n[\n2\n]\n. Traditional approaches focus on sequencing pre-defined primitives, such as the Options framework\n[\n17\n]\nor FeUdal Networks\n[\n20\n]\n. More recently, researchers have leveraged Large Language Models (LLMs) to perform high-level planning, as seen in SayCan\n[\n1\n]\nand VoxPoser\n[\n6\n]\n, which ground linguistic commands into robotic affordances.\nHowever, a key gap remains in the\ncoordination of heterogeneous skills\n. Most existing HRL frameworks treat sub-policies as homogeneous blocks, ignoring the distinct sensory requirements of different phases (e.g., Geometry vs. Semantics). Furthermore, learning these hierarchies from scratch is notoriously sample-inefficient. While hybrid methods like DAPG\n[\n13\n]\nor Residual Policy Learning\n[\n7\n]\ncombine demonstrations with RL, they are rarely applied to hierarchically decoupled systems for mobile manipulation. Our work fills this gap by proposing the HMER framework. By explicitly decoupling sensor modalities for Navigation and Manipulation experts and co-optimizing them via a hybrid pipeline, we effectively solve the â€œcold startâ€ problem and achieve the centimeter-level precision required for autonomous forklifts.\nIII\nPROBLEM FORMULATION\nFigure 1:\nIllustration of the Long-Horizon Material Handling Task.\nThe operational lifecycle is decomposed into four sequential phases:\n(1)\nDeparture\nfrom the docking station\nğ’«\ns\nâ€‹\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\nt\n\\mathcal{P}_{start}\n;\n(2)\nSearch & Pick\nat the randomized cargo location\nğ’«\no\nâ€‹\nb\nâ€‹\nj\n\\mathcal{P}_{obj}\n;\n(3)\nTransport\nthrough dynamic obstacles; and\n(4)\nPrecision Placement\nat the target slot\nğ’«\ng\nâ€‹\no\nâ€‹\na\nâ€‹\nl\n\\mathcal{P}_{goal}\n.\nThe heterogeneous sensory inputs (LiDAR for navigation, RGB for manipulation) act as triggers for the state transitions.\nIII-A\nTask Definition\nWe formulate the autonomous mobile manipulation task as a long-horizon sequential decision-making problem within a stochastic warehouse environment. The operational lifecycle is decomposed into four distinct phases, as illustrated in Fig.\n1\n:\n1.\nDeparture:\nThe agent navigates from a docking station\nğ’«\ns\nâ€‹\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\nt\n\\mathcal{P}_{start}\nto a designated transfer zone.\n2.\nSearch & Pick:\nThe agent requires visual identification and precise alignment with a target cargo initialized at a stochastic pose\nğ’«\no\nâ€‹\nb\nâ€‹\nj\n\\mathcal{P}_{obj}\n.\n3.\nTransport:\nThe loaded forklift must navigate through dynamic obstacles under kinematic constraints.\n4.\nPrecision Placement:\nThe cargo must be delivered to a storage slot\nğ’«\ng\nâ€‹\no\nâ€‹\na\nâ€‹\nl\n\\mathcal{P}_{goal}\nwith sub-centimeter accuracy.\nThe episode terminates successfully upon satisfying the placement tolerance metric, or aborts upon collision or timeout.\nIII-B\nMDP Formulation\nFormally, we model this problem as a Markov Decision Process (MDP) represented by the tuple\nâ„³\n=\nâŸ¨\nğ’®\n,\nğ’œ\n,\nğ’«\n,\nâ„›\n,\nÎ³\nâŸ©\n\\mathcal{M}=\\langle\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma\\rangle\n.\nIII-B\n1\nState Space\nThe state space\nğ’®\n\\mathcal{S}\nis high-dimensional and heterogeneous, encapsulating the multi-modal sensory data required for different operational phases. We define the composite state vector at time\nt\nt\nas\ns\nt\n=\n[\ns\nl\nâ€‹\ni\nâ€‹\nd\nâ€‹\na\nâ€‹\nr\n,\nt\n,\ns\nr\nâ€‹\ng\nâ€‹\nb\n,\nt\n,\ns\ne\nâ€‹\ng\nâ€‹\no\n,\nt\n]\nT\ns_{t}=[s_{lidar,t},s_{rgb,t},s_{ego,t}]^{T}\n, where:\nâ€¢\ns\nl\nâ€‹\ni\nâ€‹\nd\nâ€‹\na\nâ€‹\nr\n,\nt\nâˆˆ\nâ„\nN\ns\nâ€‹\nc\nâ€‹\na\nâ€‹\nn\ns_{lidar,t}\\in\\mathbb{R}^{N_{scan}}\ndenotes the\ngeometric perception vector\nderived from 2D laser scans, encoding the spatial occupancy required for collision-free navigation.\nâ€¢\ns\nr\nâ€‹\ng\nâ€‹\nb\n,\nt\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\ns_{rgb,t}\\in\\mathbb{R}^{H\\times W\\times 3}\ndenotes the\nsemantic perception tensor\ncaptured by the onboard RGB camera, providing dense visual cues for object detection and fine-grained alignment.\nâ€¢\ns\ne\nâ€‹\ng\nâ€‹\no\n,\nt\nâˆˆ\nâ„\nN\np\nâ€‹\nr\nâ€‹\no\nâ€‹\np\ns_{ego,t}\\in\\mathbb{R}^{N_{prop}}\nrepresents the\nproprioceptive state vector\n, comprising the vehicleâ€™s kinematic status (linear velocity\nv\nt\nv_{t}\n, angular velocity\nÏ‰\nt\n\\omega_{t}\n), actuator status (fork height\nh\nt\nh_{t}\n), and relative goal coordinates.\nIII-B\n2\nAction Space\nThe action space\nğ’œ\nâŠ‚\nâ„\n4\n\\mathcal{A}\\subset\\mathbb{R}^{4}\ndefines the continuous control inputs for the non-holonomic mobile manipulator. An action\na\nt\n=\n[\nv\nt\n,\nÏ‰\nt\n,\nh\nË™\nt\n,\nÎ´\nt\n]\nT\na_{t}=[v_{t},\\omega_{t},\\dot{h}_{t},\\delta_{t}]^{T}\ncorresponds to the linear velocity, steering angular velocity, fork lifting speed, and a discrete clamping trigger, respectively.\nIII-B\n3\nOptimization Objective\nThe system transitions according to the dynamics\nğ’«\nâ€‹\n(\ns\nt\n+\n1\n|\ns\nt\n,\na\nt\n)\n\\mathcal{P}(s_{t+1}|s_{t},a_{t})\n, which implicitly models vehicle kinematics and contact physics. The objective is to learn a policy\nÏ€\nÎ¸\nâ€‹\n(\na\nt\n|\ns\nt\n)\n\\pi_{\\theta}(a_{t}|s_{t})\nthat maximizes the expected cumulative discounted reward:\nJ\nâ€‹\n(\nÏ€\nÎ¸\n)\n=\nğ”¼\nÏ„\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n[\nâˆ‘\nt\n=\n0\nT\nÎ³\nt\nâ€‹\nR\nâ€‹\n(\ns\nt\n,\na\nt\n)\n]\nJ(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^{T}\\gamma^{t}R(s_{t},a_{t})\\right]\n(1)\nwhere\nÎ³\n\\gamma\nis the discount factor and\nR\nâ€‹\n(\ns\nt\n,\na\nt\n)\nR(s_{t},a_{t})\nis a composite reward function balancing task completion, efficiency, and safety.\nIII-C\nThe Optimization Challenge\nDirect maximization of Eq. (\n1\n) via a monolithic End-to-End policy is ill-posed due to\noptimization interference\n. The state components\ns\nl\nâ€‹\ni\nâ€‹\nd\nâ€‹\na\nâ€‹\nr\n,\nt\ns_{lidar,t}\nand\ns\nr\nâ€‹\ng\nâ€‹\nb\n,\nt\ns_{rgb,t}\nserve conflicting objectives: global navigation dictates high sensitivity to\ns\nl\nâ€‹\ni\nâ€‹\nd\nâ€‹\na\nâ€‹\nr\n,\nt\ns_{lidar,t}\nfor maximizing traversal speed, whereas local manipulation demands strict adherence to\ns\nr\nâ€‹\ng\nâ€‹\nb\n,\nt\ns_{rgb,t}\nfor minimizing alignment error.\nCompelling a single encoder to simultaneously extract features from these disparate modalities results in\ngradient dominance\n, where the high-magnitude gradients from collision penalties suppress the learning of subtle manipulation policies. Furthermore, the reward signal for precision placement is extremely sparse, rendering unguided exploration inefficient. These theoretical challenges motivate the\nHeterogeneous Multi-Expert (HMER)\nframework, which explicitly decouples the optimization landscape into specialized sub-manifolds.\nIV\nHeterogeneous Multi-Expert Learning System\nTo address the fundamental challenges of optimization interference and sample inefficiency inherent in long-horizon mobile manipulation, we propose the\nHeterogeneous Multi-Expert Reinforcement Learning (HMER)\nframework. We formally model the problem as a Semi-Markov Decision Process (SMDP), decomposing the intractable long-horizon mission into a bi-level hierarchy.\nThe core hypothesis of HMER is that conflicting spatiotemporal requirements must be resolved through\nstructural decoupling\nrather than joint optimization. By explicitly assigning macro-level geometric navigation and micro-level semantic manipulation to specialized modules, we isolate their respective gradient landscapes. As illustrated in the overall system architecture depicted in Fig.\n2\n, our framework consists of a high-level\nSemantic Task Planner\nthat operates on discrete events to constrain the solution space, and a set of low-level\nModality-Decoupled Expert Policies\nexecuting high-frequency continuous control.\nFigure 2:\nThe HMER Framework Architecture.\nThe high-level Semantic Task Planner observes discrete semantic states to orchestrate active low-level experts. Each expert is structurally decoupled, utilizing specialized encoders for distinct modalities (sparse geometric LiDAR vs. dense semantic RGB) to eliminate optimization interference.\nIV-A\nHierarchical Semantic Task Planning\nThe\nSemantic Task Planner\n, denoted as\nÏ€\nH\n\\pi_{H}\n, serves as the cognitive core of the system, functioning as a temporal abstraction mechanism. Formally structured as a deterministic finite state automaton governing the task lifecycle, its primary role in the learning framework is to act as a\nsemantic manifold constrainer\n.\nUnlike end-to-end approaches that must implicitly discover task sequences from high-dimensional observations, our planner explicitly encodes the logical dependencies of the material handling cycle. As detailed in the state transition logic diagram in Fig.\n3\n, the planner observes a high-level semantic state vector\ns\nH\ns_{H}\nâ€”comprising abstract features such as topological location and discrete cargo statusâ€”to activate a specific expert skill\no\nâˆˆ\nğ’ª\no\\in\\mathcal{O}\n. By conditioning expert selection on this discrete semantic state, the planner effectively masks invalid transitions, collapsing the exploration space to physically plausible sequences.\nFigure 3:\nSemantic Task Planner Logic.\nThe finite state machine governs transitions between operational phases based on semantic predicates, acting as a manifold constrainer for the learning agents.\nUpon activation at decision step\nt\nk\nt_{k}\n, the chosen expert policy\nÏ€\no\n\\pi_{o}\nassumes full control. The execution of skill\no\no\ncontinues until a termination condition\nÎ²\no\nâ€‹\n(\ns\nt\nk\n+\nk\n)\n\\beta_{o}(s_{t_{k}+k})\nis met, signaling the achievement of a sub-goal or a safety violation. This hierarchical structure bridges the gap between discrete symbolic planning and continuous sensorimotor control.\nIV-B\nModality-Decoupled Expert Skills\nA critical innovation of our system is the architectural decoupling of sensory modalities at the lower level to create distinct optimization sub-spaces. Instead of a shared encoder prone to feature collapse, we design specialized neural experts with distinct encoder backbones tailored to the topology of their specific input modality.\nIV-B\n1\nNavigation Expert (Geometric Stream)\nThe navigation policy,\nÏ€\nn\nâ€‹\na\nâ€‹\nv\n\\pi_{nav}\n, focuses exclusively on macro-level geometric perception for robust path planning in unstructured environments. Its state space input,\ns\nn\nâ€‹\na\nâ€‹\nv\ns_{nav}\n, combines sparse 360-degree 2D LiDAR point clouds with relative polar coordinates. We design a specialized 1D Convolutional Neural Network (1D-CNN) encoder to extract spatial occupancy features from the sparse scan data. This architecture intrinsically filters high-frequency visual noise irrelevant to geometric collision avoidance. The policy outputs velocity commands optimized via a dense reward function\nR\nn\nâ€‹\na\nâ€‹\nv\nR_{nav}\nincentivizing efficiency and smoothness.\nIV-B\n2\nPicking Expert (Semantic Stream)\nThe picking policy,\nÏ€\np\nâ€‹\ni\nâ€‹\nc\nâ€‹\nk\n\\pi_{pick}\n, governs fine-grained interaction, requiring high-resolution semantic understanding for 6-DoF alignment. Unlike navigation, its input\ns\np\nâ€‹\ni\nâ€‹\nc\nâ€‹\nk\ns_{pick}\nis derived primarily from dense RGB camera data. We utilize a deep 2D-CNN ResNet backbone to extract complex texture and edge features necessary for precise object pose estimation. The policy outputs synchronized chassis and lift control actions. To ensure robustness against approach errors, the reward\nR\np\nâ€‹\ni\nâ€‹\nc\nâ€‹\nk\nR_{pick}\nis formulated as a pose minimization problem, utilizing quaternion arithmetic to enforce precise angular alignment.\nIV-B\n3\nPlacing Expert (Precision Stream)\nThe placing policy,\nÏ€\np\nâ€‹\nl\nâ€‹\na\nâ€‹\nc\nâ€‹\ne\n\\pi_{place}\n, addresses the â€œprecision bottleneckâ€ of industrial stacking, where sub-centimeter accuracy is mandatory. Standard distance-based reward functions suffer from vanishing gradients as the error approaches zero, halting learning near the target. To overcome this, we introduce a novel\nReciprocal Reward Formulation\n:\nR\np\nâ€‹\nl\nâ€‹\na\nâ€‹\nc\nâ€‹\ne\nâ€‹\n(\ns\nt\n)\n=\nÎ»\np\nâ€‹\nr\nâ€‹\ne\nâ€‹\nc\nd\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nt\nâ€‹\n(\ns\nt\n)\n+\nÏµ\ns\nâ€‹\nt\nâ€‹\na\nâ€‹\nb\n+\nR\ns\nâ€‹\nu\nâ€‹\nc\nâ€‹\nc\nâ€‹\ne\nâ€‹\ns\nâ€‹\ns\nR_{place}(s_{t})=\\frac{\\lambda_{prec}}{d_{target}(s_{t})+\\epsilon_{stab}}+R_{success}\n(2)\nwhere\nd\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nt\nd_{target}\nquantifies the combined Euclidean and angular deviation. This formulation generates increasingly sharp gradients as\nd\nt\nâ€‹\na\nâ€‹\nr\nâ€‹\ng\nâ€‹\ne\nâ€‹\nt\nâ†’\n0\nd_{target}\\to 0\n, providing a strong, continuous optimization signal that incentivizes the agent to relentlessly refine its position to the sub-centimeter level.\nIV-C\nHybrid Imitation-Reinforcement Training Strategy\nTraining this multi-expert system from scratch is non-trivial due to the extreme sparsity of the global task reward and high-dimensional state space. We introduce a two-stage\nHybrid Imitation-Reinforcement Strategy\n, depicted in the training pipeline in Fig.\n4\n, addressing the exploration-exploitation paradox in hierarchical learning.\nFigure 4:\nHybrid Imitation-Reinforcement Training Strategy.\nPhase 1 utilizes Behavioral Cloning for stable manifold initialization. Phase 2 employs PPO for end-to-end residual refinement, enabling the experts to learn complex contact dynamics.\nIV-C\n1\nPhase 1: Manifold Initialization via Behavioral Cloning\nWe first define a safe operating manifold by cloning the behavior of rule-based heuristic experts. We collect a dataset of successful demonstrations and optimize each expert policy to minimize the Negative Log-Likelihood of the expert actions. This supervised learning phase rapidly initializes the neural weights, ensuring agents possess baseline kinematic understanding and reducing early-stage catastrophic failures.\nIV-C\n2\nPhase 2: Contact-Rich Refinement via PPO\nHeuristic demonstrations are inherently sub-optimal and fail to model complex contact dynamics essential for high-precision insertion. To surpass this baseline, we transition to Reinforcement Learning for\nresidual refinement\n. In this phase, the pre-trained agents explore the state space around the cloned trajectories, discovering non-intuitive micro-adjustments to handle complex physical interactions. We maximize the expected cumulative reward using Proximal Policy Optimization (PPO). This phase is critical for bridging the gap between idealized heuristic planning and the physical realities of high-precision industrial manipulation.\nV\nExperimental Setup and Implementation Details\nTo ensure the reproducibility of our results and provide a rigorous benchmark for the proposed HMER framework, we detail the high-fidelity simulation environment, the specialized neural architectures designed for heterogeneous modalities, and the practical implementation protocols of the hybrid training strategy. A summary of the key implementation parameters is provided in Table\nI\n.\nTABLE I:\nHYPERPARAMETERS AND SYSTEM CONFIGURATION\nCategory\nParameter\nValue\nSimulation\nPhysics Engine\nOpen Dynamics Engine (ODE)\nPhysics Time Step (\nÎ”\nâ€‹\nt\ns\nâ€‹\ni\nâ€‹\nm\n\\Delta t_{sim}\n)\n0.001\nâ€‹\ns\n0.001s\nControl Frequency\n10\nâ€‹\nH\nâ€‹\nz\n10Hz\nParallel Environments (\nN\ne\nâ€‹\nn\nâ€‹\nv\nN_{env}\n)\n32\nNetwork Architecture\nActor/Critic Hidden Layers\n[256, 256]\nActivation Function\nTanh\nOptimizer\nAdam\nPPO (RL Phase)\nDiscount Factor (\nÎ³\n\\gamma\n)\n0.99\nGAE Parameter (\nÎ»\n\\lambda\n)\n0.95\nClip Range (\nÎµ\n\\varepsilon\n)\n0.2\nEntropy Coefficient (c2)\n0.01\nValue Loss Coefficient (c1)\n0.5\nBatch Size\n2048\nLearning Rate (\nÎ±\nr\nâ€‹\nl\n\\alpha_{rl}\n)\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\n(Linear Decay)\nBC (IL Phase)\nLearning Rate (\nÎ±\nb\nâ€‹\nc\n\\alpha_{bc}\n)\n1\nÃ—\n10\nâˆ’\n3\n1\\times 10^{-3}\nBC Epochs\n50\nExpert Dataset Size\n10,000 Trajectories\nGeneral\nRandom Seed\n42\nV-A\nHigh-Fidelity Simulation Environment\nAll experimental validations are conducted within the\nGazebo 11\nsimulator, integrated with ROS Noetic. To bridge the â€œreality gapâ€ and accurately model the crucial non-linear contact dynamics between the forklift tines and deformable cargo (cotton bales), we utilize the\nOpen Dynamics Engine (ODE)\nas the physics solver with a physics time step of\nÎ”\nâ€‹\nt\ns\nâ€‹\ni\nâ€‹\nm\n=\n0.001\nâ€‹\ns\n\\Delta t_{sim}=0.001s\n.\nWe implement a highly parallelized training architecture to accelerate sample collection. The environment is containerized using Docker, allowing us to instantiate\nN\ne\nâ€‹\nn\nâ€‹\nv\n=\n32\nN_{env}=32\nsynchronous simulation instances on a high-performance workstation equipped with dual NVIDIA RTX 4090 GPUs and an Intel Core i9-14900K CPU. This distributed setup enables the collection of approximately\n2.5\nÃ—\n10\n5\n2.5\\times 10^{5}\ninteraction steps per hour. Furthermore, to enhance the robustness of the learned policies against environmental uncertainties, we employ\nDomain Randomization\n: at the start of each episode, physical parameters (cargo mass, ground friction coefficients) and scene layouts (obstacle distribution) are perturbed according to predefined ranges.\nV-B\nSpecialized Neural Architectures\nA key feature of HMER is the use of specialized neural architectures to practically realize the\nstructural decoupling\nintroduced in Section\nIV\n. The policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nand value function\nV\nÏ•\nV_{\\phi}\nfor each expert are parameterized by deep neural networks tailored to the specific topology of their sensory inputs, effectively isolating gradient updates.\nV-B\n1\nNavigation Expert (Geometric Stream)\nThe input\ns\nn\nâ€‹\na\nâ€‹\nv\ns_{nav}\ncomprises 360-dimensional sparse LiDAR range data and a low-dimensional proprioceptive goal vector. To extract geometric features crucial for obstacle avoidance while ignoring visual noise, we process the 1D LiDAR scan using a\n1D Convolutional Neural Network (1D-CNN)\nconsisting of three layers with kernel sizes of\n[\n8\n,\n4\n,\n3\n]\n[8,4,3]\nand strides of\n[\n4\n,\n2\n,\n1\n]\n[4,2,1]\n, followed by Tanh activations. The extracted geometric embedding is concatenated with the goal vector before being fed into MLP decision heads consisting of two hidden layers containing 256 units each.\nV-B\n2\nPicking Expert (Semantic Stream)\nRelying on visual servoing for alignment, this input\ns\np\nâ€‹\ni\nâ€‹\nc\nâ€‹\nk\ns_{pick}\nis dominated by a\n84\nÃ—\n84\nÃ—\n3\n84\\times 84\\times 3\nRGB image. We employ a visual encoder based on the standard\nNatureCNN\narchitecture, comprising three 2D convolutional layers. This structure is designed to capture rich semantic cues such as object edges and textures. The resulting flattened spatial latent features are fused with the proprioceptive fork height state.\nV-B\n3\nPlacing Expert (Precision Stream)\nDemanding centimeter-level accuracy, the input\ns\np\nâ€‹\nl\nâ€‹\na\nâ€‹\nc\nâ€‹\ne\ns_{place}\nis a low-dimensional vector representing the relative 6-DoF pose error derived from the perception system. Unlike visual agents, this compact state vector does not require convolutional processing and is fed directly into a\nMulti-Layer Perceptron (MLP)\nencoder to project the raw state into a dense latent embedding optimized for precise coordinate corrections.\nFor all experts, the specialized embeddings are fed into separate MLP networks for the Actor and Critic heads. The Actor outputs the mean\nÎ¼\n\\mu\nof a diagonal Gaussian distribution, while the log-standard deviation\nÏƒ\n\\sigma\nis learned as a state-independent parameter.\nV-C\nImplementation of the Hybrid Training Protocol\nTo ensure stable convergence in this multi-stage curriculum, we implement specific protocols for phase transition and signal normalization.\nV-C\n1\nPhase 1: Kinematic Manifold Initialization (BC)\nWe utilize the expert dataset solely to initialize the policy network weights into a\nSafe Kinematic Manifold\n. The primary goal here is not perfect imitation, but to endow the agent with basic functional competency to prevent immediate collisions during early RL exploration. We employ a relatively high learning rate (\nÎ±\nb\nâ€‹\nc\n=\n1\nÃ—\n10\nâˆ’\n3\n\\alpha_{bc}=1\\times 10^{-3}\n) to rapidly fit the expertâ€™s geometric paths. This phase is terminated early (after 50 epochs) to prevent the policy from overfitting to the expertâ€™s inherent sub-optimality.\nV-C\n2\nPhase 2: Residual Refinement via PPO\nUpon transitioning to PPO for residual learning, we introduce two critical mechanisms to manage the competing optimization processes of different experts:\nâ€¢\nRunning Reward Normalization:\nSince navigation tasks (measured in meters) and manipulation tasks (measured in radians or pixels) operate on vastly different scales, their raw reward signals are heterogeneous. We implement running mean-variance normalization for the rewards of each expert independently. This ensures that the advantage estimates across different skills share a similar magnitude, preventing the optimizer from biasing towards the task with larger raw reward magnitudes.\nâ€¢\nLearning Rate Annealing:\nTo perform fine-grained residual learning without destroying the structural knowledge acquired during the BC phase, we anneal the learning rate linearly from an initial\nÎ±\nr\nâ€‹\nl\n=\n1\nÃ—\n10\nâˆ’\n4\n\\alpha_{rl}=1\\times 10^{-4}\nto zero over the course of training.\nVI\nExperimental Evaluation\nIn this section, we present a rigorous quantitative evaluation of the proposed HMER framework. Utilizing the high-fidelity Gazebo simulation environment detailed in Section\nV\n, we aim to answer three critical research questions:\nâ€¢\nQ1 (Training Efficiency):\nDoes the hybrid imitation-reinforcement strategy effectively resolve the cold-start exploration problem and accelerate convergence compared to learning from scratch?\nâ€¢\nQ2 (Overall Performance):\nCan the structurally decoupled architecture achieve superior task success rates and operational efficiency compared to monolithic or sequential baselines?\nâ€¢\nQ3 (Precision & Robustness):\nDoes the system meet strict industrial standards for placement accuracy (sub-2.0 cm) and maintain robustness under environmental perturbations?\nVI-A\nExperimental Baselines\nWe evaluate the framework in a\n20\nâ€‹\nm\nÃ—\n20\nâ€‹\nm\n20m\\times 20m\nsimulated warehouse populated with static storage racks and dynamic obstacles. The objective is to complete a full Departure-Pick-Transport-Place cycle. To comprehensively validate the specific contributions of the HMER frameworkâ€”namely the Semantic Task Planner, Modality Decoupling, and the Hybrid Training strategyâ€”we compare it against five distinct baselines:\n1.\nFlat-BC (Monolithic Imitation):\nAn end-to-end policy trained solely via Behavioral Cloning. It maps stacked raw sensor inputs (LiDAR + RGB) directly to motor actions using a single large network. This baseline highlights the impact of modality interference and the covariate shift inherent in non-hierarchical imitation.\n2.\nRule-Based (Sequential Heuristics):\nA traditional modular approach executing classical algorithms in a fixed sequence: A* planning for navigation and heuristic visual servoing for manipulation. This baseline acts as the â€Expertâ€ for generating demonstration data and serves as a benchmark for adaptability and execution speed.\n3.\nHBC (Hierarchical Imitation):\nIncorporates the same hierarchical architecture as HMER (Semantic Planner + Experts) but is trained exclusively via Behavioral Cloning (Phase 1 only), without RL fine-tuning. This evaluates the precision bottleneck of pure supervised learning.\n4.\nHRL (Hierarchical RL from Scratch):\nThe complete HMER architecture trained solely using PPO (Phase 2 only), without the warm-start initialization from demonstrations. This tests the necessity of the hybrid strategy in overcoming exploration sparsity.\n5.\nSeq-Hybrid (w/o Semantic Planner):\nA variation where the Heterogeneous Experts are trained via the full hybrid strategy but executed in a fixed, open-loop sequence without the high-level Semantic Task Planner. This validates the necessity of the semantic layer for state-aware transitions and error recovery.\n6.\nHMER (Ours):\nThe proposed full framework featuring the Semantic Task Planner, Modality-Decoupled Experts, and the unified Hybrid Imitation-Reinforcement training strategy.\nFigure 5:\nTraining Dynamics Analysis.\nTask success rate vs. environment interaction steps. HMER (Ours) achieves faster convergence and higher asymptotic performance compared to HRL from scratch, while monolithic baselines fail to learn.\nVI-B\nTraining Dynamics Analysis\nWe first analyze the sample efficiency of the proposed training pipeline, as illustrated in Fig.\n5\nshowing task success rates over 10 million interaction steps.\nThe results clearly demonstrate the challenge of long-horizon mobile manipulation. The Flat-BC baseline fails to generalize, stagnating below 15% success, confirming that simple imitation cannot handle compounding errors in complex tasks. The HRL (From Scratch) agent demonstrates extremely slow convergence, requiring approximately 8.0 million steps to reach an 85% success rate, highlighting the difficulty of exploration in sparse-reward settings.\nIn contrast, HMER (Ours) starts at a robust baseline of approximately 65% (inherited from the stable kinematic manifold of the BC phase) and rapidly converges to its peak performance of 94.2% within just 4.5 million steps. This represents a 43.7% reduction in training samples to reach asymptotic performance compared to HRL, validating the critical role of the warm-start strategy in resolving the cold-start problem.\nVI-C\nQuantitative Performance Comparison\nWe conducted 500 separate evaluation episodes with randomized start and target poses for each method. The comprehensive quantitative results are summarized in Table\nII\n.\nTABLE II:\nOVERALL PERFORMANCE COMPARISON (500 Evaluation Trials)\nMethod\nTask Success Rate\n(\nâ†‘\n\\uparrow\n)\nAvg. Cycle Time\n(\nâ†“\n\\downarrow\n)\nMean Placement Error\n(\nâ†“\n\\downarrow\n)\nCollision Rate\n(\nâ†“\n\\downarrow\n)\nFlat-BC\n12.4%\nN/A\nN/A\n82.5%\nRule-Based (Expert)\n84.2%\n55.6 s\n4.1 cm\n5.8%\nHBC (Hierarchical BC)\n76.5%\n48.2 s\n3.9 cm\n10.2%\nHRL (RL from Scratch)\n88.1%\n45.0 s\n1.8 cm\n1.5%\nSeq-Hybrid (w/o Planner)\n68.5%\n41.8 s\n1.5 cm\n28.4%\nHMER (Ours)\n94.2%\n42.5 s\n1.5 cm\n2.1%\nNote: Arrows indicate whether higher (\nâ†‘\n\\uparrow\n) or lower (\nâ†“\n\\downarrow\n) values are better. N/A indicates the method rarely completed the task. Best results are highlighted in bold.\nVI-C\n1\nTask Success and Efficiency\nAs shown in Table\nII\n, HMER achieves the highest task success rate of 94.2%, significantly outperforming the Rule-Based expert (84.2%) and the open-loop Seq-Hybrid baseline (68.5%). The poor performance of Seq-Hybrid highlights the necessity of the Semantic Task Planner; without closed-loop state monitoring, minor failures in the pick phase propagate irreversibly, leading to high collision rates during transport.\nRegarding operational efficiency, HMER achieves an average cycle time of 42.5s. This constitutes a 23.6% reduction compared to the Rule-Based expert (55.6s). This efficiency gain stems from the concurrent optimization of all phases via RL: the Navigation Expert learns to cut corners and optimize path curvature better than A*, while the Manipulation Experts develop smoother, non-stop interaction policies compared to the stop-and-wait logic of visual servoing.\nVI-C\n2\nPlacement Precision and Robustness\nTerminal accuracy is the most critical metric for industrial viability. The methods relying solely on imitation (Rule-Based and HBC) plateau at a mean placement error of approximately 4.0 cm, limited by the quality of the heuristic demonstrations and sensor noise.\nBy leveraging Residual Learning with the proposed reciprocal reward, both HRL and HMER reduce this error significantly to 1.5 cm, meeting strict industrial tolerances. The error distribution analysis (referenced in Fig.\n6\n) reveals that 88% of successful HMER episodes result in an error of less than 2.0 cm, compared to only 25% for the Rule-Based baseline. This confirms that the contact-rich refinement phase enables the agent to learn superior micro-adjustment strategies that exceed the capabilities of the supervisor.\nFigure 6:\nPlacement Accuracy Distribution.\nCumulative distribution function (CDF) of final placement error. HMER significantly outperforms imitation-based methods, achieving sub-2cm precision in the majority of trials.\nVI-D\nAblation Analysis\nTo systematically isolate the contributions of framework components, we analyze the performance drops in the ablated variants presented in Table\nII\n.\nImpact of Modality Decoupling:\nThe catastrophic failure of the Flat-BC baseline (12.4% success) compared to the hierarchical HBC baseline (76.5% success) provides empirical evidence for the necessity of structural decoupling. Forcing a single network to process conflicting geometric and semantic gradients leads to optimization interference, preventing the learning of even basic kinematic behaviors.\nImpact of Hybrid Training:\nComparing HBC, HRL, and HMER reveals the distinct roles of the two training phases. HBC shows that BC alone provides stable but sub-optimal capability (high precision error). HRL shows that RL alone achieves high precision but suffers from sample inefficiency and high early-stage training variance. HMER combines the best of both, utilizing BC for robust initialization and RL for precision capability, resulting in the highest overall success rate.\nImpact of Semantic Planning:\nThe comparison between Seq-Hybrid and HMER isolates the role of the high-level planner. While Seq-Hybrid achieves excellent precision and speed when it succeeds, its low success rate (68.5%) and high collision rate (28.4%) indicate brittleness. The Semantic Task Planner in HMER acts as a crucial safety layer, enabling error recovery (e.g., retrying a failed grasp instead of proceeding to transport), which is essential for robustness in unstructured environments.\nVII\nConclusion\nFuture work will focus on two main directions. We aim to transfer the learned policies to a real-world forklift platform using Sim-to-Real techniques such as Domain Randomization and Domain Adaptation. Additionally, we plan to extend the high-level policy to handle multi-agent coordination, enabling a fleet of autonomous forklifts to collaborate efficiently in dynamic, shared workspaces.\nIn this work, we addressed the fundamental challenge of autonomous mobile manipulation in unstructured warehouses, where the conflicting requirements of large-scale navigation and high-precision stacking create a significant barrier for monolithic learning approaches. We proposed a Collaborative Multi-Expert Reinforcement Learning (CMRL) framework tailored for autonomous forklifts. By decoupling the sensory processing streams, we successfully resolved the â€Modality Interferenceâ€ problem, enabling a geometry-aware Navigation Expert and semantics-aware Manipulation Experts to cooperate seamlessly under a semantic task sequencer. Furthermore, our â€Imitate-then-Reinforceâ€ hybrid training strategy proved effective in solving the exploration-precision trade-off. We demonstrated that while Imitation Learning provides a safe kinematic initialization, the subsequent Residual Learning phase is indispensable for mastering the contact-rich dynamics required for industrial-grade precision.\nExtensive evaluations in a high-fidelity Gazebo simulation validated the efficacy of our approach. The CMRL system achieved a task success rate of 94.2 %, significantly outperforming end-to-end baselines (12.4%) which failed to converge due to gradient conflicts. Most notably, our method broke the precision ceiling of heuristic experts, reducing the placement error to 1.5 cm and meeting strict industrial tolerances. The system also demonstrated superior operational efficiency, reducing the cycle time by 21.4% compared to sequential execution baselines, and exhibited robust performance even under severe environmental perturbations.\nDespite these promising results, we acknowledge several limitations. First, our validation is currently restricted to simulation. While we modeled physics fidelity carefully, real-world factors such as sensor latency and uneven floor friction may introduce discrepancies. Second, the current high-level sequencer operates on a discrete logic basis; it does not yet possess the capability to learn continuous, creative recovery strategies outside its pre-defined semantic transitions.\nFuture work will focus on two primary directions. To bridge the Sim-to-Real gap, we plan to implement domain adaptation techniques to transfer the visual encoders to physical hardware. Additionally, we aim to extend the framework to Multi-Agent Reinforcement Learning, coordinating a fleet of autonomous forklifts to optimize global warehouse throughput while managing traffic congestion in shared workspaces.\nReferences\n[1]\nM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman,\net al.\n(2022)\nDo as i can, not as i say: grounding language in robotic affordances\n.\nIn\nConference on Robot Learning (CoRL)\n,\npp.Â 571â€“582\n.\nCited by:\nÂ§\nII-C\n.\n[2]\nA. G. Barto and S. Mahadevan\n(2003)\nRecent advances in hierarchical reinforcement learning\n.\nDiscrete Event Dynamic Systems\n13\n,\npp.Â 41â€“77\n.\nCited by:\nÂ§\nII-C\n.\n[3]\nC. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song\n(2023)\nDiffusion policy: visuomotor policy learning via action diffusion\n.\nIn\nRobotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-B\n.\n[4]\nC. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-PÃ©rez\n(2021)\nIntegrated task and motion planning\n.\nAnnual Review of Control, Robotics, and Autonomous Systems\n4\n,\npp.Â 265â€“293\n.\nCited by:\nÂ§\nII-A\n.\n[5]\nJ. Ho and S. Ermon\n(2016)\nGenerative adversarial imitation learning\n.\nAdvances in Neural Information Processing Systems\n29\n.\nCited by:\nÂ§\nII-B\n.\n[6]\nW. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei\n(2023)\nVoxposer: composable 3d value maps for robotic manipulation with language models\n.\nIn\nConference on Robot Learning (CoRL)\n,\npp.Â 540â€“562\n.\nCited by:\nÂ§\nII-C\n.\n[7]\nT. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea, E. Solowjow, and S. Levine\n(2019)\nResidual reinforcement learning for robot control\n.\nIn\n2019 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6023â€“6029\n.\nCited by:\nÂ§\nII-C\n.\n[8]\nL. P. Kaelbling and T. Lozano-PÃ©rez\n(2011)\nHierarchical task and motion planning in the now\n.\nIn\n2011 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 1470â€“1477\n.\nCited by:\nÂ§\nII-A\n.\n[9]\nL. P. Kaelbling and T. Lozano-PÃ©rez\n(2013)\nIntegrated task and motion planning in belief space\n.\nThe International Journal of Robotics Research\n32\n(\n9-10\n),\npp.Â 1194â€“1227\n.\nCited by:\nÂ§\nII-A\n.\n[10]\nD. Kalashnikov, A. Irpan, P. Pastor, J. Julian, D. Quillen, E. Jang, M. Wulfmeier,\net al.\n(2018)\nQt-opt: scalable deep reinforcement learning for vision-based robotic manipulation\n.\nIn\nConference on Robot Learning (CoRL)\n,\npp.Â 651â€“673\n.\nCited by:\nÂ§\nII-B\n.\n[11]\nS. Levine, C. Finn, T. Darrell, and P. Abbeel\n(2016)\nEnd-to-end training of deep visuomotor policies\n.\nThe Journal of Machine Learning Research\n17\n(\n1\n),\npp.Â 1334â€“1373\n.\nCited by:\nÂ§\nII-B\n.\n[12]\nA. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni, L. Fei-Fei, S. Savarese, Y. Zhu, and R. MartÃ­n-MartÃ­n\n(2021)\nWhat matters in learning from offline human demonstrations for robot manipulation\n.\nIn\nConference on Robot Learning (CoRL)\n,\npp.Â 1678â€“1690\n.\nCited by:\nÂ§\nII-B\n.\n[13]\nA. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine\n(2018)\nLearning complex dexterous manipulation with deep reinforcement learning and demonstrations\n.\nIn\nRobotics: Science and Systems (RSS)\n,\nCited by:\nÂ§\nII-C\n.\n[14]\nS. Ross, G. Gordon, and D. Bagnell\n(2011)\nA reduction of imitation learning and structured prediction to no-regret online learning\n.\nIn\nProceedings of the fourteenth international conference on artificial intelligence and statistics (AISTATS)\n,\npp.Â 627â€“635\n.\nCited by:\nÂ§\nII-B\n.\n[15]\nA. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitzmann\n(2022)\nNeural descriptor fields: se(3)-equivariant object representations for manipulation\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6394â€“6400\n.\nCited by:\nÂ§\nII-A\n.\n[16]\nS. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel\n(2014)\nCombined task and motion planning through an extensible planner-independent interface layer\n.\nIn\n2014 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 639â€“646\n.\nCited by:\nÂ§\nII-A\n.\n[17]\nR. S. Sutton, D. Precup, and S. Singh\n(1999)\nBetween mdps and semi-mdps: a framework for temporal abstraction in reinforcement learning\n.\nArtificial intelligence\n112\n(\n1-2\n),\npp.Â 181â€“211\n.\nCited by:\nÂ§\nII-C\n.\n[18]\nS. Thrun, W. Burgard, and D. Fox\n(2005)\nProbabilistic robotics\n.\nMIT press\n.\nCited by:\nÂ§\nII-A\n.\n[19]\nM. Toussaint\n(2015)\nLogic-geometric programming: an optimization-based approach to combined task and motion planning\n.\nIn\nProceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)\n,\nCited by:\nÂ§\nII-A\n.\n[20]\nA. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu\n(2017)\nFeudal networks for hierarchical reinforcement learning\n.\nIn\nInternational Conference on Machine Learning (ICML)\n,\npp.Â 3540â€“3549\n.\nCited by:\nÂ§\nII-C\n.\n[21]\nF. Xia, C. Li, R. Martin-Martin, O. Litany, A. Toshev, and S. Savarese\n(2020)\nRelmogen: integrating motion generation in reinforcement learning for mobile manipulation\n.\nIn\n2020 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 4583â€“4590\n.\nCited by:\nÂ§\nII-B\n.\n[22]\nT. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn\n(2020)\nGradient surgery for multi-task learning\n.\nAdvances in Neural Information Processing Systems\n33\n,\npp.Â 5824â€“5836\n.\nCited by:\nÂ§\nII-B\n.",
    "preview_text": "Autonomous mobile manipulation in unstructured warehouses requires a balance between efficient large-scale navigation and high-precision object interaction. Traditional end-to-end learning approaches often struggle to handle the conflicting demands of these distinct phases. Navigation relies on robust decision-making over large spaces, while manipulation needs high sensitivity to fine local details. Forcing a single network to learn these different objectives simultaneously often causes optimization interference, where improving one task degrades the other. To address these limitations, we propose a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts. HMER decomposes long-horizon tasks into specialized sub-policies controlled by a Semantic Task Planner. This structure separates macro-level navigation from micro-level manipulation, allowing each expert to focus on its specific action space without interference. The planner coordinates the sequential execution of these experts, bridging the gap between task planning and continuous control. Furthermore, to solve the problem of sparse exploration, we introduce a Hybrid Imitation-Reinforcement Training Strategy. This method uses expert demonstrations to initialize the policy and Reinforcement Learning for fine-tuning. Experiments in Gazebo simulations show that HMER significantly outperforms sequential and end-to-end baselines. Our method achieves a task success rate of 94.2\\% (compared to 62.5\\% for baselines), reduces operation time by 21.4\\%, and maintains placement error within 1.5 cm, validating its efficacy for precise material handling.\n\n\\useunder\n\\ul\nHeterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts\nYun Chen, Bowei Huang, Fan Guo, Kang Song\nThis work was supported in part by the National Key Research and Development Program of China under Grant 2022YFE0100100, the Tianjin Municipal Science and Technology Pro",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³è‡ªä¸»å‰è½¦åœ¨é•¿æ—¶åŸŸå¤šç›®æ ‡ä»»åŠ¡ä¸­å¯¼èˆªä¸æ“ä½œçš„å†²çªé—®é¢˜ã€‚",
    "detailed_summary": "## è®ºæ–‡è¯¦ç»†æ‘˜è¦\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nåœ¨æ™ºèƒ½ç‰©æµé¢†åŸŸï¼Œè‡ªä¸»ç§»åŠ¨æ“ä½œï¼ˆå¦‚æ— äººå‰è½¦ï¼‰çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚è¿™ç±»ä»»åŠ¡é€šå¸¸æ¶‰åŠ**é•¿æ—¶ç¨‹ã€å¤šç›®æ ‡**çš„å¤æ‚æ“ä½œï¼Œä¾‹å¦‚å¤§è§„æ¨¡å¯¼èˆªä¸å˜ç±³çº§ç²¾åº¦çš„å †å ã€‚ä¼ ç»Ÿæ–¹æ³•é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼šç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ å­˜åœ¨**ä¼˜åŒ–å¹²æ‰°**é—®é¢˜ï¼Œå•ä¸€ç½‘ç»œéš¾ä»¥åŒæ—¶å­¦ä¹ å¯¼èˆªçš„å‡ ä½•ç‰¹å¾ä¸æ“ä½œçš„è¯­ä¹‰ç‰¹å¾ï¼›æ¨¡å—åŒ–é¡ºåºæ‰§è¡Œæ–¹æ³•åˆ™å­˜åœ¨**è¯¯å·®ä¼ æ’­**å’Œæ— æ³•æ¢å¤çš„é—®é¢˜ï¼›è€Œçº¯æ¨¡ä»¿å­¦ä¹ åˆ™å—é™äºä¸“å®¶æ€§èƒ½ç“¶é¢ˆï¼Œéš¾ä»¥è¾¾åˆ°å·¥ä¸šçº§ç²¾åº¦ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ç§èƒ½å…¼é¡¾æ•ˆç‡ã€é²æ£’æ€§ä¸é«˜ç²¾åº¦çš„æ–°æ¡†æ¶ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº†**å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **ç»“æ„åŒ–è§£è€¦**ï¼šè®¾è®¡äº†ä¸€ä¸ªç”±**è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨**åè°ƒçš„å±‚æ¬¡åŒ–ç³»ç»Ÿï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºä¸“æ³¨äº**å®è§‚å‡ ä½•å¯¼èˆª**çš„ä¸“å®¶å’Œä¸“æ³¨äº**å¾®è§‚è¯­ä¹‰æ“ä½œ**çš„ä¸“å®¶ï¼Œä»æ ¹æœ¬ä¸Šé¿å…äº†ä¼˜åŒ–å¹²æ‰°ã€‚\n- **æ··åˆè®­ç»ƒç­–ç•¥**ï¼šé‡‡ç”¨**â€œæ¨¡ä»¿-å¼ºåŒ–â€æ··åˆè®­ç»ƒ**ã€‚é¦–å…ˆé€šè¿‡è¡Œä¸ºå…‹éš†åˆ©ç”¨ä¸“å®¶æ¼”ç¤ºè¿›è¡Œç­–ç•¥åˆå§‹åŒ–ï¼Œè§£å†³å†·å¯åŠ¨å’Œç¨€ç–å¥–åŠ±é—®é¢˜ï¼›éšåä½¿ç”¨PPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œä½¿ç­–ç•¥èƒ½å­¦ä¹ åˆ°è¶…è¶Šä¸“å®¶æ¼”ç¤ºçš„ã€æ›´ç²¾å¦™çš„æ¥è§¦åŠ¨åŠ›å­¦æ§åˆ¶ã€‚\n- **ä¸“ç”¨å¥–åŠ±å‡½æ•°**ï¼šé’ˆå¯¹é«˜ç²¾åº¦æ”¾ç½®ä»»åŠ¡ï¼Œè®¾è®¡äº†**å€’æ•°å‹å¥–åŠ±å‡½æ•°**ï¼Œåœ¨è¯¯å·®è¶‹è¿‘äºé›¶æ—¶æä¾›æ›´å¼ºçš„æ¢¯åº¦ä¿¡å·ï¼Œé©±åŠ¨æ™ºèƒ½ä½“å®ç°äºšå˜ç±³çº§ç²¾åº¦ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨é«˜ä¿çœŸGazeboä»¿çœŸç¯å¢ƒä¸­è¿›è¡ŒéªŒè¯ï¼ŒHMERæ¡†æ¶æ˜¾è‘—ä¼˜äºå¤šç§åŸºçº¿æ–¹æ³•ï¼š\n- **ä»»åŠ¡æˆåŠŸç‡**ï¼šè¾¾åˆ°**94.2%**ï¼Œè¿œé«˜äºç«¯åˆ°ç«¯åŸºçº¿ï¼ˆ12.4%ï¼‰å’Œè§„åˆ™ä¸“å®¶ç³»ç»Ÿï¼ˆ84.2%ï¼‰ã€‚\n- **æ“ä½œæ•ˆç‡**ï¼šå¹³å‡ä»»åŠ¡å‘¨æœŸæ—¶é—´ç¼©çŸ­äº†**21.4%**ã€‚\n- **æ”¾ç½®ç²¾åº¦**ï¼šå¹³å‡æ”¾ç½®è¯¯å·®æ§åˆ¶åœ¨**1.5å˜ç±³ä»¥å†…**ï¼Œä¸”88%çš„æˆåŠŸè¯•éªŒè¯¯å·®å°äº2å˜ç±³ï¼Œæ»¡è¶³äº†ä¸¥æ ¼çš„å·¥ä¸šæ ‡å‡†ã€‚\n- **è®­ç»ƒæ•ˆç‡**ï¼šæ··åˆè®­ç»ƒç­–ç•¥ä½¿æ”¶æ•›æ‰€éœ€çš„äº¤äº’æ­¥æ•°æ¯”çº¯å¼ºåŒ–å­¦ä¹ å‡å°‘äº†çº¦43.7%ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬å·¥ä½œä¸ºå¤æ‚é•¿æ—¶ç¨‹æœºå™¨äººä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„**å±‚æ¬¡åŒ–ã€è§£è€¦å¼å­¦ä¹ èŒƒå¼**ã€‚å…¶å®é™…ä»·å€¼åœ¨äºä¸ºæ— äººå‰è½¦ç­‰å·¥ä¸šç§»åŠ¨æ“ä½œå¹³å°æä¾›äº†å…¼å…·**é«˜é²æ£’æ€§ã€é«˜ç²¾åº¦å’Œé«˜æ•ˆç‡**çš„æ™ºèƒ½æ§åˆ¶è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ›æ¨åŠ¨äº†æ™ºèƒ½ä»“å‚¨çš„è‡ªåŠ¨åŒ–è¿›ç¨‹ã€‚ç†è®ºä»·å€¼åœ¨äºæˆåŠŸè§£å†³äº†å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„ä¼˜åŒ–å¹²æ‰°ä¸ç¨€ç–æ¢ç´¢éš¾é¢˜ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†å¯å€Ÿé‰´çš„æ¡†æ¶ã€‚æœªæ¥å·¥ä½œå°†é›†ä¸­äºå‘çœŸå®å¹³å°çš„**ä»¿çœŸåˆ°ç°å®è¿ç§»**ä»¥åŠ**å¤šæ™ºèƒ½ä½“ååŒ**çš„æ‰©å±•ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡é’ˆå¯¹**è‡ªä¸»å‰è½¦åœ¨éç»“æ„åŒ–ä»“åº“ä¸­æ‰§è¡Œé•¿æ—¶ç¨‹ã€å¤šç›®æ ‡ä»»åŠ¡**æ—¶é¢ä¸´çš„**æ ¹æœ¬æ€§æ§åˆ¶éš¾é¢˜**ï¼š\n1.  **ä¼˜åŒ–å¹²æ‰°é—®é¢˜**ï¼šä¼ ç»Ÿçš„ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¦æ±‚å•ä¸€ç½‘ç»œåŒæ—¶å¤„ç†**å¤§è§„æ¨¡å¯¼èˆªï¼ˆä¾èµ–å‡ ä½•æ„ŸçŸ¥ï¼Œå¦‚LiDARï¼‰** å’Œ**é«˜ç²¾åº¦æ“ä½œï¼ˆä¾èµ–è¯­ä¹‰æ„ŸçŸ¥ï¼Œå¦‚RGBè§†è§‰ï¼‰**ã€‚è¿™ä¸¤ç§ä»»åŠ¡çš„ç›®æ ‡å’Œæ‰€éœ€ç‰¹å¾å­˜åœ¨å†²çªï¼Œå¯¼è‡´æ¢¯åº¦ç›¸äº’å¹²æ‰°ï¼Œç½‘ç»œéš¾ä»¥åŒæ—¶ä¼˜åŒ–ï¼Œæœ€ç»ˆæ”¶æ•›å¤±è´¥æˆ–æ€§èƒ½ä½ä¸‹ã€‚\n2.  **æ¢ç´¢ç¨€ç–æ€§é—®é¢˜**ï¼šé•¿æ—¶ç¨‹ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·æå…¶ç¨€ç–ï¼ˆä»…åœ¨æœ€ç»ˆæˆåŠŸæ”¾ç½®æ—¶è·å¾—é«˜å¥–åŠ±ï¼‰ï¼Œä½¿å¾—ä»é›¶å¼€å§‹çš„å¼ºåŒ–å­¦ä¹ æ ·æœ¬æ•ˆç‡æä½ï¼Œè®­ç»ƒç¼“æ…¢ã€‚\n3.  **ç²¾åº¦ç“¶é¢ˆé—®é¢˜**ï¼šçº¯ç²¹çš„æ¨¡ä»¿å­¦ä¹ å—é™äºä¸“å®¶æ¼”ç¤ºæ•°æ®çš„è´¨é‡ä¸Šé™ï¼Œæ— æ³•é€šè¿‡ç¯å¢ƒäº¤äº’å­¦ä¹ åˆ°è¶…è¶Šä¸“å®¶çš„ã€æ›´ç²¾ç»†çš„æ§åˆ¶ç­–ç•¥ï¼Œéš¾ä»¥è¾¾åˆ°å·¥ä¸šåº”ç”¨æ‰€éœ€çš„äºšå˜ç±³çº§æ”¾ç½®ç²¾åº¦ã€‚\n\n### **äºŒã€ è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **â€œå¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ â€** çš„å®Œæ•´æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªç´§å¯†å…³è”çš„æ ¸å¿ƒåˆ›æ–°ï¼š\n\n1.  **å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶**\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼š**ç»“æ„æ€§è§£è€¦**ã€‚ä¸å†ä½¿ç”¨å•ä¸€ç½‘ç»œï¼Œè€Œæ˜¯å°†é•¿æ—¶ç¨‹ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ª**å¼‚æ„çš„ä¸“å®¶ç­–ç•¥**ï¼Œæ¯ä¸ªä¸“å®¶ä¸“æ³¨äºç‰¹å®šçš„å­ä»»åŠ¡å’Œä¼ æ„Ÿå™¨æ¨¡æ€ã€‚\n    *   **å…·ä½“è®¾è®¡**ï¼š\n        *   **å¯¼èˆªä¸“å®¶**ï¼šä¸“æ³¨äºå®è§‚å‡ ä½•æ„ŸçŸ¥ï¼ˆå¤„ç†LiDARç‚¹äº‘ï¼‰ï¼Œè´Ÿè´£é«˜æ•ˆã€æ— ç¢°æ’çš„è·¯å¾„è§„åˆ’ã€‚\n        *   **æ‹¾å–ä¸“å®¶**ï¼šä¸“æ³¨äºå¾®è§‚è¯­ä¹‰æ„ŸçŸ¥ï¼ˆå¤„ç†RGBå›¾åƒï¼‰ï¼Œè´Ÿè´£è§†è§‰ä¼ºæœå’Œå¯¹å‡†ã€‚\n        *   **æ”¾ç½®ä¸“å®¶**ï¼šä¸“æ³¨äºæè‡´ç²¾åº¦ï¼ˆå¤„ç†6-DoFä½å§¿è¯¯å·®ï¼‰ï¼Œè´Ÿè´£äºšå˜ç±³çº§çš„ç²¾å‡†å †å ã€‚\n    *   **ä»·å€¼**ï¼šä»æ ¹æœ¬ä¸Šæ¶ˆé™¤äº†ä¸åŒä»»åŠ¡é—´çš„æ¢¯åº¦å†²çªï¼Œä½¿æ¯ä¸ªä¸“å®¶èƒ½åœ¨ç‹¬ç«‹çš„ä¼˜åŒ–å­ç©ºé—´ä¸­é«˜æ•ˆå­¦ä¹ ã€‚\n\n2.  **åˆ†å±‚è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨**\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼š**é«˜å±‚åè°ƒä¸çŠ¶æ€æ„ŸçŸ¥**ã€‚è®¾è®¡ä¸€ä¸ªåŸºäºæœ‰é™çŠ¶æ€æœºçš„é«˜å±‚è§„åˆ’å™¨ï¼Œä½œä¸ºç³»ç»Ÿçš„â€œå¤§è„‘â€ã€‚\n    *   **å…·ä½“åŠŸèƒ½**ï¼š\n        *   æ ¹æ®é«˜å±‚è¯­ä¹‰çŠ¶æ€ï¼ˆå¦‚â€œæ˜¯å¦åˆ°è¾¾ç›®æ ‡åŒºåŸŸâ€ã€â€œè´§ç‰©æ˜¯å¦è¢«æŠ“å–â€ï¼‰æ¥**åŠ¨æ€è°ƒåº¦å’Œåˆ‡æ¢**åº•å±‚çš„ä¸“å®¶ç­–ç•¥ã€‚\n        *   æä¾›**é—­ç¯å†³ç­–å’Œé”™è¯¯æ¢å¤èƒ½åŠ›**ã€‚ä¾‹å¦‚ï¼Œå½“æ‹¾å–å¤±è´¥æ—¶ï¼Œè§„åˆ’å™¨å¯ä»¥å‘½ä»¤é‡æ–°å°è¯•ï¼Œè€Œä¸æ˜¯é”™è¯¯åœ°è¿›å…¥è¿è¾“é˜¶æ®µï¼Œä»è€Œè§£å†³äº†ä¼ ç»Ÿé¡ºåºå¼æ–¹æ³•ä¸­é”™è¯¯ä¼ æ’­ä¸å¯é€†çš„é—®é¢˜ã€‚\n\n3.  **ç»Ÿä¸€çš„æ··åˆæ¨¡ä»¿-å¼ºåŒ–è®­ç»ƒç­–ç•¥**\n    *   **æ ¸å¿ƒæ€æƒ³**ï¼š**å…ˆæ¨¡ä»¿åå¼ºåŒ–ï¼Œå–é•¿è¡¥çŸ­**ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿ã€‚\n    *   **å…·ä½“æµç¨‹**ï¼š\n        *   **é˜¶æ®µä¸€ï¼ˆæ¨¡ä»¿å­¦ä¹ ï¼‰**ï¼šä½¿ç”¨è¡Œä¸ºå…‹éš†ï¼Œåˆ©ç”¨ä¸“å®¶æ¼”ç¤ºæ•°æ®å¿«é€Ÿåˆå§‹åŒ–æ‰€æœ‰ä¸“å®¶ç­–ç•¥ï¼Œä½¿å…¶è·å¾—åŸºæœ¬çš„è¿åŠ¨èƒ½åŠ›å’Œå®‰å…¨æ€§ï¼Œè§£å†³äº†å¼ºåŒ–å­¦ä¹ â€œå†·å¯åŠ¨â€çš„æ¢ç´¢éš¾é¢˜ã€‚\n        *   **é˜¶æ®µäºŒï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰**ï¼šä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç­‰RLç®—æ³•ï¼Œåœ¨æ¨¡ä»¿å­¦ä¹ æä¾›çš„â€œå®‰å…¨æµå½¢â€åŸºç¡€ä¸Šè¿›è¡Œ**æ®‹å·®ç²¾è°ƒ**ã€‚æ­¤é˜¶æ®µå…è®¸æ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œå­¦ä¹ åˆ°ä¸“å®¶æ¼”ç¤ºä¸­ä¸å­˜åœ¨çš„ã€æ›´ç²¾å¦™çš„æ¥è§¦åŠ¨åŠ›å­¦å’Œå¾®è°ƒç­–ç•¥ï¼Œä»è€Œçªç ´æ¨¡ä»¿å­¦ä¹ çš„ç²¾åº¦ä¸Šé™ã€‚\n    *   **å…³é”®æŠ€æœ¯**ï¼šä¸ºæ”¾ç½®ä¸“å®¶è®¾è®¡äº†**å€’æ•°å¥–åŠ±å‡½æ•°**ï¼Œåœ¨è¯¯å·®æ¥è¿‘é›¶æ—¶æä¾›æ›´å¼ºçš„æ¢¯åº¦ä¿¡å·ï¼Œé©±åŠ¨ç­–ç•¥å‘è¶…é«˜ç²¾åº¦ä¼˜åŒ–ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆçš„æ€»ç»“**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ª**å±‚æ¬¡åŒ–ã€è§£è€¦åŒ–çš„ç³»ç»Ÿæ¶æ„**ï¼ˆHMERæ¡†æ¶ï¼‰å’Œä¸€ä¸ª**åˆ†é˜¶æ®µçš„æ··åˆè®­ç»ƒèŒƒå¼**ï¼Œç³»ç»Ÿæ€§åœ°è§£å†³äº†è‡ªä¸»ç§»åŠ¨æ“ä½œä¸­çš„å¤šç›®æ ‡å†²çªå’Œè®­ç»ƒéš¾é¢˜ã€‚\n\n1.  **â€œåˆ†è€Œæ²»ä¹‹â€**ï¼šç”¨å¤šä¸ªå¼‚æ„ä¸“å®¶åˆ†åˆ«å¤„ç†å¯¼èˆªå’Œæ“ä½œï¼Œè§£å†³äº†**ä¼˜åŒ–å¹²æ‰°**ã€‚\n2.  **â€œç»Ÿç­¹è§„åˆ’â€**ï¼šç”¨è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨è¿›è¡Œé«˜å±‚åè°ƒå’ŒçŠ¶æ€ç›‘æ§ï¼Œè§£å†³äº†**é”™è¯¯ä¼ æ’­**ã€‚\n3.  **â€œå…ˆç¨³åç²¾â€**ï¼šç”¨æ··åˆè®­ç»ƒç­–ç•¥ï¼Œå…ˆé€šè¿‡æ¨¡ä»¿å­¦ä¹ è·å¾—ç¨³å¥åŸºç¡€ï¼Œå†é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿½æ±‚æè‡´æ€§èƒ½ï¼ŒåŒæ—¶è§£å†³äº†**æ¢ç´¢ç¨€ç–æ€§**å’Œ**ç²¾åº¦ç“¶é¢ˆ**ã€‚\n\n**å®é™…ä»·å€¼**ï¼šåœ¨Gazeboé«˜ä¿çœŸä»¿çœŸä¸­ï¼Œè¯¥æ–¹æ³•å°†ä»»åŠ¡æˆåŠŸç‡ä»åŸºçº¿æ–¹æ³•çš„62.5%æå‡è‡³**94.2%**ï¼Œæ“ä½œæ—¶é—´å‡å°‘**21.4%**ï¼Œå¹¶å°†å¹³å‡æ”¾ç½®è¯¯å·®æ§åˆ¶åœ¨**1.5å˜ç±³**ä»¥å†…ï¼ŒéªŒè¯äº†å…¶æ»¡è¶³å·¥ä¸šçº§ç²¾å‡†ç‰©æ–™æ¬è¿éœ€æ±‚çš„æ½œåŠ›ã€‚",
            "timestamp": "2026-01-21T12:10:12.881796",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘æ¢³ç†\n\næœ¬æ–‡æå‡ºçš„**å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ ï¼ˆHMERï¼‰**æ¡†æ¶ï¼Œæ˜¯é’ˆå¯¹è‡ªä¸»å‰è½¦é•¿æ—¶ç¨‹ã€å¤šç›®æ ‡ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚å…¶æ ¸å¿ƒåœ¨äº**ç»“æ„åŒ–è§£è€¦**ä¸**æ··åˆè®­ç»ƒ**ï¼Œä»¥è§£å†³ç§»åŠ¨æ“ä½œä¸­**å¯¼èˆªï¼ˆå¤§å°ºåº¦å‡ ä½•ï¼‰**ä¸**æ“ä½œï¼ˆç²¾ç»†è¯­ä¹‰ï¼‰**ä¹‹é—´çš„**ä¼˜åŒ–å¹²æ‰°**é—®é¢˜ã€‚åŸºäºå…¶å†…å®¹ä¸å‰ä½œï¼ˆç›¸å…³å·¥ä½œä¸­å¼•ç”¨çš„æ–‡çŒ®ï¼‰ï¼Œå¯ä»¥æ¢³ç†å‡ºè¯¥é¢†åŸŸä»**ç»å…¸è§„åˆ’**åˆ°**æ•°æ®é©±åŠ¨å­¦ä¹ **ï¼Œå†åˆ°**åˆ†å±‚ä¸æ··åˆæ–¹æ³•**çš„æ¼”è¿›è„‰ç»œï¼Œä¸»è¦æ²¿ç€ä¸‰ä¸ªå‘å±•æ–¹å‘æ·±åŒ–ã€‚\n\n### å‘å±•è„‰ç»œä¸æ ¸å¿ƒæ”¹è¿›\n\n#### æ–¹å‘ä¸€ï¼šä»**åŸºäºæ¨¡å‹çš„ç»å…¸è§„åˆ’**åˆ°**æ•°æ®é©±åŠ¨çš„æ„ŸçŸ¥-åŠ¨ä½œç«¯åˆ°ç«¯å­¦ä¹ **\n*   **å‰ä½œä»£è¡¨**ï¼šä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ–¹æ³•ï¼Œå¦‚ Kaelbling & Lozano-PÃ©rez (2011, 2013) å’Œ Garrett ç­‰äºº (2021) çš„å·¥ä½œã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå°†ä»»åŠ¡åˆ†è§£ä¸ºç¬¦å·åŒ–çš„åŠ¨ä½œåºåˆ—ï¼Œå¹¶ä¸ºæ¯ä¸ªåŠ¨ä½œæ±‚è§£è¿ç»­çš„è¿åŠ¨è½¨è¿¹ã€‚ä¾èµ–ç²¾ç¡®çš„ç¯å¢ƒæ¨¡å‹ï¼ˆé«˜æ¸…åœ°å›¾ã€3Dç‰©ä½“æ¨¡å‹ï¼‰å’ŒçŠ¶æ€ä¼°è®¡ã€‚\n*   **å±€é™æ€§**ï¼šåœ¨éç»“æ„åŒ–ã€åŠ¨æ€ç¯å¢ƒä¸­ç¼ºä¹çµæ´»æ€§ï¼Œä¸¥é‡ä¾èµ–â€œç‰¹æƒä¿¡æ¯â€ï¼Œéš¾ä»¥é€‚åº”ä»“åº“å¸ƒå±€åŠ¨æ€å˜åŒ–ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›ä¸å®šä½**ï¼šHMERæ¡†æ¶**æ‘’å¼ƒäº†å¯¹ç²¾ç¡®å…ˆéªŒæ¨¡å‹çš„ä¾èµ–**ï¼Œå®Œå…¨ä¾é è½¦è½½ä¼ æ„Ÿå™¨ï¼ˆLiDAR, RGBï¼‰è¿›è¡Œåœ¨çº¿æ„ŸçŸ¥ä¸å†³ç­–ã€‚å®ƒé€šè¿‡**è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨**å®ç°äº†ç±»ä¼¼TAMPçš„**é€»è¾‘åºåˆ—æ§åˆ¶**ï¼Œä½†åº•å±‚æ˜¯æ•°æ®é©±åŠ¨çš„ç­–ç•¥ï¼Œä»è€Œå…¼å…·äº†ä»»åŠ¡çš„ç»“æ„æ€§å’Œå¯¹åŠ¨æ€ç¯å¢ƒçš„é€‚åº”æ€§ã€‚\n\n#### æ–¹å‘äºŒï¼šä»**å•ä¸€çš„ç«¯åˆ°ç«¯å­¦ä¹ **åˆ°**åˆ†å±‚çš„ã€å¼‚æ„çš„ä¸“å®¶ç³»ç»Ÿ**\n*   **å‰ä½œä»£è¡¨**ï¼š\n    1.  **ç«¯åˆ°ç«¯RL**ï¼šLevineç­‰äºº (2016) å°è¯•ç›´æ¥ä»åŸå§‹ä¼ æ„Ÿå™¨æ˜ å°„åˆ°æ§åˆ¶åŠ¨ä½œï¼Œä½†åœ¨é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­æ˜“å—ä¼˜åŒ–å¹²æ‰°ï¼ˆYuç­‰äººï¼Œ 2020ï¼‰ã€‚\n    2.  **åˆ†å±‚RLï¼ˆHRLï¼‰**ï¼šå¦‚Optionsæ¡†æ¶ (Suttonç­‰äººï¼Œ 1999)ã€FeUdalç½‘ç»œ (Vezhnevetsç­‰äººï¼Œ 2017)ï¼Œä»¥åŠç»“åˆLLMçš„è§„åˆ’æ–¹æ³•ï¼ˆå¦‚SayCan [Ahnç­‰äººï¼Œ 2022]ï¼‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³ä¸å±€é™æ€§**ï¼š\n    *   **ç«¯åˆ°ç«¯RL**ï¼šå•ä¸€ç½‘ç»œå¤„ç†å¼‚æ„æ¨¡æ€ï¼ˆå‡ ä½•vsè¯­ä¹‰ï¼‰å¯¼è‡´**æ¢¯åº¦å†²çª**ï¼Œéš¾ä»¥åŒæ—¶ä¼˜åŒ–å†²çªçš„ç›®æ ‡ã€‚\n    *   **ä¼ ç»ŸHRL**ï¼šè™½è¿›è¡Œäº†æ—¶é—´æŠ½è±¡ï¼Œä½†å¸¸å°†å­ç­–ç•¥è§†ä¸ºåŒè´¨æ¨¡å—ï¼Œ**å¿½ç•¥äº†ä¸åŒé˜¶æ®µï¼ˆå¯¼èˆª/æ“ä½œï¼‰åœ¨æ„ŸçŸ¥æ¨¡æ€å’Œä¼˜åŒ–ç›®æ ‡ä¸Šçš„æ ¹æœ¬å·®å¼‚**ï¼Œæœªèƒ½å½»åº•è§£å†³æ¨¡æ€å¹²æ‰°ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›**ï¼šHMERçš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**æ˜¾å¼çš„æ¨¡æ€è§£è€¦**ã€‚å®ƒå¹¶éç®€å•çš„æ—¶é—´åˆ†å±‚ï¼Œè€Œæ˜¯**ç»“æ„åˆ†å±‚**ï¼š\n    *   **å¯¼èˆªä¸“å®¶**ï¼šä¸“ç”¨1D-CNNå¤„ç†ç¨€ç–LiDARç‚¹äº‘ï¼Œä¸“æ³¨äºå‡ ä½•ç©ºé—´å ç”¨å’Œé¿éšœã€‚\n    *   **æ“ä½œä¸“å®¶ï¼ˆæ‹¾å–/æ”¾ç½®ï¼‰**ï¼šä¸“ç”¨2D-CNNï¼ˆå¦‚ResNetï¼‰å¤„ç†ç¨ å¯†RGBå›¾åƒï¼Œä¸“æ³¨äºè¯­ä¹‰ç†è§£å’Œç²¾ç»†å¯¹é½ã€‚\n    *   **æ”¹è¿›ç‚¹**ï¼šé€šè¿‡ä¸ºä¸åŒä¸“å®¶è®¾è®¡**ä¸“ç”¨ç¼–ç å™¨æ¶æ„**ï¼Œä»æ ¹æœ¬ä¸Š**éš”ç¦»äº†æ¢¯åº¦æ›´æ–°ç©ºé—´**ï¼Œè§£å†³äº†ä¼˜åŒ–å¹²æ‰°é—®é¢˜ã€‚è¿™æ˜¯å¯¹ä¼ ç»ŸHRLå°†å­ç­–ç•¥â€œåŒè´¨åŒ–â€å‡è®¾çš„é‡è¦çªç ´ã€‚\n\n#### æ–¹å‘ä¸‰ï¼šä»**çº¯æ¨¡ä»¿å­¦ä¹ **æˆ–**çº¯å¼ºåŒ–å­¦ä¹ **åˆ°**æ··åˆæ¨¡ä»¿-å¼ºåŒ–è®­ç»ƒç­–ç•¥**\n*   **å‰ä½œä»£è¡¨**ï¼š\n    1.  **æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰**ï¼šåŒ…æ‹¬è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰ã€ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼ˆGAIL, Ho & Ermonï¼Œ 2016ï¼‰å’Œæ‰©æ•£ç­–ç•¥ï¼ˆDiffusion Policy, Chiç­‰äººï¼Œ 2023ï¼‰ã€‚å—é™äº**åå˜é‡åç§»**å’Œ**æ¼”ç¤ºè€…æ€§èƒ½ç“¶é¢ˆ**ï¼ˆMandlekarç­‰äººï¼Œ 2021ï¼‰ã€‚\n    2.  **çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼šåœ¨ç¨€ç–å¥–åŠ±çš„é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­é¢ä¸´**æ¢ç´¢æ•ˆç‡æä½**çš„â€œå†·å¯åŠ¨â€é—®é¢˜ã€‚\n    3.  **æ··åˆæ–¹æ³•å°è¯•**ï¼šå¦‚DAPG (Rajeswaranç­‰äººï¼Œ 2018) å’Œæ®‹å·®ç­–ç•¥å­¦ä¹ ï¼ˆJohanninkç­‰äººï¼Œ 2019ï¼‰ï¼Œä½†å®ƒä»¬å¤šç”¨äºå•ä¸€ç­–ç•¥æˆ–æœªåœ¨è§£è€¦çš„å¼‚æ„ç³»ç»Ÿä¸­å……åˆ†åº”ç”¨ã€‚\n*   **æ ¸å¿ƒæ€æƒ³ä¸å±€é™æ€§**ï¼šILç¨³å®šä½†æ€§èƒ½æœ‰ä¸Šé™ï¼ŒRLæ½œåŠ›å¤§ä½†æ¢ç´¢éš¾ã€‚å…ˆå‰æ··åˆæ–¹æ³•å¤šå…³æ³¨äºä¸ºå•ä¸€ç­–ç•¥æä¾›è¾…åŠ©ã€‚\n*   **æœ¬æ–‡çš„æ”¹è¿›**ï¼šæå‡ºäº†**ç»Ÿä¸€çš„ã€é’ˆå¯¹åˆ†å±‚å¼‚æ„ç³»ç»Ÿçš„æ··åˆè®­ç»ƒç­–ç•¥**ï¼š\n    1.  **é˜¶æ®µ1ï¼ˆBCåˆå§‹åŒ–ï¼‰**ï¼šç”¨å¯å‘å¼è§„åˆ™æ¼”ç¤ºåˆå§‹åŒ–å„ä¸“å®¶ç­–ç•¥ï¼Œå¿«é€Ÿå»ºç«‹**å®‰å…¨è¿åŠ¨æµå½¢**ï¼Œè§£å†³RLå†·å¯åŠ¨é—®é¢˜ã€‚\n    2.  **é˜¶æ®µ2ï¼ˆPPOå¾®è°ƒï¼‰**ï¼šåœ¨BCæä¾›çš„å®‰å…¨åŸºçº¿ä¹‹ä¸Šï¼Œè¿›è¡Œ**æ®‹å·®ç²¾ä¿®**ã€‚å…³é”®æ”¹è¿›åœ¨äºå¼•å…¥äº†**å€’æ•°å¥–åŠ±å‡½æ•°**ï¼Œåœ¨æ¥è¿‘ç›®æ ‡æ—¶æä¾›æ›´å¼ºçš„æ¢¯åº¦ä¿¡å·ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿ**å­¦ä¹ åˆ°è¶…è¶Šæ¼”ç¤ºè€…ï¼ˆå¯å‘å¼è§„åˆ™ï¼‰çš„ã€åº”å¯¹å¤æ‚æ¥è§¦åŠ¨åŠ›å­¦çš„å¾®è°ƒç­–ç•¥**ï¼Œä»è€Œçªç ´æ¨¡ä»¿å­¦ä¹ çš„ç²¾åº¦ç“¶é¢ˆã€‚\n    3.  **ç³»ç»Ÿæ€§æ•´åˆ**ï¼šè¯¥ç­–ç•¥è¢«åº”ç”¨äºæ•´ä¸ªè§£è€¦çš„ä¸“å®¶ç³»ç»Ÿï¼Œå¹¶é…åˆ**è¿è¡Œå¥–åŠ±å½’ä¸€åŒ–**ç­‰æŠ€æœ¯ï¼Œåè°ƒå¤šä¸ªä¸“å®¶çš„å¼‚æ­¥ä¼˜åŒ–è¿‡ç¨‹ã€‚\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªä¸åŒæ–¹å‘æ€»ç»“\n\n1.  **æ¶æ„è®¾è®¡æ–¹å‘ï¼šä»â€œæ—¶é—´åˆ†å±‚â€åˆ°â€œæ¨¡æ€ä¸ç»“æ„è§£è€¦â€**\n    *   **æ¼”è¿›**ï¼šä¼ ç»ŸHRLå…³æ³¨â€œä½•æ—¶åšä»€ä¹ˆâ€ï¼ˆæ—¶é—´æŠ½è±¡ï¼‰ï¼Œæœ¬æ–‡çš„HMERè¿›ä¸€æ­¥æ·±å…¥ä¸ºâ€œç”¨ä»€ä¹ˆæ„ŸçŸ¥åšä»€ä¹ˆï¼Œä»¥åŠå¦‚ä½•æœ€ä¼˜åœ°åšâ€ï¼ˆæ¨¡æ€ä¸ç»“æ„æŠ½è±¡ï¼‰ã€‚\n    *   **ä»·å€¼**ï¼šæ›´è´´è¿‘ç§»åŠ¨æ“ä½œä»»åŠ¡çš„ç‰©ç†æœ¬è´¨ï¼Œé€šè¿‡ä¸“ç”¨ç½‘ç»œæ¶æ„å®ç°æ›´å¹²å‡€ã€æ›´é«˜æ•ˆçš„ä¼˜åŒ–ï¼Œæ˜¯æå‡æ€§èƒ½çš„åŸºç¡€ã€‚\n\n2.  **è®­ç»ƒèŒƒå¼æ–¹å‘ï¼šä»â€œå­¤ç«‹å­¦ä¹ â€åˆ°â€œååŒçš„è¯¾ç¨‹åŒ–æ··åˆå­¦ä¹ â€**\n    *   **æ¼”è¿›**ï¼šå°†æ¨¡ä»¿å­¦ä¹ ï¼ˆæä¾›å®‰å…¨æ€§ä¸å…ˆéªŒï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆæä¾›ä¼˜åŒ–ä¸è¶…è¶Šèƒ½åŠ›ï¼‰ä¸å†æ˜¯æ›¿ä»£æˆ–ç®€å•æ‹¼æ¥ï¼Œè€Œæ˜¯è®¾è®¡ä¸ºä¸€ä¸ª**æœ‰åºçš„è¯¾ç¨‹**ï¼šBCå¥ å®šåŸºç¡€ï¼ŒRLç²¾ä¿®çªç ´ã€‚åŒæ—¶ï¼Œè¯¥èŒƒå¼è¢«**ç³»ç»Ÿæ€§åœ°åº”ç”¨äºä¸€ä¸ªå¼‚æ„ä¸“å®¶ç³»ç»Ÿ**ã€‚\n    *   **ä»·å€¼**ï¼šåŒæ—¶è§£å†³äº†é•¿æ—¶ç¨‹ä»»åŠ¡ä¸­çš„**æ¢ç´¢ç¨€ç–æ€§**å’Œ**å·¥ä¸šçº§ç²¾åº¦è¦æ±‚**ä¸¤å¤§éš¾é¢˜ï¼Œå®ç°äº†æ ·æœ¬æ•ˆç‡ä¸æœ€ç»ˆæ€§èƒ½çš„å¹³è¡¡ã€‚\n\n3.  **ç³»ç»Ÿé›†æˆæ–¹å‘ï¼šä»â€œå¼€ç¯åºåˆ—â€åˆ°â€œé—­ç¯è¯­ä¹‰ä»»åŠ¡è§„åˆ’â€**\n    *   **æ¼”è¿›**ï¼šç›¸è¾ƒäºç®€å•çš„æ¨¡å—é¡ºåºæ‰§è¡Œï¼ˆæ˜“è¯¯å·®ä¼ æ’­ã€æ— æ³•æ¢å¤ï¼‰ï¼ŒHMERå¼•å…¥äº†**åŸºäºè¯­ä¹‰çŠ¶æ€çš„æœ‰é™çŠ¶æ€æœº**ä½œä¸ºé«˜å±‚è§„åˆ’å™¨ã€‚\n    *   **ä»·å€¼**ï¼šå®ƒ**æ¡¥æ¥äº†ç¦»æ•£ä»»åŠ¡é€»è¾‘ä¸è¿ç»­æ§åˆ¶**ï¼Œèƒ½å¤Ÿæ ¹æ®é«˜å±‚è¯­ä¹‰çŠ¶æ€ï¼ˆå¦‚â€œæ˜¯å¦æˆåŠŸæŠ“å–â€ï¼‰åŠ¨æ€åˆ‡æ¢ä¸“å®¶å¹¶å¯åŠ¨é”™è¯¯æ¢å¤æµç¨‹ï¼Œæ˜¾è‘—æå‡äº†åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„**é²æ£’æ€§**ã€‚è¿™ç›¸è¾ƒäºå®Œå…¨ä¾èµ–å­¦ä¹ éšå«åºåˆ—çš„ç«¯åˆ°ç«¯æ–¹æ³•æ›´å…·å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚\n\n**ç»“è®º**ï¼šæœ¬æ–‡çš„HMERæ¡†æ¶ä»£è¡¨äº†è‡ªä¸»ç§»åŠ¨æ“ä½œé¢†åŸŸä¸€ä¸ªé‡è¦çš„èåˆä¸æ·±åŒ–æ–¹å‘ã€‚å®ƒå¹¶éå•ä¸€æŠ€æœ¯çš„çªç ´ï¼Œè€Œæ˜¯**å°†å¼‚æ„æ¶æ„è®¾è®¡ã€æ··åˆè®­ç»ƒç­–ç•¥å’Œé—­ç¯è¯­ä¹‰è§„åˆ’ä¸‰è€…ç³»ç»Ÿæ€§åœ°æ•´åˆ**ï¼Œé’ˆå¯¹å·¥ä¸šåœºæ™¯ï¼ˆå¦‚æ— äººå‰è½¦ï¼‰çš„æ ¸å¿ƒç—›ç‚¹â€”â€”**æ•ˆç‡ã€ç²¾åº¦ã€é²æ£’æ€§**â€”â€”æä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚å…¶å‘å±•è„‰ç»œæ¸…æ™°ä½“ç°äº†ä»ä¾èµ–æ¨¡å‹ã€åˆ°æ•°æ®é©±åŠ¨ã€å†åˆ°å¯»æ±‚**ç»“æ„å…ˆéªŒä¸æ•°æ®å­¦ä¹ æœ€ä¼˜ç»“åˆ**çš„æ¼”è¿›è¶‹åŠ¿ã€‚",
            "timestamp": "2026-01-21T12:11:07.058312",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®æ‚¨æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œè¯¥è®ºæ–‡å¹¶æœªæ˜ç¡®åˆ—å‡ºä½œè€…å›¢é˜Ÿæ­¤å‰å‘è¡¨çš„ã€ä¸æœ¬æ–‡ç›´æ¥ç›¸å…³çš„å…·ä½“â€œå‰ä½œâ€ã€‚è®ºæ–‡çš„â€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†ï¼ˆSection IIï¼‰æ˜¯å¯¹æ•´ä¸ªé¢†åŸŸå†…**å…¶ä»–ç ”ç©¶è€…**å·¥ä½œçš„ç»¼è¿°ï¼Œè€Œéä½œè€…è‡ªèº«çš„å‰æœŸç ”ç©¶ã€‚\n\nå› æ­¤ï¼Œ**æ— æ³•ç›´æ¥æ„å»ºä¸€ä¸ªå…³äºâ€œä»–çš„å‰ä½œâ€çš„è¡¨æ ¼**ã€‚\n\nä¸è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥ä»è®ºæ–‡çš„**é—®é¢˜é™ˆè¿°ã€æ–¹æ³•åˆ›æ–°å’Œå®éªŒå¯¹æ¯”**ä¸­ï¼Œæ¸…æ™°åœ°æ¨æ–­å‡ºæœ¬æ–‡æ‰€é’ˆå¯¹çš„**ç ”ç©¶èŒƒå¼**åŠå…¶ç›¸å¯¹äºè¿™äº›èŒƒå¼çš„**æ ¸å¿ƒæ”¹è¿›**ã€‚è¿™äº›èŒƒå¼å¯ä»¥è§†ä¸ºæœ¬æ–‡æ„å›¾è¶…è¶Šçš„â€œå¹¿ä¹‰å‰ä½œâ€ã€‚\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ†æäº†æœ¬æ–‡æå‡ºçš„ **HMERæ¡†æ¶** ä¸æ–‡çŒ®ä¸­æåˆ°çš„å‡ ç§ä¸»æµç ”ç©¶èŒƒå¼ä¹‹é—´çš„åŒºåˆ«ä¸æ”¹è¿›ï¼š\n\n| å¯¹æ¯”èŒƒå¼ (ä½œä¸ºâ€œå¹¿ä¹‰å‰ä½œâ€) | æ ¸å¿ƒæ–¹æ³• / ç‰¹ç‚¹ | å­˜åœ¨çš„ä¸»è¦é—®é¢˜ (æœ¬æ–‡æŒ‡å‡º) | **HMERæ¡†æ¶çš„å…·ä½“æ”¹è¿›ä¸åˆ›æ–°** | **æ”¹å–„çš„é‡ç‚¹ä¸ä»·å€¼** |\n| :--- | :--- | :--- | :--- | :--- |\n| **1. ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹  (End-to-End RL)** | ä½¿ç”¨å•ä¸€ç¥ç»ç½‘ç»œï¼Œç›´æ¥ä»åŸå§‹ä¼ æ„Ÿå™¨è¾“å…¥æ˜ å°„åˆ°æ§åˆ¶æŒ‡ä»¤ã€‚ | **ä¼˜åŒ–å¹²æ‰°**ï¼šå¯¼èˆªï¼ˆä¾èµ–å‡ ä½•LiDARç‰¹å¾ï¼‰ä¸æ“çºµï¼ˆä¾èµ–è¯­ä¹‰RGBç‰¹å¾ï¼‰çš„ç›®æ ‡å†²çªï¼Œå¯¼è‡´æ¢¯åº¦ç«äº‰ï¼Œä½¿å•ä¸€ç½‘ç»œéš¾ä»¥åŒæ—¶ä¼˜åŒ–ï¼Œå¸¸æ”¶æ•›å¤±è´¥æˆ–å¾—åˆ°æ¬¡ä¼˜ç­–ç•¥ã€‚ | **1. ç»“æ„è§£è€¦**ï¼šè®¾è®¡å¼‚æ„ä¸“å®¶ï¼ˆå¯¼èˆªä¸“å®¶ã€æŠ“å–ä¸“å®¶ã€æ”¾ç½®ä¸“å®¶ï¼‰ï¼Œæ¯ä¸ªä¸“å®¶ä½¿ç”¨**ä¸“ç”¨ç¼–ç å™¨**å¤„ç†ç‰¹å®šæ¨¡æ€æ•°æ®ï¼ˆå¦‚1D-CNNå¤„ç†LiDARï¼Œ2D-CNNå¤„ç†RGBï¼‰ï¼Œä»æ ¹æœ¬ä¸Šéš”ç¦»äº†å†²çªçš„æ¢¯åº¦æµã€‚<br>**2. åˆ†å±‚è§„åˆ’**ï¼šå¼•å…¥é«˜å±‚è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨ï¼Œåè°ƒä¸“å®¶æ‰§è¡Œåºåˆ—ï¼Œå°†é•¿è§†ç•Œä»»åŠ¡åˆ†è§£ã€‚ | **è§£å†³äº†æ¨¡æ€å†²çª**ã€‚é€šè¿‡æ¶æ„è®¾è®¡é¿å…äº†ç‰¹å¾æå–ä¸­çš„æ¢¯åº¦å¹²æ‰°ï¼Œä½¿å¾—å®è§‚å¯¼èˆªå’Œå¾®è§‚æ“çºµèƒ½ç‹¬ç«‹ã€é«˜æ•ˆåœ°ä¼˜åŒ–ï¼Œè¿™æ˜¯æ€§èƒ½æå‡çš„åŸºç¡€ã€‚ |\n| **2. é¡ºåºå¼æ¨¡å—åŒ–æ–¹æ³• (Sequential Modular, e.g., Rule-Based)** | å°†ä»»åŠ¡åˆ†è§£ä¸ºå¯¼èˆªã€æŠ“å–ã€æ”¾ç½®ç­‰å­æ¨¡å—ï¼ŒæŒ‰å›ºå®šé¡ºåºæ‰§è¡Œç»å…¸ç®—æ³•ï¼ˆå¦‚A*è·¯å¾„è§„åˆ’ã€è§†è§‰ä¼ºæœï¼‰ã€‚ | **è¯¯å·®ä¼ æ’­ä¸ä¸å¯é€†æ€§**ï¼šå„æ¨¡å—ç‹¬ç«‹è¿è¡Œï¼Œç¼ºä¹é«˜å±‚åè°ƒã€‚å‰ä¸€é˜¶æ®µï¼ˆå¦‚å¯¼èˆªï¼‰ç§¯ç´¯çš„å¾®å°çŠ¶æ€åå·®ä¼šç›´æ¥ä¼ é€’ç»™ä¸‹ä¸€é˜¶æ®µï¼ˆå¦‚æ“çºµï¼‰ï¼Œä¸”ç³»ç»Ÿæ— æ³•è‡ªä¸»å›é€€æˆ–é‡è§„åˆ’ï¼Œå¯¼è‡´ä»»åŠ¡å¤±è´¥ã€‚ | **1. é—­ç¯è¯­ä¹‰è§„åˆ’å™¨**ï¼šè®¾è®¡äº†ä¸€ä¸ªåŸºäºæœ‰é™çŠ¶æ€æœºçš„é«˜å±‚è§„åˆ’å™¨ã€‚å®ƒæ ¹æ®**è¯­ä¹‰çŠ¶æ€**ï¼ˆå¦‚â€œæ˜¯å¦æŠµè¾¾ç›®æ ‡é™„è¿‘â€ã€â€œè´§ç‰©æ˜¯å¦è¢«æŠ“å–â€ï¼‰åŠ¨æ€å†³å®šæ¿€æ´»å“ªä¸ªä¸“å®¶ï¼Œå¹¶èƒ½è§¦å‘é”™è¯¯æ¢å¤ï¼ˆå¦‚æŠ“å–å¤±è´¥åé‡è¯•ï¼‰ï¼Œè€Œéæœºæ¢°åœ°é¡ºåºæ‰§è¡Œã€‚<br>**2. ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**ï¼šæ‰€æœ‰ä¸“å®¶åœ¨RLé˜¶æ®µè¢«å…±åŒä¼˜åŒ–ï¼Œå­¦ä¹ æ›´å¹³æ»‘ã€é«˜æ•ˆçš„ååŒç­–ç•¥ã€‚ | **å¢å¼ºäº†ç³»ç»Ÿé²æ£’æ€§ä¸é€‚åº”æ€§**ã€‚ä»â€œå¼€ç¯æµæ°´çº¿â€å‡çº§ä¸ºâ€œé—­ç¯æ™ºèƒ½ä½“â€ï¼Œèƒ½å¤Ÿå¤„ç†ä¸ç¡®å®šæ€§å¹¶ä»é”™è¯¯ä¸­æ¢å¤ï¼Œæ˜¾è‘—é™ä½äº†å› è¯¯å·®ä¼ æ’­å¯¼è‡´çš„å¤±è´¥ç‡ã€‚ |\n| **3. æ¨¡ä»¿å­¦ä¹  (Imitation Learning, IL)** | é€šè¿‡è¡Œä¸ºå…‹éš†ç­‰æ–¹å¼ï¼Œä»ä¸“å®¶æ¼”ç¤ºæ•°æ®ä¸­ç›´æ¥å­¦ä¹ ç­–ç•¥ã€‚ | **1. åå˜é‡åç§»**ï¼šæ‰§è¡Œä¸­çš„å¾®å°åå·®ä¼šå¯¼è‡´çŠ¶æ€åˆ†å¸ƒè¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´ï¼Œäº§ç”Ÿå¤åˆè¯¯å·®ã€‚<br>**2. ç²¾åº¦ç“¶é¢ˆ**ï¼šç­–ç•¥æ€§èƒ½ä¸¥æ ¼å—é™äºæ¼”ç¤ºæ•°æ®çš„è´¨é‡ï¼Œæ— æ³•è¶…è¶Šä¸“å®¶ï¼Œéš¾ä»¥è¾¾åˆ°å·¥ä¸šçº§ï¼ˆäºšå˜ç±³çº§ï¼‰ç²¾åº¦è¦æ±‚ã€‚ | **æå‡ºæ··åˆæ¨¡ä»¿-å¼ºåŒ–è®­ç»ƒç­–ç•¥**ï¼š<br>**é˜¶æ®µ1 (BCåˆå§‹åŒ–)**ï¼šç”¨ä¸“å®¶æ¼”ç¤ºæ•°æ®é¢„çƒ­æ‰€æœ‰ä¸“å®¶ç­–ç•¥ï¼Œå¿«é€Ÿè·å¾—åŸºæœ¬èƒ½åŠ›ï¼Œè§£å†³RLå†·å¯åŠ¨é—®é¢˜ã€‚<br>**é˜¶æ®µ2 (PPOå¾®è°ƒ)**ï¼šåœ¨BCæä¾›çš„â€œå®‰å…¨è¿åŠ¨æµå½¢â€åŸºç¡€ä¸Šï¼Œä½¿ç”¨RLè¿›è¡Œ**æ®‹å·®ç²¾ç‚¼**ã€‚å…³é”®åˆ›æ–°æ˜¯**è®¾è®¡äº†å€’æ•°å¥–åŠ±å‡½æ•°**ï¼Œåœ¨æ¥è¿‘ç›®æ ‡æ—¶æä¾›æ›´å¼ºçš„æ¢¯åº¦ä¿¡å·ï¼Œé©±åŠ¨ç­–ç•¥æ¢ç´¢å¹¶å­¦ä¹ è¶…è¶Šæ¼”ç¤ºæ•°æ®çš„ã€æ›´ç²¾å¯†çš„æ¥è§¦åŠ¨åŠ›å­¦è°ƒæ•´ã€‚ | **çªç ´äº†æ¨¡ä»¿å­¦ä¹ çš„æ€§èƒ½ä¸Šé™**ã€‚ç»“åˆäº†ILçš„**æ ·æœ¬æ•ˆç‡**å’ŒRLçš„**è¶…è¶Šèƒ½åŠ›**ã€‚ä½¿ç³»ç»Ÿä¸ä»…èƒ½ç¨³å®šå¯åŠ¨ï¼Œè¿˜èƒ½é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å‘ç°æ›´ä¼˜ç­–ç•¥ï¼Œæœ€ç»ˆå®ç°**1.5å˜ç±³**çš„æ”¾ç½®ç²¾åº¦ï¼Œè¿™æ˜¯çº¯æ¨¡ä»¿å­¦ä¹ æ— æ³•è¾¾åˆ°çš„ã€‚ |\n| **4. åˆ†å±‚å¼ºåŒ–å­¦ä¹  (HRL) from Scratch** | ä½¿ç”¨åˆ†å±‚ç»“æ„ï¼Œä½†å®Œå…¨ä»é›¶å¼€å§‹é€šè¿‡RLè®­ç»ƒï¼Œæ²¡æœ‰åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ã€‚ | **æ¢ç´¢ç¨€ç–æ€§**ï¼šåœ¨é•¿è§†ç•Œã€ç¨€ç–å¥–åŠ±çš„ä»»åŠ¡ä¸­ï¼Œä»å¤´å¼€å§‹æ¢ç´¢æ•ˆç‡æä½ï¼Œè®­ç»ƒæ”¶æ•›ç¼“æ…¢ä¸”ä¸ç¨³å®šã€‚ | **å¼•å…¥äº†åˆ†é˜¶æ®µçš„æ··åˆè®­ç»ƒèŒƒå¼**ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œå…ˆç”¨BCè¿›è¡Œ**æµå½¢åˆå§‹åŒ–**ï¼Œå¤§å¹…ç¼©çŸ­äº†RLæ¢ç´¢æ‰€éœ€çš„æ—¶é—´ã€‚å®éªŒè¡¨æ˜ï¼ŒHMERæ¯”çº¯HRLï¼ˆä»å¤´å¼€å§‹ï¼‰**èŠ‚çœäº†43.7%çš„è®­ç»ƒæ ·æœ¬**å³è¾¾åˆ°æ›´é«˜æ€§èƒ½ã€‚ | **æå¤§æå‡äº†è®­ç»ƒæ•ˆç‡**ã€‚è§£å†³äº†å¤æ‚åˆ†å±‚æ¨¡å‹â€œå†·å¯åŠ¨â€éš¾çš„é—®é¢˜ï¼Œä½¿HMERè¿™ç§å¤æ‚æ¡†æ¶åœ¨å®é™…è®­ç»ƒä¸­å˜å¾—å¯è¡Œã€‚ |\n| **5. æ— è§„åˆ’å™¨çš„æ··åˆä¸“å®¶ (Seq-Hybrid)** | å…·å¤‡HMERçš„å¼‚æ„ä¸“å®¶å’Œæ··åˆè®­ç»ƒï¼Œä½†**ç¼ºå°‘é«˜å±‚è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨**ï¼Œä¸“å®¶æŒ‰å›ºå®šé¡ºåºæ‰§è¡Œã€‚ | **ç¼ºä¹çŠ¶æ€æ„ŸçŸ¥ä¸é”™è¯¯æ¢å¤**ï¼šè™½ç„¶ä¸“å®¶æœ¬èº«èƒ½åŠ›å¼ºï¼Œä½†å›ºå®šçš„æ‰§è¡Œé¡ºåºæ— æ³•åº”å¯¹æ„å¤–ã€‚ä¾‹å¦‚ï¼ŒæŠ“å–è½»å¾®å¤±è´¥åä»ä¼šå¼ºè¡Œè¿›å…¥è¿è¾“é˜¶æ®µï¼Œå¯¼è‡´ç¢°æ’æˆ–æ”¾ç½®å¤±è´¥ã€‚ | **å¢åŠ äº†è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨ä½œä¸ºâ€œå¤§è„‘â€**ã€‚è¯¥è§„åˆ’å™¨æ ¹æ®é«˜çº§è¯­ä¹‰çŠ¶æ€åšå†³ç­–ï¼Œå®ç°äº†**è‡ªé€‚åº”çš„æŠ€èƒ½åˆ‡æ¢**å’Œ**è‡ªä¸»é”™è¯¯æ¢å¤**ã€‚ | **æ˜ç¡®äº†é«˜å±‚å†³ç­–çš„æ ¸å¿ƒä»·å€¼**ã€‚å¯¹æ¯”å®éªŒï¼ˆSeq-Hybrid vs. HMERï¼‰æ¸…æ™°è¡¨æ˜ï¼Œå³ä½¿åº•å±‚ä¸“å®¶èƒ½åŠ›ç›¸åŒï¼Œç¼ºå°‘ä¸€ä¸ªæ™ºèƒ½çš„ã€é—­ç¯çš„åè°ƒå™¨ï¼Œæ•´ä½“ä»»åŠ¡æˆåŠŸç‡ä¼šå¤§å¹…ä¸‹é™ï¼ˆä»94.2%é™è‡³68.5%ï¼‰ï¼Œç¢°æ’ç‡æ€¥å‰§ä¸Šå‡ã€‚è¿™è¯æ˜äº†**å°†ä»»åŠ¡é€»è¾‘æ˜¾å¼åŒ–ã€é—­ç¯åŒ–**å¯¹äºå®é™…æœºå™¨äººç³»ç»Ÿé²æ£’æ€§è‡³å…³é‡è¦ã€‚ |\n\n### æ€»ç»“ï¼šæœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®ä¸æ”¹è¿›\n\næœ¬æ–‡å¹¶éåœ¨ä½œè€…è‡ªèº«ä¸€ç³»åˆ—æ–‡ç« ä¸Šçš„è¿­ä»£ï¼Œè€Œæ˜¯**é’ˆå¯¹è‡ªä¸»ç§»åŠ¨æ“çºµé¢†åŸŸé•¿æœŸå­˜åœ¨çš„å‡ ä¸ªå…±æ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€å¥—é›†æˆæ€§è§£å†³æ–¹æ¡ˆ**ã€‚å…¶æ ¸å¿ƒæ”¹è¿›åœ¨äºï¼š\n\n1.  **æ¶æ„ä¸Š**ï¼šé€šè¿‡**å¼‚æ„ä¸“å®¶ç»“æ„**è§£å†³äº†â€œä¼˜åŒ–å¹²æ‰°â€é—®é¢˜ã€‚\n2.  **è®­ç»ƒä¸Š**ï¼šé€šè¿‡**æ··åˆæ¨¡ä»¿-å¼ºåŒ–ç­–ç•¥**è§£å†³äº†â€œæ¢ç´¢ç¨€ç–æ€§â€å’Œâ€œç²¾åº¦ç“¶é¢ˆâ€é—®é¢˜ã€‚\n3.  **æ§åˆ¶æµä¸Š**ï¼šé€šè¿‡**è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨**è§£å†³äº†â€œè¯¯å·®ä¼ æ’­â€å’Œâ€œç¼ºä¹é€‚åº”æ€§â€é—®é¢˜ã€‚\n\næœ¬æ–‡çš„åˆ›æ–°æ˜¯**ç³»ç»Ÿæ€§çš„**ï¼Œå®ƒå°†å¤šä¸ªå·²è¢«å•ç‹¬è®¨è®ºçš„æ€æƒ³ï¼ˆåˆ†å±‚ã€å¼‚æ„ã€æ··åˆè®­ç»ƒï¼‰åˆ›é€ æ€§åœ°æ•´åˆå¹¶é€‚é…åˆ°â€œè‡ªåŠ¨é©¾é©¶å‰è½¦â€è¿™ä¸€å…·ä½“ä¸”å¤æ‚çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œå¹¶é€šè¿‡ä¸¥è°¨çš„æ¶ˆèå®éªŒéªŒè¯äº†æ¯ä¸ªç»„ä»¶çš„å¿…è¦æ€§ã€‚",
            "timestamp": "2026-01-21T12:11:57.375906",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†åˆ†æè¿™ç¯‡è®ºæ–‡æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡é’ˆå¯¹è‡ªä¸»å‰è½¦åœ¨éç»“æ„åŒ–ä»“åº“ä¸­æ‰§è¡Œé•¿æ—¶ç¨‹ã€å¤šç›®æ ‡ä»»åŠ¡æ—¶é¢ä¸´çš„â€œä¼˜åŒ–å¹²æ‰°â€å’Œâ€œæ¢ç´¢ç¨€ç–â€ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º**å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ **çš„å®Œæ•´æ¡†æ¶ã€‚å…¶å…³é”®æŠ€æœ¯æ–¹æ³•å¯å½’çº³ä¸ºä»¥ä¸‹ä¸‰ä¸ªç›¸äº’å…³è”çš„ç»„æˆéƒ¨åˆ†ï¼š\n\n### 1. å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶\n**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡**ç»“æ„åŒ–è§£è€¦**ï¼Œå°†å®è§‚å¯¼èˆªä¸å¾®è§‚æ“ä½œä»»åŠ¡åˆ†é…ç»™ä¸åŒçš„ä¸“å®¶ç­–ç•¥ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³å•ä¸€ç½‘ç»œå› å¤„ç†å¼‚è´¨ä¼ æ„Ÿå™¨æ¨¡æ€ï¼ˆå‡ ä½•LiDAR vs. è¯­ä¹‰RGBï¼‰è€Œå¯¼è‡´çš„æ¢¯åº¦å†²çªé—®é¢˜ã€‚\n\n| ç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸æ¶æ„ |\n| :--- | :--- | :--- |\n| **å¯¼èˆªä¸“å®¶** | **ç›®æ ‡**ï¼šä¸“æ³¨äºåŸºäºå‡ ä½•ä¿¡æ¯çš„é²æ£’è·¯å¾„è§„åˆ’ã€‚<br>**çŠ¶æ€ç©ºé—´**ï¼š\\( s_{nav} = [s_{lidar}, s_{ego, nav}]^T \\)ï¼Œå…¶ä¸­ \\( s_{lidar} \\in \\mathbb{R}^{360} \\)ï¼ˆ360ç»´2Dæ¿€å…‰é›·è¾¾æ‰«æï¼‰ï¼Œ\\( s_{ego, nav} \\) ä¸ºåŒ…å«ç›¸å¯¹ç›®æ ‡æåæ ‡çš„æœ¬ä½“æ„ŸçŸ¥çŠ¶æ€ã€‚<br>**ç½‘ç»œæ¶æ„**ï¼šé‡‡ç”¨**1D-CNN**å¤„ç†ç¨€ç–çš„LiDARç‚¹äº‘ï¼Œæå–ç©ºé—´å ç”¨ç‰¹å¾ï¼Œè¿‡æ»¤ä¸å¯¼èˆªæ— å…³çš„è§†è§‰å™ªå£°ã€‚ç‰¹å¾ä¸ç›®æ ‡å‘é‡æ‹¼æ¥åï¼Œè¾“å…¥MLPå†³ç­–å¤´ã€‚<br>**å¥–åŠ±å‡½æ•°**ï¼š\\( R_{nav} \\) ä¸ºå¯†é›†å¥–åŠ±ï¼Œæ¿€åŠ±è·¯å¾„æ•ˆç‡ï¼ˆæ¥è¿‘ç›®æ ‡ï¼‰å’Œå¹³æ»‘æ€§ï¼ˆé¿å…å‰§çƒˆè½¬å‘ï¼‰ã€‚ | â€¢ **è¾“å…¥**ï¼š360ç»´LiDAR + ä½ç»´ç›®æ ‡å‘é‡ã€‚<br>â€¢ **1D-CNNç¼–ç å™¨**ï¼š3å±‚ï¼Œæ ¸å¤§å° `[8, 4, 3]`ï¼Œæ­¥é•¿ `[4, 2, 1]`ï¼Œæ¿€æ´»å‡½æ•° `Tanh`ã€‚<br>â€¢ **å†³ç­–å¤´MLP**ï¼š2ä¸ªéšè—å±‚ï¼Œæ¯å±‚256ä¸ªå•å…ƒã€‚ |\n| **æŠ“å–ä¸“å®¶** | **ç›®æ ‡**ï¼šå®ç°åŸºäºè§†è§‰çš„6è‡ªç”±åº¦ç²¾ç¡®å¯¹é½ã€‚<br>**çŠ¶æ€ç©ºé—´**ï¼š\\( s_{pick} = [s_{rgb}, s_{ego, pick}]^T \\)ï¼Œå…¶ä¸­ \\( s_{rgb} \\in \\mathbb{R}^{84 \\times 84 \\times 3} \\)ï¼ˆRGBå›¾åƒï¼‰ï¼Œ\\( s_{ego, pick} \\) åŒ…å«è´§å‰é«˜åº¦ç­‰çŠ¶æ€ã€‚<br>**ç½‘ç»œæ¶æ„**ï¼šé‡‡ç”¨åŸºäº **NatureCNN** çš„2D-CNNï¼ˆResNetä¸»å¹²ï¼‰ä»RGBå›¾åƒä¸­æå–çº¹ç†ã€è¾¹ç¼˜ç­‰è¯­ä¹‰ç‰¹å¾ã€‚è§†è§‰ç‰¹å¾ä¸æœ¬ä½“çŠ¶æ€èåˆåè¾“å…¥MLPã€‚<br>**å¥–åŠ±å‡½æ•°**ï¼š\\( R_{pick} \\) è¢«æ„é€ ä¸ºä¸€ä¸ª**å§¿æ€æœ€å°åŒ–é—®é¢˜**ï¼Œä½¿ç”¨å››å…ƒæ•°è¿ç®—æ¥ç²¾ç¡®æƒ©ç½šè§’åº¦åå·®ã€‚ | â€¢ **è¾“å…¥**ï¼š84x84x3 RGBå›¾åƒ + è´§å‰é«˜åº¦ã€‚<br>â€¢ **è§†è§‰ç¼–ç å™¨**ï¼šæ ‡å‡†NatureCNNï¼Œ3ä¸ª2Då·ç§¯å±‚ã€‚<br>â€¢ **å†³ç­–å¤´MLP**ï¼š2ä¸ªéšè—å±‚ï¼Œæ¯å±‚256ä¸ªå•å…ƒã€‚ |\n| **æ”¾ç½®ä¸“å®¶** | **ç›®æ ‡**ï¼šå®ç°äºšå˜ç±³çº§ç²¾åº¦çš„å †å æ“ä½œï¼Œå…‹æœâ€œç²¾åº¦ç“¶é¢ˆâ€ã€‚<br>**çŠ¶æ€ç©ºé—´**ï¼š\\( s_{place} \\) ä¸ºä½ç»´å‘é‡ï¼Œè¡¨ç¤ºä»æ„ŸçŸ¥ç³»ç»Ÿå¾—åˆ°çš„ç›¸å¯¹6-DoFä½å§¿è¯¯å·®ã€‚<br>**ç½‘ç»œæ¶æ„**ï¼šç”±äºè¾“å…¥æ˜¯ç´§å‡‘çš„ä½å§¿è¯¯å·®å‘é‡ï¼Œç›´æ¥ä½¿ç”¨**MLPç¼–ç å™¨**å°†å…¶æ˜ å°„ä¸ºç¨ å¯†æ½œåœ¨åµŒå…¥ã€‚<br>**å¥–åŠ±å‡½æ•°ï¼ˆå…³é”®åˆ›æ–°ï¼‰**ï¼šæå‡º**å€’æ•°å¥–åŠ±å…¬å¼**ï¼Œä»¥è§£å†³æ¥è¿‘ç›®æ ‡æ—¶åŸºäºè·ç¦»çš„å¥–åŠ±æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š<br>\\[ R_{place}(s_t) = \\frac{\\lambda_{prec}}{d_{target}(s_t) + \\epsilon_{stab}} + R_{success} \\]<br>å…¶ä¸­ \\( d_{target}(s_t) \\) ä¸ºç»¼åˆæ¬§æ°è·ç¦»å’Œè§’åº¦åå·®ã€‚å½“ \\( d_{target} \\to 0 \\) æ—¶ï¼Œå¥–åŠ±æ¢¯åº¦ \\( \\nabla R_{place} \\approx -\\frac{\\lambda_{prec}}{(d_{target}+\\epsilon_{stab})^2} \\nabla d_{target} \\) ä¼šæ€¥å‰§å¢å¤§ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›å¼ºçƒˆçš„è¿ç»­ä¼˜åŒ–ä¿¡å·ï¼Œé©±åŠ¨å…¶è¿›è¡Œå¾®è°ƒã€‚ | â€¢ **è¾“å…¥**ï¼šä½ç»´6-DoFä½å§¿è¯¯å·®å‘é‡ã€‚<br>â€¢ **ç¼–ç å™¨**ï¼šMLPã€‚<br>â€¢ **å†³ç­–å¤´MLP**ï¼š2ä¸ªéšè—å±‚ï¼Œæ¯å±‚256ä¸ªå•å…ƒã€‚<br>â€¢ **å¥–åŠ±å‚æ•°**ï¼š\\( \\lambda_{prec} \\) ä¸ºç²¾åº¦æƒé‡ç³»æ•°ï¼Œ\\( \\epsilon_{stab} \\) ä¸ºæ•°å€¼ç¨³å®šå¸¸æ•°ã€‚ |\n\n### 2. åˆ†å±‚è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨\n**æ ¸å¿ƒæ€æƒ³**ï¼šä½œä¸ºä¸€ä¸ªé«˜çº§è®¤çŸ¥æ ¸å¿ƒï¼Œå°†ç¦»æ•£ä»»åŠ¡é€»è¾‘ä¸è¿ç»­æ§åˆ¶æ¡¥æ¥èµ·æ¥ï¼Œé€šè¿‡çŠ¶æ€æ„ŸçŸ¥çš„é—­ç¯å†³ç­–ç®¡ç†ä¸“å®¶åºåˆ—çš„æ‰§è¡Œä¸é”™è¯¯æ¢å¤ã€‚\n\n| ç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸æ¶æ„ |\n| :--- | :--- | :--- |\n| **è§„åˆ’å™¨é€»è¾‘** | **å½¢å¼åŒ–**ï¼šè§„åˆ’å™¨ \\( \\pi_H \\) è¢«å»ºæ¨¡ä¸ºä¸€ä¸ª**ç¡®å®šæ€§æœ‰é™çŠ¶æ€è‡ªåŠ¨æœº**ï¼Œåœ¨é«˜å±‚è¯­ä¹‰çŠ¶æ€ \\( s_H \\)ï¼ˆå¦‚æ‹“æ‰‘ä½ç½®ã€è´§ç‰©çŠ¶æ€ç¦»æ•£æ ‡ç­¾ï¼‰ä¸Šæ“ä½œã€‚<br>**æ•°å­¦åŸç†**ï¼šå®ƒå°†é•¿æ—¶ç¨‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹åˆ†è§£ä¸ºä¸€ä¸ªåŠé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€‚åœ¨å†³ç­–æ­¥ \\( t_k \\)ï¼Œè§„åˆ’å™¨æ ¹æ® \\( s_H(t_k) \\) é€‰æ‹©ä¸€ä¸ªä¸“å®¶æŠ€èƒ½ \\( o \\in \\mathcal{O} \\)ã€‚è¯¥ä¸“å®¶æŒç»­æ‰§è¡Œç›´åˆ°å…¶ç»ˆæ­¢æ¡ä»¶ \\( \\beta_o(s_{t_k+k}) \\) è¢«æ»¡è¶³ï¼ˆå­ç›®æ ‡è¾¾æˆæˆ–å®‰å…¨è¿è§„ï¼‰ã€‚è¿™å®ç°äº†æ—¶é—´æŠ½è±¡ï¼Œå¹¶**å°†æ¢ç´¢ç©ºé—´çº¦æŸåœ¨ç‰©ç†ä¸Šåˆç†çš„åºåˆ—ä¸Š**ï¼Œé¿å…äº†æ— æ•ˆæ¢ç´¢ã€‚<br>**ä½œç”¨**ï¼š1) **åºåˆ—åè°ƒ**ï¼šæŒ‰â€œå‡ºå‘â†’æŠ“å–â†’è¿è¾“â†’æ”¾ç½®â€é€»è¾‘æ¿€æ´»ä¸“å®¶ã€‚2) **é”™è¯¯æ¢å¤**ï¼šä¾‹å¦‚ï¼Œå½“æŠ“å–å¤±è´¥ï¼ˆ\\( \\beta_{pick} \\) è¿”å›å¤±è´¥ï¼‰ï¼Œè§„åˆ’å™¨å¯é‡æ–°æ¿€æ´»æŠ“å–ä¸“å®¶ï¼Œè€Œéé”™è¯¯åœ°è¿›å…¥è¿è¾“é˜¶æ®µï¼Œä»è€Œè§£å†³ä¼ ç»Ÿæ¨¡å—åŒ–æ–¹æ³•ä¸­çš„é”™è¯¯ä¼ æ’­é—®é¢˜ã€‚ | â€¢ **çŠ¶æ€è¾“å…¥** \\( s_H \\)ï¼šæŠ½è±¡è¯­ä¹‰ç‰¹å¾ï¼ˆå¦‚ `at_transfer_zone`, `cargo_grabbed`, `near_goal`ï¼‰ã€‚<br>â€¢ **æŠ€èƒ½é›†** \\( \\mathcal{O} \\)ï¼š`{nav, pick, place}`ã€‚<br>â€¢ **ç»ˆæ­¢æ¡ä»¶** \\( \\beta_o \\)ï¼šåŸºäºè·ç¦»é˜ˆå€¼ã€ä¼ æ„Ÿå™¨è§¦å‘æˆ–è¶…æ—¶çš„è§„åˆ™ã€‚ |\n\n### 3. æ··åˆæ¨¡ä»¿-å¼ºåŒ–è®­ç»ƒç­–ç•¥\n**æ ¸å¿ƒæ€æƒ³**ï¼šé‡‡ç”¨â€œæ¨¡ä»¿é¢„çƒ­ï¼Œå¼ºåŒ–å¾®è°ƒâ€çš„ä¸¤é˜¶æ®µèŒƒå¼ï¼Œè§£å†³ä»é›¶å¼€å§‹è®­ç»ƒé•¿æ—¶ç¨‹ä»»åŠ¡çš„**æ¢ç´¢ç¨€ç–æ€§**é—®é¢˜ï¼Œå¹¶çªç ´æ¨¡ä»¿å­¦ä¹ å›ºæœ‰çš„**ç²¾åº¦å¤©èŠ±æ¿**ã€‚\n\n| é˜¶æ®µ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- |\n| **é˜¶æ®µ1ï¼šé€šè¿‡è¡Œä¸ºå…‹éš†è¿›è¡Œæµå½¢åˆå§‹åŒ–** | **ç›®æ ‡**ï¼šåˆ©ç”¨ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼Œå°†ç­–ç•¥ç½‘ç»œæƒé‡åˆå§‹åŒ–ä¸ºä¸€ä¸ª**å®‰å…¨è¿åŠ¨æµå½¢**ï¼Œè·å¾—åŸºç¡€ä»»åŠ¡èƒ½åŠ›ï¼Œé¿å…å¼ºåŒ–å­¦ä¹ æ—©æœŸé˜¶æ®µçš„ç¾éš¾æ€§å¤±è´¥ã€‚<br>**æ•°å­¦åŸç†**ï¼šå¯¹äºæ¯ä¸ªä¸“å®¶ç­–ç•¥ \\( \\pi_\\theta \\)ï¼Œæœ€å°åŒ–å…¶åŠ¨ä½œåˆ†å¸ƒä¸ä¸“å®¶æ¼”ç¤ºåŠ¨ä½œä¹‹é—´çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š<br>\\[ \\mathcal{L}_{BC}(\\theta) = -\\mathbb{E}_{(s,a) \\sim \\mathcal{D}_{expert}} \\left[ \\log \\pi_\\theta(a | s) \\right] \\]<br>å…¶ä¸­ \\( \\mathcal{D}_{expert} \\) ä¸ºç”±è§„åˆ™åŸºä¸“å®¶ç”Ÿæˆçš„æ¼”ç¤ºæ•°æ®é›†ã€‚æ­¤é˜¶æ®µæ—¨åœ¨å¿«é€Ÿæ‹Ÿåˆï¼Œè€Œéè¿‡åº¦è¿½æ±‚ç²¾åº¦ã€‚ | â€¢ **æ•°æ®é›†**ï¼š10,000æ¡æ¼”ç¤ºè½¨è¿¹ã€‚<br>â€¢ **ä¼˜åŒ–å™¨**ï¼šAdamã€‚<br>â€¢ **å­¦ä¹ ç‡** \\( \\alpha_{bc} \\)ï¼š\\( 1 \\times 10^{-3} \\)ã€‚<br>â€¢ **è®­ç»ƒè½®æ•°**ï¼š50è½®ã€‚ |\n| **é˜¶æ®µ2ï¼šé€šè¿‡PPOè¿›è¡Œæ¥è§¦å¼ç²¾ç»†å¾®è°ƒ** | **ç›®æ ‡**ï¼šåœ¨åˆå§‹åŒ–ç­–ç•¥çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ä¸ç¯å¢ƒäº¤äº’è¿›è¡Œ**æ®‹å·®å­¦ä¹ **ï¼Œå‘ç°è¶…è¶Šæ¼”ç¤ºä¸“å®¶èƒ½åŠ›çš„ã€éç›´è§‚çš„å¾®è°ƒç­–ç•¥ï¼Œä»¥å¤„ç†å¤æ‚çš„æ¥è§¦åŠ¨åŠ›å­¦ï¼Œå®ç°é«˜ç²¾åº¦ã€‚<br>**æ•°å­¦åŸç†**ï¼šä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±ï¼š<br>\\[ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t R(s_t, a_t) \\right] \\]<br>PPOé€šè¿‡é‡è¦æ€§é‡‡æ ·å’Œè£å‰ªæœºåˆ¶ï¼Œåœ¨ä¿è¯è®­ç»ƒç¨³å®šæ€§çš„å‰æä¸‹è¿›è¡Œç­–ç•¥æ›´æ–°ï¼š<br>\\[ L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t) \\right] \\]<br>å…¶ä¸­ \\( r_t(\\theta) = \\frac{\\pi_\\theta(a_t\\|s_t)}{\\pi_{\\theta_{old}}(a_t\\|s_t)} \\)ï¼Œ\\( \\hat{A}_t \\) æ˜¯ä¼˜åŠ¿ä¼°è®¡ã€‚<br>**å…³é”®æŠ€æœ¯ç‚¹**ï¼š<br>1. **è¿è¡Œå¥–åŠ±å½’ä¸€åŒ–**ï¼š",
            "timestamp": "2026-01-21T12:13:02.565776",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\næœ¬æ–‡é€šè¿‡é«˜ä¿çœŸä»¿çœŸå®éªŒå¯¹æå‡ºçš„**å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ ï¼ˆHMERï¼‰æ¡†æ¶**è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å…¶è¯„ä»·ä½“ç³»å›´ç»•**ä»»åŠ¡æˆåŠŸç‡ã€æ•ˆç‡ã€ç²¾åº¦å’Œé²æ£’æ€§**å››å¤§æ ¸å¿ƒç»´åº¦æ„å»ºï¼Œå¹¶åœ¨ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ä»¿çœŸæ•°æ®é›†ä¸­è¿›è¡ŒéªŒè¯ã€‚\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡è¯¦æƒ…\n\nä¸‹è¡¨æ€»ç»“äº†è®ºæ–‡ä¸­ä½¿ç”¨çš„æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡åŠå…¶å…·ä½“å«ä¹‰ï¼š\n\n| è¯„ä»·æŒ‡æ ‡ | ç¬¦å·/ç®€ç§° | è®¡ç®—æ–¹å¼ä¸ç»†èŠ‚ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **ä»»åŠ¡æˆåŠŸç‡** | Task Success Rate (â†‘) | åœ¨å›ºå®šæ•°é‡çš„è¯„ä¼°å›åˆï¼ˆå¦‚500æ¬¡ï¼‰ä¸­ï¼ŒæˆåŠŸå®Œæˆæ•´ä¸ªâ€œå‡ºå‘-æ‹¾å–-è¿è¾“-æ”¾ç½®â€å¾ªç¯çš„å›åˆæ¯”ä¾‹ã€‚**æˆåŠŸæ ‡å‡†**ï¼šåœ¨æ— ç¢°æ’ã€æœªè¶…æ—¶çš„å‰æä¸‹ï¼Œæœ€ç»ˆæ”¾ç½®è¯¯å·®æ»¡è¶³å·¥ä¸šå®¹å·®ï¼ˆå¦‚ < 2.0 cmï¼‰ã€‚ | è¡¡é‡æ¡†æ¶å®Œæˆ**é•¿è§†é‡ã€å¤šç›®æ ‡ä»»åŠ¡**çš„æ•´ä½“å¯é æ€§å’Œé²æ£’æ€§ã€‚æ˜¯æœ€é«˜å±‚æ¬¡çš„ç»¼åˆæ€§èƒ½æŒ‡æ ‡ã€‚ |\n| **å¹³å‡å¾ªç¯æ—¶é—´** | Avg. Cycle Time (â†“) | ä»ä»»åŠ¡å¼€å§‹ï¼ˆç¦»å¼€åœé ç«™ï¼‰åˆ°æˆåŠŸæ”¾ç½®è´§ç‰©æ‰€ç»å†çš„å¹³å‡æ—¶é—´ï¼ˆå•ä½ï¼šç§’ï¼‰ã€‚åœ¨æ¯”è¾ƒä¸­ï¼Œ**HMER (42.5s) æ¯”åŸºäºè§„åˆ™çš„ä¸“å®¶ (55.6s) å¿«23.6%**ã€‚ | è¯„ä¼°ç³»ç»Ÿçš„**æ“ä½œæ•ˆç‡**ã€‚æ›´çŸ­çš„æ—¶é—´æ„å‘³ç€æ›´é«˜çš„ä»“åº“ååé‡ï¼Œå…·æœ‰ç›´æ¥çš„ç»æµä»·å€¼ã€‚ |\n| **å¹³å‡æ”¾ç½®è¯¯å·®** | Mean Placement Error (â†“) | ä»»åŠ¡æˆåŠŸæ—¶ï¼Œè´§ç‰©æœ€ç»ˆä½å§¿ä¸ç›®æ ‡ä½å§¿ä¹‹é—´çš„**ç»¼åˆè¯¯å·®**ã€‚é€šå¸¸è®¡ç®—**6è‡ªç”±åº¦ä½å§¿è¯¯å·®**ï¼ŒåŒ…æ‹¬**å¹³ç§»ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰å’Œæ—‹è½¬ï¼ˆè§’åº¦åå·®ï¼‰**ã€‚è®ºæ–‡ä¸­HMERè¾¾åˆ° **1.5 cm** çš„å¹³å‡è¯¯å·®ã€‚ | è¡¡é‡**å¾®æ“ä½œç²¾åº¦**ï¼Œæ˜¯å·¥ä¸šåº”ç”¨ï¼ˆå¦‚ç²¾å‡†å †å ï¼‰çš„**å…³é”®å®‰å…¨ä¸æŠ€æœ¯æŒ‡æ ‡**ã€‚è¯æ˜äº†å…¶è¶…è¶Šå¯å‘å¼ä¸“å®¶çš„èƒ½åŠ›ã€‚ |\n| **ç¢°æ’ç‡** | Collision Rate (â†“) | è¯„ä¼°å›åˆä¸­å‘ç”Ÿä¸ç¯å¢ƒï¼ˆé™æ€è´§æ¶ã€åŠ¨æ€éšœç¢ç‰©ï¼‰æˆ–è‡ªèº«ç¢°æ’çš„å›åˆæ¯”ä¾‹ã€‚**Seq-HybridåŸºçº¿å› æ— è¯­ä¹‰è§„åˆ’å™¨ï¼Œç¢°æ’ç‡é«˜è¾¾28.4%**ï¼Œè€ŒHMERä»…ä¸º2.1%ã€‚ | è¯„ä¼°ç³»ç»Ÿçš„**å®‰å…¨æ€§**å’Œ**åœ¨åŠ¨æ€éç»“æ„åŒ–ç¯å¢ƒä¸­çš„é¿éšœèƒ½åŠ›**ã€‚ä½ç¢°æ’ç‡æ˜¯å®é™…éƒ¨ç½²çš„å‰æã€‚ |\n| **è®­ç»ƒæ ·æœ¬æ•ˆç‡** | Training Sample Efficiency | é€šè¿‡ç»˜åˆ¶â€œä»»åŠ¡æˆåŠŸç‡ vs. ç¯å¢ƒäº¤äº’æ­¥æ•°â€æ›²çº¿ï¼ˆå¦‚å›¾5ï¼‰è¿›è¡Œåˆ†æã€‚**HMERä»…ç”¨450ä¸‡æ­¥è¾¾åˆ°å³°å€¼æ€§èƒ½ï¼Œæ¯”ä»é›¶å¼€å§‹çš„HRLï¼ˆ800ä¸‡æ­¥ï¼‰èŠ‚çœ43.7%çš„æ ·æœ¬**ã€‚ | éªŒè¯**æ··åˆæ¨¡ä»¿-å¼ºåŒ–è®­ç»ƒç­–ç•¥**åœ¨è§£å†³**ç¨€ç–å¥–åŠ±æ¢ç´¢**å’Œ**å†·å¯åŠ¨é—®é¢˜**ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ |\n| **ç²¾åº¦åˆ†å¸ƒ** | Accuracy Distribution | é€šè¿‡**ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å›¾**ï¼ˆå¦‚å›¾6ï¼‰å±•ç¤ºï¼Œä¾‹å¦‚â€œ**88%çš„HMERæˆåŠŸå›åˆè¯¯å·®å°äº2.0 cm**â€ï¼Œè€ŒåŸºäºè§„åˆ™çš„ä¸“å®¶ä»…25%ã€‚ | æ›´ç»†è‡´åœ°å±•ç¤ºç³»ç»Ÿç²¾åº¦çš„**ç»Ÿè®¡åˆ†å¸ƒå’Œä¸€è‡´æ€§**ï¼Œè€Œä¸ä»…ä»…æ˜¯å¹³å‡å€¼ï¼Œæ›´èƒ½è¯´æ˜å…¶å·¥ä¸šå¯é æ€§ã€‚ |\n\n### äºŒã€ æ•°æ®é›†ä¸å®éªŒç¯å¢ƒç»†èŠ‚\n\næœ¬æ–‡çš„ç ”ç©¶å®Œå…¨åœ¨**ä»¿çœŸç¯å¢ƒ**ä¸­è¿›è¡Œï¼Œæœªä½¿ç”¨å…¬å¼€çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ã€‚å…¶â€œæ•°æ®é›†â€æœ¬è´¨ä¸Šæ˜¯**åœ¨è‡ªå®šä¹‰çš„é«˜ä¿çœŸä»¿çœŸç¯å¢ƒä¸­ç”Ÿæˆçš„äº¤äº’ç»éªŒ**ã€‚\n\n| ç±»åˆ« | åç§°/ç±»å‹ | å…·ä½“ç»†èŠ‚ä¸é…ç½® | ä½œç”¨ä¸ç‰¹ç‚¹ |\n| :--- | :--- | :--- | :--- |\n| **ä»¿çœŸå¹³å°** | Gazebo 11 + ROS Noetic | - **ç‰©ç†å¼•æ“**ï¼šOpen Dynamics Engine (ODE)ï¼Œç”¨äºç²¾ç¡®æ¨¡æ‹Ÿæ¥è§¦åŠ¨åŠ›å­¦ï¼ˆå¦‚å‰é½¿ä¸æ£‰åŒ…çš„å˜å½¢äº¤äº’ï¼‰ã€‚<br>- **ç‰©ç†æ—¶é—´æ­¥é•¿**ï¼š0.001ç§’ï¼Œç¡®ä¿ä»¿çœŸè¶³å¤Ÿç²¾ç»†ã€‚<br>- **æ§åˆ¶é¢‘ç‡**ï¼š10 Hzã€‚ | æ„å»ºäº†ä¸€ä¸ª**å¯é‡å¤ã€å¯æ§ã€é«˜ä¿çœŸ**çš„æµ‹è¯•ç¯å¢ƒï¼Œä»¥æ¨¡æ‹ŸçœŸå®å‰è½¦çš„è¿åŠ¨å­¦å’ŒåŠ¨åŠ›å­¦ï¼Œç‰¹åˆ«æ˜¯**æ¥è§¦ç‰©ç†**ã€‚ |\n| **è®­ç»ƒç¯å¢ƒ** | å¹¶è¡ŒåŒ–ä»¿çœŸå®ä¾‹ | - **å¹¶è¡Œç¯å¢ƒæ•°**ï¼š32ä¸ªã€‚é€šè¿‡Dockerå®¹å™¨åŒ–å®ç°ã€‚<br>- **ç¡¬ä»¶**ï¼šåŒNVIDIA RTX 4090 GPU + Intel i9-14900K CPUã€‚<br>- **é‡‡æ ·é€Ÿåº¦**ï¼šçº¦ **25ä¸‡æ­¥/å°æ—¶**ã€‚ | **å¤§å¹…åŠ é€Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹**ï¼Œä½¿å¾—åœ¨åˆç†æ—¶é—´å†…è®­ç»ƒé•¿è§†é‡ä»»åŠ¡æˆä¸ºå¯èƒ½ã€‚ |\n| **ä»»åŠ¡åœºæ™¯** | åŠ¨æ€éç»“æ„åŒ–ä»“åº“ | - **åœºæ™¯å¤§å°**ï¼š20m Ã— 20m çš„æ¨¡æ‹Ÿä»“åº“ã€‚<br>- **è¦ç´ **ï¼šé™æ€å­˜å‚¨è´§æ¶ã€éšæœºåˆå§‹åŒ–çš„ç›®æ ‡è´§ç‰©ã€**åŠ¨æ€éšœç¢ç‰©**ã€‚<br>- **ä»»åŠ¡**ï¼šå®Œæ•´çš„â€œå‡ºå‘-æœç´¢æ‹¾å–-è¿è¾“-ç²¾å‡†æ”¾ç½®â€å››é˜¶æ®µå¾ªç¯ã€‚ | æ¨¡æ‹Ÿäº†çœŸå®ç‰©æµä»“åº“çš„**æ ¸å¿ƒæŒ‘æˆ˜**ï¼šå¤§èŒƒå›´å¯¼èˆªã€åŠ¨æ€é¿éšœã€è§†è§‰æœç´¢ã€å˜ç±³çº§æ“ä½œã€‚ |\n| **é¢†åŸŸéšæœºåŒ–** | Domain Randomization | æ¯ä¸ªè®­ç»ƒå›åˆå¼€å§‹æ—¶ï¼ŒéšæœºåŒ–ä»¥ä¸‹å‚æ•°ï¼š<br>1. **ç‰©ç†å‚æ•°**ï¼šè´§ç‰©è´¨é‡ã€åœ°é¢æ‘©æ“¦ç³»æ•°ã€‚<br>2. **åœºæ™¯å¸ƒå±€**ï¼šéšœç¢ç‰©åˆ†å¸ƒã€è´§ç‰©åˆå§‹ä½å§¿(`P_obj`)ã€ç›®æ ‡ä½å§¿(`P_goal`)ã€‚ | **å¢å¼ºç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§**ï¼Œä½¿å…¶ä¸ä¾èµ–äºç‰¹å®šç¯å¢ƒé…ç½®ï¼Œä¸ºâ€œä»¿çœŸåˆ°ç°å®â€è¿ç§»æ‰“ä¸‹åŸºç¡€ã€‚ |\n| **ä¸“å®¶æ¼”ç¤ºæ•°æ®é›†** | ç”¨äºè¡Œä¸ºå…‹éš†çš„è½¨è¿¹é›† | - **ç”Ÿæˆæ–¹å¼**ï¼šç”±**åŸºäºè§„åˆ™çš„ä¸“å®¶ç³»ç»Ÿ**ï¼ˆA*è·¯å¾„è§„åˆ’ + å¯å‘å¼è§†è§‰ä¼ºæœï¼‰åœ¨ä»¿çœŸä¸­è¿è¡Œäº§ç”Ÿã€‚<br>- **æ•°æ®é›†è§„æ¨¡**ï¼š**10,000æ¡æˆåŠŸè½¨è¿¹**ã€‚<br>- **ç”¨é€”**ï¼šç”¨äº**æ··åˆè®­ç»ƒç­–ç•¥çš„ç¬¬ä¸€é˜¶æ®µï¼ˆè¡Œä¸ºå…‹éš†ï¼‰**ï¼Œä¸ºç­–ç•¥ç½‘ç»œæä¾›å®‰å…¨çš„è¿åŠ¨å­¦æµå½¢åˆå§‹åŒ–ã€‚ | æä¾›äº†å¯é çš„**å…ˆéªŒçŸ¥è¯†**ï¼Œè§£å†³å¼ºåŒ–å­¦ä¹ ä»é›¶å¼€å§‹çš„æ¢ç´¢éš¾é¢˜ã€‚ä½†è¯¥æ•°æ®é›†çš„æ€§èƒ½å­˜åœ¨**ç²¾åº¦ç“¶é¢ˆ**ï¼ˆ~4cmè¯¯å·®ï¼‰ã€‚ |\n| **è¯„ä¼°åè®®** | å›ºå®šæ¬¡æ•°çš„ç‹¬ç«‹æµ‹è¯• | - **è¯„ä¼°å›åˆæ•°**ï¼šæ¯ä¸ªå¯¹æ¯”æ–¹æ³•è¿›è¡Œ **500æ¬¡** ç‹¬ç«‹æµ‹è¯•ã€‚<br>- **éšæœºåŒ–**ï¼šæ¯æ¬¡æµ‹è¯•çš„èµ·å§‹ç‚¹ã€è´§ç‰©ä½ç½®ã€ç›®æ ‡ä½ç½®å‡éšæœºç”Ÿæˆã€‚<br>- **ç»Ÿè®¡æŒ‡æ ‡**ï¼šè®¡ç®—ä¸Šè¿°æ‰€æœ‰è¯„ä»·æŒ‡æ ‡çš„å‡å€¼ã€åˆ†å¸ƒç­‰ã€‚ | ç¡®ä¿æ€§èƒ½æ¯”è¾ƒçš„**ç»Ÿè®¡æ˜¾è‘—æ€§å’Œå…¬å¹³æ€§**ï¼Œå…¨é¢åæ˜ å„æ–¹æ³•åœ¨å¤šå˜åœºæ™¯ä¸‹çš„å¹³å‡è¡¨ç°ã€‚ |\n\n### æ€»ç»“\nè¯¥è®ºæ–‡çš„è¯„ä»·ä½“ç³»**ç´§å¯†ç»“åˆå·¥ä¸šåº”ç”¨éœ€æ±‚**ï¼Œä¸ä»…å…³æ³¨æœ€ç»ˆçš„æˆåŠŸç‡ï¼Œæ›´æ·±å…¥è¯„ä¼°äº†æ•ˆç‡ï¼ˆæ—¶é—´ï¼‰ã€ç²¾åº¦ï¼ˆè¯¯å·®ï¼‰å’Œå®‰å…¨ï¼ˆç¢°æ’ï¼‰ç­‰å…³é”®å®æ“æŒ‡æ ‡ã€‚å…¶â€œæ•°æ®é›†â€æ˜¯ä¸€ä¸ªé€šè¿‡**é«˜åº¦å·¥ç¨‹åŒ–çš„ä»¿çœŸç¯å¢ƒ**å’Œ**é¢†åŸŸéšæœºåŒ–æŠ€æœ¯**ç”Ÿæˆçš„åŠ¨æ€äº¤äº’ç¯å¢ƒï¼Œæ—¨åœ¨å…¨é¢ã€å¯æ§åœ°éªŒè¯ç®—æ³•åœ¨é€¼è¿‘çœŸå®åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚è¿™ç§åŸºäºä»¿çœŸçš„è¯„ä¼°æ–¹æ³•ï¼Œä¸ºåç»­çš„â€œä»¿çœŸåˆ°ç°å®â€è¿ç§»ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚",
            "timestamp": "2026-01-21T12:13:52.674075",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒä¸è¯„ä¼°åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†ä¸ä»¿çœŸç¯å¢ƒ\nè®ºæ–‡**æœªä½¿ç”¨**ä¼ ç»Ÿçš„ç¦»çº¿æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚æ‰€æœ‰å®éªŒå‡åœ¨**é«˜ä¿çœŸä»¿çœŸç¯å¢ƒ**ä¸­å®Œæˆï¼Œå…·ä½“é…ç½®å¦‚ä¸‹ï¼š\n\n- **ä»¿çœŸå¹³å°**ï¼šGazebo 11ï¼ˆé›†æˆROS Noeticï¼‰ã€‚\n- **ç‰©ç†å¼•æ“**ï¼šOpen Dynamics Engine (ODE)ï¼Œæ—¶é—´æ­¥é•¿ Î”t = 0.001ç§’ã€‚\n- **ç¯å¢ƒè®¾ç½®**ï¼š\n    - æ¨¡æ‹Ÿä¸€ä¸ª **20m Ã— 20m** çš„åŠ¨æ€ä»“åº“åœºæ™¯ã€‚\n    - åŒ…å«é™æ€è´§æ¶ã€éšæœºåˆ†å¸ƒçš„åŠ¨æ€éšœç¢ç‰©ã€‚\n    - è´§ç‰©ï¼ˆæ¨¡æ‹Ÿæ£‰åŒ…ï¼‰çš„åˆå§‹ä½ç½®ã€è´¨é‡ã€åœ°é¢æ‘©æ“¦ç³»æ•°ç­‰å‚æ•°åœ¨æ¯è½®è®­ç»ƒä¸­**éšæœºåŒ–**ï¼ˆDomain Randomizationï¼‰ï¼Œä»¥å¢å¼ºç­–ç•¥çš„é²æ£’æ€§ã€‚\n- **è®­ç»ƒæ¶æ„**ï¼šé‡‡ç”¨**å¹¶è¡ŒåŒ–è®­ç»ƒ**ï¼Œåœ¨32ä¸ªåŒæ­¥ä»¿çœŸå®ä¾‹ä¸­è¿›è¡Œï¼Œä»¥åŠ é€Ÿæ ·æœ¬æ”¶é›†ï¼ˆçº¦25ä¸‡æ­¥/å°æ—¶ï¼‰ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é€šè¿‡ä»¥ä¸‹å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡å…¨é¢è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼š\n\n| æŒ‡æ ‡ | è¯´æ˜ | ä¼˜åŒ–æ–¹å‘ |\n| :--- | :--- | :--- |\n| **ä»»åŠ¡æˆåŠŸç‡** | æˆåŠŸå®Œæˆâ€œå‡ºå‘-æ‹¾å–-è¿è¾“-æ”¾ç½®â€å®Œæ•´å‘¨æœŸçš„æ¯”ä¾‹ã€‚ | è¶Šé«˜è¶Šå¥½ (â†‘) |\n| **å¹³å‡å‘¨æœŸæ—¶é—´** | å®Œæˆä¸€ä¸ªå®Œæ•´ç‰©æ–™æ¬è¿ä»»åŠ¡æ‰€éœ€çš„å¹³å‡æ—¶é—´ï¼ˆç§’ï¼‰ã€‚ | è¶Šä½è¶Šå¥½ (â†“) |\n| **å¹³å‡æ”¾ç½®è¯¯å·®** | è´§ç‰©æœ€ç»ˆæ”¾ç½®ä½ç½®ä¸ç›®æ ‡ä½ç½®ä¹‹é—´çš„æ¬§æ°è·ç¦»ï¼ˆå˜ç±³ï¼‰ã€‚ | è¶Šä½è¶Šå¥½ (â†“) |\n| **ç¢°æ’ç‡** | æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿç¢°æ’çš„ä»»åŠ¡æ¯”ä¾‹ã€‚ | è¶Šä½è¶Šå¥½ (â†“) |\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸å¯¹æ¯”ç»“æœ\nè®ºæ–‡å°†æå‡ºçš„ **HMERæ¡†æ¶** ä¸5ä¸ªåŸºçº¿æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”å®éªŒï¼ˆåŸºäº500æ¬¡éšæœºåŒ–æµ‹è¯•ï¼‰ã€‚**HMERåœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šå‡å–å¾—æ˜¾è‘—æå‡**ã€‚\n\n**ä¸»è¦æ€§èƒ½æå‡æ€»ç»“å¦‚ä¸‹ï¼š**\n\n1.  **ä»»åŠ¡æˆåŠŸç‡**ï¼š\n    - **HMER**: **94.2%**\n    - **æœ€ä½³åŸºçº¿ (HRL)**: 88.1%\n    - **ä¼ ç»Ÿè§„åˆ™æ–¹æ³•**: 84.2%\n    - **æ€§èƒ½æå‡**: HMERæ¯”æœ€ä½³åŸºçº¿**æå‡6.1ä¸ªç™¾åˆ†ç‚¹**ï¼Œæ¯”ä¼ ç»Ÿè§„åˆ™æ–¹æ³•**æå‡10ä¸ªç™¾åˆ†ç‚¹**ã€‚\n\n2.  **æ“ä½œæ•ˆç‡**ï¼š\n    - **HMERå¹³å‡å‘¨æœŸæ—¶é—´**: **42.5ç§’**\n    - **è§„åˆ™åŸºçº¿æ—¶é—´**: 55.6ç§’\n    - **æ•ˆç‡æå‡**: å‘¨æœŸæ—¶é—´**å‡å°‘21.4%**ã€‚è¿™å¾—ç›ŠäºRLä¼˜åŒ–å‡ºçš„æ›´ä¼˜è·¯å¾„å’Œæ›´æµç•…çš„æ“ä½œç­–ç•¥ã€‚\n\n3.  **æ”¾ç½®ç²¾åº¦**ï¼š\n    - **HMERå¹³å‡æ”¾ç½®è¯¯å·®**: **1.5å˜ç±³**\n    - **è§„åˆ™åŸºçº¿è¯¯å·®**: 4.1å˜ç±³\n    - **ç²¾åº¦æå‡**: è¯¯å·®**å‡å°‘63.4%**ï¼ŒæˆåŠŸè¾¾åˆ°å·¥ä¸šçº§äºšå˜ç±³ç²¾åº¦è¦æ±‚ï¼ˆ<2.0å˜ç±³ï¼‰ã€‚è¯¯å·®åˆ†å¸ƒåˆ†ææ˜¾ç¤ºï¼ŒHMERåœ¨88%çš„æˆåŠŸä»»åŠ¡ä¸­è¯¯å·®å°äº2å˜ç±³ï¼Œè€Œè§„åˆ™æ–¹æ³•ä»…ä¸º25%ã€‚\n\n4.  **è®­ç»ƒæ•ˆç‡**ï¼š\n    - HMERé‡‡ç”¨æ··åˆè®­ç»ƒç­–ç•¥ï¼Œä»…éœ€**450ä¸‡æ­¥**å³å¯æ”¶æ•›è‡³å³°å€¼æ€§èƒ½ã€‚\n    - ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»é›¶å¼€å§‹çš„å¼ºåŒ–å­¦ä¹ åŸºçº¿éœ€è¦çº¦**800ä¸‡æ­¥**ã€‚\n    - **æ ·æœ¬æ•ˆç‡æå‡**: è¾¾åˆ°ç›¸åŒæ€§èƒ½æ‰€éœ€çš„äº¤äº’æ­¥æ•°**å‡å°‘çº¦43.7%**ï¼Œæœ‰æ•ˆè§£å†³äº†é•¿è§†é‡ä»»åŠ¡çš„â€œå†·å¯åŠ¨â€æ¢ç´¢éš¾é¢˜ã€‚\n\n### å››ã€å…³é”®ç»“è®º\n- **æŠ€æœ¯åˆ›æ–°ä»·å€¼**ï¼šHMERæ¡†æ¶é€šè¿‡**å¼‚æ„ä¸“å®¶ç»“æ„åˆ†è§£**å’Œ**æ··åˆæ¨¡ä»¿-å¼ºåŒ–è®­ç»ƒç­–ç•¥**ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†ç§»åŠ¨æ“ä½œä¸­â€œæ¨¡æ€å¹²æ‰°â€å’Œâ€œç¨€ç–å¥–åŠ±â€ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚\n- **å®é™…åº”ç”¨ä»·å€¼**ï¼šå®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…èƒ½**å¤§å¹…æå‡ä»»åŠ¡æˆåŠŸç‡å’Œæ•ˆç‡**ï¼Œæ›´èƒ½**çªç ´å¯å‘å¼æ¼”ç¤ºçš„æ€§èƒ½ä¸Šé™**ï¼Œå®ç°æ»¡è¶³ä¸¥æ ¼å·¥ä¸šæ ‡å‡†ï¼ˆ1.5å˜ç±³ï¼‰çš„é«˜ç²¾åº¦æ“ä½œï¼Œä¸ºæ— äººå‰è½¦åœ¨éç»“æ„åŒ–ä»“åº“ä¸­çš„å®é™…éƒ¨ç½²æä¾›äº†å¼ºæœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
            "timestamp": "2026-01-21T12:14:20.989196",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…æ˜ç¡®æŒ‡å‡ºäº†æ‰€æå‡ºçš„**å¼‚æ„å¤šä¸“å®¶å¼ºåŒ–å­¦ä¹ ï¼ˆHMERï¼‰æ¡†æ¶**å­˜åœ¨çš„å‡ ä¸ªå…³é”®å±€é™æ€§ï¼š\n\n- **éªŒè¯ç¯å¢ƒå±€é™äºä»¿çœŸ**ï¼šæ‰€æœ‰å®éªŒå‡åœ¨**Gazeboé«˜ä¿çœŸä»¿çœŸç¯å¢ƒ**ä¸­è¿›è¡Œã€‚è™½ç„¶é€šè¿‡ç²¾ç»†çš„ç‰©ç†å¼•æ“ï¼ˆODEï¼‰å’Œé¢†åŸŸéšæœºåŒ–æŠ€æœ¯åŠ›æ±‚çœŸå®ï¼Œä½†**ç°å®ä¸–ç•Œçš„ä¸ç¡®å®šæ€§**ï¼ˆå¦‚ä¼ æ„Ÿå™¨å»¶è¿Ÿã€åœ°é¢æ‘©æ“¦ä¸å‡ã€è§†è§‰å™ªå£°ã€æœºæ¢°ç£¨æŸç­‰ï¼‰å¯èƒ½å¼•å…¥æ€§èƒ½å·®è·ï¼Œå³å­˜åœ¨â€œ**ä»¿çœŸåˆ°ç°å®çš„é¸¿æ²Ÿ**â€ã€‚\n\n- **é«˜å±‚ä»»åŠ¡è§„åˆ’å™¨çš„èƒ½åŠ›æœ‰é™**ï¼šå½“å‰çš„**è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨**åŸºäº**ç¦»æ•£é€»è¾‘çš„æœ‰é™çŠ¶æ€æœº**è¿è¡Œã€‚å®ƒè™½ç„¶èƒ½è¿›è¡ŒçŠ¶æ€æ„ŸçŸ¥å’Œé”™è¯¯æ¢å¤ï¼ˆå¦‚é‡è¯•å¤±è´¥çš„æŠ“å–ï¼‰ï¼Œä½†**ç¼ºä¹å­¦ä¹ å’Œåˆ›é€ èƒ½åŠ›**ï¼š\n    - æ— æ³•å­¦ä¹ **è¿ç»­**çš„æ¢å¤ç­–ç•¥ã€‚\n    - æ— æ³•ç”Ÿæˆ**é¢„å®šä¹‰è¯­ä¹‰è½¬æ¢ä¹‹å¤–**çš„åˆ›é€ æ€§è§£å†³æ–¹æ¡ˆã€‚\n    - å…¶å†³ç­–é€»è¾‘æ˜¯ç¡¬ç¼–ç çš„ï¼Œä¸å…·å¤‡ä»ç»éªŒä¸­è‡ªæˆ‘ä¼˜åŒ–çš„èƒ½åŠ›ã€‚\n\n- **ç³»ç»ŸèŒƒå›´çš„é™åˆ¶**ï¼š\n    - **å•æ™ºèƒ½ä½“è®¾å®š**ï¼šå½“å‰æ¡†æ¶ä»…é’ˆå¯¹å•ä¸ªè‡ªä¸»å‰è½¦è¿›è¡Œä¼˜åŒ–ï¼Œæœªè€ƒè™‘å¤šæ™ºèƒ½ä½“åä½œåœºæ™¯ä¸­å¯èƒ½å‡ºç°çš„**äº¤é€šæ‹¥å µã€ä»»åŠ¡åˆ†é…å’Œé¿è®©**ç­‰å¤æ‚é—®é¢˜ã€‚\n    - **ä¾èµ–ä¸“å®¶æ¼”ç¤ºæ•°æ®**ï¼šæ··åˆè®­ç»ƒç­–ç•¥çš„ç¬¬ä¸€é˜¶æ®µï¼ˆè¡Œä¸ºå…‹éš†ï¼‰éœ€è¦é«˜è´¨é‡çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®é›†ã€‚æ•°æ®æ”¶é›†çš„æˆæœ¬å’Œæ¼”ç¤ºç­–ç•¥æœ¬èº«çš„è´¨é‡ä¸Šé™å¯èƒ½æˆä¸ºå®é™…éƒ¨ç½²çš„ç“¶é¢ˆã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†æ˜ç¡®è§„åˆ’äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸»è¦é›†ä¸­åœ¨ä»¥ä¸‹ä¸¤ç‚¹ï¼š\n\n1.  **ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»**ï¼š\n    - åº”ç”¨**é¢†åŸŸè‡ªé€‚åº”**å’Œ**é¢†åŸŸéšæœºåŒ–**æŠ€æœ¯ï¼Œå°†å­¦ä¹ åˆ°çš„ç­–ç•¥ï¼ˆç‰¹åˆ«æ˜¯è§†è§‰ç¼–ç å™¨ï¼‰è¿ç§»åˆ°çœŸå®çš„å‰è½¦ç¡¬ä»¶å¹³å°ã€‚\n    - é‡ç‚¹è§£å†³**è§†è§‰åŸŸå·®å¼‚**ï¼ˆå…‰ç…§ã€çº¹ç†ï¼‰å’Œ**åŠ¨åŠ›å­¦åŸŸå·®å¼‚**ï¼ˆæ‘©æ“¦ã€æƒ¯æ€§ï¼‰å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»¥ç¼©å°ä»¿çœŸä¸ç°å®çš„æ€§èƒ½å·®è·ã€‚\n\n2.  **æ‰©å±•è‡³å¤šæ™ºèƒ½ä½“ååŒ**ï¼š\n    - å°†æ¡†æ¶æ‰©å±•è‡³**å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ **ï¼Œä»¥åè°ƒä¸€ä¸ª**è‡ªä¸»å‰è½¦è½¦é˜Ÿ**ã€‚\n    - ç ”ç©¶ç›®æ ‡åŒ…æ‹¬ï¼šä¼˜åŒ–å…¨å±€ä»“åº“ååé‡ã€åŠ¨æ€ä»»åŠ¡åˆ†é…ã€åœ¨å…±äº«å·¥ä½œç©ºé—´ä¸­ç®¡ç†äº¤é€šæ‹¥å µã€å®ç°é«˜æ•ˆçš„é¿ç¢°ä¸åä½œæ¬è¿ã€‚\n\n### æ½œåœ¨çš„å…¶ä»–æ”¹è¿›æ–¹å‘ï¼ˆåŸºäºå±€é™æ€§åˆ†æå¼•ç”³ï¼‰\né™¤äº†è®ºæ–‡æ˜ç¡®æŒ‡å‡ºçš„ä¸¤ç‚¹ï¼Œä»ä¸Šè¿°å±€é™æ€§è¿˜å¯ä»¥æ¨å¯¼å‡ºå…¶ä»–å¯èƒ½çš„æ”¹è¿›æ–¹å‘ï¼š\n\n3.  **å¢å¼ºé«˜å±‚è§„åˆ’å™¨çš„æ™ºèƒ½**ï¼š\n    - å°†é«˜å±‚è¯­ä¹‰ä»»åŠ¡è§„åˆ’å™¨æ›¿æ¢æˆ–å¢å¼ºä¸º**å¯å­¦ä¹ çš„ç­–ç•¥**ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ æˆ–å¤§è¯­è¨€æ¨¡å‹çš„è§„åˆ’å™¨ï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿä»äº¤äº’ä¸­å­¦ä¹ æ›´çµæ´»ã€æ›´ä¼˜çš„ä»»åŠ¡åˆ†è§£å’Œæ¢å¤ç­–ç•¥ã€‚\n\n4.  **å‡å°‘å¯¹ä¸“å®¶æ•°æ®çš„ä¾èµ–**ï¼š\n    - æ¢ç´¢æ›´é«˜æ•ˆçš„**æ— ç¤ºèŒƒæˆ–å°‘é‡ç¤ºèŒƒ**çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥é™ä½æ•°æ®æ”¶é›†æˆæœ¬ã€‚\n    - ç ”ç©¶**è‡ªç›‘ç£å­¦ä¹ **æ–¹æ³•ï¼Œä»å¤§é‡æ— æ ‡ç­¾çš„äº¤äº’æ•°æ®ä¸­é¢„è®­ç»ƒæ„ŸçŸ¥æ¨¡å—ã€‚\n\n5.  **æå‡ç³»ç»Ÿçš„è‡ªé€‚åº”ä¸é²æ£’æ€§**ï¼š\n    - å¼•å…¥**åœ¨çº¿å­¦ä¹ **æˆ–**å…ƒå­¦ä¹ **æœºåˆ¶ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨éƒ¨ç½²åæŒç»­é€‚åº”æ–°çš„ç¯å¢ƒå˜åŒ–æˆ–æœªçŸ¥æ‰°åŠ¨ã€‚\n    - å¼€å‘æ›´å¼ºå¤§çš„**æ•…éšœæ£€æµ‹ä¸æ¢å¤**æ¨¡å—ï¼Œä»¥å¤„ç†æ›´å¹¿æ³›çš„å¼‚å¸¸æƒ…å†µã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡å·¥ä½œçš„ä¸»è¦å±€é™æ€§åœ¨äºå…¶**ä»¿çœŸéªŒè¯çš„è¾¹ç•Œ**å’Œ**é«˜å±‚è§„åˆ’å™¨çš„å›ºå®šé€»è¾‘**ã€‚æœªæ¥çš„æ ¸å¿ƒæ”¹è¿›è·¯å¾„æ˜¯**çªç ´ä»¿çœŸå£å’å®ç°ç°å®éƒ¨ç½²**ï¼Œå¹¶**ä»å•æ™ºèƒ½ä½“æ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ**ï¼Œä»¥è§£å†³å®é™…ç‰©æµä»“åº“ä¸­çš„è§„æ¨¡åŒ–ååŒä½œä¸šé—®é¢˜ã€‚",
            "timestamp": "2026-01-21T12:14:47.352490",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T08:27:24Z",
    "created_at": "2026-01-21T12:09:08.870622",
    "updated_at": "2026-01-21T12:09:08.870630"
}