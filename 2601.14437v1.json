{
    "id": "2601.14437v1",
    "title": "Agentic AI Meets Edge Computing in Autonomous UAV Swarms",
    "authors": [
        "Thuan Minh Nguyen",
        "Vu Tuan Truong",
        "Long Bao Le"
    ],
    "abstract": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ï¼Œå‡­å€Ÿå…¶è‡ªä¸»æ¨ç†ã€è§„åˆ’ä¸æ‰§è¡Œèƒ½åŠ›ï¼Œä¸æ— äººæœºé›†ç¾¤çš„èåˆä¸ºæ–°å‹ä½œä¸šæ¨¡å¼åˆ›é€ äº†å¯èƒ½ï¼Œä¹Ÿä½¿æ— äººæœºç‰©è”ç½‘çš„æ„¿æ™¯æ›´æ¥è¿‘ç°å®ã€‚ç„¶è€Œï¼ŒåŸºç¡€è®¾æ–½é™åˆ¶ã€åŠ¨æ€ç¯å¢ƒä»¥åŠå¤šæ™ºèƒ½ä½“åè°ƒçš„é«˜è®¡ç®—éœ€æ±‚ï¼Œåˆ¶çº¦äº†å…¶åœ¨é‡ç«æ•‘æ´ä¸ç¾å®³å“åº”ç­‰é«˜å±åœºæ™¯ä¸­çš„å®é™…éƒ¨ç½²ã€‚æœ¬æ–‡ç ”ç©¶åŸºäºLLMçš„æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ä¸è¾¹ç¼˜è®¡ç®—çš„èåˆï¼Œä»¥å®ç°æ— äººæœºé›†ç¾¤çš„å¯æ‰©å±•æ€§ä¸é²æ£’æ€§è‡ªä¸»èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆæ¢è®¨æ”¯æŒæ— äººæœºé›†ç¾¤çš„ä¸‰ç§æ¶æ„â€”â€”ç‹¬ç«‹éƒ¨ç½²ã€è¾¹ç¼˜èµ‹èƒ½éƒ¨ç½²åŠè¾¹ç¼˜-äº‘æ··åˆéƒ¨ç½²ï¼Œæ¯ç§æ¶æ„å‡é’ˆå¯¹ä¸åŒè‡ªä¸»æ€§ä¸è¿æ¥æ°´å¹³è¿›è¡Œä¼˜åŒ–ã€‚éšåï¼Œé€šè¿‡è®¾è®¡é‡ç«æœæ•‘åº”ç”¨æ¡ˆä¾‹ï¼ŒéªŒè¯è¾¹ç¼˜èµ‹èƒ½æ¶æ„çš„æ•ˆèƒ½ï¼šä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¶æ„èƒ½å®ç°æ›´é«˜çš„æœæ•‘è¦†ç›–ç‡ã€ç¼©çŸ­ä»»åŠ¡å®Œæˆæ—¶é—´ï¼Œå¹¶æå‡è‡ªä¸»åŒ–æ°´å¹³ã€‚æœ€åï¼Œæˆ‘ä»¬æŒ‡å‡ºåœ¨å…³é”®ä»»åŠ¡å‹æ— äººæœºé›†ç¾¤åº”ç”¨ä¸­æ•´åˆLLMä¸è¾¹ç¼˜è®¡ç®—ä»é¢ä¸´çš„å¼€æ”¾æŒ‘æˆ˜ã€‚",
    "url": "https://arxiv.org/abs/2601.14437v1",
    "html_url": "https://arxiv.org/html/2601.14437v1",
    "html_content": "Agentic AI Meets Edge Computing in Autonomous UAV Swarms\nThuan Minh Nguyen, Vu Tuan Truong, and Long Bao Le\nThe authors are with INRS, University of QuÃ©bec, MontrÃ©al, QC H5A 1K6, Canada. Emails: minh-thuan.nguyen@inrs.ca,\ntuan.vu.truong@inrs.ca, long.le@inrs.ca.\nAbstract\nThe integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.\nI\nIntroduction\nUnmanned aerial vehicles (UAVs), or drones, have attracted growing attention in recent years due to their capabilities in wide-area coverage, real-time sensing, and rapid deployment. When operating collaboratively as swarms, UAVs amplify these advantages and advance the vision of the Internet of Drones\n[\n1\n]\n. Such drone swarms are particularly effective for demanding tasks, such as environmental monitoring, search and rescue (SAR) operations, and wireless network provisioning.\nHowever, real-world UAV deployments face numerous challenges arising from dynamic conditions (e.g., environmental changes) and practical constraints such as limited Global Positioning System (GPS) access, unreliable network connectivity, and restricted human supervision. Consequently, ensuring effective UAV swarm operation under such conditions remains a major challenge. Recent advances in agentic AI, powered by large language models (LLMs) with strong reasoning capabilities\n[\n9\n,\n13\n]\n, offer promising solutions to these issues. By integrating agentic AI, UAVs can autonomously reason, plan, and make decisions with minimal human intervention, enabling rapid adaptation to environmental dynamics and unforeseen events. For instance, an LLM-based agentic AI embedded within a UAV can perform functions traditionally handled by humans (e.g., navigation and control) through natural-language-driven instruction and decision-making. Moreover, a more advanced paradigm, known as multi-agentic AI\n[\n6\n,\n4\n]\n, can be employed to further enhance the capabilities of UAV swarms. In this framework, each drone functions as an autonomous agent that communicates, reasons, and collaborates with others to collectively make decisions and address complex tasks.\nNevertheless, recent LLMs require substantial storage, computational, and energy resources, which are often limited on small aerial systems like UAVs. In addition, real-time missions typically demand low-latency decision-making, exceeding the computational capability of UAVsâ€™ embedded hardware. Although model compression techniques such as quantization, pruning, and knowledge distillation can be employed to mitigate these challenges, they often degrade the reasoning and contextual understanding abilities of the integrated LLMs, resulting in lower mission success rates and reduced overall swarm performance.\nTo sustain high performance in multi-agentic AI systems without exceeding the hardware limitations of UAVs, alternative computing paradigms such as edge and cloud computing can be employed\n[\n5\n,\n12\n]\n. Although cloud computing offers abundant storage and computational capacity, it often introduces considerable latency due to long-distance data transmission. Moreover, UAV swarms operating in harsh or remote environments (e.g., disaster zones or wilderness regions) may experience unstable connectivity to cloud servers, potentially causing service disruptions and mission failures. To balance the aforementioned trade-offs, edge computing offers an effective alternative by distributing inference, reasoning, and data processing across UAVs and nearby edge servers\n[\n14\n]\n.\nHowever, the literature does not provide a detailed and unified account of integrating edge computing with LLM-based agentic AI for scalable, resilient, low-latency UAV swarm autonomy.\nThis paper aims to address this research gap by first providing background on agentic AI, followed by a description of three alternative architectures for UAV swarmsâ€”standalone, edge-enabled, and edge/cloud-enabled UAV swarms.\nWe then propose a hybrid, edge-enabled LLM-based agentic AI framework for the SAR use case using UAV swarms.\nThe remainder of this paper is organized as follows. Section\nII\nprovides background on agentic AI and presents an overview of UAV swarm design based on practical multi-agentic AI frameworks. Section\nIII\ndiscusses three deployment strategies for UAV swarms powered by multi-agentic AI, namely fully standalone, edge-enabled, and hybrid edge-cloud architectures. Section\nIV\nillustrates the potential of agentic-AI-powered UAV swarms through a wildfire SAR use case. Finally, Section\nV\nhighlights open challenges for such systems, and Section\nVI\nconcludes the paper.\nII\nAgentic AI Systems\nThis section provides background on agentic AI with the introduction of several representative frameworks for practical implementation.\nII-A\nBackground on Agentic AI\nAgentic AI refers to intelligent systems that function as autonomous, goal-directed agents capable of reasoning, adapting, and interacting with their environment in a human-like manner.\nAgentic AI systems are particularly well-suited for complex domains like UAV swarm operations, where autonomy, adaptability, and coordinated decision-making are critical. Multi-agentic AI settings further enable collaborative behaviors among agents, allowing UAV swarms to respond collectively to evolving conditions and unforeseen challenges. In the following, we present core components of an agentic AI system.\nPerception Module.\nThis module captures and processes diverse inputs (e.g., natural language instructions, sensor readings, visual streams, and data from external Application Programming Interfaces (APIs)) into structured representations that can be interpreted by the reasoning module. It establishes situational awareness and contextual grounding, both of which are essential for accurate understanding and informed decision-making.\nReasoning Module.\nThis module processes inputs from the perception layer to interpret objectives, decompose tasks, and generate strategic plans. Using methods such as Chain-of-Thought (CoT) reasoning and iterative self-reflection, it adapts strategies to dynamic conditions and system constraints while pursuing mission goals.\nAction Execution Module.\nThis module materializes plans generated by the reasoning module through interfaces with external tools, APIs, and physical actuators. It converts high-level reasoning outputs into executable actions such as code generation, API calls, and control commands for embodied agents like UAVs and robots.\nMemory Module.\nThis module stores past observations, interactions, and learned knowledge, which are vital for coherent multi-step reasoning and long-horizon task execution. It encompasses short-term memory, which maintains active task states and recent episodic events, and long-term memory, which accumulates experiences and contextual knowledge over time. By retaining prior interactions and outcomes, this enables experiential learning, behavioral adaptation, and continuity across sequential decision-making processes.\nCoordination Module.\nThis module facilitates multi-agent and human-AI interactions by managing dialogue flow, negotiating task allocations, and fostering consensus among agents, thereby enabling effective operation in distributed environments such as UAV swarms.\nModern agentic AI-based platforms such as humanoid robots and drones can leverage recent visionâ€“languageâ€“action (VLA) models to build their perception, reasoning, and execution modules. These models are capable of translating multimodal observations and natural-language instructions directly to executable actions, such as navigation or control commands. For instance, RT-2\n[\n15\n]\nbuilds upon large pretrained visionâ€“language models (VLMs) and adapts them for robotic control through fine-tuning on task-specific robotic datasets. OpenVLA\n[\n7\n]\nextends this paradigm with an open-source, scalable architecture trained on the Open-X-Embodiment dataset, offering reproducible pipelines and adaptability across a wide range of robotic platforms.\nII-B\nMulti-agentic AI Frameworks\nA variety of LLM-based multi-agent frameworks have recently emerged to coordinate complex and collaborative workflows in robotics. In swarm robotics, these frameworks facilitate persistent state sharing, dynamic task allocation, adaptive planning, and other capabilities crucial for maintaining autonomy under real-world constraints. Several representative frameworks applicable for UAV applications are summarized below.\nLangGraph\nis a graph-based, stateful, and event-driven multi-agent framework in which nodes represent agents, tools, or memory modules, and edges define control and data flows. It supports cyclic and asynchronous execution, allowing UAVs to continuously sense, plan, and coordinate without waiting for centralized directives. In swarm mode, LangGraph eliminates single points of control by enabling agents to hand off tasks directly to peers via handoff tools. This decentralized design is particularly well-suited for autonomous UAV swarms operating in unpredictable or communication-constrained environments.\nAutoGen\nis a conversation-driven framework that enables multi-agent collaboration through structured dialogue\n[\n10\n]\n. Agents assume roles such as planner, executor, or verifier and interact via prompt engineering and message passing. In robotics, AutoGen supports high-level mission planning, tool invocation (e.g., perception or actuation APIs), and reflective reasoning via critic agents. It accommodates both human-in-the-loop (HITL) and fully autonomous operations, making it well-suited for UAV swarms. Its modular design and tool integration allow seamless interfacing with robotic subsystems, including sensor fusion and navigation modules.\nCrewAI\nis a lightweight, role-based framework for coordinating teams of autonomous agents. Inspired by human workflows, it assigns predefined roles (e.g., Leader, Analyzer, or Marketer) to LLM-based agents collaborating on tasks. Unlike graph-based systems, CrewAI emphasizes sequential, collaborative execution through intuitive workflows and modular role definitions. This approach is particularly well-suited for robotics applications involving modular subsystems, including perception, planning, navigation, and communication.\nThese frameworks differ in architectural philosophy: LangGraph uses stateful, event-driven graph orchestration; AutoGen relies on dialogue-driven coordination; and CrewAI employs role-based sequential execution. In the following, we demonstrate how LangGraph can support the practical, end-to-end design of UAV swarms, illustrating its integration with perception, reasoning, memory, and execution modules.\nII-C\nUAV Swarm Design with LangGraph\nWhile the preceding section discussed multi-agent frameworks, their deployment in UAV swarms requires a systematic integration of perception, reasoning, communication, and action modules. Perception is realized by transforming multimodal UAV sensor data into structured textual representations. Onboard algorithms such as YOLO perform real-time object detection on RGB and thermal imagery, while lightweight vision-language models such as LLaVA provide contextual descriptions. Together, these outputs enable the reasoning module to fuse symbolic and perceptual information.\nIn the LangGraph-based UAV swarm, perception functions operate as local graph nodes within each agent, supporting autonomous situational awareness. When necessary, processed outputs are shared via the wireless mesh network to update the persistent state, allowing peers to adjust mission plans based on shared observations.\nDecision-making and task planning are distributed throughout the swarm. Each UAV includes a lightweight LLM (e.g., TinyLLaMA or Phi-3-mini) to enable low-latency plan adjustments, hazard avoidance, and local decision-making. Through LangGraphâ€™s swarm handoff mechanism, UAVs can dynamically delegate tasks to their peers.\nThe action execution layer translates reasoning outputs into concrete flight and payload actions. LangGraph tool interfaces connect with the UAV platformâ€™s native Software Development Kit (SDK), enabling simple commands to directly control navigation, hovering, landing, or payload release.\nA robust memory module is essential for continuity in multistep reasoning, enabling UAVs to adapt effectively in dynamic environments. Short-term memory holds active task states and recent events for immediate decisions, while long-term memory retains mission history, environment maps, and coordination patterns to guide future task allocation and hazard response. In LangGraph, memory modules serve as persistent state stores accessible to relevant nodes, allowing agents to recall detections, mission context, and lessons from prior deployments.\nA self-organizing mesh network is utilized to maintain communication and coordination within the swarm. This network allows for continuous peer-to-peer negotiation without centralized control, utilizing technologies such as Wi-Fi. The LangGraph swarmâ€™s asynchronous message-passing architecture facilitates the rapid redistribution of workloads in response to changing operational conditions, thereby enhancing system resilience and guaranteeing efficient area coverage.\nIII\nMulti-Agentic AI for UAV Swarms\nWe now consider alternative designs for system-level deployment of UAV swarms. Specifically, we analyze three architecturesâ€”standalone, edge-enabled, and edge/cloud-enabled UAV swarmsâ€”as illustrated in Fig.\n1\n.\nFigure 1:\nThree multi-agentic AI deployment architectures for UAV swarms:\n(a) Standalone UAV swarm;\n(b) Edge-enabled UAV swarm;\n(c) Edge/cloud-enabled UAV swarm;\n(d) Applications;\n(e) UAV agent\nIII-A\nStandalone UAV Swarm\nThe first design, shown in Fig.\n1\n(a), targets extreme environments with little or no communication infrastructure, such as disaster zones, remote wilderness, or adversarial regions.\nIn these scenarios, each UAV functions as a self-contained intelligent agent equipped with a multimodal sensor suite that includes optical and infrared cameras, Global Navigation Satellite System (GNSS), light detection and ranging (LiDAR), inertial measurement units (IMUs), and millimeter-wave radars for strong autonomous perception. Onboard processors such as NVIDIA Jetson Orin or Qualcomm RB5 support lightweight or quantized LLMs (e.g., TinyLLaMA), enabling onboard reasoning for decision-making and task planning.\nThe execution layer transforms reasoning outputs into specific actions by interacting with APIs and integrated actuators. Contemporary UAVs, such as the Skydio X10, already incorporate sophisticated control and navigation systems for autonomous obstacle evasion and accurate flight in GPS-denied or congested environments, allowing each UAV to sense, analyze, and perform tasks independently of external infrastructure.\nUAVs communicate through a multi-agent AI framework, enabling the swarm to function as a cohesive, goal-directed system without centralized control and to remain resilient under communication loss or unit failure. A self-organizing wireless mesh network using technology such as Wi-Fi supports mission updates, task allocation, and route synchronization with the ground station. This decentralized design enhances autonomy and adaptability but requires energy-efficient AI, lightweight embedded LLMs, and careful safeguards to mitigate risks from reduced human oversight.\nIII-B\nEdge-enabled UAV Swarm\nTo enhance swarm performance while preserving decentralized autonomy, mobile ground stations serving as edge servers can be employed to support UAV swarms, as shown in\nFig.\n1\n(b).\nIn fact, mobile ground stations can maintain reliable and low-latency communication links with the UAV swarm, enabling high-level oversight and HITL collaboration.\nIn addition, they provide user interfaces that enable command issuance, swarm monitoring, and real-time panoramic mapping of UAV positions and situational contexts.\nMoreover, these edge servers leverage high-capacity processors to run full-scale LLMs for advanced reasoning, event validation, and mission planning, while aggregating logs and sensor data for forensic analysis, post-mission learning, and regulatory compliance. This architecture balances autonomy with flexibility, making it well-suited for semi-connected, dynamic environments.\nIII-C\nEdge/Cloud-enabled UAV Swarm\nFig.\n1\n(c) illustrates the third design, intended for large-scale missions where reliable communication and integrated edge-cloud infrastructure support the UAV swarm. This design enables the UAV swarm to operate efficiently over large areas by combining onboard decision-making with powerful edge-cloud support.\nIn particular, each UAV is equipped with components, including multi-modal sensors and embedded lightweight LLMs for onboard reasoning, planning, and decentralized communication, as in the previous designs.\nNonetheless, lightweight onboard LLMs may experience reasoning and planning limitationsâ€”including hallucinations and reduced accuracyâ€”which raises concerns about safety, ethics, and task reliability.\nThe ground stations can be leveraged to solve this problem\nwhere UAVs run a smaller onboard LLM to produce approximate reasoning results while the edge ground station (EGS) relies on a larger LLM to validate and, if necessary, correct these outputs.\nThis approach enables rapid UAV decision-making, speeds up edge inference through parallel token verification, and reduces communication by sending concise outputs. Confidence-based uploads\n[\n8\n]\nfurther save bandwidth, while the EGS aggregates UAV inputs to build a shared situational view and support global task redistribution.\nCloud platforms extend system capabilities with persistent data storage, global map construction, cross-swarm collaboration, and continuous model updates. The architecture also enables remote monitoring and high-level command from centralized locations. By integrating edge and cloud infrastructure, multi-agent AI-driven UAV swarms achieve scalable, intelligent, and resilient operations for large, long-duration missions.\nIII-D\nApplications\nAgentic AIâ€“powered UAV swarms can be applied across diverse domains as follows.\nSafety and Defense:\nAutonomous UAV swarms enable persistent surveillance, border monitoring, and threat detection in contested or infrastructure-denied environments. Through onboard reasoning and decentralized coordination, they remain resilient and maintain situational awareness under hostile conditions.\nEnvironmental Monitoring and Disaster Relief:\nUAV swarms enable real-time environmental assessment, from air quality monitoring and wildfire tracking to search-and-rescue missions. Linked with EGSs, they rapidly identify survivors and hazard zones while adapting to dynamic disaster scenarios such as floods, earthquakes, and wildfires.\nAgriculture:\nUAV swarms support precision agriculture by monitoring crop health, analyzing soil, and managing livestock. With distributed reasoning and multi-modal sensing, they provide large-area coverage and real-time decision-making, enhancing efficiency and sustainability in farming.\nSmart City:\nIn urban environments, UAV swarms support infrastructure inspection, traffic monitoring, and public safety. Cloud integration enables global coordination, data sharing, and remote access for city authorities, improving responsiveness and efficiency.\nThese applications highlight the versatility of multi-agent AI-enabled UAV swarms, especially when coupled with edge and cloud platforms. The architecture delivers reliable autonomy, advanced reasoning, and resilient performance across diverse real-world scenarios.\nIV\nUse Case: Wildfire Search-and-Rescue (SAR)\nRecent wildfires, such as those in Los\nAngeles in\nJanuary 2025\n1\n1\n1\nâ€œSpread of the Palisades and Eaton Fires â€“ January 2025,â€ NASA Scientific Visualization Studio, 11 July 2025. [Online]. Available:\nhttps://svs.gsfc.nasa.gov/5558/\n.\n, highlight the limits of traditional SAR operations in dynamic, hazardous environments. To overcome these challenges, we propose an autonomous SAR system combining multi-agent AI, UAV swarms, and edge computing.\nIV-A\nMission Objectives\nOur proposed system is designed to enable adaptive and resilient UAV swarm operations in dynamic and hazardous wildfire environments. Specifically, the design seeks to achieve the following mission objectives.\nFirst, it dynamically segments wildfire zones and assigns subregions to UAVs for autonomous surveying and survivor detection.\nThe design supports onboard LLM-based UAV route planning, detect human presence and residual hazards using multimodal sensing. Also, the design allows transmitting situational awareness data from UAVs to the EGS for real-time monitoring and assessment.\nIV-B\nSystem Architecture and Operational Workflow\nOur proposed system for wildfire SAR integrates three core componentsâ€”satellite image-based planning by an EGS, distributed swarm intelligence, and resilient humanâ€“machine teamingâ€”into a unified workflow (Fig.\n2\n). It is designed for deployment on advanced UAVs such as the Skydio X10, which combines multimodal sensing, powerful onboard processing, and wireless communications. The framework is designed so that mission goals can be achieved by tackling four different tasks: 1) Image segmentation task to identify the wildfire area, 2) Survey-point creation task, 3) Survey-point assignment task, and 4) UAV path planning task. The first three tasks are executed by the EGS to guarantee global situational awareness and effective resource coordination, whereas the final task is performed by each UAV to achieve localized autonomy and responsiveness. Detailed operations and decision-making processes of the EGS and UAVs are described in Algorithm\n1\n.\n2\n2\n2\nTo account for the UAVsâ€™ limited energy, computing, and storage capacities, the lightweight TinyLLaMA model is used onboard for route planning, while the more capable GPT-4.1 model is employed at the EGS, which has a richer resource pool.\nFigure 2:\nProposed multi-agentic AI-driven UAV swarm system for wildfire SAR.\nInput:\nSystem state for\nN\nN\nUAVs\nğ’°\n=\n{\nU\n1\n,\nU\n2\n,\nâ€¦\n,\nU\nN\n}\n\\mathcal{U}=\\{U_{1},U_{2},\\dots,U_{N}\\}\n, satellite image\nI\nâ€‹\n(\nt\n)\nI(t)\n, ground station position\nG\nG\n, mission command\nM\nM\nAt Edge Ground Station:\nwhile\nwildfire relief is active\ndo\n- Acquire latest satellite image\nI\nâ€‹\n(\nt\n)\nI(t)\n- Extract fire boundary\nâ„¬\nâ€‹\n(\nt\n)\n\\mathcal{B}(t)\nfrom\nI\nâ€‹\n(\nt\n)\nI(t)\nusing U\n-\nNet segmentation model\n- Generate uniform grid within\nâ„¬\nâ€‹\n(\nt\n)\n\\mathcal{B}(t)\nand define set of survey points\nğ’®\nâ€‹\n(\nt\n)\n=\n{\ns\nk\n}\n\\mathcal{S}(t)=\\{s_{k}\\}\nat cell centroids\n- Construct mission prompt\nğ’«\n=\n(\nM\n,\nğ’°\n,\nğ’®\nâ€‹\n(\nt\n)\n)\n\\mathcal{P}=(M,\\mathcal{U},\\mathcal{S}(t))\n- Obtain survey-point assignment solution via LLM-based mission planner:\n{\nğ’œ\ni\n}\ni\n=\n1\nN\nâ†\nPlanner\nâ€‹\n(\nğ’«\n)\n\\{\\mathcal{A}_{i}\\}_{i=1}^{N}\\leftarrow\\texttt{Planner}(\\mathcal{P})\nif\nâˆ‘\ni\n|\nğ’œ\ni\n|\n>\n|\nğ’®\nâ€‹\n(\nt\n)\n|\n\\sum_{i}|\\mathcal{A}_{i}|>|\\mathcal{S}(t)|\nthen\n- Append â€œYou are hallucinating, creating more survey points than required. Do not invent, modify, or add any new points.â€ to\nğ’«\n\\mathcal{P}\n- Re-plan to obtain new solution of survey-point assignments via LLM-based mission planner:\n{\nğ’œ\ni\n}\ni\n=\n1\nN\nâ†\nPlanner\nâ€‹\n(\nğ’«\n)\n\\{\\mathcal{A}_{i}\\}_{i=1}^{N}\\leftarrow\\texttt{Planner}(\\mathcal{P})\nelse if\nâˆ‘\ni\n|\nğ’œ\ni\n|\n<\n|\nğ’®\nâ€‹\n(\nt\n)\n|\n\\sum_{i}|\\mathcal{A}_{i}|<|\\mathcal{S}(t)|\nthen\n- Append â€œYou have not assigned all survey points to UAVs. You must allocate all survey points to UAVs.â€ to\nğ’«\n\\mathcal{P}\n- Re-plan to obtain new solution of survey-point assignments via LLM-based mission planner:\n{\nğ’œ\ni\n}\ni\n=\n1\nN\nâ†\nPlanner\nâ€‹\n(\nğ’«\n)\n\\{\\mathcal{A}_{i}\\}_{i=1}^{N}\\leftarrow\\texttt{Planner}(\\mathcal{P})\n- Transmit validated survey-point assignments\n{\nğ’œ\ni\n}\n\\{\\mathcal{A}_{i}\\}\nto UAVs via wireless communications links\nend while\nAt UAVs (executed in parallel at\nN\nN\nUAVs):\nfor\ni\n=\n1\ni=1\nto\nN\nN\ndo\n- UAV\ni\ni\nreceives assigned survey points\nğ’œ\ni\n\\mathcal{A}_{i}\n- Form local prompt\nğ’«\nl\n=\n(\nğ’œ\ni\n,\nlocal perception\n)\n\\mathcal{P}_{l}=(\\mathcal{A}_{i},\\text{local perception})\n- Generate flight route via TinyLLaMA based planner:\nğ’¯\ni\nâ†\nTinyLLaMA\nâ€‹\n(\nğ’«\nl\n)\n\\mathcal{T}_{i}\\leftarrow\\texttt{TinyLLaMA}(\\mathcal{P}_{l})\nif\n|\nğ’¯\ni\n|\n>\n|\nğ’œ\ni\n|\n|\\mathcal{T}_{i}|>|\\mathcal{A}_{i}|\nthen\n- Append â€œYou have used more survey points than required. Do not invent, modify, or add any new points.â€ to\nğ’«\nl\n\\mathcal{P}_{l}\n- Generate new flight route via TinyLLaMA based planner:\nğ’¯\ni\nâ†\nTinyLLaMA\nâ€‹\n(\nğ’«\nl\n)\n\\mathcal{T}_{i}\\leftarrow\\texttt{TinyLLaMA}(\\mathcal{P}_{l})\nelse if\n|\nğ’¯\ni\n|\n<\n|\nğ’œ\ni\n|\n|\\mathcal{T}_{i}|<|\\mathcal{A}_{i}|\nthen\n- Append â€œYou have generated a flight route not including all assigned survey points. You must visit every assigned survey point.â€ to\nğ’«\nl\n\\mathcal{P}_{l}\n- Generate new flight route via TinyLLaMA based planner:\nğ’¯\ni\nâ†\nTinyLLaMA\nâ€‹\n(\nğ’«\nl\n)\n\\mathcal{T}_{i}\\leftarrow\\texttt{TinyLLaMA}(\\mathcal{P}_{l})\n- Execute\nğ’¯\ni\n\\mathcal{T}_{i}\n: navigate waypoints, survey grids, perform real-time detection, and coordinate with peers\n- Transmit state information (position, battery, progress, detections) to EGS for task updates\nend for\nAlgorithmÂ 1\nWorkflow of Proposed Design\nThe proposed framework operates as described below.\n3\n3\n3\nOptimization objectives and constraints for the survey-point assignment and route planning optimization are integrated into the corresponding prompts for the LLMs employed by the EGS and\nUAVs, respectively. Detailed prompt designs are not presented due to the space constraint.\nUpon receiving wildfire alerts and an image of the region of interest from the satellite system, the EGS is deployed near the affected region to process incoming data and coordinate swarm operations. First,\nthe EGS employs the U-Net segmentation\nmodel\n4\n4\n4\nAvailable at\nhttps://github.com/yueureka/WildFireDetection.git\nto delineate wildfire boundaries. As new satellite data arrives, the fire boundary maps are continuously updated, maintaining situational awareness and supporting adaptive sub-region assignment for UAVs. The delineated region is discretized into uniform grid cells whose centroids serve as survey points for swarm deployment.\nSubsequently, the LLM-based mission planner at the EGS analyzes segmented region and system state information such as UAVsâ€™ remaining energy and current locations to assign survey points to different UAVs.\nThe prompt of the LLM-based planner is engineered to explicitly optimize the coverage efficiency, travel distance for each UAV, balance workload distribution among UAVs, and prevent overlap of UAVsâ€™ survey points. To tackle the LLM hallucinations, the output of the EGSâ€™s mission planner is validated by comparing the actual number of survey points and the number of survey points assigned to all UAVs.\nWhen inconsistencies are detected, such as extra or missing points, the prompt is automatically updated to initiate re-planning.\nFor instance, when the EGSâ€™s LLM hallucinates by assigning more survey points than are actually available to the UAVs, we append the following statement to the prompt during re-planning:\nâ€œYou are hallucinating, creating more survey points than required. Do not invent, modify, or add any new points.â€ \nOnce validated, each UAV is provided with its subregion map (i.e., assigned survey points), task instructions, and coordination requirements, establishing the foundation for autonomous swarm collaboration.\nUpon deployment, the UAVs automatically establish a self-organizing wireless network, enabling resilient peer-to-peer communication and communication between UAVs and the EGS.\nEach UAV uses the TinyLLaMA model for onboard planning, enabling it to generate and adjust its flight path based on the survey points assigned by the EGS and real-time sensory data collected onboard.\nThis capability enables the UAVs to achieve the mission goals described above. The flight routes generated by TinyLLaMA are verified using the same consistency checks applied to the mission plannerâ€™s output, ensuring overall mission reliability.\nOnce the flight route is validated, the execution layer directs UAVs to their waypoints, conducts independent grid surveys, performs real-time fire and survivor detection, adapts to environmental changes, and coordinates with swarm peers as needed. Each UAV continuously transmits telemetry data, including position, battery status, task execution progress, and detection results, to the EGS. These data allow the EGS to perform real-time mission reconfiguration and ensure sustained operational efficiency.\nIV-C\nPerformance Metrics\nWe now define performance metrics to quantitatively evaluate the proposed design. In our simulation, these performance metrics are based on the specifications of the Skydio X10 drone, which allows human detection up to 1500 m. Accordingly, each grid cell is set to approximately 450 m Ã— 450 m for optimal coverage, dividing the Eaton fire-affected area into about 300 grid cells, corresponding to 300 survey points. The performance metrics are defined as follows:\nâ€¢\nCoverage Rate:\nRatio of the number of survey points visited to the total number of survey points.\nâ€¢\nMission Completion Time:\nTotal time elapsed from assigning tasks to the swarm until all SAR objectivesâ€”i.e., all grid cells or survey points have been visited by the UAVsâ€”are completed.\nIV-D\nState-of-the-Art Baseline\nWe adopt the baseline\n[\n2\n]\nfor performance comparison with our design. In this work,\nthe authors proposed an efficient LLM hybrid planning framework, which combines centralized planning with distributed feedback, achieving a high task success rate and desired scalability in various warehouse scenarios.\nTheir performance evaluation showed that the hybrid LLM planning framework outperforms other centralized designs.\nWe compare this baseline with our proposed design using the two performance metrics described above.\nIV-E\nSimulation Setting\nExperiments were conducted on an Intel Core i7-10750H CPU (2.60 GHz), 16 GB RAM, and an NVIDIA GeForce GTX 1650 Ti GPU running Windows 11 (64-bit). The software environment used Python 3.11.13 and PyTorch 2.7.1 with CUDA 12.8. To simulate the full-scale LLM at the EGS, we employ GPT-4.1 via the OpenAI API for survey-point assignments, ensuring that survey points assigned for different UAVs are non-overlapping and optimized for route planning efficiency. Each UAV used TinyLLaMA-1.1B (int4) for onboard route planning, enabling it to minimize flight distance over its assigned survey points.\nWe assume that each UAV simulates a Skydio X10-class platform with RGB + thermal sensors (1500 m range), a cruise speed of 15 m/s, and a 9600 mAh battery. The onboard processor modeled a Jetson Orin NX (70 TOPS, 10â€“25 W), with base power 45 W, flight power 8 W per 1 m/s, TinyLLaMA idle power 5 W, and inference power 10 W. Satellite images were obtained from the sped-up NASA video footage. We assume that consecutive satellite images are taken from the video, one image for every interval of 0.37 seconds. These extracted images are segmented by the EGS to identify wildfire boundaries, enabling survey points to be obtained dynamically.\nNote that this interval corresponds to tens of minutes in reality due to the sped-up nature of the video clip.\nIV-F\nExperimental Results\n(a)\nInitial deployment phase, with all 10 UAVs stationed at the ground control station and the survey-point assignments generated by the EGS, ready for launch.\n(b)\nOperation phase, with UAVs dispersed and flying along their planned routes.\n(c)\nFire spread requiring dynamic fire boundary calculation and updated survey-point assignments for UAVs.\nFigure 3:\nVisualization of survey-point assignments for 10 UAVs across three different phases, shown alongside the expanding wildfire region.\nWe demonstrate the wildfire segmentation and GPT-4.1â€“based survey-point assignment solutions across three different phases: (1) initial phase with all 10 UAVs still stationed at the ground station and survey-point assignment solution generated by the EGS, ready for launch, as shown in Fig.\n3\n(a), (2) operation phase with UAVs dispersed and flying along their planned routes, as shown in Fig.\n3\n(b), and (3) active fire spread requiring dynamic fire boundary calculation and updated survey-point assignments for UAVs, as shown in Fig.\n3\n(c). As demonstrated in Fig.\n3\n, the proposed framework effectively segments the wildfire region, assigns survey points in clusters to UAVs, and ensures efficiency, minimal overlap, balanced workload, and adaptive response to wildfire changing conditions and swarm status.\nWe analyze the coverage rate achieved by the proposed design with different UAV fleet sizes (8 or 12 drones) to assess its scalability, adaptability to dynamic wildfire conditions captured at different satellite image update index\nt\nt\n(i.e.,\ndifferent time points during the wildfire period).\nAs shown in Fig.\n4\n, the number of survey points varies over the satellite image update index\nt\nt\n, reflecting the dynamic evolution of the wildfire region.\nThe coverage rate achieved by our design remains high across the satellite image update indices, despite the increasing number of survey points caused by the expanding wildfire region.\nFor moderate wildfire regions in small\nt\nt\n, fewer UAVs (e.g., 8 UAVs) are preferable, as GPT-4.1 allocates survey points to UAVs more efficiently. Over-deployment (e.g., 12 UAVs) can cause hallucinations, generating false survey points, missing survey points, or creating overlaps. Conversely, larger wildfire regions require more UAVs (e.g., 12) to distribute the workload effectively, as too few UAVs overload the onboard TinyLLaMA reasoning model, resulting in inefficient path planning and missed survey points.\nTherefore, selecting an appropriate fleet size relative to the wildfire scale is essential for efficient task allocation and mission planning.\n\nIt can be observed that our proposed design significantly outperforms the baseline\n[\n2\n]\nin terms of coverage rate. This improvement is attributed to the efficient coordination between the UAVs and the EGS, as well as the validation, feedback, and adaptive re-planning strategies employed in our framework to mitigate LLM hallucination.\nFigure 4:\nCoverage rate of proposed design and the baseline [13]\nTo demonstrate the efficiency of our LLM-based strategy for assigning survey points to UAVs, we compare the mission completion time of our proposed design with that of a greedy assignment strategy as well as the baseline in\n[\n2\n]\n.\nThis greedy strategy iteratively assigns survey points to UAVs based on an assignment metric, which is a weighted sum of the (UAV, survey point) distance and a UAVâ€™s workload penalty. We define each UAVâ€™s set of assigned survey points as\nğ’œ\ni\n\\mathcal{A}_{i}\n(\ni\n=\n1\n,\n2\n,\nâ€¦\n,\nN\ni=1,2,\\dots,N\n), which is initially empty. For each assignment, the survey point\nP\nj\nP_{j}\nis assigned to UAV\nU\ni\nU_{i}\nif the assignment metric\nC\ni\nâ€‹\nj\nC_{ij}\nis the smallest among all available (UAV, survey point) pairs.\nLet\nn\nÂ¯\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\n|\nğ’œ\ni\n|\n\\bar{n}=\\frac{1}{N}\\sum_{i=1}^{N}|\\mathcal{A}_{i}|\nbe the average number of survey points assigned per UAV; then the\nworkload penalty is defined as\nPenalty\nâ€‹\n(\nU\ni\n)\n=\nmax\nâ¡\n(\n0\n,\n|\nğ’œ\ni\n|\nâˆ’\nn\nÂ¯\n)\nâ‹…\nB\n,\n\\text{Penalty}(U_{i})=\\max(0,|\\mathcal{A}_{i}|-\\bar{n})\\cdot B,\n(1)\nwhere\nB\nB\nis a coefficient set as\nB\n=\n800\nB=800\nin the\nsimulation, and\n|\nğ’œ\ni\n|\n|\\mathcal{A}_{i}|\nrepresents the cardinality of set\nğ’œ\ni\n\\mathcal{A}_{i}\n.\nThe assignment metric is defined as\nC\ni\nâ€‹\nj\n=\n\\displaystyle C_{ij}=\nDistance\n(\nU\ni\n.\ncurrent_pos\n,\nP\nj\n)\n+\nÎ»\nâ‹…\nPenalty\n(\nU\ni\n)\n,\n\\displaystyle\\>\\text{Distance}(U_{i}.\\text{current\\_pos},P_{j})+\\lambda\\cdot\\text{Penalty}(U_{i}),\n(2)\nwhere\nDistance\n(\nU\ni\n.\ncurrent_pos\n,\nP\nj\n)\n\\text{Distance}(U_{i}.\\text{current\\_pos},P_{j})\ndenotes the distance between the current position of UAV\nU\ni\nU_{i}\nand survey point\nP\nj\nP_{j}\n, and\nÎ»\n\\lambda\nis the weighting parameter.\nAs shown in Fig.\n5\n, our design consistently achieves shorter mission completion time across all satellite image update indices\nt\nt\ncompared to that due to the greedy strategy, especially as the number of survey points increases. Moreover, our proposed framework and the baseline in\n[\n2\n]\nachieve similar mission completion time.\nWith 8 UAVs, missions finish in under 25 minutes for our design, compared to over 30 minutes for the greedy algorithm. Scaling up to 12 UAVs reduces the completion time to under 17 minutes, compared with over 22 minutes for the greedy baseline.\nThese results demonstrate the scalability and efficiency of our approach in dynamic, real-world conditions.\nFigure 5:\nMission completion time of proposed design and other baselines\nV\nOpen Challenges\nWe outline key open challenges for future research in the following.\nV-A\nEfficient Onboard LLMs\nAlthough lightweight LLMs like TinyLLaMA are emerging, achieving reliable, low-latency reasoning on resource-constrained UAVs remains challenging. Future research could explore efficient model design and inference techniques, such as quantization, knowledge distillation, sparsification, and structural optimization, to enable practical onboard reasoning. Additionally, hardwareâ€“software co-design offers a promising direction for robust onboard autonomy.\nV-B\nHallucination Mitigation\nHallucination remains a critical challenge in LLMs and LLM-driven agentic AI systems, referring to the generation of factually inaccurate content\n[\n3\n]\n. In multi-agent systems, this problem is amplified through inter-agent communication: a hallucination by one agent can propagate across the network, triggering cascades of misinformation. Mitigation requires both detecting and correcting errors at the individual agent level and controlling information flow to prevent systemic error amplification. Thus, addressing hallucination in multi-agent AI demands a combination of local robustness and global coordination.\nV-C\nRobust Multi-Agent Collaboration\nReal-world disaster zones often feature harsh conditions, such as damaged communication infrastructure, GPS failures, and unpredictable environments. Ensuring resilient UAV swarm coordination, self-organizing mesh networks, and reliable peer-to-peer communication under these circumstances requires further research in decentralized reasoning, self-healing networks, and fault-tolerant protocols.\nV-D\nEdge Infrastructure Scalability and Reliability\nDeploying mobile edge ground stations that support full-scale LLM inference, situational visualization, and HITL integrationâ€”while remaining portable and resilient in extreme conditionsâ€”remains a technical challenge. Future designs must balance computational power, energy efficiency, and operational resilience.\nV-E\nEvaluation and Benchmarks\nDespite growing interest in LLM-based multi-agent AI, most research focuses on evaluating individual agents within narrow contexts\n[\n11\n]\n. Comprehensive benchmarks for LLM-based UAV swarm autonomy are lacking, with no established metrics for reasoning accuracy, collaboration efficiency, or mission success in realistic SAR scenarios. Developing simulation platforms, conducting field tests, and designing evaluation frameworks are essential for a thorough performance assessment.\nV-F\nUAV Technical Limitations\nDespite advances in autonomy and swarm coordination, current UAV platforms face several hardware and operational constraints that impact mission performance and scalability. Flight time and energy limitations restrict the coverage area and duration of missions, particularly in dynamic environments. Payload and sensor constraints introduce trade-offs between weight, sensing capabilities, and mission requirements. Communication and computation bottlenecks can reduce responsiveness and efficiency, especially when relying on lightweight edge devices. These constraints affect the planning, execution, and coordination of UAV swarms, influencing mission completion time and reliability. Addressing these limitations through energy-aware planning, adaptive payload management, and hierarchical computation offloading remains critical challenges for practical deployments.\nVI\nConclusion\nThis paper investigated the integration of agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We discussed three swarm deployment strategies: standalone, edge-enabled, and edge/cloud hybrid UAV swarms. A wildfire SAR use case was studied to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion time, and a higher level of autonomy compared to existing approaches.\nThe results demonstrate that the edge-enabled architecture effectively adapts to dynamic environments while balancing autonomy and resource efficiency.\nMoreover, we identified essential research challenges to guide future investigations. These findings establish a technical foundation for advancing next-generation intelligent swarm systems and provide guidance for researchers and stakeholders in robotics, edge AI, and disaster response seeking to deploy UAV swarms in real-world missions.\nReferences\n[1]\nL. Abualigah, A. Diabat, P. Sumari, and A. H. Gandomi\n(2021)\nApplications, deployments, and integration of Internet of Drones (IoD): a review\n.\nIEEE Sens. J.\n21\n(\n22\n),\npp.Â 25532â€“25546\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[2]\nY. Chen, J. Arkin, Y. Zhang, N. Roy, and C. Fan\n(2024-05)\nScalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?\n.\nIn\nProc. IEEE Int. Conf. Robot. Autom. (ICRA)\n,\npp.Â 4311â€“4317\n.\nCited by:\nÂ§\nIV-D\n,\nÂ§\nIV-F\n,\nÂ§\nIV-F\n,\nÂ§\nIV-F\n.\n[3]\nS. Farquhar, J. Kossen, L. Kuhn, and Y. Gal\n(2024)\nDetecting hallucinations in large language models using semantic entropy\n.\nNature\n630\n(\n8017\n),\npp.Â 625â€“630\n.\nCited by:\nÂ§\nV-B\n.\n[4]\nP. Feng, T. Yang, M. Liang, L. Wang, and Y. Gao\n(2025)\nOC-HMAS: dynamic self-organization and self-correction in heterogeneous multiagent systems using multimodal large models\n.\nIEEE Internet Things J.\n12\n(\n10\n),\npp.Â 13538â€“13555\n.\nCited by:\nÂ§I\n.\n[5]\nY. He, J. Fang, F. R. Yu, and V. C. Leung\n(2024)\nLarge language models (LLMs) inference offloading and resource allocation in cloud-edge computing: an active inference approach\n.\nIEEE Trans. Mobile Comput.\n23\n(\n12\n),\npp.Â 11253â€“11264\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[6]\nS. S. Kannan, V. L. Venkatesh, and B. Min\n(2024)\nSMART-LLM: smart multi-agent robot task planning using large language models\n.\nIn\nProc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)\n,\npp.Â 12140â€“12147\n.\nCited by:\nÂ§I\n.\n[7]\nM. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster, P. R. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn\n(2025-06â€“09 Nov)\nOpenVLA: an open-source vision-language-action model\n.\nIn\nProc. Conf. Robot Learn. (CoRL)\n,\nVol.\n270\n,\npp.Â 2679â€“2713\n.\nCited by:\nÂ§\nII-A\n.\n[8]\nY. Wang, K. Chen, H. Tan, and K. Guo\n(2023-05)\nTabi: an efficient multi-level inference system for large language models\n.\nIn\nProc. Eur. Conf. Comput. Syst. (EuroSys)\n,\npp.Â 233â€“248\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-C\n.\n[9]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou\n(2022-12)\nChain-of-thought prompting elicits reasoning in large language models\n.\nIn\nProc. Adv. Neural Inf. Process. Syst. (NeurIPS)\n,\nVol.\n35\n,\npp.Â 24824â€“24837\n.\nCited by:\nÂ§I\n.\n[10]\nQ. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu,\net al.\n(2024-10)\nAutoGen: enabling next-gen LLM applications via multi-agent conversations\n.\nIn\nProc. Conf. Lang. Model. (COLM)\n,\nCited by:\nÂ§\nII-B\n.\n[11]\nL. Xu, Z. Hu, D. Zhou, H. Ren, Z. Dong, K. Keutzer, S. Ng, and J. Feng\n(2024-11)\nMAgIC: investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration\n.\nIn\nProc. Conf. Empir. Methods Nat. Lang. Process. (EMNLP)\n,\npp.Â 7315â€“7332\n.\nCited by:\nÂ§\nV-E\n.\n[12]\nT. Yang, P. Feng, Q. Guo, J. Zhang, X. Zhang, J. Ning, X. Wang, and Z. Mao\n(2025)\nAutoHMA-LLM: efficient task coordination and execution in heterogeneous multi-agent systems using hybrid large language models\n.\nIEEE Trans. Cogn. Commun. Netw.\n11\n(\n2\n),\npp.Â 987â€“998\n.\nCited by:\nÂ§I\n.\n[13]\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan\n(2023-12)\nTree of Thoughts: deliberate problem solving with large language models\n.\nIn\nProc. Adv. Neural Inf. Process. Syst.\n,\nVol.\n36\n,\npp.Â 11809â€“11822\n.\nCited by:\nÂ§I\n.\n[14]\nM. Zhang, X. Shen, J. Cao, Z. Cui, and S. Jiang\n(2025)\nEdgeShard: efficient LLM inference via collaborative edge computing\n.\nIEEE Internet Things J.\n12\n(\n10\n),\npp.Â 13119â€“13131\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[15]\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,\net al.\n(2023-11)\nRT-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nProc. Conf. Robot Learn. (CoRL)\n,\nVol.\n229\n,\npp.Â 2165â€“2183\n.\nCited by:\nÂ§\nII-A\n.",
    "preview_text": "The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.\n\nAgentic AI Meets Edge Computing in Autonomous UAV Swarms\nThuan Minh Nguyen, Vu Tuan Truong, and Long Bao Le\nThe authors are with INRS, University of QuÃ©bec, MontrÃ©al, QC H5A 1K6, Canada. Emails: minh-thuan.nguyen@inrs.ca,\ntuan.vu.truong@inrs.ca, long.le@inrs.ca.\nAbstract\nThe integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI ",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "LLM",
        "edge computing",
        "UAV swarms",
        "autonomous reasoning",
        "planning",
        "execution",
        "wildfire search and rescue"
    ],
    "one_line_summary": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½AIä¸è¾¹ç¼˜è®¡ç®—åœ¨æ— äººæœºé›†ç¾¤ä¸­çš„é›†æˆï¼Œä»¥æå‡è‡ªä¸»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä½†æœªæ¶‰åŠå¼ºåŒ–å­¦ä¹ ã€VLAã€æ‰©æ•£æ¨¡å‹ã€æµåŒ¹é…ã€è¿åŠ¨æ§åˆ¶ã€VLMæˆ–å…¨èº«æ§åˆ¶ç­‰å…³é”®è¯ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T19:45:33Z",
    "created_at": "2026-01-27T15:53:14.678320",
    "updated_at": "2026-01-27T15:53:14.678328"
}