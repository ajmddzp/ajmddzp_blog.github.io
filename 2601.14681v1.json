{
    "id": "2601.14681v1",
    "title": "FARE: Fast-Slow Agentic Robotic Exploration",
    "authors": [
        "Shuhao Liao",
        "Xuxin Lv",
        "Jeric Lew",
        "Shizhe Zhang",
        "Jingsong Liang",
        "Peizhuo Li",
        "Yuhong Cao",
        "Wenjun Wu",
        "Guillaume Sartoretti"
    ],
    "abstract": "æœ¬ç ”ç©¶é€šè¿‡æ•´åˆæ™ºèƒ½ä½“å±‚é¢çš„è¯­ä¹‰æ¨ç†ä¸å¿«é€Ÿå±€éƒ¨æ§åˆ¶ï¼Œæ¨åŠ¨äº†è‡ªä¸»æœºå™¨äººæ¢ç´¢æŠ€æœ¯çš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºFAREâ€”â€”ä¸€ç§åˆ†å±‚å¼è‡ªä¸»æ¢ç´¢æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç”¨äºå…¨å±€æ¨ç†çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è´Ÿè´£å±€éƒ¨å†³ç­–çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ç›¸ç»“åˆã€‚FAREéµå¾ªå¿«æ…¢æ€ç»´ååŒèŒƒå¼ï¼šæ…¢æ€ç»´LLMæ¨¡å—è§£ææœªçŸ¥ç¯å¢ƒçš„ç®€æ˜æ–‡æœ¬æè¿°ï¼Œç”Ÿæˆæ™ºèƒ½ä½“çº§æ¢ç´¢ç­–ç•¥ï¼Œå¹¶é€šè¿‡æ‹“æ‰‘å›¾å°†å…¶å…·è±¡åŒ–ä¸ºå…¨å±€èˆªç‚¹åºåˆ—ï¼›è¯¥æ¨¡å—é‡‡ç”¨åŸºäºæ¨¡å—åŒ–çš„å‰ªææœºåˆ¶ï¼Œé€šè¿‡ç²¾ç®€å†—ä½™å›¾ç»“æ„è¿›ä¸€æ­¥æå‡æ¨ç†æ•ˆç‡ã€‚å¿«æ€ç»´RLæ¨¡å—åœ¨LLMç”Ÿæˆçš„å…¨å±€èˆªç‚¹å¼•å¯¼ä¸‹ï¼Œæ ¹æ®å±€éƒ¨è§‚æµ‹å®æ—¶æ‰§è¡Œæ¢ç´¢ä»»åŠ¡ï¼›å…¶ç­–ç•¥é€šè¿‡å¥–åŠ±é¡¹è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥å¥–åŠ±é¡¹é¼“åŠ±éµå¾ªå…¨å±€èˆªç‚¹ï¼Œä»è€Œå½¢æˆè¿è´¯ä¸”é²æ£’çš„é—­ç¯è¡Œä¸ºã€‚è¿™ç§æ¶æ„å°†è¯­ä¹‰æ¨ç†ä¸å‡ ä½•å†³ç­–è§£è€¦ï¼Œä½¿å„æ¨¡å—èƒ½åœ¨é€‚å®œçš„æ—¶ç©ºå°ºåº¦ä¸­è¿ä½œã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»¿çœŸç¯å¢ƒä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜FAREç›¸æ¯”å‰æ²¿åŸºçº¿æ–¹æ³•åœ¨æ¢ç´¢æ•ˆç‡ä¸Šå–å¾—æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†FAREéƒ¨ç½²äºç¡¬ä»¶å¹³å°ï¼Œå¹¶åœ¨å¤æ‚çš„å¤§è§„æ¨¡å»ºç­‘ç¯å¢ƒï¼ˆ200mÃ—130mï¼‰ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.14681v1",
    "html_url": "https://arxiv.org/html/2601.14681v1",
    "html_content": "FARE: Fast-Slow Agentic Robotic Exploration\nShuhao Liao\n1,2\n, Xuxin Lv\n1\n, Jeric Lew\n2\n, Shizhe Zhang\n2\n, Jingsong Liang\n2\n, Peizhuo Li\n2\n,\nYuhong Cao\n2\n, Wenjun Wu\n1\n, Guillaume Sartoretti\n2\n1\nBeihang University, China\n2\nDepartment of Mechanical Engineering, National University of Singapore, Singapore\nAbstract\nThis work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale\n200\nâ€‹\nm\nÃ—\n130\nâ€‹\nm\n200m\\times 130m\nbuilding environment.\nI\nINTRODUCTION\nAutonomous exploration is a core capability for mobile robots operating in unknown environments, where the objective is to efficiently acquire geometric information. The robot acquires environmental data through sensors such as 3D LiDARs or cameras and incrementally builds a representation of the environment, commonly encoded as an occupancy grid or voxel map.\nWith the reliability of modern LiDAR odometry and SLAM (Simultaneous Localization and Mapping), recent work has increasingly focused on the exploration problem itself, without concerns about mapping or localization accuracy\n[\n21\n,\n4\n,\n13\n,\n20\n,\n16\n,\n18\n,\n6\n]\n. Yet, despite substantial progress, efficient exploration in complex real-world environments remains challenging. Existing planners struggle to leverage long-term structural information embedded in the evolving map and often rely on fixed heuristics or rigid decision rules that limit their ability to adapt exploration strategies to different environmental characteristics.\nExisting autonomous exploration approaches are largely divided into conventional methods and learning based methods. Conventional planners maintain a geometric belief map and plan motions that trade off investigating new regions and further refining partially explored areas\n[\n28\n,\n2\n,\n21\n,\n13\n]\n. To improve scalability in large and complex environments, recent conventional planners increasingly rely on hierarchical planning, which decouples exploration into a low-resolution global path computed over a coarse belief and a high-resolution local path computed over the nearby detailed map\n[\n29\n,\n4\n,\n16\n,\n18\n]\n. The key insight there is that fine reasoning is most effective locally, while coarse inference suffices for guiding long-term exploration. By contrast, learning-based methods formulate exploration as a partially observable Markov decision process and use RL to learn policies that map observations to next waypoint\n[\n9\n,\n10\n,\n5\n,\n7\n]\n. These methods improve exploration efficiency through reward designs that encourage informative observations and reduce redundant motions, often supported by neural architectures that capture spatial correlations in the evolving map.\nDespite these advances, existing methods still struggle to exploit long-term information and to adapt their exploration strategy in response to evolving environments.\nConventional planners rely on fixed hyperparameters that determine how they balance path length with information gain or how they choose the resolution of the global representation. These parameters typically remain constant throughout deployment, which prevents the planner from adjusting its behavior in different environments. As a result, the planner may act too conservatively in open spaces or spend unnecessary effort refining cluttered areas. Even hierarchical planners face similar constraints. Their performance depends on a predefined coarse-to-fine decomposition of the map, which restricts how global structural information can influence local decisions.\nLearning-based planners encounter a different but equally important limitation. The objective in autonomous exploration is to minimize the overall time or distance required to complete coverage, yet this objective only becomes observable at the end of an episode. The reward signal therefore becomes extremely sparse, making it difficult for the policy to learn behaviors that depend on long-term consequences. Existing methods address this issue by introducing dense rewards that provide immediate feedback proportional to newly observed information\n[\n9\n,\n10\n,\n26\n,\n30\n]\n. Although this improves training stability, it also biases the optimization process toward short-term, immediately observable information gain.\nAs a result, long-horizon credit assignment becomes significantly more challenging in practice, making it difficult for the policy to reliably acquire behaviors such as long-distance backtracking or exploiting distant structural cues that only become relevant later in the mission.\nConsequently, both conventional and learning-based approaches still struggle to utilize long-term information effectively and to adjust their exploration strategy as environmental conditions change.\nInspired by the impressive intelligence emerging in recent AI developments, we propose FARE, a novel hierarchical fastâ€“slow thinking framework that separates global reasoning from local decision making.\nOur slow-thinking module operates at the global level and reasons over a sparse belief graph derived from community detection and modularity-based clustering\n[\n6\n]\n. Given a concise textual description of the environment, our slow-thinking module extracts high-level spatial characteristics and obstacle configurations, which are then used to synthesize an exploration strategy tailored to the overall map structure. This strategy is grounded onto the global belief graph to generate a sequence of global waypoints that reflect long-range coverage priorities and adapt as the partial map is incrementally updated.\nOur fast-thinking module operates at the local level and uses a reinforcement learning policy to select actions based on dense local map information, including frontiers and nearby geometric features. During deployment, the policy reacts in real time to local observations while also incorporating the global waypoints supplied by the slow-thinking module. To ensure that the learned policy can follow long-horizon guidance without relying on overly myopic reward shaping, the training process includes an additional reward term that encourages consistency with the global waypoints. This design allows the policy to remain responsive to local conditions while still pursuing long-term objectives.\nThrough this structured interaction, our framework effectively integrates global reasoning with local decision, enabling the robot to exploit long-term information and to adapt its exploration behavior as the environment unfolds.\nAs demonstrated in simulations and real-world robot validation, this design reduces unnecessary backtracking and yields more efficient exploration trajectories.\nII\nRELATED WORK\nII-A\nAutonomous Exploration\nAutonomous exploration has been widely studied using both conventional and learning-based approaches.\nConventional planners are predominantly frontier-based, selecting exploration targets at the boundary between known and unknown regions, where early methods directly navigate to individual frontier points, leading to greedy and myopic behavior\n[\n14\n]\n.\nLater works reformulate exploration as a viewpoint selection problem, in which candidate viewpoints are sampled and evaluated based on expected information gain and feasibility, enabling non-myopic planning through sampling-based strategies\n[\n2\n,\n27\n]\n.\nTo improve scalability in large environments, many works adopt hierarchical designs that combine fine-grained local planning with coarse global representations.\nRepresentative approaches construct sparse global frontier graphs or viewpoint graphs to guide long-range navigation, while local planners handle detailed viewpoint sampling and collision checking\n[\n4\n,\n16\n]\n.\nWhile effective, these methods rely on manually designed global representations and planning heuristics, and require careful balancing between representation sparsity and coverage, which is difficult to achieve in environments with unknown or evolving structures.\nLearning-based exploration methods aim to alleviate manual design by training policies to guide exploration decisions.\nSome works employ supervised learning to explicitly predict occupancy or semantic maps from partial observations, and then integrate the predicted maps into conventional frontier-based planners\n[\n9\n,\n26\n]\n.\nMore recent approaches formulate exploration as a sequential decision-making problem and use reinforcement learning to estimate long-term returns from observations.\nMany of these methods rely on convolutional neural networks (CNNs) to process fixed-size map representations or local occupancy grids\n[\n19\n,\n9\n,\n26\n,\n30\n]\n, with different action abstractions ranging from frontier or viewpoint selection\n[\n9\n,\n26\n]\nto direct navigation commands such as velocity or steering control\n[\n30\n,\n10\n]\n.\nTo address the limited scalability and robustness of CNN-based planners, recent works adopt graph learning-based representations and formulate exploration as a next-viewpoint selection problem on a graph\n[\n5\n,\n7\n,\n6\n]\n, enabling variable-sized environments.\nDespite these advances, existing methods still struggle to exploit long-term information and adapt their exploration behavior as the environment evolves, largely because exploration decisions are conditioned on local observations or fixed representations, without explicitly leveraging readily available, high-level environment priors to guide planning.\nII-B\nLLM-based Graph Reasoning\nLLM-based graph reasoning has advanced substantially in recent years, as an increasing body of work shows that large language models can exploit structured relational information when graphs are presented in suitable, structured formats.\nA central challenge in this line of research lies in how graph structures are encoded and provided to LLMs in a way that preserves relational semantics while remaining compatible with text-based or embedding-based model interfaces.\nTo this end, widely adopted approaches introduce structured graph verbalizers, embedding projectors, or instruction-tuned schemas that convert nodes, edges, and local neighborhoods into sequential representations that LLMs can reliably process\n[\n15\n,\n23\n,\n11\n,\n12\n]\n.\nBuilding on such representations, another line of work focuses on strengthening LLM reasoning capabilities over graphs.\nRecent methods incorporate guided graph traversal, community-aware pruning, and explicitly structured reasoning steps to facilitate multi-hop inference and long-range dependency modeling on complex graphs\n[\n22\n,\n1\n,\n17\n]\n.\nIn parallel, hybrid approaches couple LLMs with dedicated graph encoders or multimodal modules, allowing symbolic or geometric graph features to complement language-based reasoning, as exemplified by GraphLLM and multimodal extensions such as GITA\n[\n8\n,\n24\n]\n.\nDespite these advances, existing LLM-based graph reasoning methods are evaluated primarily on static, offline benchmarks, and rarely address the challenges posed by dynamic, partially observed, and incrementally evolving graph structures that arise in real-world robotic environments.\nIII\nPROBLEM FORMULATION\nLet\nâ„°\nâŠ‚\nâ„\n2\n\\mathcal{E}\\subset\\mathbb{R}^{2}\ndenote a bounded and initially unknown environment, represented by a two-dimensional occupancy grid. During exploration, the robot incrementally constructs a partial map\nâ„³\n\\mathcal{M}\n, which is decomposed into known and unknown regions,\nâ„³\n=\nâ„³\nu\nâˆª\nâ„³\nk\n.\n\\mathcal{M}=\\mathcal{M}_{\\text{u}}\\cup\\mathcal{M}_{\\text{k}}.\n(1)\nThe known region\nâ„³\nk\n\\mathcal{M}_{\\text{k}}\nis further partitioned into free and occupied space\nâ„³\nk\n=\nâ„³\nf\nâˆª\nâ„³\no\n\\mathcal{M}_{\\text{k}}=\\mathcal{M}_{\\text{f}}\\cup\\mathcal{M}_{\\text{o}}\n, where\nâ„³\nf\n\\mathcal{M}_{\\text{f}}\ndenotes traversable space and\nâ„³\no\n\\mathcal{M}_{\\text{o}}\ndenotes occupied space.\nAt the beginning of exploration, the environment is completely unknown, i.e.,\nâ„³\n=\nâ„³\nu\n\\mathcal{M}=\\mathcal{M}_{\\text{u}}\n. At each decision step, the robot acquires observations using an omnidirectional LiDAR with sensing range\nd\ns\nd_{s}\n, and cells within the sensing range are classified as either free or occupied according to traversability.\nThe objective of autonomous exploration is to compute a collision-free trajectory that minimizes the total traversal cost while completing the exploration,\nÏ„\nâˆ—\n=\nargmin\nÏ„\nâˆˆ\nğ’¯\nâ€‹\nL\nâ€‹\n(\nÏ„\n)\n,\ns.t.\nâ€‹\nâ„³\nk\n=\nâ„³\ng\n\\tau^{*}=\\underset{\\tau\\in\\mathcal{T}}{\\operatorname{argmin}}L(\\tau),\\quad\\text{ s.t. }\\mathcal{M}_{\\text{k}}=\\mathcal{M}_{g}\n(2)\nwhere\nğ’¯\n\\mathcal{T}\ndenotes the set of feasible trajectories and\nL\n:\nÏ„\nâ†’\nâ„\n+\nL:\\tau\\rightarrow\\mathbb{R}^{+}\nmaps a trajectory to its total path length. While the ground-truth map\nâ„³\ng\n\\mathcal{M}_{g}\nis not available during real-world deployment, it is accessible in simulation and benchmark settings for evaluation. In practice, exploration completion is commonly approximated by the absence of remaining unknown or frontier regions.\nIV\nMETHODOLOGY\nIV-A\nHierarchical Robot Belief Graph\nIn this work, we construct a hierarchical robot belief graph through the community-based method proposed in\n[\n6\n]\n. It represents the neighboring area of the robot as a dense local graph, which will serve as the input for the fast-thinking module, and the distant areas as a sparse global graph, which will serve as the input for the slow-thinking module.\nIV-A\n1\nLocal Belief Graph\nThe robot belief is represented by a collision-free graph constructed from onboard sensor data. The robot trajectory is defined as a sequence of viewpoints\nÏ„\n=\n(\nv\n0\n,\nv\n1\n,\nâ€¦\n)\n\\tau=(v_{0},v_{1},\\dots)\n, where each waypoint\nv\ni\nâˆˆ\nâ„³\nf\nv_{i}\\in\\mathcal{M}_{f}\nlies in free space.\nAt each decision step\nt\nt\n, a set of candidate viewpoints\nV\nt\n=\n{\nv\n0\n,\nv\n1\n,\nâ€¦\n}\nV_{t}=\\{v_{0},v_{1},\\dots\\}\n, with\nv\ni\n=\n(\nx\ni\n,\ny\ni\n)\nâˆˆ\nâ„³\nf\nv_{i}=(x_{i},y_{i})\\in\\mathcal{M}_{f}\n, is uniformly sampled from the current free space following the approach in TARE. Each viewpoint is connected to its\nk\nk\nnearest neighbors, and edges intersecting occupied or unknown regions are removed, yielding a collision-free graph\nG\nt\n=\n(\nV\nt\n,\nE\nt\n)\nG_{t}=(V_{t},E_{t})\n.\nThe robot selects viewpoints sequentially, forming a trajectory\nÏ„\ni\nâˆˆ\nV\nt\n\\tau_{i}\\in V_{t}\n. Each node\nv\ni\nv_{i}\nis assigned a utility value\nu\ni\nu_{i}\n, defined as the number of observable frontiers within sensor range. A frontier\nf\nj\nf_{j}\nis observable from\nv\ni\nv_{i}\nif the line segment\nL\nâ€‹\n(\nv\ni\n,\nf\nj\n)\nL(v_{i},f_{j})\nis collision-free and\nâ€–\nf\nj\nâˆ’\nv\ni\nâ€–\nâ‰¤\nd\ns\n\\|f_{j}-v_{i}\\|\\leq d_{s}\n. The utility is defined as\nu\ni\n=\n|\nF\no\n,\ni\n|\n,\n\\displaystyle u_{i}=|F_{o,i}|,\n(3)\nâˆ€\nf\nj\nâˆˆ\nF\no\n,\ni\n,\nâ€–\nf\nj\nâˆ’\nv\ni\nâ€–\nâ‰¤\nd\ns\n,\n\\displaystyle\\forall f_{j}\\in F_{o,i},\\quad\\|f_{j}-v_{i}\\|\\leq d_{s},\nL\nâ€‹\n(\nv\ni\n,\nf\nj\n)\nâˆ©\n(\nâ„³\nâˆ’\nâ„³\nf\n)\n=\nâˆ…\n.\n\\displaystyle L(v_{i},f_{j})\\cap(\\mathcal{M}-\\mathcal{M}_{f})=\\emptyset.\nwhere\nF\no\n,\ni\nF_{o,i}\ndenotes the set of observable frontiers at\nv\ni\nv_{i}\n. Then we introduce a square sliding window\nğ’²\nlocal\n\\mathcal{W}_{\\text{local}}\nof size\nd\nÃ—\nd\nd\\times d\n, centered at the robotâ€™s current position\nv\ncur\nv_{\\text{cur}}\n, and construct the\nlocal belief graph\nG\nlocal\n=\n(\nV\nlocal\n,\nE\nlocal\n)\nG_{\\text{local}}=(V_{\\text{local}},E_{\\text{local}})\n. Here,\nV\nlocal\nâŠ‚\nV\nt\nV_{\\text{local}}\\subset V_{t}\ncontains all candidate nodes within the window, and\nE\nlocal\nâŠ‚\nE\nt\nE_{\\text{local}}\\subset E_{t}\nincludes the edges connecting those nodes.\nIV-A\n2\nGlobal Belief Graph with Modularity-Based Pruning\nWe construct the\nglobal belief graph\nG\nglobal\n=\n(\nV\nglobal\n,\nE\nglobal\n)\nG_{\\text{global}}=(V_{\\text{global}},E_{\\text{global}})\nby jointly performing community detection and modularity-based pruning on the collision-free graph\nG\nt\nG_{t}\n. Instead of retaining all detected communities, we explicitly select a subset of structurally informative communities according to their modularity contribution, and only these communities are promoted as high-level nodes for global reasoning.\nSpecifically, we first apply modularity-based community detection on\nG\nt\nG_{t}\n[\n3\n]\n, where the graph modularity is defined as\nQ\n=\n1\n2\nâ€‹\nm\nâ€‹\nâˆ‘\ni\n,\nj\n[\nA\ni\nâ€‹\nj\nâˆ’\nk\ni\nâ€‹\nk\nj\n2\nâ€‹\nm\n]\nâ€‹\nÎ´\nâ€‹\n(\nc\ni\n,\nc\nj\n)\n,\nQ=\\frac{1}{2m}\\sum_{i,j}\\left[A_{ij}-\\frac{k_{i}k_{j}}{2m}\\right]\\delta(c_{i},c_{j}),\n(4)\nwith\nm\nm\ndenoting the total number of edges,\nA\ni\nâ€‹\nj\nA_{ij}\nthe adjacency matrix,\nk\ni\nk_{i}\nthe degree of node\ni\ni\n, and\nÎ´\nâ€‹\n(\nâ‹…\n)\n\\delta(\\cdot)\nthe community indicator.\nLet\nğ’\n\\mathcal{C}\ndenote the set of communities detected from\nG\nt\nG_{t}\n. For simplicity, we ignore edge weights and directions and reorganize the modularity objective at the community level:\nQ\n=\n1\n2\nâ€‹\nm\nâ€‹\nâˆ‘\nc\nâˆˆ\nğ’\nâˆ‘\ni\n,\nj\nâˆˆ\nc\n[\nA\ni\nâ€‹\nj\nâˆ’\nk\ni\nâ€‹\nk\nj\n2\nâ€‹\nm\n]\n.\nQ=\\frac{1}{2m}\\sum_{c\\in\\mathcal{C}}\\sum_{i,j\\in c}\\left[A_{ij}-\\frac{k_{i}k_{j}}{2m}\\right].\n(5)\nFor each community\nc\nc\n, let\nâˆ‘\nin\n\\sum{\\mathrm{in}}\ndenote the number of edges in\nc\nc\n, and let\nâˆ‘\ntot\n\\sum{\\mathrm{tot}}\ndenote the number of edges connected to\nc\nc\n. Thus, we have:\nâˆ‘\ni\n,\nj\nâˆˆ\nc\nk\ni\nâ€‹\nk\nj\n2\nâ€‹\nm\n=\n(\nâˆ‘\ntot\n)\n2\n2\nâ€‹\nm\n,\n\\sum_{i,j\\in c}\\frac{k_{i}k_{j}}{2m}=\\frac{(\\sum{\\mathrm{tot}})^{2}}{2m},\n(6)\nthe modularity can be rewritten as\nQ\n=\n1\n2\nâ€‹\nm\nâ€‹\nâˆ‘\nc\nâˆˆ\nğ’\n[\nâˆ‘\nin\nâˆ’\n(\nâˆ‘\ntot\n)\n2\n2\nâ€‹\nm\n]\n.\nQ=\\frac{1}{2m}\\sum_{c\\in\\mathcal{C}}\\left[\\sum{\\mathrm{in}}-\\frac{(\\sum{\\mathrm{tot}})^{2}}{2m}\\right].\n(7)\nThen, the modularity of community\nc\nc\nis\nQ\nâ€‹\n(\nc\n)\n=\nâˆ‘\nin\nâˆ’\n(\nâˆ‘\ntot\n)\n2\n2\nâ€‹\nm\n.\nQ(c)=\\sum{\\mathrm{in}}-\\frac{(\\sum{\\mathrm{tot}})^{2}}{2m}.\n(8)\nRather than preserving all communities, we directly integrate a pruning step into the construction of\nG\nglobal\nG_{\\text{global}}\nby retaining only the top-\nk\nk\ncommunities with the highest modularity contributions:\nğ’\nâ€²\n:=\nargtopk\nc\nâŠ†\nğ’\nâ¡\nQ\nâ€‹\n(\nc\n)\n.\n\\mathcal{C}^{\\prime}:=\\operatorname{argtopk}_{c\\subseteq\\mathcal{C}}Q(c).\n(9)\nEach retained community\nc\nâŠ†\nğ’\nâ€²\nc\\subseteq\\mathcal{C}^{\\prime}\nis abstracted as a node in\nV\nglobal\nV_{\\text{global}}\n, while edges in\nE\nglobal\nE_{\\text{global}}\nare induced by inter-community connectivity in\nG\nt\nG_{t}\n. In this way, the global belief graph is constructed directly from a pruned set of structurally coherent communities, yielding a compact yet informative high-level representation that significantly reduces reasoning complexity while preserving the dominant topological structure of the environment.\nIV-B\nSlow-Thinking Module\nIV-B\n1\nEnvironment Conditioned Strategy Generation\nThis part provides a principled mechanism for translating high-level environment descriptions into strategy-level guidance for autonomous exploration. Rather than directly prescribing actions or trajectories, it operates at the level of environment-conditioned reasoning, bridging semantic understanding and long-horizon planning objectives.\nAccording to the intuitive impression formed after a brief exposure to the environment, we give a concise natural language description that summarizes the environment type and its composition. The slow-thinking module first performs structured environment characterization using an LLM. To ensure interpretability and robustness, the analysis is constrained to a predefined schema that decomposes the environment into three complementary aspects.\nSpatial characteristics\ncapture the global layout and navigability of the environment, including openness, structural complexity, topological connectivity, and corridor width.\nObstacle characteristics\ndescribe the distribution and structure of obstacles, such as their density, predictability, and vertical variation.\nExploration challenges\nreflect task-level difficulties induced by the environment, including navigation difficulty, the likelihood of dead ends, and the necessity of backtracking.\nEnvironment Characterization Example\nâ€\nspatial characteristics\nâ€:\nopenness\n: â€confinedâ€,\ncomplexity\n: â€moderateâ€,\nconnectivity\n: â€lowâ€,\ncorridor width\n: â€narrowâ€\nâ€\nobstacle characteristics\nâ€:\ndensity\n: â€moderateâ€,\npredictability\n: â€irregularâ€,\nheight variation\n: â€flatâ€\nâ€\nexploration challenges\nâ€:\nnavigation difficulty\n: â€moderateâ€,\ndead end probability\n: â€moderateâ€,\nbacktracking necessity\n: â€moderateâ€\nBased on the extracted environment characteristics, we instantiate an exploration strategy by populating a set of predefined strategy dimensions. These dimensions provide a structured representation of agent-level exploration behavior and are designed to capture recurring decision patterns across diverse environments. Specifically, the strategy is parameterized along four complementary axes:\nspatial strategy\n,\nefficiency strategy\n,\nsafety strategy\n, and\ntask strategy\n. Each axis governs a distinct aspect of exploration behavior, including coverage patterns and traversal order, energy and time trade-offs, risk sensitivity and obstacle handling, and completion objectives and information priorities. The environment characteristics directly condition the values assigned to these dimensions. For instance, environments exhibiting low topological connectivity increase tolerance for backtracking and enforce stronger awareness of escape routes, reflecting the higher risk of entrapment. Dense or irregular obstacles bias the strategy toward conservative motion, larger obstacle clearance, and cautious treatment of unexplored regions. Similarly, high navigation difficulty favors exploration behaviors that prioritize reliability and path quality over speed. In this way, environment semantics are systematically translated into structured exploration strategies that remain interpretable and explicitly grounded in environmental factors.\nStrategy Example\nâ€™\ndescription\nâ€™: â€™Outdoor environment with natural obstacles and terrain variationsâ€™,\nâ€™\nspatial\nâ€™: \nâ€™coverage strategyâ€™: â€™boundary firstâ€™,\nâ€™direction biasâ€™: â€™perimeter followingâ€™,\nâ€™depth strategyâ€™: â€™balanced depth breadthâ€™,\nâ€™corridor handlingâ€™: â€™natural pathâ€™\n,\nâ€™\nefficiency\nâ€™: \nâ€™energy policyâ€™: â€™conservativeâ€™,\nâ€™time constraintâ€™: â€™moderateâ€™,\nâ€™backtrack toleranceâ€™: â€™moderateâ€™,\nâ€™revisit policyâ€™: â€™avoidâ€™\n,\nâ€™\nsafety\nâ€™: \nâ€™obstacle clearanceâ€™: â€™conservativeâ€™,\nâ€™unknown area approachâ€™: â€™standardâ€™,\nâ€™dead end handlingâ€™: â€™explore carefullyâ€™,\nâ€™escape route awarenessâ€™: â€™always maintainâ€™\n,\nâ€™\ntask\nâ€™: \nâ€™completion criteriaâ€™: â€™time limitedâ€™,\nâ€™information priorityâ€™: â€™object detectionâ€™,\nâ€™quality vs speedâ€™: â€™balancedâ€™\nIV-B\n2\nGraph Reasoning\nGiven the pruned global belief graph\nG\nglobal\nG_{\\text{global}}\n, a textual strategy prompt\nx\nx\nand the episode memory\nm\nm\n, we perform iterative graph reasoning with the instance\nÎ \n\\Pi\nof LLMs. At reasoning depth\ni\ni\n, the LLMs perform:\nÏ„\ng\n=\nÎ \nâ€‹\n(\nG\nglobal\n,\nx\n,\nm\n)\n\\tau_{g}=\\Pi(G_{\\text{global}},\\,x,m)\n(10)\nwhere\nÏ„\ng\n=\n[\nv\ncur\n,\nv\n1\n,\nâ€¦\n,\nv\nm\n]\n\\tau_{g}=[v_{\\mathrm{cur}},v_{1},\\dots,v_{m}]\nis a global path from the current node to an unexplored node.\nIV-C\nFast-Thinking Module\nIV-C\n1\nPolicy Network\nThe fast-thinking module operates on a structured observation that integrates the local graph, utility, and global path. At each time step, the observation is defined as\no\nt\n=\n(\nG\nâˆ—\n,\nÏ„\nt\n)\no_{t}=\\left(G^{*},\\tau_{t}\\right)\n.\nFollowing\n[\n5\n,\n7\n,\n6\n]\n, we build an informative graph\nG\nâˆ—\n=\n(\nV\nl\nâˆ—\n,\nE\nl\n)\nG^{*}=\\left(V_{l}^{*},E_{l}\\right)\n, which shares the same edge set and node positions with the local graph\nG\nloacl\nG_{\\text{loacl}}\n. In addition to the position\n(\nx\ni\n,\ny\ni\n)\n\\left(x_{i},y_{i}\\right)\n, each node\nv\ni\nâ€²\n=\n(\nx\ni\n,\ny\ni\n,\nu\ni\n,\ng\ni\n)\nâˆˆ\nV\nl\nâˆ—\nv_{i}^{\\prime}=\\left(x_{i},y_{i},u_{i},g_{i}\\right)\\in V_{l}^{*}\nin the informative graph has two more properties: the utility\nu\ni\nu_{i}\nand guidepost\ng\ni\ng_{i}\n(a binary signal that denotes whether the location of the node is in the global paths).\nThe planner selects a neighboring node\nv\ni\nâˆˆ\nğ’©\nâ€‹\n(\nv\ncur\n)\nv_{i}\\in\\mathcal{N}\\left(v_{\\text{cur }}\\right)\nas the next waypoint\nw\nt\nw_{t}\n, and the robot executes the action\na\nt\na_{t}\nto move toward\nw\nt\nw_{t}\n.\nOur policy network comprises attention-based encoder and decoder modules tailored to graph-structured inputs. We first compute query (\nğª\ni\n\\mathbf{q}_{i}\n), key (\nğ¤\ni\n\\mathbf{k}_{i}\n), and value (\nğ¯\ni\n\\mathbf{v}_{i}\n) vectors via learned linear transformations:\nğª\ni\n=\nW\nq\nâ€‹\nğ¡\ni\n(\nq\n)\n,\nğ¤\ni\n=\nW\nk\nâ€‹\nğ¡\ni\n(\nk\n,\nv\n)\n,\nğ¯\ni\n=\nW\nv\nâ€‹\nğ¡\ni\n(\nk\n,\nv\n)\n,\n\\mathbf{q}_{i}=W^{q}\\mathbf{h}_{i}^{(q)},\\quad\\mathbf{k}_{i}=W^{k}\\mathbf{h}_{i}^{(k,v)},\\quad\\mathbf{v}_{i}=W^{v}\\mathbf{h}_{i}^{(k,v)},\n(11)\nwhere\nğ¡\ni\nâˆˆ\nâ„\nd\nf\n\\mathbf{h}_{i}\\in\\mathbb{R}^{d_{f}}\ndenotes the input node feature vector associated with node\nv\ni\nâ€²\nv_{i}^{\\prime}\n,\nW\nq\n,\nW\nk\n,\nW\nv\nâˆˆ\nâ„\nd\nf\nÃ—\nd\nf\nW^{q},W^{k},W^{v}\\in\\mathbb{R}^{d_{f}\\times d_{f}}\nare learnable projection matrices, and superscripts\n(\nq\n)\n(q)\nand\n(\nk\n,\nv\n)\n(k,v)\nindicate query or key/value projections, respectively. The scaled dot-product attention scores are then computed between pairs of nodes as\nu\ni\nâ€‹\nj\n=\nğª\ni\nâŠ¤\nâ€‹\nğ¤\nj\nd\nf\nu_{ij}=\\frac{\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}}{\\sqrt{d_{f}}}\nand transformed into normalized attention weights subject to edge constraints indicated by an adjacency-based mask matrix\nM\nM\n:\nw\ni\nâ€‹\nj\n\\displaystyle w_{ij}\n=\nexp\nâ¡\n(\nu\ni\nâ€‹\nj\n)\nâ‹…\n(\n1\nâˆ’\nM\ni\nâ€‹\nj\n)\nâˆ‘\nj\nâ€²\n=\n1\nn\nexp\nâ¡\n(\nu\ni\nâ€‹\nj\nâ€²\n)\nâ‹…\n(\n1\nâˆ’\nM\ni\nâ€‹\nj\nâ€²\n)\n,\n\\displaystyle=\\frac{\\exp(u_{ij})\\cdot(1-M_{ij})}{\\sum_{j^{\\prime}=1}^{n}\\exp(u_{ij^{\\prime}})\\cdot(1-M_{ij^{\\prime}})},\n(12)\nM\ni\nâ€‹\nj\n\\displaystyle M_{ij}\n=\n{\n0\n,\n(\nv\ni\n,\nv\nj\n)\nâˆˆ\nE\n1\n,\n(\nv\ni\n,\nv\nj\n)\nâˆ‰\nE\n\\displaystyle=\nwhere\nE\nE\nis the edge set defining the current adjacency relationships. Finally, the output node features\nğ¡\ni\nâ€²\n=\nâˆ‘\nj\n=\n1\nn\nw\ni\nâ€‹\nj\nâ€‹\nğ¯\nj\n\\mathbf{h}_{i}^{\\prime}=\\sum_{j=1}^{n}w_{ij}\\mathbf{v}_{j}\naggregates the weighted values across neighbors.\nIV-C\n2\nInstruction Following\nFigure 1\n:\nIllustration of the environment state at timestep\nt\nt\n, showing the robot at position\np\nt\np_{t}\n, the policy-selected waypoint\nw\nt\nw_{t}\n, and the globally guided next waypoint\nw\nt\nâˆ—\nw_{t}^{*}\n.\nBuilding upon the reward design in\n[\n6\n]\n, we introduce an instruction following objective to encourage the fast-thinking policy to adhere to the long horizon guidance generated by the slow-thinking module. At decision step\nt\nt\n, the fast-thinking policy is guided by a global path\nÏ„\nt\nâˆ—\n\\tau_{t}^{*}\n, whose next waypoint is denoted as\nw\nt\nâˆ—\nw_{t}^{*}\n, and selects a local waypoint\nw\nt\nw_{t}\nfor execution.\nAt a high level, following the global guidance reduces unnecessary detours and implicitly shortens the executed trajectory. While this objective can be expressed in terms of path length deviation, directly optimizing such quantities is impractical for reinforcement learning. Instead, we optimize a smooth surrogate that preserves the monotonic relationship with deviation from the global guidance. We define the normalized deviation between the policy-selected waypoint and the globally guided waypoint as\nd\nt\n=\nâ€–\nw\nt\nâˆ’\nw\nt\nâˆ—\nâ€–\n4\nâ€‹\nÎ”\nnode\nâ€‹\n2\n,\nd_{t}=\\frac{\\|w_{t}-w_{t}^{*}\\|}{4\\,\\Delta_{\\text{node}}\\sqrt{2}},\n(13)\nwhere\nÎ”\nnode\n\\Delta_{\\text{node}}\ndenotes the node resolution. The normalization constant corresponds to an upper bound on admissible deviation measured in grid units, ensuring scale invariance across environments. The instruction following reward is then defined as an exponential penalty on the normalized deviation,\nr\nt\ndev\n=\nâˆ’\ne\nd\nt\nâˆ’\n1\ne\nâˆ’\n1\n,\nr_{t}^{\\text{dev}}=-\\frac{e^{d_{t}}-1}{e-1},\n(14)\n(a)\n(b)\n(c)\nFigure 2\n:\nPerformance comparison between FARE and conventional baselines across 10 runs per method in each environment.\nTABLE I\n:\nComparison results in three environment.\n(a)\nIndoor\nDistance (\nm\nm\n)\nTime (\ns\ns\n)\nDSVP\n1511\nâ€‹\n(\nÂ±\n75\n)\n1511(\\pm 75)\n931\nâ€‹\n(\nÂ±\n61\n)\n931(\\pm 61)\nTARE\n1209\nâ€‹\n(\nÂ±\n42\n)\n1209(\\pm 42)\n658\nâ€‹\n(\nÂ±\n23\n)\n658(\\pm 23)\nARiADNE\n1053\nâ€‹\n(\nÂ±\n63\n)\n1053(\\pm 63)\n610\nâ€‹\n(\nÂ±\n24\n)\n610(\\pm 24)\nHEADER\n1030\nâ€‹\n(\nÂ±\n40\n)\n1030(\\pm 40)\n576\nâ€‹\n(\nÂ±\n26\n)\n576(\\pm 26)\nFARE\n1048\nâ€‹\n(\nÂ±\n13\n)\n\\textbf{1048}(\\pm 13)\n590\nâ€‹\n(\nÂ±\n10\n)\n\\textbf{590}(\\pm 10)\n(b)\nForest\nDistance (\nm\nm\n)\nTime (\ns\ns\n)\nDSVP\n2058\nâ€‹\n(\nÂ±\n92\n)\n2058(\\pm 92)\n1083\nâ€‹\n(\nÂ±\n60\n)\n1083(\\pm 60)\nTARE\n1363\nâ€‹\n(\nÂ±\n43\n)\n1363(\\pm 43)\n711\nâ€‹\n(\nÂ±\n21\n)\n711(\\pm 21)\nARiADNE\n1320\nâ€‹\n(\nÂ±\n81\n)\n1320(\\pm 81)\n790\nâ€‹\n(\nÂ±\n62\n)\n790(\\pm 62)\nHEADER\n1230\nâ€‹\n(\nÂ±\n72\n)\n1230(\\pm 72)\n725\nâ€‹\n(\nÂ±\n36\n)\n725(\\pm 36)\nFARE\n1090\nâ€‹\n(\nÂ±\n21\n)\n\\textbf{1090}(\\pm 21)\n680\nâ€‹\n(\nÂ±\n10\n)\n\\textbf{680}(\\pm 10)\n(c)\nWarehouse\nDistance (\nm\nm\n)\nTime (\ns\ns\n)\nDSVP\n869\nâ€‹\n(\nÂ±\n42\n)\n869(\\pm 42)\n582\nâ€‹\n(\nÂ±\n32\n)\n582(\\pm 32)\nTARE\n652\nâ€‹\n(\nÂ±\n31\n)\n652(\\pm 31)\n366\nâ€‹\n(\nÂ±\n22\n)\n366(\\pm 22)\nARiADNE\n521\nâ€‹\n(\nÂ±\n16\n)\n521(\\pm 16)\n362\nâ€‹\n(\nÂ±\n40\n)\n362(\\pm 40)\nHEADER\n492\nâ€‹\n(\nÂ±\n17\n)\n492(\\pm 17)\n286\nâ€‹\n(\nÂ±\n16\n)\n286(\\pm 16)\nFARE\n441\nâ€‹\n(\nÂ±\n15\n)\n\\textbf{441}(\\pm 15)\n252\nâ€‹\n(\nÂ±\n8\n)\n\\textbf{252}(\\pm 8)\n(a)\n(b)\n(c)\nFigure 3\n:\nDemonstration of the exploration trajectories output by FARE in both indoor and outdoor Gazebo simulations. The Trajectory is color-coded to represent the robotâ€™s movement over time.\nwhich yields\nr\nt\ndev\nâˆˆ\n[\nâˆ’\n1\n,\n0\n]\nr_{t}^{\\text{dev}}\\in[-1,0]\n. This formulation assigns mild penalties for small deviations while increasingly suppressing large deviations from the global guidance, providing smooth and stable gradients for policy optimization. By this construction, the waypoint deviation\nd\nt\nd_{t}\nis monotonically related to detours from the global path. Therefore, minimizing the cumulative instruction-following penalty implicitly encourages shorter executed trajectories, while remaining fully compatible with efficient reinforcement learning.\nV\nExperiments\nWe conduct a set of experiments to evaluate the effectiveness and robustness of FARE. We first perform comparative evaluations in Gazebo simulation across three representative environmentsâ€”\nindoor\n,\nforest\n, and\nwarehouse\nâ€”to assess exploration performance under diverse structural characteristics. We then deploy FARE on a real mobile robot and validate its performance in a large-scale campus environment. All experiments use the same trained model and identical system configurations unless otherwise specified.\nV-A\nComparison Analysis\n(a)\n(b)\n(c)\nFigure 4\n:\nValidations of FARE in real-world scenarios on a wheeled robot.\nWe integrate FARE into the robot operating system (ROS) and compare it with representative state-of-the-art exploration planners, including\nTARE\n[\n4\n]\n,\nDSVP\n[\n29\n]\n,\nARiADNE\n[\n7\n]\nand\nHEADER\n[\n6\n]\n. For a fair comparison, ARiADNE is implemented with the same graph rarefaction strategy as in\n[\n7\n]\n, and all graph-related settings are kept consistent across methods.\nThe comparison is conducted in Gazebo simulation across three benchmark environments: an\nindoor\nenvironment, a\nforest\nenvironment, and a\nwarehouse\nenvironment. All experiments are performed on a four-wheeled differential-drive robot equipped with a 16-channel 3D LiDAR, with a maximum speed of\n2\nâ€‹\nm\n/\ns\n2\\,\\mathrm{m/s}\n.\nFor different environments, FARE only adjusts the node resolution parameter\nÎ”\nnode\n\\Delta_{\\text{node}}\n, which is varied from\n1.2\nâ€‹\nm\n1.2\\,\\mathrm{m}\nto\n2.8\nâ€‹\nm\n2.8\\,\\mathrm{m}\n. The baseline planners generally require tuning of multiple parameters to achieve their best performance.\nIn addition, FARE is provided with a short environment description specific to each benchmark environment to condition the generation of the exploration strategy, with the slow-thinking module instantiated as\nQwen3-14B\n. The environment descriptions are as follows: Indoorâ€”modern indoor office building with long corridors, meeting rooms, and cubicle areas; Forestâ€”outdoor forest environment with natural obstacles, trees, and uneven terrain; and Warehouseâ€”indoor warehouse composed of densely arranged box stacks forming narrow aisles.\nEach method is executed for 10 runs per environment. Travel distance and explored volume are reported in Fig.\n2\n, and quantitative results are summarized in Table\nI\n.\nFARE achieves performance on par with other baselines in the indoor environment, where the environment is compact and lacks distinctive global-level structure. In the forest environment, FARE achieves a clear reduction in travel distance and makespan, and this advantage further widens in the warehouse environment, where FARE obtains the shortest paths and fastest completion times among all methods. Specifically, as illustrated by the trajectory visualizations in Fig.\n3\n, FARE does not rely solely on local frontier or utility signals. Instead, it systematically incorporates global structural cues during exploration. After reaching the boundary of the explored region, FARE tends to complete peripheral and corner areas early, rather than postponing them. In contrast, baseline methods often defer these regions and revisit them later, resulting in additional backtracking and reduced overall efficiency.\nWe attribute FAREâ€™s performance gains to two key factors. First, our hierarchical design enables environment-adaptive planning, allowing FARE to adjust its global exploration strategy according to the environment character. Second, effective coordination between global guidance and local execution allows the agent to balance long-horizon objectives with local reactivity. The slow-thinking module provide global direction, while the fast-thinking module remain flexible to exploit nearby informative regions.\nV-B\nHardware Validation\nWe validate\nFARE\non an Agilex Scout-mini wheeled robot equipped with an onboard\nJetson AGX Orin\n, which runs the LLM-based slow-thinking module using\nQwen3-14B\n. An Ouster OS0-32 LiDAR is used for perception, and FastLIO2\n[\n25\n]\nprovides odometry and mapping. For all experiments, the maximum robot speed is set to\n1\nâ€‹\nm\n/\ns\n1\\,\\mathrm{m/s}\n, the sensor range to\n8\nâ€‹\nm\n8\\,\\mathrm{m}\n, and the fast-thinking module replanning frequency to\n1\nâ€‹\nHz\n1\\,\\mathrm{Hz}\n.\nReal-world validation is conducted in a\n200\nâ€‹\nm\nÃ—\n130\nâ€‹\nm\n200m\\times 130m\nindoor teaching building\non campus. The environment consists of long corridors, rooms, and intersections, posing challenges in global reasoning and long-horizon exploration. We set the map resolution to\nÎ”\nmap\n=\n0.4\nâ€‹\nm\n\\Delta_{\\text{map}}=0.4\\,\\mathrm{m}\nand the node resolution to\nÎ”\nnode\n=\n0.8\nâ€‹\nm\n\\Delta_{\\text{node}}=0.8\\,\\mathrm{m}\n. These parameters are selected prior to deployment and remain fixed throughout the experiment.\nDuring deployment,\nFARE\nsuccessfully explores the entire building without manual intervention. The system maintains stable runtime performance during global guidance generation and local policy execution. This hardware experiment demonstrates that\nFARE\ncan effectively operate with onboard LLM inference and transfer from simulation to real-world environments.\nVI\nCONCLUSIONS\nIn this work, we propose FARE, a hierarchical autonomous exploration framework that separates environment-conditioned global reasoning from fast local decision-making. FARE translates concise natural language environment descriptions into structured, interpretable exploration strategies, and performs LLM-based graph reasoning on a pruned global belief graph to generate adaptive global guidance. A fast-thinking policy integrates local graph structure, utility signals, and global paths, and is explicitly trained to follow long-horizon guidance while retaining local flexibility. Together, these components enable coherent exploration behaviors that reduce redundant backtracking and improve efficiency, as validated in both simulation and real-world experiments.\nFuture work will extend FARE to multi-robot exploration with explicit inter-agent coordination, and incorporate vision-based semantic perception to enable online detection of environment changes, better supporting hybrid environments with abrupt scene-type transitions. We also plan to investigate richer environment representations and three-dimensional action spaces to further enhance generality.\nReferences\n[1]\nM. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, and T. Hoefler\n(2024)\nGraph of thoughts: solving elaborate problems with large language models\n.\nIn\nAAAI\n,\npp.Â 17682â€“17690\n.\nCited by:\nÂ§\nII-B\n.\n[2]\nA. Bircher, M. Kamel, K. Alexis, H. Oleynikova, and R. Siegwart\n(2016)\nReceding horizonâ€ next-best-viewâ€ planner for 3d exploration\n.\nIn\n2016 IEEE international conference on robotics and automation (ICRA)\n,\npp.Â 1462â€“1468\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[3]\nV. D. Blondel, J. Guillaume, R. Lambiotte, and E. Lefebvre\n(2008)\nFast unfolding of communities in large networks\n.\nJournal of statistical mechanics: theory and experiment\n2008\n(\n10\n),\npp.Â P10008\n.\nCited by:\nÂ§\nIV-A\n2\n.\n[4]\nC. Cao, H. Zhu, H. Choset, and J. Zhang\n(2021)\nTARE: a hierarchical framework for efficiently exploring complex 3d environments.\n.\nIn\nRobotics: Science and Systems\n,\nVol.\n5\n,\npp.Â 2\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nV-A\n.\n[5]\nY. Cao, T. Hou, Y. Wang, X. Yi, and G. Sartoretti\n(2023)\nAriadne: a reinforcement learning approach using attention-based deep networks for exploration\n.\nIn\n2023 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 10219â€“10225\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nIV-C\n1\n.\n[6]\nY. Cao, Y. Wang, J. Liang, S. Liao, Y. Zhang, P. Li, and G. Sartoretti\n(2025)\nHEADER: hierarchical robot exploration via attention-based deep reinforcement learning with expert-guided reward\n.\narXiv preprint arXiv:2510.15679\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nIV-A\n,\nÂ§\nIV-C\n1\n,\nÂ§\nIV-C\n2\n,\nÂ§\nV-A\n.\n[7]\nY. Cao, R. Zhao, Y. Wang, B. Xiang, and G. Sartoretti\n(2024)\nDeep reinforcement learning-based large-scale robot exploration\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n,\nÂ§\nIV-C\n1\n,\nÂ§\nV-A\n.\n[8]\nZ. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang\n(2023)\nGraphLLM: boosting graph reasoning ability of large language model\n.\nCoRR\nabs/2310.05845\n.\nCited by:\nÂ§\nII-B\n.\n[9]\nF. Chen, S. Bai, T. Shan, and B. Englot\n(2019)\nSelf-learning exploration and mapping for mobile robots via deep reinforcement learning\n.\nIn\nAiaa scitech 2019 forum\n,\npp.Â 0396\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-A\n.\n[10]\nF. Chen, J. D. Martin, Y. Huang, J. Wang, and B. Englot\n(2020)\nAutonomous exploration under uncertainty via deep reinforcement learning on graphs\n.\nIn\n2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 6140â€“6147\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-A\n.\n[11]\nR. Chen, T. Zhao, A. Jaiswal, N. Shah, and Z. Wang\n(2024)\nLlaga: large language and graph assistant\n.\narXiv preprint arXiv:2402.08170\n.\nCited by:\nÂ§\nII-B\n.\n[12]\nZ. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang, D. Yin, W. Fan, H. Liu,\net al.\n(2024)\nExploring the potential of large language models (llms) in learning on graphs\n.\nACM SIGKDD Explorations Newsletter\n25\n(\n2\n),\npp.Â 42â€“61\n.\nCited by:\nÂ§\nII-B\n.\n[13]\nT. Dang, M. Tranzatto, S. Khattak, F. Mascarich, K. Alexis, and M. Hutter\n(2020)\nGraph-based subterranean exploration path planning using aerial and legged robots\n.\nJournal of Field Robotics\n37\n(\n8\n),\npp.Â 1363â€“1388\n.\nCited by:\nÂ§I\n,\nÂ§I\n.\n[14]\nH. H. GonzÃ¡lez-Banos and J. Latombe\n(2002)\nNavigation strategies for exploring indoor environments\n.\nThe International Journal of Robotics Research\n21\n(\n10-11\n),\npp.Â 829â€“848\n.\nCited by:\nÂ§\nII-A\n.\n[15]\nJ. Guo, L. Du, H. Liu, M. Zhou, X. He, and S. Han\n(2023)\nGpt4graph: can large language models understand graph structured data? an empirical evaluation and benchmarking\n.\narXiv preprint arXiv:2305.15066\n.\nCited by:\nÂ§\nII-B\n.\n[16]\nJ. Huang, B. Zhou, Z. Fan, Y. Zhu, Y. Jie, L. Li, and H. Cheng\n(2023)\nFAEL: fast autonomous exploration for large-scale environments with a mobile robot\n.\nIEEE robotics and automation letters\n8\n(\n3\n),\npp.Â 1667â€“1674\n.\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nII-A\n.\n[17]\nX. Liang and Z. Gu\n(2025)\nFast think-on-graph: wider, deeper and faster reasoning of large language model on knowledge graph\n.\nIn\nAAAI\n,\npp.Â 24558â€“24566\n.\nCited by:\nÂ§\nII-B\n.\n[18]\nS. Long, Y. Li, C. Wu, B. Xu, and W. Fan\n(2024)\nHPHS: hierarchical planning based on hybrid frontier sampling for unknown environments exploration\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\npp.Â 12056â€“12063\n.\nCited by:\nÂ§I\n,\nÂ§I\n.\n[19]\nF. Niroui, K. Zhang, Z. Kashino, and G. Nejat\n(2019)\nDeep reinforcement learning robot for search and rescue applications: exploration in unknown cluttered environments\n.\nIEEE Robotics and Automation Letters\n4\n(\n2\n),\npp.Â 610â€“617\n.\nCited by:\nÂ§\nII-A\n.\n[20]\nO. Peltzer, A. Bouman, S. Kim, R. Senanayake, J. Ott, H. Delecki, M. Sobue, M. Kochenderfer, M. Schwager, J. Burdick,\net al.\n(2022)\nFIG-op: exploring large-scale unknown environments on a fixed time budget\n.\narXiv preprint arXiv:2203.06316\n.\nCited by:\nÂ§I\n.\n[21]\nM. Selin, M. Tiger, D. Duberg, F. Heintz, and P. Jensfelt\n(2019)\nEfficient autonomous exploration planning of large-scale 3-d environments\n.\nIEEE Robotics and Automation Letters\n4\n(\n2\n),\npp.Â 1699â€“1706\n.\nCited by:\nÂ§I\n,\nÂ§I\n.\n[22]\nJ. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, L. M. Ni, H. Shum, and J. Guo\n(2024)\nThink-on-graph: deep and responsible reasoning of large language model on knowledge graph\n.\nIn\nICLR\n,\nCited by:\nÂ§\nII-B\n.\n[23]\nJ. Wang, J. Wu, Y. Wu, Y. Liu, M. Gao, and J. McAuley\nInstructGraph: boosting large language models via graph-centric instruction tuning and preference alignment\n.\nExternal Links:\n2402.08785\nCited by:\nÂ§\nII-B\n.\n[24]\nY. Wei, S. Fu, W. Jiang, Z. Zhang, Z. Zeng, Q. Wu, J. T. Kwok, and Y. Zhang\n(2024)\nGITA: graph to visual and textual integration for vision-language graph reasoning\n.\nIn\nNeurIPS\n,\nCited by:\nÂ§\nII-B\n.\n[25]\nW. Xu, Y. Cai, D. He, J. Lin, and F. Zhang\n(2022)\nFast-lio2: fast direct lidar-inertial odometry\n.\nIEEE Transactions on Robotics\n38\n(\n4\n),\npp.Â 2053â€“2073\n.\nCited by:\nÂ§\nV-B\n.\n[26]\nY. Xu, J. Yu, J. Tang, J. Qiu, J. Wang, Y. Shen, Y. Wang, and H. Yang\n(2022)\nExplore-bench: data sets, metrics and evaluations for frontier-based and deep-reinforcement-learning-based autonomous exploration\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 6225â€“6231\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[27]\nZ. Xu, D. Deng, and K. Shimada\n(2021)\nAutonomous uav exploration of dynamic environments via incremental sampling and probabilistic roadmap\n.\nIEEE Robotics and Automation Letters\n6\n(\n2\n),\npp.Â 2729â€“2736\n.\nCited by:\nÂ§\nII-A\n.\n[28]\nB. Yamauchi\n(1997)\nA frontier-based approach for autonomous exploration\n.\nIn\nProceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRAâ€™97.â€™Towards New Computational Principles for Robotics and Automationâ€™\n,\npp.Â 146â€“151\n.\nCited by:\nÂ§I\n.\n[29]\nH. Zhu, C. Cao, Y. Xia, S. Scherer, J. Zhang, and W. Wang\n(2021)\nDSVP: dual-stage viewpoint planner for rapid exploration by dynamic expansion\n.\nIn\n2021 IEEE/RSJ international conference on intelligent robots and systems (IROS)\n,\npp.Â 7623â€“7630\n.\nCited by:\nÂ§I\n,\nÂ§\nV-A\n.\n[30]\nS. Zhu, J. Zhou, A. Chen, M. Bai, J. Chen, and J. Xu\n(2024)\nMAexp: a generic platform for rl-based multi-agent exploration\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 5155â€“5161\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.",
    "preview_text": "This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\\times130m$ building environment.\n\nFARE: Fast-Slow Agentic Robotic Exploration\nShuhao Liao\n1,2\n, Xuxin Lv\n1\n, Jeric Lew\n2\n, Shizhe Zhang\n2\n, Jingsong Liang\n2\n, Peizhuo Li\n2\n,\nYuhong Cao\n2\n, Wenjun Wu\n1\n, Guillaume Sartoretti\n2\n1\nBeihang University, China\n2\nDepartment of Mechanical Engineering, National University of Singapore, Singapore\nAbstract\nThis work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language mode",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "FAREæ˜¯ä¸€ä¸ªåˆ†å±‚è‡ªä¸»æ¢ç´¢æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨å±€è¯­ä¹‰æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œå±€éƒ¨å†³ç­–ï¼Œæå‡æœºå™¨äººæ¢ç´¢æ•ˆç‡ã€‚",
    "detailed_summary": "## FAREè®ºæ–‡è¯¦ç»†æ‘˜è¦\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nè‡ªä¸»æ¢ç´¢æ˜¯ç§»åŠ¨æœºå™¨äººåœ¨æœªçŸ¥ç¯å¢ƒä¸­è¿è¡Œçš„æ ¸å¿ƒèƒ½åŠ›ï¼Œæ—¨åœ¨é«˜æ•ˆè·å–ç¯å¢ƒå‡ ä½•ä¿¡æ¯ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¾èµ–å›ºå®šå¯å‘å¼è§„åˆ™çš„**ä¼ ç»Ÿè§„åˆ’å™¨**å’Œæ˜“å—ç¨€ç–å¥–åŠ±é—®é¢˜å›°æ‰°çš„**åŸºäºå­¦ä¹ çš„æ–¹æ³•**ã€‚ä¸¤è€…å‡éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨ç¯å¢ƒä¸­çš„**é•¿æœŸç»“æ„ä¿¡æ¯**ï¼Œå¹¶éšç€ç¯å¢ƒæ¼”å˜è‡ªé€‚åº”è°ƒæ•´æ¢ç´¢ç­–ç•¥ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºFAREæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆ**å…¨å±€è¯­ä¹‰æ¨ç†**ä¸**å±€éƒ¨å¿«é€Ÿå†³ç­–**æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\nFAREé‡‡ç”¨**å¿«-æ…¢æ€ç»´åˆ†å±‚æ¶æ„**ï¼Œå°†å…¨å±€æ¨ç†ä¸å±€éƒ¨å†³ç­–è§£è€¦ï¼š\n- **æ…¢æ€ç»´æ¨¡å—ï¼ˆå…¨å±€æ¨ç†ï¼‰**ï¼šåˆ©ç”¨**å¤§è¯­è¨€æ¨¡å‹**è§£æç¯å¢ƒçš„ç®€æ´æ–‡æœ¬æè¿°ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ã€é€‚åº”ç¯å¢ƒç‰¹æ€§çš„**æ¢ç´¢ç­–ç•¥**ã€‚è¯¥ç­–ç•¥é€šè¿‡ä¸€ä¸ªç»è¿‡**æ¨¡å—åŒ–å‰ªæ**çš„ç¨€ç–å…¨å±€ä¿¡å¿µå›¾è¿›è¡Œè½åœ°ï¼Œç”Ÿæˆä¸€ç³»åˆ—æŒ‡å¯¼é•¿æœŸæ¢ç´¢çš„å…¨å±€è·¯å¾„ç‚¹ã€‚å‰ªææœºåˆ¶ä¿ç•™äº†æœ€å…·ç»“æ„ä¿¡æ¯é‡çš„ç¤¾åŒºï¼Œæ˜¾è‘—é™ä½äº†LLMçš„æ¨ç†å¤æ‚åº¦ã€‚\n- **å¿«æ€ç»´æ¨¡å—ï¼ˆå±€éƒ¨å†³ç­–ï¼‰**ï¼šé‡‡ç”¨**åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å›¾ç¥ç»ç½‘ç»œå¼ºåŒ–å­¦ä¹ ç­–ç•¥**ï¼Œå¤„ç†åŒ…å«å±€éƒ¨å›¾ç»“æ„ã€ä¿¡æ¯æ•ˆç”¨å’Œå…¨å±€è·¯å¾„ç‚¹çš„è§‚æµ‹ã€‚ç­–ç•¥è®­ç»ƒä¸­å¼•å…¥äº†ä¸€ä¸ª**æŒ‡ä»¤è·Ÿéšå¥–åŠ±é¡¹**ï¼Œé¼“åŠ±ç­–ç•¥è¾“å‡ºä¸å…¨å±€è·¯å¾„ç‚¹ä¿æŒä¸€è‡´ï¼Œä»è€Œåè°ƒé•¿æœŸç›®æ ‡ä¸å±€éƒ¨ååº”èƒ½åŠ›ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨Gazeboæ¨¡æ‹Ÿçš„å®¤å†…ã€æ£®æ—å’Œä»“åº“ä¸‰ç§å…¸å‹ç¯å¢ƒä¸­ï¼ŒFAREä¸å¤šç§å‰æ²¿åŸºçº¿æ–¹æ³•ï¼ˆTARE, DSVP, ARiADNE, HEADERï¼‰è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼š\n- åœ¨**ç»“æ„å¤æ‚çš„ä»“åº“ç¯å¢ƒ**ä¸­ï¼ŒFAREä¼˜åŠ¿æœ€æ˜æ˜¾ï¼Œå–å¾—äº†**æœ€çŸ­çš„æ¢ç´¢è·¯å¾„ï¼ˆ441mï¼‰å’Œæœ€å¿«çš„å®Œæˆæ—¶é—´ï¼ˆ252sï¼‰**ã€‚\n- è½¨è¿¹å¯è§†åŒ–æ˜¾ç¤ºï¼ŒFAREèƒ½æ›´æ—©åœ°æ¢ç´¢å¤–å›´å’Œè§’è½åŒºåŸŸï¼Œ**å‡å°‘äº†ä¸å¿…è¦çš„å›æº¯**ï¼Œæ¢ç´¢è½¨è¿¹æ›´é«˜æ•ˆã€‚\n- ç¡¬ä»¶ä¸Šï¼ŒFAREæˆåŠŸéƒ¨ç½²äºè½®å¼æœºå™¨äººï¼Œåœ¨**200mÃ—130mçš„å¤§å‹æ•™å­¦æ¥¼**ä¸­å®ç°äº†å®Œå…¨è‡ªä¸»æ¢ç´¢ï¼ŒéªŒè¯äº†å…¶ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\nFAREçš„åˆ›æ–°ä¸ä»·å€¼åœ¨äºï¼š\n- **æ–¹æ³•è®ºåˆ›æ–°**ï¼šé¦–æ¬¡å°†LLMçš„è¯­ä¹‰ç†è§£ä¸RLçš„å±€éƒ¨æ§åˆ¶èƒ½åŠ›åœ¨æœºå™¨äººæ¢ç´¢ä»»åŠ¡ä¸­æ·±åº¦ç»“åˆï¼Œæå‡ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„ã€ç¯å¢ƒè‡ªé€‚åº”çš„åˆ†å±‚å†³ç­–æ¡†æ¶ã€‚\n- **æ€§èƒ½æå‡**ï¼šé€šè¿‡å…¨å±€è¯­ä¹‰æŒ‡å¯¼ä¸å±€éƒ¨çµæ´»æ‰§è¡Œçš„ååŒï¼Œæœ‰æ•ˆåˆ©ç”¨äº†é•¿æœŸç»“æ„ä¿¡æ¯ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­æ˜¾è‘—æå‡äº†æ¢ç´¢æ•ˆç‡ã€‚\n- **åº”ç”¨å‰æ™¯**ï¼šä¸ºå¤„ç†åŠ¨æ€ã€å¤§è§„æ¨¡æœªçŸ¥ç¯å¢ƒçš„è‡ªä¸»ç³»ç»Ÿæä¾›äº†æ–°æ€è·¯ï¼Œå…¶æ¡†æ¶æ˜“äºæ‰©å±•è‡³å¤šæœºå™¨äººåä½œã€èå…¥è§†è§‰è¯­ä¹‰æ„ŸçŸ¥ç­‰æ–¹å‘ï¼Œå…·æœ‰é‡è¦çš„å®é™…åº”ç”¨ä»·å€¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šFARE\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nç°æœ‰æœºå™¨äººè‡ªä¸»æ¢ç´¢æ–¹æ³•åœ¨**å¤æ‚ã€å¤§è§„æ¨¡ã€æœªçŸ¥ç¯å¢ƒ**ä¸­ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨**é•¿æœŸç»“æ„ä¿¡æ¯**å¹¶**åŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥**ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼ˆå¦‚ä¸å¿…è¦çš„æŠ˜è¿”ã€çŸ­è§†è¡Œä¸ºï¼‰ã€‚å…·ä½“è¡¨ç°ä¸ºï¼š\n- **ä¼ ç»Ÿæ–¹æ³•**ï¼šä¾èµ–å›ºå®šçš„å¯å‘å¼è§„åˆ™å’Œè¶…å‚æ•°ï¼Œæ— æ³•æ ¹æ®ç¯å¢ƒç‰¹å¾ï¼ˆå¦‚å¼€é˜”åœ°ã€ç‹­çª„èµ°å»Šï¼‰è‡ªé€‚åº”è°ƒæ•´å…¨å±€ç­–ç•¥ã€‚\n- **å­¦ä¹ æ–¹æ³•**ï¼šå—é™äºç¨€ç–å¥–åŠ±å’ŒçŸ­æœŸä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œéš¾ä»¥å­¦ä¹ ä¾èµ–é•¿è¿œåæœçš„è¡Œä¸ºï¼ˆå¦‚é•¿è·ç¦»å›æº¯ã€åˆ©ç”¨è¿œè·ç¦»ç»“æ„çº¿ç´¢ï¼‰ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nFARE æå‡ºäº†ä¸€ç§**åˆ†å±‚å¼â€œå¿«-æ…¢æ€è€ƒâ€æ™ºèƒ½ä½“æ¡†æ¶**ï¼Œå°†**åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…¨å±€è¯­ä¹‰æ¨ç†**ä¸**åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å±€éƒ¨å†³ç­–**è§£è€¦å¹¶ååŒå·¥ä½œã€‚\n\n1.  **â€œæ…¢æ€è€ƒâ€å…¨å±€æ¨ç†æ¨¡å—**\n    - **æŠ€æœ¯åˆ›æ–°**ï¼š\n        - **ç¯å¢ƒæ¡ä»¶åŒ–ç­–ç•¥ç”Ÿæˆ**ï¼šåˆ©ç”¨LLMè§£æç¯å¢ƒçš„**ç®€æ´æ–‡æœ¬æè¿°**ï¼ˆå¦‚â€œå…·æœ‰ç‹­çª„é€šé“çš„ä»“åº“â€ï¼‰ï¼Œè¾“å‡ºç»“æ„åŒ–çš„æ¢ç´¢ç­–ç•¥ï¼ˆæ¶µç›–ç©ºé—´ã€æ•ˆç‡ã€å®‰å…¨ã€ä»»åŠ¡å››ä¸ªç»´åº¦ï¼‰ã€‚\n        - **åŸºäºæ¨¡å—åŒ–çš„å›¾å‰ªæ**ï¼šå¯¹å…¨å±€ä¿¡å¿µå›¾è¿›è¡Œç¤¾åŒºæ£€æµ‹ï¼Œå¹¶ä»…ä¿ç•™**æ¨¡å—åº¦è´¡çŒ®æœ€é«˜**çš„ç¤¾åŒºèŠ‚ç‚¹ï¼Œæ„å»ºæåº¦ç¨€ç–ä½†ä¿¡æ¯ä¸°å¯Œçš„å…¨å±€å›¾ï¼Œ**å¤§å¹…é™ä½LLMçš„æ¨ç†å¤æ‚åº¦**ã€‚\n        - **LLMå›¾æ¨ç†**ï¼šåœ¨å‰ªæåçš„å…¨å±€å›¾ä¸Šï¼ŒLLMç»“åˆç¯å¢ƒç­–ç•¥å’Œæ¢ç´¢è®°å¿†ï¼Œç”Ÿæˆä¸€ç³»åˆ—æŒ‡å¯¼é•¿æœŸæ¢ç´¢çš„**å…¨å±€è·¯å¾„ç‚¹**ã€‚\n\n2.  **â€œå¿«æ€è€ƒâ€å±€éƒ¨æ‰§è¡Œæ¨¡å—**\n    - **æŠ€æœ¯åˆ›æ–°**ï¼š\n        - **å›¾æ³¨æ„åŠ›ç½‘ç»œç­–ç•¥**ï¼šç­–ç•¥ç½‘ç»œä»¥**å±€éƒ¨ç¨ å¯†å›¾**ï¼ˆåŒ…å«èŠ‚ç‚¹ä½ç½®ã€æ•ˆç”¨å€¼ã€å…¨å±€è·¯å¾„ç‚¹å¼•å¯¼ä¿¡å·ï¼‰ä¸ºè¾“å…¥ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©ä¸‹ä¸€ä¸ªå±€éƒ¨è·¯å¾„ç‚¹ã€‚\n        - **æŒ‡ä»¤è·Ÿéšå¥–åŠ±è®¾è®¡**ï¼šåœ¨RLè®­ç»ƒä¸­å¼•å…¥**`r_t_dev`å¥–åŠ±é¡¹**ï¼Œæƒ©ç½šæ™ºèƒ½ä½“é€‰æ‹©çš„è·¯å¾„ç‚¹ä¸â€œæ…¢æ€è€ƒâ€æ¨¡å—æä¾›çš„å…¨å±€æŒ‡å¯¼è·¯å¾„ç‚¹ä¹‹é—´çš„åå·®ã€‚è¯¥å¥–åŠ±å¹³æ»‘å¯å¾®ï¼Œé¼“åŠ±ç­–ç•¥åœ¨ä¿æŒå±€éƒ¨çµæ´»æ€§çš„åŒæ—¶ï¼Œ**éµå¾ªé•¿æœŸå…¨å±€æŒ‡å¯¼**ã€‚\n\n3.  **æ ¸å¿ƒæ¶æ„åˆ›æ–°ï¼šæ—¶ç©ºè§£è€¦ä¸ååŒ**\n    - **â€œæ…¢æ€è€ƒâ€** åœ¨**ä½é¢‘ã€å…¨å±€å°ºåº¦**ä¸Šè¿è¡Œï¼Œè¿›è¡Œè¯­ä¹‰ç†è§£å’Œé•¿å‘¨æœŸè§„åˆ’ã€‚\n    - **â€œå¿«æ€è€ƒâ€** åœ¨**é«˜é¢‘ã€å±€éƒ¨å°ºåº¦**ä¸Šè¿è¡Œï¼Œè¿›è¡Œå®æ—¶ååº”å’Œç²¾ç»†æ§åˆ¶ã€‚\n    - ä¸¤è€…é€šè¿‡**å…¨å±€è·¯å¾„ç‚¹åºåˆ—**å’Œ**æŒ‡ä»¤è·Ÿéšå¥–åŠ±**ç´§å¯†è€¦åˆï¼Œå®ç°äº†**è¯­ä¹‰æ¨ç†ä¸å‡ ä½•å†³ç­–çš„æœ‰æ•ˆæ•´åˆ**ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ€»ç»“**\nFAREé€šè¿‡ä¸€ä¸ª**åˆ†å±‚çš„ã€æ™ºèƒ½ä½“åŒ–çš„æ¡†æ¶**æ¥è§£å†³è‡ªä¸»æ¢ç´¢çš„é•¿æœŸä¾èµ–å’Œè‡ªé€‚åº”éš¾é¢˜ï¼š\n1.  **é—®é¢˜è¯†åˆ«ä¸æŠ½è±¡**ï¼šå°†ç¯å¢ƒç‰¹å¾è½¬åŒ–ä¸ºæ–‡æœ¬ï¼Œå°†æ¢ç´¢ç›®æ ‡è½¬åŒ–ä¸ºç»“æ„åŒ–ç­–ç•¥ã€‚\n2.  **é«˜æ•ˆå…¨å±€è§„åˆ’**ï¼šåˆ©ç”¨å›¾å‰ªæå’ŒLLMï¼Œåœ¨ç¨€ç–çš„æ‹“æ‰‘å›¾ä¸Šè¿›è¡Œä½æˆæœ¬ã€é€‚åº”æ€§çš„é•¿ç¨‹æ¨ç†ã€‚\n3.  **é²æ£’å±€éƒ¨æ‰§è¡Œ**ï¼šè®­ç»ƒä¸€ä¸ªæ—¢èƒ½å“åº”å±€éƒ¨ä¿¡æ¯ï¼ˆå‰æ²¿ã€éšœç¢ï¼‰ï¼Œåˆèƒ½éµä»å…¨å±€æŒ‡å¼•çš„RLç­–ç•¥ã€‚\n4.  **ç«¯åˆ°ç«¯ååŒ**ï¼šé€šè¿‡å¥–åŠ±æœºåˆ¶å°†å…¨å±€ç›®æ ‡â€œçŒè¾“â€ç»™å±€éƒ¨ç­–ç•¥ï¼Œå½¢æˆé—­ç¯çš„ã€è¿è´¯çš„æ¢ç´¢è¡Œä¸ºã€‚\n\n### **å››ã€ å®é™…ä»·å€¼**\n- **æå‡æ¢ç´¢æ•ˆç‡**ï¼šåœ¨ä»¿çœŸå’ŒçœŸå®åœºæ™¯ï¼ˆ200m x 130må»ºç­‘ï¼‰ä¸­éªŒè¯ï¼Œç›¸æ¯”å‰æ²¿æ–¹æ³•ï¼ˆå¦‚TARE, HEADERï¼‰ï¼Œèƒ½å‡å°‘ä¸å¿…è¦çš„æŠ˜è¿”ï¼Œè·å¾—æ›´çŸ­çš„æ¢ç´¢è·¯å¾„å’Œè€—æ—¶ã€‚\n- **å¢å¼ºè‡ªé€‚åº”èƒ½åŠ›**ï¼šä»…é€šè¿‡æ”¹å˜ç¯å¢ƒæ–‡æœ¬æè¿°ï¼Œå³å¯ä½¿ç³»ç»Ÿé‡‡ç”¨ä¸åŒçš„æ¢ç´¢ç­–ç•¥ï¼Œé€‚åº”å®¤å†…ã€æ£®æ—ã€ä»“åº“ç­‰å¤šæ ·åŒ–ç¯å¢ƒã€‚\n- **å®ç°é«˜å±‚è¯­ä¹‰æ§åˆ¶**ï¼šä¸ºäººç±»æä¾›äº†ä¸€ç§ç›´è§‚çš„ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰æ¥å£æ¥å½±å“æœºå™¨äººçš„é«˜çº§æ¢ç´¢è¡Œä¸ºï¼Œæé«˜äº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚\n- **ä¸ºå…·èº«æ™ºèƒ½æä¾›æ–°èŒƒå¼**ï¼šå±•ç¤ºäº†LLMä½œä¸ºâ€œé«˜å±‚å†³ç­–å¤§è„‘â€ä¸ä¼ ç»Ÿæœºå™¨äººæ§åˆ¶/å­¦ä¹ æ¨¡å—ååŒå·¥ä½œçš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚",
            "timestamp": "2026-01-27T16:01:37.821087",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠFARE: Fast-Slow Agentic Robotic Explorationã€‹æŠ€æœ¯è„‰ç»œä¸å‘å±•æ–¹å‘åˆ†æ\n\nåŸºäºè®ºæ–‡å†…å®¹åŠå…¶å¼•ç”¨çš„ç›¸å…³å·¥ä½œï¼Œå¯ä»¥æ¢³ç†å‡ºæœºå™¨äººè‡ªä¸»æ¢ç´¢é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯è¯¥å›¢é˜Ÿç ”ç©¶æ–¹å‘çš„æ¼”è¿›è„‰ç»œã€‚å…¶å‘å±•å‘ˆç°å‡ºä»**ä¼ ç»Ÿå‡ ä½•æ–¹æ³•**åˆ°**å­¦ä¹ å‹æ–¹æ³•**ï¼Œå†åˆ°**èåˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å±‚æ¬¡åŒ–æ™ºèƒ½ä½“**çš„æ¸…æ™°è·¯å¾„ã€‚æ¯ä¸€æ­¥çš„æ”¹è¿›éƒ½æ—¨åœ¨è§£å†³å‰ä»£æ–¹æ³•çš„æ ¹æœ¬æ€§å±€é™ã€‚\n\n### å‘å±•è„‰ç»œæ¢³ç†\n\n#### 1. ä¼ ç»Ÿå‰æ²¿æ¢ç´¢ä¸å±‚æ¬¡åŒ–è§„åˆ’ (Conventional & Hierarchical Planning)\n- **æ—©æœŸå·¥ä½œ (å¦‚ [14])**: é‡‡ç”¨**è´ªå©ªçš„å‰æ²¿æ¢ç´¢**ã€‚æœºå™¨äººç›´æ¥å¯¼èˆªè‡³æœ€è¿‘çš„å·²çŸ¥-æœªçŸ¥è¾¹ç•Œï¼ˆå‰æ²¿ç‚¹ï¼‰ã€‚è¿™ç§æ–¹æ³•**çŸ­è§†**ï¼Œå®¹æ˜“äº§ç”Ÿå¤§é‡å¾€è¿”å’Œä½æ•ˆè·¯å¾„ã€‚\n- **æ”¹è¿›çš„é‡‡æ ·è§„åˆ’ (å¦‚ [2], [27])**: å°†æ¢ç´¢è§†ä¸º**è§†ç‚¹é€‰æ‹©é—®é¢˜**ã€‚é€šè¿‡é‡‡æ ·å€™é€‰è§†ç‚¹å¹¶è¯„ä¼°å…¶ä¿¡æ¯å¢ç›Šå’Œå¯è¡Œæ€§ï¼Œå®ç°**éçŸ­è§†è§„åˆ’**ã€‚ä½†è®¡ç®—å¼€é”€å¤§ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­æ‰©å±•æ€§å·®ã€‚\n- **å±‚æ¬¡åŒ–è§„åˆ’ (å¦‚ [4], [16], [29])**: ä¸ºåº”å¯¹å¤§è§„æ¨¡ç¯å¢ƒï¼Œå°†è§„åˆ’åˆ†è§£ä¸º**å…¨å±€ç²—ç²’åº¦**å’Œ**å±€éƒ¨ç»†ç²’åº¦**ä¸¤å±‚ã€‚\n    - **å…¨å±€å±‚**: åœ¨ç¨€ç–çš„å›¾è¡¨ç¤ºï¼ˆå¦‚å‰æ²¿å›¾ã€æ‹“æ‰‘å›¾ï¼‰ä¸Šè®¡ç®—é•¿è·ç¦»è·¯å¾„ã€‚\n    - **å±€éƒ¨å±‚**: å¤„ç†è¯¦ç»†çš„è§†ç‚¹é‡‡æ ·ã€é¿éšœå’Œè¿åŠ¨æ‰§è¡Œã€‚\n- **æ”¹è¿›ç‚¹**: å¼•å…¥äº†**ç©ºé—´å°ºåº¦ä¸Šçš„è§£è€¦**ï¼Œæé«˜äº†å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„è§„åˆ’æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚\n- **é—ç•™é—®é¢˜**: ä¸¥é‡ä¾èµ–**æ‰‹å·¥è®¾è®¡çš„å¯å‘å¼è§„åˆ™**å’Œ**å›ºå®šçš„è¶…å‚æ•°**ï¼ˆå¦‚ä¿¡æ¯å¢ç›Šä¸è·¯å¾„æˆæœ¬çš„æƒè¡¡ï¼‰ã€‚å…¨å±€è¡¨ç¤ºï¼ˆå¦‚å›¾çš„ç¨€ç–åº¦ï¼‰éš¾ä»¥é€‚åº”ç¯å¢ƒç»“æ„çš„å˜åŒ–ï¼Œç¼ºä¹**è¯­ä¹‰ç†è§£**å’Œ**é•¿æœŸç­–ç•¥è‡ªé€‚åº”**èƒ½åŠ›ã€‚\n\n#### 2. åŸºäºå­¦ä¹ çš„æ¢ç´¢ (Learning-based Exploration)\nä¸ºäº†å‡å°‘æ‰‹å·¥è®¾è®¡ï¼Œç ”ç©¶è€…å¼€å§‹ç”¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•å­¦ä¹ æ¢ç´¢ç­–ç•¥ã€‚\n- **åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„RL (å¦‚ [9], [19], [30])**: å°†å±€éƒ¨å æ®æ …æ ¼å›¾ä½œä¸ºè¾“å…¥ï¼Œç›´æ¥è¾“å‡ºåŠ¨ä½œï¼ˆé€Ÿåº¦ã€è½¬å‘ï¼‰æˆ–é€‰æ‹©å‰æ²¿ç‚¹ã€‚é€šè¿‡è®¾è®¡å¯†é›†å¥–åŠ±ï¼ˆå¦‚æ–°å‘ç°åŒºåŸŸé¢ç§¯ï¼‰æ¥ä¿ƒè¿›å­¦ä¹ ã€‚\n- **æ”¹è¿›ç‚¹**: èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¤æ‚çš„æ˜ å°„å…³ç³»ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Š**è‡ªé€‚åº”**å±€éƒ¨ç¯å¢ƒã€‚\n- **é—ç•™é—®é¢˜**:\n    1.  **å¥–åŠ±ç¨€ç–æ€§**: æœ€ç»ˆç›®æ ‡ï¼ˆæœ€å°åŒ–æ€»æ¢ç´¢æ—¶é—´/è·¯å¾„ï¼‰æ˜¯ç¨€ç–çš„ï¼Œéš¾ä»¥è¿›è¡Œ**é•¿æœŸä¿¡ç”¨åˆ†é…**ã€‚\n    2.  **çŸ­è§†åè§**: ä¾èµ–å¯†é›†çš„å³æ—¶ä¿¡æ¯å¢ç›Šå¥–åŠ±ï¼Œä½¿ç­–ç•¥åå‘**çŸ­æœŸæ”¶ç›Š**ï¼Œéš¾ä»¥å­¦ä¹ â€œé•¿é€”æŠ˜è¿”â€ã€â€œåˆ©ç”¨è¿œæœŸç»“æ„çº¿ç´¢â€ç­‰é•¿æœŸè¡Œä¸ºã€‚\n    3.  **è¡¨ç¤ºå±€é™æ€§**: CNNå¤„ç†å›ºå®šå°ºå¯¸è¾“å…¥ï¼Œå¯¹**å¯å˜å¤§å°çš„ç¯å¢ƒ**å’Œ**å›¾ç»“æ„ä¿¡æ¯**çš„å»ºæ¨¡èƒ½åŠ›æœ‰é™ã€‚\n\n#### 3. åŸºäºå›¾è¡¨ç¤ºä¸å­¦ä¹ çš„æ¢ç´¢ (Graph-based Learning)\nä¸ºäº†å…‹æœCNNçš„å±€é™æ€§ï¼Œè¿‘æœŸå·¥ä½œè½¬å‘å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€‚\n- **å›¾å­¦ä¹ æ¢ç´¢ (å¦‚ [5], [7])**: å°†ç¯å¢ƒè¡¨ç¤ºä¸º**å›¾**ï¼ˆèŠ‚ç‚¹ä¸ºå€™é€‰è§†ç‚¹ï¼Œè¾¹è¡¨ç¤ºè¿é€šæ€§ï¼‰ï¼Œå°†æ¢ç´¢å»ºæ¨¡ä¸ºå›¾ä¸Šçš„**åºåˆ—å†³ç­–é—®é¢˜**ï¼ˆé€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼‰ã€‚ä½¿ç”¨GNNæˆ–æ³¨æ„åŠ›æœºåˆ¶å¤„ç†å›¾ç»“æ„è¾“å…¥ã€‚\n- **æ”¹è¿›ç‚¹**:\n    1.  **çµæ´»è¡¨ç¤º**: å¤©ç„¶æ”¯æŒ**å¯å˜å¤§å°çš„ç¯å¢ƒ**ã€‚\n    2.  **ç»“æ„æ„ŸçŸ¥**: GNNèƒ½æ›´å¥½åœ°æ•è·ç¯å¢ƒä¸­çš„**æ‹“æ‰‘å…³ç³»**ã€‚\n- **é—ç•™é—®é¢˜**: è™½ç„¶åˆ©ç”¨äº†å›¾ç»“æ„ï¼Œä½†å†³ç­–ä»ä¸»è¦åŸºäº**å±€éƒ¨å‡ ä½•è§‚å¯Ÿ**æˆ–**å›ºå®šçš„å›¾è¡¨ç¤º**ï¼Œç¼ºä¹åˆ©ç”¨**é«˜å±‚ç¯å¢ƒå…ˆéªŒ**è¿›è¡Œæ˜¾å¼ã€è‡ªé€‚åº”çš„**å…¨å±€æ¨ç†**èƒ½åŠ›ã€‚è¿™æ­£æ˜¯å…¶å‰ä½œ **HEADER [6]** è¯•å›¾è§£å†³çš„é—®é¢˜ã€‚\n\n#### 4. å‰ä½œï¼šHEADER - å¼•å…¥ä¸“å®¶å¼•å¯¼çš„å±‚æ¬¡åŒ–RL\n- **æ ¸å¿ƒæ€æƒ³ [6]**: åœ¨åŸºäºå›¾å­¦ä¹ çš„æ¡†æ¶ä¸Šï¼Œå¼•å…¥**å±‚æ¬¡åŒ–**å’Œ**ä¸“å®¶å¼•å¯¼**ã€‚\n    - **å±‚æ¬¡åŒ–**: æ„å»ºå±€éƒ¨ç¨ å¯†å›¾å’Œå…¨å±€ç¨€ç–å›¾ã€‚\n    - **ä¸“å®¶å¼•å¯¼**: ä½¿ç”¨ä¸€ä¸ª**ä¼ ç»Ÿè§„åˆ’å™¨ï¼ˆå¦‚TAREï¼‰** åœ¨å…¨å±€ç¨€ç–å›¾ä¸Šç”Ÿæˆä¸“å®¶è·¯å¾„ï¼Œå¹¶è®¾è®¡å¥–åŠ±å‡½æ•°é¼“åŠ±RLç­–ç•¥è·Ÿéšè¯¥è·¯å¾„ã€‚\n- **æ”¹è¿›ç‚¹**:\n    1.  **æ˜¾å¼é•¿æœŸå¼•å¯¼**: é€šè¿‡ä¸“å®¶è·¯å¾„ä¸ºRLç­–ç•¥æä¾›äº†**é•¿è§†é‡çš„å‚è€ƒ**ï¼Œç¼“è§£äº†å¥–åŠ±ç¨€ç–å’ŒçŸ­è§†é—®é¢˜ã€‚\n    2.  **æ€§èƒ½æå‡**: ç›¸æ¯”çº¯RLæˆ–ä¼ ç»Ÿæ–¹æ³•ï¼Œå–å¾—äº†æ›´å¥½çš„æ¢ç´¢æ•ˆç‡ã€‚\n- **é—ç•™é—®é¢˜**:\n    1.  **å¼•å¯¼æºåƒµåŒ–**: ä¸“å®¶è·¯å¾„æ¥è‡ª**å›ºå®šçš„ä¼ ç»Ÿè§„åˆ’å™¨**ï¼Œå…¶æœ¬èº«å›ºæœ‰çš„å±€é™æ€§ï¼ˆå¦‚å›ºå®šå¯å‘å¼ï¼‰ä¾ç„¶å­˜åœ¨ã€‚\n    2.  **ç¼ºä¹è¯­ä¹‰ä¸è‡ªé€‚åº”**: æ— æ³•æ ¹æ®ç¯å¢ƒçš„é«˜å±‚è¯­ä¹‰ç‰¹å¾ï¼ˆå¦‚â€œä»“åº“â€ã€â€œæ£®æ—â€ï¼‰**åŠ¨æ€è°ƒæ•´æ¢ç´¢ç­–ç•¥**ã€‚ç­–ç•¥æ˜¯â€œè·Ÿéšä¸€ä¸ªå›ºå®šä¸“å®¶â€ï¼Œè€Œéâ€œç†è§£ç¯å¢ƒååˆ¶å®šç­–ç•¥â€ã€‚\n\n#### 5. æœ¬æ–‡å·¥ä½œï¼šFARE - èåˆLLMçš„â€œå¿«æ…¢æ€è€ƒâ€æ™ºèƒ½ä½“æ¡†æ¶\nFARE åœ¨ HEADER çš„å±‚æ¬¡åŒ–å›¾å­¦ä¹ åŸºç¡€ä¸Šï¼Œåšå‡ºäº†**æ ¹æœ¬æ€§çš„èŒƒå¼åˆ›æ–°**ï¼šç”¨ **LLMé©±åŠ¨çš„è¯­ä¹‰æ¨ç†æ¨¡å—** æ›¿ä»£äº† **å›ºå®šçš„ä¼ ç»Ÿè§„åˆ’å™¨** ä½œä¸ºå…¨å±€å¼•å¯¼æºã€‚\n- **æ ¸å¿ƒæ”¹è¿›**:\n    1.  **å¼•å…¥è¯­ä¹‰æ¨ç†ï¼ˆæ…¢æ€è€ƒï¼‰**:\n        - **è¾“å…¥**: ç¯å¢ƒçš„**æ–‡æœ¬æè¿°**ï¼ˆå¦‚â€œå…·æœ‰ç‹­çª„é€šé“çš„å®¤å†…ä»“åº“â€ï¼‰ã€‚\n        - **è¿‡ç¨‹**: LLMæ ¹æ®æè¿°ï¼Œé€šè¿‡é¢„å®šä¹‰çš„æ¨¡å¼ï¼ˆç©ºé—´ç‰¹å¾ã€éšœç¢ç‰©ç‰¹å¾ã€æ¢ç´¢æŒ‘æˆ˜ï¼‰è¿›è¡Œ**ç»“æ„åŒ–ç¯å¢ƒè¡¨å¾**ï¼Œå¹¶ç”Ÿæˆ**å‚æ•°åŒ–çš„æ¢ç´¢ç­–ç•¥**ï¼ˆè¦†ç›–ç­–ç•¥ã€æ•ˆç‡ç­–ç•¥ã€å®‰å…¨ç­–ç•¥ç­‰ï¼‰ã€‚\n        - **è¾“å‡º**: å°†ç­–ç•¥â€œè½åœ°â€åˆ°ç»è¿‡**æ¨¡å—åŒ–å‰ªæ**çš„å…¨å±€ä¿¡å¿µå›¾ä¸Šï¼Œç”Ÿæˆ**è‡ªé€‚åº”**çš„å…¨å±€è·¯å¾„ç‚¹åºåˆ—ã€‚\n    2.  **åŠ¨æ€ç­–ç•¥ç”Ÿæˆ**: æ¢ç´¢ç­–ç•¥ä¸å†æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯**æ¡ä»¶äºç¯å¢ƒè¯­ä¹‰**ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿é€šæ€§ä½çš„ç¯å¢ƒä¸­ï¼Œç­–ç•¥ä¼šæé«˜â€œæŠ˜è¿”å®¹å¿åº¦â€å’Œâ€œé€ƒç”Ÿè·¯çº¿æ„è¯†â€ã€‚\n    3.  **é«˜æ•ˆçš„å›¾æ¨ç†**: é€šè¿‡**åŸºäºæ¨¡å—åº¦çš„å‰ªæ**ï¼Œå¤§å¹…å‡å°‘å…¨å±€å›¾çš„èŠ‚ç‚¹æ•°é‡ï¼ˆä»…ä¿ç•™ç»“æ„ä¿¡æ¯æœ€ä¸°å¯Œçš„ç¤¾åŒºï¼‰ï¼Œé™ä½äº†LLMè¿›è¡Œå›¾æ¨ç†çš„å¤æ‚åº¦ã€‚\n    4.  **å¢å¼ºçš„æŒ‡ä»¤è·Ÿéšï¼ˆå¿«æ€è€ƒï¼‰**: å±€éƒ¨RLç­–ç•¥çš„å¥–åŠ±å‡½æ•°ä¸­ï¼Œé™¤äº†ä¿¡æ¯å¢ç›Šï¼Œè¿˜åŠ å…¥äº†**è·Ÿéšå…¨å±€è·¯å¾„ç‚¹**çš„å¥–åŠ±é¡¹ï¼Œç¡®ä¿å±€éƒ¨æ‰§è¡Œä¸é•¿æœŸè¯­ä¹‰è§„åˆ’çš„ä¸€è‡´æ€§ã€‚\n- **æ ¹æœ¬æ€§çªç ´**: å®ç°äº† **â€œè¯­ä¹‰ç†è§£â€** ä¸ **â€œå‡ ä½•å†³ç­–â€**ã€**â€œé•¿æœŸæ¨ç†â€** ä¸ **â€œå¿«é€Ÿååº”â€** åœ¨**æ—¶ç©ºå°ºåº¦ä¸Šçš„è§£è€¦ä¸èåˆ**ã€‚æœºå™¨äººä¸ä»…èƒ½â€œçœ‹åˆ°â€å‡ ä½•å‰æ²¿ï¼Œè¿˜èƒ½â€œç†è§£â€ç¯å¢ƒç±»å‹å¹¶æ®æ­¤åˆ¶å®šå’Œè°ƒæ•´é«˜å±‚ç­–ç•¥ã€‚\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªæ–¹å‘\n\nåŸºäºä¸Šè¿°è„‰ç»œï¼Œè¯¥å›¢é˜Ÿçš„ç ”ç©¶å‘ˆç°å‡ºå‡ ä¸ªæ˜ç¡®çš„ã€ç›¸äº’å…³è”çš„å‘å±•æ–¹å‘ï¼š\n\n1.  **ä»å‡ ä½•åˆ°è¯­ä¹‰çš„æ¼”è¿›æ–¹å‘**:\n    - **æ—©æœŸ**: çº¯å‡ ä½•ä¿¡æ¯ï¼ˆå‰æ²¿ã€å æ®æ …æ ¼ï¼‰ã€‚\n    - **ä¸­æœŸ**: å›¾ç»“æ„è¡¨ç¤ºçš„å‡ ä½•å…³ç³»ã€‚\n    - **å½“å‰**: **å‡ ä½•ï¼ˆå›¾ï¼‰ + è¯­ä¹‰ï¼ˆæ–‡æœ¬æè¿°ï¼‰**ã€‚LLMä½œä¸ºæ¡¥æ¢ï¼Œå°†äººç±»å¯¹ç¯å¢ƒçš„å¸¸è¯†å’Œè¯­ä¹‰ç†è§£æ³¨å…¥åˆ°æœºå™¨äººè§„åˆ’ä¸­ã€‚\n\n2.  **å†³ç­–æ¶æ„çš„æ¼”è¿›æ–¹å‘**:\n    - **æ—©æœŸ**: å•å±‚ã€ååº”å¼æ¶æ„ã€‚\n    - **ä¸­æœŸ**: ä¸¤å±‚ï¼ˆå…¨å±€/å±€éƒ¨ï¼‰å‡ ä½•ä¼˜åŒ–æ¶æ„ã€‚\n    - **å½“å‰**: **â€œå¿«-æ…¢æ€è€ƒâ€åŒè¿›ç¨‹æ™ºèƒ½ä½“æ¶æ„**ã€‚æ…¢æ€è€ƒï¼ˆLLMï¼‰è´Ÿè´£ç­–ç•¥æ€§ã€è¯­ä¹‰æ€§ã€é•¿å‘¨æœŸçš„æ¨ç†ï¼›å¿«æ€è€ƒï¼ˆRLï¼‰è´Ÿè´£æˆ˜æœ¯æ€§ã€å‡ ä½•æ€§ã€å®æ—¶çš„å†³ç­–ä¸æ‰§è¡Œã€‚è¿™æ¨¡ä»¿äº†äººç±»çš„è®¤çŸ¥å†³ç­–è¿‡ç¨‹ã€‚\n\n3.  **ç¯å¢ƒè¡¨ç¤ºä¸æ¨ç†çš„æ¼”è¿›æ–¹å‘**:\n    - **æ—©æœŸ**: å‡åŒ€æ …æ ¼/å›ºå®šå›¾ã€‚\n    - **ä¸­æœŸ**: å±‚æ¬¡åŒ–å›¾ï¼ˆç¨ å¯†å±€éƒ¨å›¾+ç¨€ç–å…¨å±€å›¾ï¼‰ã€‚\n    - **å½“å‰**: **åŸºäºç¤¾åŒºæ£€æµ‹å’Œæ¨¡å—åº¦å‰ªæçš„ã€è¯­ä¹‰å¢å¼ºçš„å±‚æ¬¡åŒ–å›¾**ã€‚ä¸ä»…åˆ†å±‚ï¼Œè¿˜å¯¹å…¨å±€å›¾è¿›è¡Œ**æ™ºèƒ½å‹ç¼©**ï¼Œä¿ç•™æœ€å…·ä»£è¡¨æ€§çš„æ‹“æ‰‘ç»“æ„ï¼Œä½¿å…¶æ›´é€‚åˆLLMå¤„ç†ï¼Œå®ç°äº†**è¡¨ç¤ºæ•ˆç‡**ä¸**ä¿¡æ¯é‡**çš„å¹³è¡¡ã€‚\n\n4.  **å¼•å¯¼ä¸å­¦ä¹ èåˆçš„æ¼”è¿›æ–¹å‘**:\n    - **æ—©æœŸ**: æ— å¼•å¯¼çš„RLï¼ˆéš¾ä»¥è®­ç»ƒï¼‰ã€‚\n    - **ä¸­æœŸ**: ç”±**å›ºå®šç®—æ³•**ç”Ÿæˆçš„ä¸“å®¶è·¯å¾„è¿›è¡Œå¼•å¯¼ï¼ˆHEADERï¼‰ã€‚\n    - **å½“å‰**: ç”±**æ¡ä»¶åŒ–ã€å¯è§£é‡Šçš„è¯­ä¹‰ç­–ç•¥**ç”Ÿæˆçš„åŠ¨æ€è·¯å¾„è¿›è¡Œå¼•å¯¼ï¼ˆFAREï¼‰ã€‚å¼•å¯¼æºæœ¬èº«å…·å¤‡äº†**è‡ªé€‚åº”å’Œå¯è§£é‡Šæ€§**ã€‚\n\n**æ€»ç»“**ï¼šè¯¥æ–¹å‘çš„å‘å±•è„‰ç»œæ ¸å¿ƒæ˜¯ä¸æ–­æå‡æœºå™¨äººæ¢ç´¢çš„**æ™ºèƒ½æ°´å¹³**â€”â€”ä»å¯¹å‡ ä½•ç¯å¢ƒçš„**ååº”**ï¼Œåˆ°å¯¹å›¾ç»“æ„çš„**å­¦ä¹ **ï¼Œæœ€ç»ˆè¿›åŒ–åˆ°åˆ©ç”¨å¤§æ¨¡å‹è¿›è¡Œ**ç¯å¢ƒè¯­ä¹‰ç†è§£ä¸ç­–ç•¥æ€§æ¨ç†**ã€‚æ¯ä¸€æ­¥éƒ½æ—¨åœ¨è§£å†³å‰ä¸€æ­¥åœ¨**é•¿æœŸè§„åˆ’**ã€**è‡ªé€‚åº”èƒ½åŠ›**å’Œ**åˆ©ç”¨å…ˆéªŒçŸ¥è¯†**æ–¹é¢çš„ä¸è¶³ã€‚FARE ä»£è¡¨äº†å½“å‰å°† **LLM çš„é«˜å±‚æ¨ç†èƒ½åŠ›** ä¸ **RL çš„ä½å±‚æ„ŸçŸ¥æ§åˆ¶èƒ½åŠ›** è¿›è¡Œæ·±åº¦èåˆçš„å‰æ²¿æ¢ç´¢ï¼Œä¸ºæ„å»ºæ›´é€šç”¨ã€æ›´æ™ºèƒ½çš„è‡ªä¸»æœºå™¨äººç³»ç»Ÿæä¾›äº†é‡è¦æ€è·¯ã€‚",
            "timestamp": "2026-01-27T16:02:52.029728",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## FARE ä¸å‰ä½œå…³ç³»åˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒFARE çš„ç ”ç©¶å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯æ¥è‡ªæ–°åŠ å¡å›½ç«‹å¤§å­¦æœºæ¢°å·¥ç¨‹ç³»çš„ä½œè€…ï¼‰åœ¨æœºå™¨äººè‡ªä¸»æ¢ç´¢é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³çš„å‰æœŸå·¥ä½œã€‚è¿™äº›å‰ä½œå…±åŒæ„æˆäº†ä¸€ä¸ªé€æ­¥æ¼”è¿›çš„ç ”ç©¶è„‰ç»œï¼ŒFARE æ˜¯è¿™ä¸€è„‰ç»œçš„æœ€æ–°æˆæœã€‚\n\n### å‰ä½œæ¢³ç†ä¸å¯¹æ¯”\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†åˆ—å‡ºäº†ä¸ FARE ç›´æ¥ç›¸å…³çš„ä¸»è¦å‰ä½œï¼Œå¹¶åˆ†æäº† FARE ç›¸å¯¹äºæ¯ç¯‡å‰ä½œçš„å…·ä½“æ”¹è¿›å’ŒåŒºåˆ«ã€‚\n\n| å‰ä½œè®ºæ–‡ (å¹´ä»½) | æ ¸å¿ƒæ–¹æ³• | ä¸ FARE çš„ä¸»è¦åŒºåˆ« | FARE çš„ä¸»è¦æ”¹è¿›/åˆ›æ–°ç‚¹ |\n| :--- | :--- | :--- | :--- |\n| **ARiADNE** (2023) [5] | **åŸºäºæ³¨æ„åŠ›çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¢ç´¢**ã€‚å°†æ¢ç´¢å»ºæ¨¡ä¸ºå›¾ä¸Šçš„ä¸‹ä¸€ä¸ªè§†ç‚¹é€‰æ‹©é—®é¢˜ï¼Œä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç½‘ç»œå¤„ç†å±€éƒ¨å›¾ç»“æ„ï¼Œä»¥å­¦ä¹ é•¿æœŸæ¢ç´¢ç­–ç•¥ã€‚ | 1. **å†³ç­–å±‚é¢**ï¼šARiADNE å®Œå…¨ä¾èµ–äºä»å±€éƒ¨å›¾è§‚æµ‹ä¸­å­¦ä¹ ç­–ç•¥ï¼Œç¼ºä¹**æ˜¾å¼çš„ã€è¯­ä¹‰å±‚é¢çš„å…¨å±€è§„åˆ’**ã€‚<br>2. **ä¿¡æ¯åˆ©ç”¨**ï¼šä¸»è¦åˆ©ç”¨**å‡ ä½•ä¿¡æ¯**ï¼ˆå¦‚å‰æ²¿ã€æ•ˆç”¨ï¼‰ï¼Œæœªå¼•å…¥ç¯å¢ƒè¯­ä¹‰å…ˆéªŒæˆ–é«˜çº§æ¨ç†ã€‚<br>3. **æ¶æ„**ï¼šæ˜¯å•ä¸€çš„ã€æ•°æ®é©±åŠ¨çš„å­¦ä¹ æ¡†æ¶ã€‚ | 1. **å¼•å…¥å±‚æ¬¡åŒ–â€œå¿«-æ…¢æ€è€ƒâ€æ¶æ„**ï¼šFARE å°† ARiADNE çš„â€œå¿«é€Ÿâ€å±€éƒ¨å†³ç­–æ¨¡å—ï¼Œä¸ä¸€ä¸ªå…¨æ–°çš„â€œæ…¢é€Ÿâ€å…¨å±€è¯­ä¹‰æ¨ç†æ¨¡å—ç›¸ç»“åˆï¼Œå®ç°äº†**å†³ç­–å°ºåº¦çš„è§£è€¦**ã€‚<br>2. **èå…¥å¤§è¯­è¨€æ¨¡å‹**ï¼šåˆ©ç”¨ LLM è§£æç¯å¢ƒæ–‡æœ¬æè¿°ï¼Œç”Ÿæˆ**é¢å‘æ™ºèƒ½ä½“ï¼ˆagent-levelï¼‰çš„æ¢ç´¢ç­–ç•¥**ï¼Œä¸ºå±€éƒ¨å†³ç­–æä¾›é•¿æœŸã€è‡ªé€‚åº”çš„å…¨å±€èˆªç‚¹åºåˆ—ï¼Œè§£å†³äº† ARiADNE åœ¨**é•¿è§†é‡ä¿¡ç”¨åˆ†é…**ä¸Šçš„æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ |\n| **Deep RL-based Large-Scale Exploration** (2024) [7] | **åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å¤§è§„æ¨¡æœºå™¨äººæ¢ç´¢**ã€‚åŒæ ·é‡‡ç”¨å›¾è¡¨ç¤ºå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä½†ä¾§é‡äº**å¤§è§„æ¨¡ç¯å¢ƒ**çš„å¯æ‰©å±•æ€§ï¼Œå¯èƒ½æ¶‰åŠæ›´é«˜æ•ˆçš„å›¾ç¨€ç–åŒ–æˆ–é‡‡æ ·ç­–ç•¥ã€‚ | 1. **è§„åˆ’èŒƒå¼**ï¼šä»ç„¶æ˜¯**å•ä¸€çš„å­¦ä¹ å‹è§„åˆ’å™¨**ï¼Œå…¶â€œå…¨å±€â€è§„åˆ’ä¾èµ–äºå¯¹ç¨€ç–åŒ–å…¨å±€å›¾çš„ç›´æ¥å­¦ä¹ ï¼Œè€Œé**ç¬¦å·åŒ–æˆ–è¯­ä¹‰æ¨ç†**ã€‚<br>2. **é€‚åº”æ€§**ï¼šç­–ç•¥çš„é€‚åº”æ€§æ¥æºäºæ•°æ®ä¸­çš„æ¨¡å¼ï¼Œç¼ºä¹æ ¹æ®**é«˜å±‚ç¯å¢ƒæè¿°ï¼ˆå¦‚â€œä»“åº“â€ã€â€œæ£®æ—â€ï¼‰** è¿›è¡Œæ˜¾å¼ç­–ç•¥è°ƒæ•´çš„æœºåˆ¶ã€‚ | 1. **ç¯å¢ƒæ¡ä»¶åŒ–ç­–ç•¥ç”Ÿæˆ**ï¼šFARE çš„æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€æ˜¯å…è®¸ç”¨æˆ·æˆ–ç³»ç»Ÿæä¾›**ç®€æ´çš„è‡ªç„¶è¯­è¨€ç¯å¢ƒæè¿°**ï¼ŒLLM æ®æ­¤ç”Ÿæˆç»“æ„åŒ–çš„æ¢ç´¢ç­–ç•¥ï¼ˆè¦†ç›–ç©ºé—´ã€æ•ˆç‡ã€å®‰å…¨ã€ä»»åŠ¡å››ä¸ªç»´åº¦ï¼‰ï¼Œä½¿ç³»ç»Ÿå…·å¤‡äº†**åŸºäºè¯­ä¹‰çš„å…ˆéªŒé€‚åº”èƒ½åŠ›**ï¼Œè¿™æ˜¯çº¯ç²¹æ•°æ®é©±åŠ¨æ–¹æ³•éš¾ä»¥å®ç°çš„ã€‚<br>2. **æ¨¡å—åŒ–å›¾å‰ªæ**ï¼šåœ¨æ„å»ºå…¨å±€ä¿¡å¿µå›¾æ—¶ï¼ŒFARE å¼•å…¥äº†**åŸºäºæ¨¡å—åº¦çš„å‰ªææœºåˆ¶**ï¼Œä»…ä¿ç•™æ¨¡å—åº¦è´¡çŒ®æœ€é«˜çš„ç¤¾åŒºä½œä¸ºå…¨å±€å›¾èŠ‚ç‚¹ï¼Œ**æ˜¾è‘—é™ä½äº†LLMè¿›è¡Œå›¾æ¨ç†çš„å¤æ‚åº¦**ï¼Œæå‡äº†å®æ—¶æ€§ã€‚ |\n| **HEADER** (2025) [6] | **åŸºäºæ³¨æ„åŠ›æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸ä¸“å®¶å¼•å¯¼å¥–åŠ±çš„åˆ†å±‚æœºå™¨äººæ¢ç´¢**ã€‚è¿™æ˜¯ FARE æœ€ç›´æ¥çš„å‰èº«ï¼Œé¦–æ¬¡æ˜ç¡®æå‡ºäº†**åˆ†å±‚**æ€æƒ³ï¼Œå¹¶å¼•å…¥äº†**ä¸“å®¶å¼•å¯¼çš„å¥–åŠ±**æ¥å¡‘é€ ç­–ç•¥è¡Œä¸ºã€‚ | 1. **â€œä¸“å®¶â€æ¥æºä¸æ€§è´¨**ï¼šHEADER ä¸­çš„â€œä¸“å®¶â€æä¾›çš„æ˜¯**çŸ­æœŸçš„ã€åŸºäºå½“å‰å‡ ä½•çŠ¶æ€çš„å¥–åŠ±ä¿¡å·**ï¼ˆå¦‚é¼“åŠ±æ¥è¿‘å‰æ²¿ï¼‰ï¼Œæœ¬è´¨ä¸Šæ˜¯**å¯†é›†çš„ã€å±€éƒ¨çš„æ‰‹å·¥å¥–åŠ±å‡½æ•°**ã€‚<br>2. **å…¨å±€æŒ‡å¯¼**ï¼šå…¶â€œåˆ†å±‚â€æ›´å¤šä½“ç°åœ¨**å›¾è¡¨ç¤ºçš„å±‚æ¬¡åŒ–**ï¼ˆå±€éƒ¨å›¾ vs. å…¨å±€å›¾ï¼‰ï¼Œä½†é«˜å±‚å¯¹ä½å±‚çš„æŒ‡å¯¼æ˜¯éšå¼çš„ã€é€šè¿‡å¥–åŠ±å‡½æ•°å®ç°çš„ï¼Œç¼ºä¹**å¯è§£é‡Šçš„ã€ç¬¦å·åŒ–çš„å…¨å±€è®¡åˆ’**ã€‚<br>3. **æ¨ç†èƒ½åŠ›**ï¼šä¸å…·å¤‡**è¯­ä¹‰ç†è§£ä¸æ¨ç†**èƒ½åŠ›ã€‚ | 1. **ç”¨LLMæ›¿ä»£ä¸“å®¶å¥–åŠ±**ï¼šFARE ç”¨ **LLM ç”Ÿæˆçš„å…¨å±€èˆªç‚¹åºåˆ—** å–ä»£äº† HEADER ä¸­æ‰‹å·¥è®¾è®¡çš„ä¸“å®¶å¥–åŠ±ã€‚è¿™å¸¦æ¥äº†è´¨çš„é£è·ƒï¼šæŒ‡å¯¼ä»**å±€éƒ¨ã€å¯†é›†çš„å¥–åŠ±ä¿¡å·**å˜ä¸º**é•¿æœŸã€ç¨€ç–çš„è¯­ä¹‰ç›®æ ‡**ã€‚<br>2. **â€œæŒ‡ä»¤è·Ÿéšâ€å¥–åŠ±è®¾è®¡**ï¼šä¸ºæ­¤ï¼ŒFARE è®¾è®¡äº†æ–°çš„ **`r_t^dev` å¥–åŠ±é¡¹**ï¼Œæƒ©ç½šæ™ºèƒ½ä½“æ‰€é€‰å±€éƒ¨èˆªç‚¹ä¸ LLM æä¾›çš„å…¨å±€æŒ‡å¯¼èˆªç‚¹ä¹‹é—´çš„åå·®ã€‚è¿™ä½¿å¾—ç­–ç•¥å­¦ä¹ **å¦‚ä½•éµå¾ªé•¿æœŸè¯­ä¹‰æŒ‡ä»¤**ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å¤§åŒ–å³æ—¶ä¿¡æ¯å¢ç›Šã€‚<br>3. **ç­–ç•¥å¯è§£é‡Šæ€§æå‡**ï¼šLLM ç”Ÿæˆçš„ç­–ç•¥ï¼ˆå¦‚â€œè¾¹ç•Œä¼˜å…ˆâ€ã€â€œä¿å®ˆèƒ½é‡ç­–ç•¥â€ï¼‰å’Œå…¨å±€èˆªç‚¹è·¯å¾„æä¾›äº†**äººç±»å¯ç†è§£çš„å†³ç­–ä¾æ®**ï¼Œç³»ç»Ÿè¡Œä¸ºæ›´é€æ˜ã€‚ |\n\n### æ€»ç»“ï¼šFARE çš„æ ¸å¿ƒæ¼”è¿›\n\nFARE å¹¶éå¯¹å‰ä½œçš„ç®€å•å åŠ ï¼Œè€Œæ˜¯è¿›è¡Œäº†ä¸€æ¬¡**èŒƒå¼è½¬æ¢**ï¼š\n\n1.  **ä»â€œå‡ ä½•é©±åŠ¨â€åˆ°â€œè¯­ä¹‰-å‡ ä½•ååŒé©±åŠ¨â€**ï¼šå‰ä½œï¼ˆARiADNE, [7], HEADERï¼‰çš„æ ¸å¿ƒæ˜¯**ä»å‡ ä½•æ•°æ®ï¼ˆåœ°å›¾ã€å‰æ²¿å›¾ï¼‰ä¸­å­¦ä¹ ç­–ç•¥**ã€‚FARE åˆ™å¼•å…¥ **LLM ä½œä¸ºâ€œè¯­ä¹‰å¤§è„‘â€**ï¼Œå°†äººç±»å¯¹ç¯å¢ƒçš„**é«˜å±‚è®¤çŸ¥**ï¼ˆæè¿°ã€ç­–ç•¥ï¼‰è½¬åŒ–ä¸ºå¯¹å‡ ä½•æ¢ç´¢è¿‡ç¨‹çš„æŒ‡å¯¼ï¼Œå®ç°äº†ç¬¦å·çŸ¥è¯†ä¸äºšç¬¦å·æ§åˆ¶çš„èåˆã€‚\n2.  **ä»â€œéšå¼åˆ†å±‚â€åˆ°â€œæ˜¾å¼å¼‚æ„åˆ†å±‚â€**ï¼šHEADER è™½ç„¶åˆ†å±‚ï¼Œä½†ä¸¤å±‚ï¼ˆå›¾è¡¨ç¤ºå±‚å’Œå†³ç­–å±‚ï¼‰æœ¬è´¨éƒ½æ˜¯ç¥ç»ç½‘ç»œå¤„ç†å‡ ä½•ä¿¡æ¯ã€‚FARE çš„â€œå¿«-æ…¢æ€è€ƒâ€æ˜¯**å¼‚æ„çš„**ï¼šâ€œæ…¢æ€è€ƒâ€æ¨¡å—ï¼ˆLLM+å›¾æ¨ç†ï¼‰åœ¨**ç¬¦å·å’Œæ‹“æ‰‘å±‚é¢**æ“ä½œï¼Œé¢‘ç‡ä½ï¼›â€œå¿«æ€è€ƒâ€æ¨¡å—ï¼ˆRLç­–ç•¥ï¼‰åœ¨**å‡ ä½•å’Œä¼ æ„Ÿå™¨å±‚é¢**æ“ä½œï¼Œé¢‘ç‡é«˜ã€‚è¿™ç§è§£è€¦è®©æ¯ä¸ªæ¨¡å—åœ¨å…¶æœ€æ“…é•¿çš„æ—¶ç©ºå°ºåº¦ä¸Šè¿è¡Œã€‚\n3.  **ä»â€œå¥–åŠ±å·¥ç¨‹â€åˆ°â€œæŒ‡ä»¤è·Ÿéšâ€**ï¼šå‰ä½œä¾èµ–ç²¾å¿ƒè®¾è®¡çš„**å¯†é›†å¥–åŠ±å‡½æ•°**ï¼ˆå¦‚æ–°è§‚å¯Ÿä½“ç§¯ã€è·¯å¾„é•¿åº¦ï¼‰æ¥å¼•å¯¼å­¦ä¹ ï¼Œå®¹æ˜“å¯¼è‡´çŸ­è§†ã€‚FARE é€šè¿‡ **LLM ç”Ÿæˆå…¨å±€èˆªç‚¹**ï¼Œå¹¶è®¾è®¡ **â€œæŒ‡ä»¤è·Ÿéšâ€å¥–åŠ±**ï¼Œå°†ä¼˜åŒ–ç›®æ ‡è½¬åŒ–ä¸º**ä¸è¯­ä¹‰æŒ‡ä»¤çš„ä¸€è‡´æ€§**ï¼Œæ›´ç›´æ¥åœ°æœåŠ¡äºâ€œå‡å°‘æ— æ•ˆå›æº¯ã€æé«˜æ•´ä½“æ•ˆç‡â€è¿™ä¸ªç»ˆæä»»åŠ¡ç›®æ ‡ã€‚\n4.  **å®é™…ä»·å€¼æå‡**ï¼šè¿™ç§æ¶æ„ä½¿æœºå™¨äººèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ï¼Œå…ˆâ€œè§‚å¯Ÿç¯å¢ƒç‰¹ç‚¹å¹¶åˆ¶å®šå¤§è‡´è®¡åˆ’â€ï¼ˆæ…¢æ€è€ƒï¼‰ï¼Œå†â€œè¾¹æ‰§è¡Œè¾¹æ ¹æ®å±€éƒ¨æƒ…å†µå¾®è°ƒâ€ï¼ˆå¿«æ€è€ƒï¼‰ã€‚è®ºæ–‡å®éªŒè¡¨æ˜ï¼Œåœ¨å…·æœ‰æ˜æ˜¾å…¨å±€ç»“æ„ï¼ˆå¦‚ä»“åº“ã€æ£®æ—ï¼‰çš„ç¯å¢ƒä¸­ï¼Œè¿™ç§èƒ½åŠ›èƒ½æœ‰æ•ˆ**å‡å°‘ä¸å¿…è¦çš„æŠ˜è¿”å’Œæ¢ç´¢æ—¶é—´**ï¼Œåœ¨ä»¿çœŸå’ŒçœŸå®å¤§è§„æ¨¡ç¯å¢ƒä¸­å‡éªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚",
            "timestamp": "2026-01-27T16:03:47.542972",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†ä¸ºæ‚¨è¯¦ç»†è§£æè®ºæ–‡ã€ŠFARE: Fast-Slow Agentic Robotic Explorationã€‹æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åˆ†æ\n\næœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ç§**å¿«-æ…¢æ€ç»´èŒƒå¼**çš„å±‚æ¬¡åŒ–è‡ªä¸»æ¢ç´¢æ¡†æ¶ï¼Œå°†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å…¨å±€è¯­ä¹‰æ¨ç†ä¸åŸºäºå¼ºåŒ–å­¦ä¹ çš„å±€éƒ¨å†³ç­–è§£è€¦ã€‚ä»¥ä¸‹æ˜¯å¯¹å…¶å…³é”®æŠ€æœ¯æ–¹æ³•çš„è¯¦ç»†æ‹†è§£ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¡¨\n\n| æ¨¡å—åç§° | æ ¸å¿ƒåŠŸèƒ½ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•° |\n| :--- | :--- | :--- | :--- |\n| **1. å±‚æ¬¡åŒ–æœºå™¨äººä¿¡å¿µå›¾æ„å»º** | å°†ç¯å¢ƒè¡¨ç¤ºä¸º**å±€éƒ¨ç¨ å¯†å›¾**å’Œ**å…¨å±€ç¨€ç–å›¾**ï¼Œä¸ºå¿«æ…¢æ€ç»´æ¨¡å—æä¾›ç»“æ„åŒ–è¾“å…¥ã€‚ | **1.1 å±€éƒ¨ä¿¡å¿µå›¾ (`G_local`)**ï¼š<br>- **æ„å»º**ï¼šåœ¨æœºå™¨äººå½“å‰ä½ç½® `v_cur` å‘¨å›´å®šä¹‰ä¸€ä¸ªå¤§å°ä¸º `d Ã— d` çš„æ»‘åŠ¨çª—å£ `W_local`ï¼Œçª—å£å†…æ‰€æœ‰åœ¨è‡ªç”±ç©ºé—´ `M_f` ä¸­å‡åŒ€é‡‡æ ·çš„å€™é€‰è§†ç‚¹ `v_i` æ„æˆèŠ‚ç‚¹é›† `V_local`ã€‚<br>- **è¿æ¥**ï¼šæ¯ä¸ªèŠ‚ç‚¹ä¸å…¶ `k` ä¸ªæœ€è¿‘é‚»èŠ‚ç‚¹è¿æ¥ï¼Œå¹¶å‰”é™¤ä¸éšœç¢ç‰©/æœªçŸ¥åŒºåŸŸç›¸äº¤çš„è¾¹ï¼Œå½¢æˆè¾¹é›† `E_local`ã€‚<br>- **èŠ‚ç‚¹æ•ˆç”¨**ï¼šä¸ºæ¯ä¸ªèŠ‚ç‚¹ `v_i` è®¡ç®—æ•ˆç”¨å€¼ `u_i`ï¼Œå®šä¹‰ä¸ºä»è¯¥èŠ‚ç‚¹**æ— ç¢°æ’ä¸”è·ç¦»åœ¨ä¼ æ„Ÿå™¨èŒƒå›´ `d_s` å†…**å¯è§‚æµ‹åˆ°çš„è¾¹ç•Œç‚¹æ•°é‡ï¼š<br>  `u_i = |F_o,i|` <br> çº¦æŸæ¡ä»¶ï¼š`âˆ€ f_j âˆˆ F_o,i, â€–f_j - v_iâ€– â‰¤ d_s, L(v_i, f_j) âˆ© (M - M_f) = âˆ…`ã€‚<br><br>**1.2 å…¨å±€ä¿¡å¿µå›¾ (`G_global`)**ï¼š<br>- **ç¤¾åŒºæ£€æµ‹**ï¼šå¯¹å®Œæ•´ç¢°æ’è‡ªç”±å›¾ `G_t` åº”ç”¨åŸºäº**æ¨¡å—åº¦**çš„ç¤¾åŒºæ£€æµ‹ç®—æ³•ï¼ˆå¦‚Louvainæ–¹æ³•ï¼‰ã€‚æ¨¡å—åº¦ `Q` è¡¡é‡ç¤¾åŒºå†…éƒ¨è¿æ¥å¯†åº¦ç›¸å¯¹äºéšæœºè¿æ¥çš„å¼ºåº¦ï¼š<br>  `Q = 1/(2m) * Î£_{i,j} [A_{ij} - (k_i k_j)/(2m)] * Î´(c_i, c_j)` <br>  å…¶ä¸­ `m` ä¸ºæ€»è¾¹æ•°ï¼Œ`A_{ij}` ä¸ºé‚»æ¥çŸ©é˜µï¼Œ`k_i` ä¸ºèŠ‚ç‚¹åº¦ï¼Œ`Î´(c_i, c_j)` åœ¨èŠ‚ç‚¹ `i, j` å±äºåŒä¸€ç¤¾åŒº `c` æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚<br>- **æ¨¡å—åº¦è´¡çŒ®è®¡ç®—**ï¼šå°†æ¨¡å—åº¦é‡å†™ä¸ºç¤¾åŒºå±‚é¢æ±‚å’Œï¼Œæ¯ä¸ªç¤¾åŒº `c` çš„è´¡çŒ® `Q(c)` ä¸ºï¼š<br>  `Q(c) = Î£_in - (Î£_tot)Â²/(2m)` <br>  å…¶ä¸­ `Î£_in` æ˜¯ç¤¾åŒº `c` å†…éƒ¨çš„è¾¹æ•°ï¼Œ`Î£_tot` æ˜¯ä¸ç¤¾åŒº `c` å†…èŠ‚ç‚¹ç›¸è¿çš„æ€»è¾¹æ•°ã€‚<br>- **åŸºäºæ¨¡å—åº¦çš„å‰ªæ**ï¼šä¸ä¿ç•™æ‰€æœ‰ç¤¾åŒºï¼Œè€Œæ˜¯ä»…ä¿ç•™æ¨¡å—åº¦è´¡çŒ®æœ€é«˜çš„å‰ `k` ä¸ªç¤¾åŒºï¼š<br>  `C' := argtopk_{câŠ†C} Q(c)`ã€‚<br>- **å›¾æŠ½è±¡**ï¼šæ¯ä¸ªè¢«ä¿ç•™çš„ç¤¾åŒº `c âˆˆ C'` è¢«æŠ½è±¡ä¸º `G_global` ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œç¤¾åŒºé—´çš„è¿æ¥å…³ç³»è¯±å¯¼å‡º `G_global` çš„è¾¹ã€‚ | - `d`: å±€éƒ¨æ»‘åŠ¨çª—å£è¾¹é•¿<br>- `k`: å±€éƒ¨å›¾èŠ‚ç‚¹æœ€è¿‘é‚»è¿æ¥æ•°<br>- `d_s`: ä¼ æ„Ÿå™¨æ„ŸçŸ¥èŒƒå›´<br>- `k`: ä¿ç•™çš„é¡¶çº§ç¤¾åŒºæ•°é‡ï¼ˆå‰ªæå‚æ•°ï¼‰ |\n| **2. æ…¢æ€ç»´æ¨¡å— (LLMå…¨å±€æ¨ç†)** | æ ¹æ®ç¯å¢ƒæ–‡æœ¬æè¿°ç”Ÿæˆé«˜å±‚æ¢ç´¢ç­–ç•¥ï¼Œå¹¶åœ¨å‰ªæåçš„å…¨å±€ä¿¡å¿µå›¾ä¸Šè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆå…¨å±€è·¯å¾„ç‚¹åºåˆ—ã€‚ | **2.1 ç¯å¢ƒæ¡ä»¶åŒ–ç­–ç•¥ç”Ÿæˆ**ï¼š<br>- **è¾“å…¥**ï¼šç¯å¢ƒçš„ç®€çŸ­æ–‡æœ¬æè¿°ï¼ˆå¦‚â€œå…·æœ‰ç‹­çª„é€šé“çš„å®¤å†…ä»“åº“â€ï¼‰ã€‚<br>- **ç»“æ„åŒ–åˆ†æ**ï¼šLLM æŒ‰ç…§é¢„å®šæ¨¡å¼è§£æç¯å¢ƒç‰¹å¾ï¼š<br>  1. **ç©ºé—´ç‰¹å¾**ï¼šå¼€æ”¾æ€§ã€å¤æ‚æ€§ã€è¿é€šæ€§ã€é€šé“å®½åº¦ã€‚<br>  2. **éšœç¢ç‰©ç‰¹å¾**ï¼šå¯†åº¦ã€è§„å¾‹æ€§ã€é«˜åº¦å˜åŒ–ã€‚<br>  3. **æ¢ç´¢æŒ‘æˆ˜**ï¼šå¯¼èˆªéš¾åº¦ã€æ­»èƒ¡åŒæ¦‚ç‡ã€å›æº¯å¿…è¦æ€§ã€‚<br>- **ç­–ç•¥å®ä¾‹åŒ–**ï¼šæ ¹æ®ä¸Šè¿°ç‰¹å¾ï¼Œæ²¿å››ä¸ªç»´åº¦å‚æ•°åŒ–å…·ä½“æ¢ç´¢ç­–ç•¥ï¼š<br>  - **ç©ºé—´ç­–ç•¥**ï¼šè¦†ç›–æ¨¡å¼ï¼ˆå¦‚â€œè¾¹ç•Œä¼˜å…ˆâ€ï¼‰ã€æ–¹å‘åå¥½ç­‰ã€‚<br>  - **æ•ˆç‡ç­–ç•¥**ï¼šèƒ½è€—æ”¿ç­–ã€å›æº¯å®¹å¿åº¦ç­‰ã€‚<br>  - **å®‰å…¨ç­–ç•¥**ï¼šéšœç¢ç‰©æ¸…é™¤è·ç¦»ã€æœªçŸ¥åŒºåŸŸå¤„ç†æ–¹æ³•ç­‰ã€‚<br>  - **ä»»åŠ¡ç­–ç•¥**ï¼šå®Œæˆæ ‡å‡†ã€ä¿¡æ¯ä¼˜å…ˆçº§ç­‰ã€‚<br><br>**2.2 å›¾æ¨ç†**ï¼š<br>- **è¾“å…¥**ï¼šå‰ªæåçš„å…¨å±€ä¿¡å¿µå›¾ `G_global`ã€æ–‡æœ¬ç­–ç•¥æç¤º `x`ã€æ¢ç´¢å†å²è®°å¿† `m`ã€‚<br>- **è¿‡ç¨‹**ï¼šLLM (`Î `) åœ¨ `G_global` ä¸Šè¿›è¡Œè¿­ä»£æ¨ç†ï¼Œç”Ÿæˆä»å½“å‰èŠ‚ç‚¹åˆ°æœªæ¢ç´¢åŒºåŸŸçš„å…¨å±€è·¯å¾„ `Ï„_g`ï¼š<br>  `Ï„_g = Î (G_global, x, m)`ï¼Œå…¶ä¸­ `Ï„_g = [v_cur, v_1, ..., v_m]`ã€‚ | - **LLM æ¨¡å‹**ï¼šå®éªŒä¸­é‡‡ç”¨ **Qwen3-14B**ã€‚<br>- **ç­–ç•¥æ¨¡å¼**ï¼šé¢„å®šä¹‰çš„ç¯å¢ƒç‰¹å¾ç»´åº¦å’Œç­–ç•¥ç»´åº¦ï¼ˆä¸å¯è°ƒå‚ï¼Œä½†å¯æ‰©å±•ï¼‰ã€‚ |\n| **3. å¿«æ€ç»´æ¨¡å— (RLå±€éƒ¨å†³ç­–)** | åŸºäºå±€éƒ¨è§‚æµ‹ï¼ˆå›¾ç»“æ„ã€æ•ˆç”¨ã€å…¨å±€è·¯å¾„ï¼‰å®æ—¶é€‰æ‹©ä¸‹ä¸€ä¸ªå±€éƒ¨è·¯å¾„ç‚¹ï¼Œåœ¨éµå¾ªå…¨å±€å¼•å¯¼çš„åŒæ—¶ä¿æŒå±€éƒ¨çµæ´»æ€§ã€‚ | **3.1 ç­–ç•¥ç½‘ç»œæ¶æ„**ï¼š<br>- **è¾“å…¥å›¾ `G*`**ï¼šåœ¨å±€éƒ¨å›¾ `G_local` åŸºç¡€ä¸Šï¼Œä¸ºæ¯ä¸ªèŠ‚ç‚¹ `v_i'` å¢åŠ ä¸¤ä¸ªå±æ€§ï¼šæ•ˆç”¨ `u_i` å’Œ**è·¯æ ‡ä¿¡å· `g_i`**ï¼ˆäºŒè¿›åˆ¶ï¼ŒæŒ‡ç¤ºè¯¥èŠ‚ç‚¹æ˜¯å¦åœ¨å…¨å±€è·¯å¾„ `Ï„_g` ä¸Šï¼‰ã€‚èŠ‚ç‚¹ç‰¹å¾ï¼š`v_i' = (x_i, y_i, u_i, g_i)`ã€‚<br>- **æ³¨æ„åŠ›ç¼–ç å™¨-è§£ç å™¨**ï¼š<br>  1. **çº¿æ€§æŠ•å½±**ï¼šè®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼å‘é‡ï¼š<br>     `q_i = W^q * h_i^(q)`, `k_i = W^k * h_i^(k,v)`, `v_i = W^v * h_i^(k,v)`ã€‚<br>     `W^q, W^k, W^v âˆˆ â„^{d_f Ã— d_f}` ä¸ºå¯å­¦ä¹ å‚æ•°ï¼Œ`h_i âˆˆ â„^{d_f}` ä¸ºèŠ‚ç‚¹ç‰¹å¾ã€‚<br>  2. **æ©ç æ³¨æ„åŠ›**ï¼šè®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›åˆ†æ•° `u_{ij} = (q_i^T k_j) / âˆšd_f`ï¼Œå¹¶åˆ©ç”¨é‚»æ¥çŸ©é˜µæ©ç  `M` è¿›è¡Œå½’ä¸€åŒ–ï¼Œç¡®ä¿åªå…³æ³¨ç›¸è¿èŠ‚ç‚¹ï¼š<br>     `w_{ij} = exp(u_{ij}) * (1 - M_{ij}) / Î£_{j'} exp(u_{ij'}) * (1 - M_{ij'})` <br>     å…¶ä¸­ `M_{ij} = 0` å¦‚æœ `(v_i, v_j) âˆˆ E`ï¼Œå¦åˆ™ä¸º `1`ã€‚<br>  3. **ç‰¹å¾èšåˆ**ï¼šè¾“å‡ºç‰¹å¾ `h_i' = Î£_{j=1}^n w_{ij} v_j`ã€‚<br><br>**3.2 æŒ‡ä»¤è·Ÿéšå¥–åŠ±è®¾è®¡**ï¼š<br>- **ç›®æ ‡**ï¼šé¼“åŠ±RLç­–ç•¥é€‰æ‹©çš„å±€éƒ¨è·¯å¾„ç‚¹ `w_t` è´´è¿‘æ…¢æ€ç»´æ¨¡å—ç”Ÿæˆçš„å…¨å±€å¼•å¯¼è·¯å¾„ç‚¹ `w_t*`ã€‚<br>- **å½’ä¸€åŒ–åå·®**ï¼š`d_t = â€–w_t - w_t*â€– / (4 * Î”_node * âˆš2)`ã€‚<br>  - åˆ†æ¯ `4 * Î”_node * âˆš2` æ˜¯**å…è®¸åå·®çš„ä¸Šç•Œ**ï¼ˆåŸºäºç½‘æ ¼å•ä½ï¼‰ï¼Œç¡®ä¿ `d_t` å°ºåº¦ä¸å˜ä¸”é€šå¸¸å°äº1ã€‚<br>- **æŒ‡æ•°æƒ©ç½šå¥–åŠ±**ï¼š`r_t^{dev} = -(e^{d_t} - 1) / (e - 1)`ã€‚<br>  - **æ•°å­¦ç‰¹æ€§**ï¼šå½“ `d_t = 0`ï¼ˆå®Œå…¨è·Ÿéšï¼‰æ—¶ï¼Œ`r_t^{dev} = 0`ï¼›å½“ `d_t â†’ 1`ï¼ˆæœ€å¤§åå·®ï¼‰æ—¶ï¼Œ`r_t^{dev} â†’ -1`ã€‚è¯¥å‡½æ•°åœ¨ `[0,1]` åŒºé—´å†…å•è°ƒé€’å‡ï¼Œä¸ºå°åå·®æä¾›å¹³ç¼“æƒ©ç½šï¼Œå¯¹å¤§åå·®æ–½åŠ æ€¥å‰§å¢å¤§çš„æƒ©ç½šï¼Œæä¾›å¹³æ»‘çš„ä¼˜åŒ–æ¢¯åº¦ã€‚ | - `d_f`: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦<br>- `Î”_node`: èŠ‚ç‚¹åˆ†è¾¨ç‡ï¼ˆå…³é”®å‚æ•°ï¼Œä¾ç¯å¢ƒè°ƒæ•´ï¼Œè®ºæ–‡ä¸­ä¸º0.8m-2.8mï¼‰<br>- **å¥–åŠ±æƒé‡**ï¼š`r_t^{dev}` ä¸ä¿¡æ¯å¢ç›Šç­‰å¥–åŠ±é¡¹ç»“åˆï¼Œæ€»å¥–åŠ± `R_t = r_t^{info} + Î» * r_t^{dev}`ï¼ˆÎ»ä¸ºè¶…å‚æ•°ï¼Œè®ºæ–‡ä¸­æœªæ˜ç¡®",
            "timestamp": "2026-01-27T16:04:58.033253",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡ã€ŠFARE: Fast-Slow Agentic Robotic Explorationã€‹çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\næœ¬æ–‡é€šè¿‡ä»¿çœŸå’ŒçœŸå®ç¡¬ä»¶å®éªŒè¯„ä¼°FAREæ¡†æ¶çš„æ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯å…¶ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†çš„è¯¦ç»†åˆ†æã€‚\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡ä¸»è¦ä½¿ç”¨ä»¥ä¸‹ä¸¤ç±»å®šé‡æŒ‡æ ‡æ¥è¯„ä¼°æ¢ç´¢æ•ˆç‡ï¼Œæ‰€æœ‰æŒ‡æ ‡å‡ä»¥**æ›´å°å€¼ä¸ºä¼˜**ï¼ˆè¡¨ç¤ºæ›´é«˜æ•ˆï¼‰ã€‚\n\n| æŒ‡æ ‡ | æè¿° | è®¡ç®—æ–¹å¼/ç»†èŠ‚ | ç›®çš„ä¸æ„ä¹‰ |\n| :--- | :--- | :--- | :--- |\n| **æ€»è¡Œè¿›è·ç¦»** | æœºå™¨äººä»èµ·ç‚¹åˆ°å®Œæˆæ¢ç´¢æ‰€è¡Œé©¶çš„æ€»è·¯å¾„é•¿åº¦ã€‚ | å•ä½ï¼šç±³ (m)ã€‚åœ¨ä»¿çœŸä¸­é€šè¿‡è®°å½•æœºå™¨äººè½¨è¿¹ç‚¹å¹¶ç´¯åŠ è®¡ç®—ï¼›åœ¨çœŸå®å®éªŒä¸­é€šè¿‡é‡Œç¨‹è®¡æˆ–SLAMç³»ç»Ÿä¼°è®¡ã€‚ | **æ ¸å¿ƒæ•ˆç‡æŒ‡æ ‡**ã€‚ç›´æ¥åæ˜ æ¢ç´¢è·¯å¾„çš„æœ€ä¼˜æ€§ã€‚è·ç¦»è¶ŠçŸ­ï¼Œè¯´æ˜è§„åˆ’å™¨äº§ç”Ÿçš„æ— æ•ˆç§»åŠ¨ï¼ˆå¦‚å†—ä½™æ¢ç´¢ã€ä¸å¿…è¦çš„å›æº¯ï¼‰è¶Šå°‘ã€‚ |\n| **æ€»æ¢ç´¢æ—¶é—´** | æœºå™¨äººä»å¼€å§‹åˆ°å®Œæˆæ¢ç´¢æ‰€èŠ±è´¹çš„æ€»æ—¶é—´ã€‚ | å•ä½ï¼šç§’ (s)ã€‚ä»æ¢ç´¢å¼€å§‹è®¡æ—¶ï¼Œç›´åˆ°æ»¡è¶³ç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚æœªçŸ¥åŒºåŸŸä½äºé˜ˆå€¼æˆ–æ‰‹åŠ¨åˆ¤å®šå®Œæˆï¼‰ã€‚ | **å…³é”®æ€§èƒ½æŒ‡æ ‡**ã€‚ç»“åˆè¡Œè¿›è·ç¦»ï¼Œå…±åŒè¡¡é‡ç³»ç»Ÿçš„æ•´ä½“æ•ˆç‡ã€‚æ—¶é—´è¶ŠçŸ­ï¼Œè¯´æ˜ç®—æ³•åœ¨å¿«é€Ÿå†³ç­–å’Œè¿åŠ¨æ‰§è¡Œä¸Šè¶Šé«˜æ•ˆã€‚ |\n| **æ¢ç´¢ä½“ç§¯/è¦†ç›–ç‡** | åœ¨ç»™å®šæ—¶é—´æˆ–è·ç¦»å†…ï¼Œç®—æ³•æˆåŠŸå»ºå›¾çš„åŒºåŸŸå¤§å°ã€‚ | æ–‡ä¸­è™½æœªåœ¨è¡¨æ ¼ä¸­ç›´æ¥åˆ—å‡ºå…·ä½“æ•°å€¼ï¼Œä½†åœ¨**å›¾2**ä¸­ä»¥æ›²çº¿å½¢å¼å±•ç¤ºäº†â€œExplored Volume vs. Travel Distanceâ€ã€‚é€šè¿‡è®¡ç®—å·²æ¢ç´¢çš„ä½“ç´ ï¼ˆvoxelï¼‰æˆ–æ …æ ¼ï¼ˆgridï¼‰æ•°é‡æ¥è¡¡é‡ã€‚ | **è¿‡ç¨‹æ€§æŒ‡æ ‡**ã€‚ç”¨äºåˆ†ææ¢ç´¢è¿‡ç¨‹ä¸­çš„æ•ˆç‡åŠ¨æ€ã€‚æ›²çº¿ä¸Šå‡è¶Šå¿«ï¼Œè¯´æ˜å•ä½è·ç¦»/æ—¶é—´å†…è·å–çš„ä¿¡æ¯è¶Šå¤šï¼Œæ¢ç´¢ç­–ç•¥è¶Šæœ‰æ•ˆã€‚ |\n\n**æŒ‡æ ‡ä½¿ç”¨ç»†èŠ‚ï¼š**\n1.  **å¯¹æ¯”åŸºå‡†**ï¼šæ‰€æœ‰æŒ‡æ ‡å‡ç”¨äºä¸åŸºçº¿æ–¹æ³•ï¼ˆTARE, DSVP, ARiADNE, HEADERï¼‰è¿›è¡Œæ¨ªå‘æ¯”è¾ƒã€‚\n2.  **ç»Ÿè®¡å¯é æ€§**ï¼šåœ¨ä»¿çœŸå®éªŒä¸­ï¼Œæ¯ä¸ªæ–¹æ³•åœ¨æ¯ä¸ªç¯å¢ƒä¸­è¿è¡Œ**10æ¬¡**ï¼Œæ±‡æŠ¥æŒ‡æ ‡çš„**å¹³å‡å€¼å’Œæ ‡å‡†å·®**ï¼ˆå¦‚è¡¨Iä¸­çš„ `å‡å€¼(Â±æ ‡å‡†å·®)`ï¼‰ï¼Œä»¥è¯æ˜ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚\n3.  **ç¡¬ä»¶éªŒè¯**ï¼šåœ¨çœŸå®ä¸–ç•Œå®éªŒä¸­ï¼Œä¸»è¦è¿›è¡Œå®šæ€§å±•ç¤ºå’Œç³»ç»Ÿç¨³å®šæ€§éªŒè¯ï¼Œå®šé‡æŒ‡æ ‡ï¼ˆå¦‚æœ€ç»ˆæ¢ç´¢å®Œæˆçš„æ€»è·ç¦»å’Œæ—¶é—´ï¼‰è™½æœªåœ¨è¡¨æ ¼ä¸­åˆ—å‡ºï¼Œä½†å¯é€šè¿‡å®éªŒæè¿°æ¨æ–­å…¶æˆåŠŸå®Œæˆäº†å¤§è§„æ¨¡ç¯å¢ƒçš„æ¢ç´¢ã€‚\n\n### äºŒã€ æ•°æ®é›†ï¼ˆä»¿çœŸç¯å¢ƒï¼‰ä¸å®éªŒåœºæ™¯\n\nè®ºæ–‡æœªä½¿ç”¨é¢„å…ˆå½•åˆ¶çš„é™æ€æ•°æ®é›†ï¼Œè€Œæ˜¯æ„å»ºäº†**åŠ¨æ€çš„ä»¿çœŸç¯å¢ƒ**å’Œ**çœŸå®çš„ç‰©ç†åœºæ™¯**è¿›è¡Œæµ‹è¯•ã€‚\n\n#### 1. ä»¿çœŸç¯å¢ƒï¼ˆGazeboï¼‰\nåœ¨Gazeboæ¨¡æ‹Ÿå™¨ä¸­åˆ›å»ºäº†ä¸‰ä¸ªå…·æœ‰ä¸åŒç»“æ„ç‰¹å¾çš„åŸºå‡†ç¯å¢ƒï¼Œä»¥æµ‹è¯•ç®—æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n| ç¯å¢ƒåç§° | åœºæ™¯æè¿°ä¸ç‰¹ç‚¹ | å°ºå¯¸ï¼ˆçº¦ï¼‰ | æŒ‘æˆ˜ä¸è¯„ä¼°é‡ç‚¹ | åœ¨FAREä¸­çš„ç¯å¢ƒæè¿°æ–‡æœ¬ |\n| :--- | :--- | :--- | :--- | :--- |\n| **å®¤å†…åŠå…¬å®¤** | ç°ä»£åŠå…¬å»ºç­‘ï¼ŒåŒ…å«é•¿èµ°å»Šã€ä¼šè®®å®¤å’Œéš”é—´åŒºåŸŸã€‚ | æ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºç´§å‡‘å‹ç»“æ„ã€‚ | **ç»“æ„ç´§å‡‘ï¼Œç¼ºä¹æ˜¾è‘—å…¨å±€ç‰¹å¾**ã€‚ç”¨äºæµ‹è¯•ç®—æ³•åœ¨ç›¸å¯¹å‡åŒ€ç¯å¢ƒä¸­çš„åŸºç¡€æ€§èƒ½ã€‚ | â€œmodern indoor office building with long corridors, meeting rooms, and cubicle areasâ€ |\n| **æ£®æ—** | æˆ·å¤–è‡ªç„¶ç¯å¢ƒï¼ŒåŒ…å«æ ‘æœ¨ã€å¤©ç„¶éšœç¢ç‰©å’Œä¸å¹³å¦åœ°å½¢ã€‚ | æ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºå¼€æ”¾ã€ä¸è§„åˆ™ç»“æ„ã€‚ | **éšœç¢ç‰©åˆ†å¸ƒä¸è§„åˆ™ï¼Œç©ºé—´å¼€æ”¾æ€§é«˜**ã€‚ç”¨äºæµ‹è¯•ç®—æ³•åœ¨éç»“æ„åŒ–ã€å¼€é˜”ç¯å¢ƒä¸­çš„é•¿è·ç¦»è§„åˆ’å’Œé€‚åº”æ€§ã€‚ | â€œoutdoor forest environment with natural obstacles, trees, and uneven terrainâ€ |\n| **ä»“åº“** | å®¤å†…ä»“åº“ï¼Œç”±å¯†é›†æ’åˆ—çš„è´§ç®±å †å½¢æˆç‹­çª„çš„é€šé“ã€‚ | æ–‡ä¸­æœªæ˜ç¡®ç»™å‡ºï¼Œå±äºé«˜åº¦ç»“æ„åŒ–ä½†å¤æ‚çš„è¿·å®«å¼ç»“æ„ã€‚ | **é€šé“ç‹­çª„ã€ç»“æ„é‡å¤ã€æ˜“äº§ç”Ÿæ­»èƒ¡åŒ**ã€‚ç”¨äºæµ‹è¯•ç®—æ³•åœ¨å¤æ‚ç»“æ„åŒ–ç¯å¢ƒä¸­çš„å…¨å±€æ¨ç†å’Œå›æº¯ä¼˜åŒ–èƒ½åŠ›ã€‚ | â€œindoor warehouse composed of densely arranged box stacks forming narrow aislesâ€ |\n\n**ä»¿çœŸè®¾ç½®ç»Ÿä¸€æ¡ä»¶ï¼š**\n*   **æœºå™¨äººå¹³å°**ï¼šå››è½®å·®é€Ÿé©±åŠ¨æœºå™¨äººã€‚\n*   **ä¼ æ„Ÿå™¨**ï¼š16çº¿3D LiDARã€‚\n*   **æœ€å¤§é€Ÿåº¦**ï¼š2 m/sã€‚\n*   **å¯¹æ¯”åŸºå‡†**ï¼šæ‰€æœ‰åŸºçº¿æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„å›¾ç¨€ç–åŒ–ç­–ç•¥å’Œå›¾å½¢ç›¸å…³è®¾ç½®ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ã€‚\n\n#### 2. çœŸå®ä¸–ç•Œå®éªŒåœºæ™¯\n*   **åœ°ç‚¹**ï¼šä¸€ä¸ªå¤§å­¦æ ¡å›­å†…çš„æ•™å­¦å»ºç­‘ã€‚\n*   **è§„æ¨¡**ï¼š**200m Ã— 130m**ï¼Œå±äºå¤§è§„æ¨¡å®¤å†…ç¯å¢ƒã€‚\n*   **ç¯å¢ƒç‰¹ç‚¹**ï¼šç”±é•¿èµ°å»Šã€æˆ¿é—´å’Œäº¤å‰è·¯å£ç»„æˆï¼Œå¯¹**å…¨å±€æ¨ç†å’Œé•¿è§†é‡æ¢ç´¢**æ„æˆæŒ‘æˆ˜ã€‚\n*   **ç¡¬ä»¶é…ç½®**ï¼š\n    *   æœºå™¨äººï¼šAgilex Scout-mini è½®å¼æœºå™¨äººã€‚\n    *   \bè®¡ç®—å•å…ƒï¼šJetson AGX Orinï¼ˆç”¨äºè¿è¡ŒLLMæ¨¡å—ï¼‰ã€‚\n    *   ä¼ æ„Ÿå™¨ï¼šOuster OS0-32 LiDARã€‚\n    *   SLAMï¼šFastLIO2 æä¾›é‡Œç¨‹è®¡å’Œå»ºå›¾ã€‚\n*   **å‚æ•°**ï¼š\n    *   åœ°å›¾åˆ†è¾¨ç‡ (`Î”_map`): 0.4m\n    *   èŠ‚ç‚¹åˆ†è¾¨ç‡ (`Î”_node`): 0.8m\n    *   æœºå™¨äººæœ€å¤§é€Ÿåº¦ï¼š1 m/s\n    *   ä¼ æ„Ÿå™¨èŒƒå›´ï¼š8 m\n    *   å¿«é€Ÿæ€ç»´æ¨¡å—é‡è§„åˆ’é¢‘ç‡ï¼š1 Hz\n\n### æ€»ç»“\nFARE çš„è¯„ä»·ä½“ç³»ç´§å¯†ç»“åˆå…¶ç ”ç©¶ç›®æ ‡ï¼š\n*   **æŒ‡æ ‡**ï¼ˆè·ç¦»ã€æ—¶é—´ï¼‰ç›´æ¥ã€æœ‰æ•ˆåœ°åæ˜ äº†å…¶**æå‡æ¢ç´¢æ•ˆç‡**çš„æ ¸å¿ƒåˆ›æ–°ä»·å€¼ã€‚\n*   **ç¯å¢ƒ**ï¼ˆå¤šæ ·åŒ–çš„ä»¿çœŸåœºæ™¯+å¤§è§„æ¨¡çœŸå®åœºæ™¯ï¼‰å…¨é¢éªŒè¯äº†å…¶**åˆ†å±‚æ¶æ„**ï¼ˆæ…¢æ€è€ƒå…¨å±€æ¨ç†+å¿«æ€è€ƒå±€éƒ¨æ‰§è¡Œï¼‰åœ¨ä¸åŒç»“æ„å¤æ‚æ€§ç¯å¢ƒä¸­çš„**é€‚åº”æ€§å’Œé²æ£’æ€§**ã€‚\n\nå®éªŒç»“æœè¡¨æ˜ï¼ŒFARE å°¤å…¶åœ¨ç»“æ„å¤æ‚çš„â€œä»“åº“â€å’Œâ€œæ£®æ—â€ç¯å¢ƒä¸­ï¼Œå‡­å€Ÿå…¶ç¯å¢ƒè‡ªé€‚åº”çš„å…¨å±€ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸å¿…è¦çš„å›æº¯ï¼Œåœ¨**è¡Œè¿›è·ç¦»å’Œæ¢ç´¢æ—¶é—´ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•**ï¼Œè¯æ˜äº†å…¶æŠ€æœ¯åˆ›æ–°çš„å®é™…æœ‰æ•ˆæ€§ã€‚",
            "timestamp": "2026-01-27T16:05:47.044582",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†\nè®ºæ–‡åœ¨ä»¥ä¸‹ä¸¤ç±»ç¯å¢ƒä¸­è¿›è¡Œäº†å®éªŒéªŒè¯ï¼š\n\n1.  **ä»¿çœŸç¯å¢ƒï¼ˆGazebo Simulationï¼‰**ï¼š\n    - **å®¤å†…ç¯å¢ƒï¼ˆIndoorï¼‰**ï¼šæ¨¡æ‹Ÿç°ä»£åŠå…¬å»ºç­‘ï¼ŒåŒ…å«é•¿èµ°å»Šã€ä¼šè®®å®¤å’Œéš”é—´åŒºåŸŸã€‚\n    - **æ£®æ—ç¯å¢ƒï¼ˆForestï¼‰**ï¼šæ¨¡æ‹Ÿæˆ·å¤–æ£®æ—ï¼ŒåŒ…å«è‡ªç„¶éšœç¢ç‰©ã€æ ‘æœ¨å’Œä¸å¹³å¦åœ°å½¢ã€‚\n    - **ä»“åº“ç¯å¢ƒï¼ˆWarehouseï¼‰**ï¼šæ¨¡æ‹Ÿå®¤å†…ä»“åº“ï¼Œç”±å¯†é›†æ’åˆ—çš„è´§ç®±å †å½¢æˆç‹­çª„é€šé“ã€‚\n\n2.  **çœŸå®ä¸–ç•Œç¡¬ä»¶éªŒè¯**ï¼š\n    - **æ ¡å›­æ•™å­¦æ¥¼ç¯å¢ƒ**ï¼šä¸€ä¸ªå¤§å‹å®¤å†…ç¯å¢ƒï¼Œå°ºå¯¸ä¸º **200m Ã— 130m**ï¼ŒåŒ…å«é•¿èµ°å»Šã€æˆ¿é—´å’Œäº¤å‰è·¯å£ï¼Œç”¨äºéªŒè¯ç³»ç»Ÿåœ¨å¤æ‚ã€å¤§è§„æ¨¡çœŸå®åœºæ™¯ä¸­çš„éƒ¨ç½²èƒ½åŠ›ã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨ä»¥ä¸‹ä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥è¯„ä¼°è‡ªä¸»æ¢ç´¢ç³»ç»Ÿçš„æ€§èƒ½ï¼š\n\n1.  **æ€»æ¢ç´¢è·ç¦»ï¼ˆDistance, mï¼‰**ï¼šæœºå™¨äººå®Œæˆç¯å¢ƒæ¢ç´¢æ‰€è¡Œé©¶çš„æ€»è·¯å¾„é•¿åº¦ã€‚**ç›®æ ‡æ˜¯æœ€å°åŒ–æ­¤è·ç¦»**ã€‚\n2.  **æ€»æ¢ç´¢æ—¶é—´ï¼ˆTime, sï¼‰**ï¼šæœºå™¨äººå®Œæˆç¯å¢ƒæ¢ç´¢æ‰€èŠ±è´¹çš„æ€»æ—¶é—´ã€‚**ç›®æ ‡æ˜¯æœ€å°åŒ–æ­¤æ—¶é—´**ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸å¯¹æ¯”ç»“æœ\nè®ºæ–‡å°†FAREä¸å››ç§å…ˆè¿›çš„åŸºçº¿æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ï¼š**TARE**ã€**DSVP**ã€**ARiADNE** å’Œ **HEADER**ã€‚æ¯ä¸ªç¯å¢ƒè¿›è¡Œ10æ¬¡å®éªŒï¼Œç»“æœå–å¹³å‡å€¼ï¼ˆÂ±æ ‡å‡†å·®ï¼‰ã€‚\n\n**æ€§èƒ½æå‡æ€»ç»“å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼ˆFAREä¸ºæœ€ä¼˜æˆ–æ¥è¿‘æœ€ä¼˜ï¼‰**ï¼š\n\n| ç¯å¢ƒ | å¯¹æ¯”æ–¹æ³• | è·ç¦»å‡å°‘ (FARE vs. æœ€ä½³åŸºçº¿) | æ—¶é—´å‡å°‘ (FARE vs. æœ€ä½³åŸºçº¿) | FARE æ€§èƒ½è¡¨ç° |\n| :--- | :--- | :--- | :--- | :--- |\n| **å®¤å†…** | HEADER (æœ€ä½³åŸºçº¿) | **åŸºæœ¬æŒå¹³** (1048m vs. 1030m) | **ç•¥æœ‰å¢åŠ ** (590s vs. 576s) | ä¸æœ€ä½³åŸºçº¿æ€§èƒ½ç›¸å½“ï¼Œåœ¨ç´§å‡‘ã€ç¼ºä¹å…¨å±€ç»“æ„çš„ç¯å¢ƒä¸­å±•ç¤ºäº†ç«äº‰åŠ›ã€‚ |\n| **æ£®æ—** | HEADER (æœ€ä½³åŸºçº¿) | **æ˜¾è‘—å‡å°‘** (1090m vs. 1230m) | **æ˜¾è‘—å‡å°‘** (680s vs. 725s) | åœ¨å¤æ‚æˆ·å¤–ç¯å¢ƒä¸­ï¼Œ**è·ç¦»å‡å°‘çº¦11.4%ï¼Œæ—¶é—´å‡å°‘çº¦6.2%**ï¼Œä¼˜åŠ¿æ˜æ˜¾ã€‚ |\n| **ä»“åº“** | HEADER (æœ€ä½³åŸºçº¿) | **æ˜¾è‘—å‡å°‘** (441m vs. 492m) | **æ˜¾è‘—å‡å°‘** (252s vs. 286s) | åœ¨å…·æœ‰æ˜æ˜¾å…¨å±€ç»“æ„ï¼ˆç‹­çª„é€šé“ï¼‰çš„ç¯å¢ƒä¸­ï¼Œ**è·ç¦»å‡å°‘çº¦10.4%ï¼Œæ—¶é—´å‡å°‘çº¦11.9%**ï¼Œä¼˜åŠ¿è¿›ä¸€æ­¥æ‰©å¤§ã€‚ |\n\n### å››ã€æ ¸å¿ƒç»“è®ºä¸ä»·å€¼\n- **æ•´ä½“è¶‹åŠ¿**ï¼šFAREåœ¨**ç»“æ„è¶Šå¤æ‚ã€å…¨å±€ç‰¹å¾è¶Šæ˜æ˜¾**çš„ç¯å¢ƒä¸­ï¼ˆå¦‚ä»“åº“ã€æ£®æ—ï¼‰ï¼Œç›¸æ¯”åŸºçº¿æ–¹æ³•çš„æ€§èƒ½æå‡è¶Šæ˜¾è‘—ã€‚è¿™éªŒè¯äº†å…¶**åŸºäºLLMçš„å…¨å±€è¯­ä¹‰æ¨ç†**å’Œ**åˆ†å±‚å†³ç­–æ¶æ„**çš„æœ‰æ•ˆæ€§ã€‚\n- **æŠ€æœ¯åˆ›æ–°ä»·å€¼**ï¼šFAREé€šè¿‡å°†**æ…¢æ€è€ƒï¼ˆLLMå…¨å±€ç­–ç•¥ï¼‰** ä¸**å¿«æ€è€ƒï¼ˆRLå±€éƒ¨æ‰§è¡Œï¼‰** è§£è€¦ï¼ŒæˆåŠŸè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨**åˆ©ç”¨é•¿æœŸä¿¡æ¯**å’Œ**é€‚åº”ç¯å¢ƒå˜åŒ–**æ–¹é¢çš„ç“¶é¢ˆã€‚å…¶å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ¶æ„èƒ½å¤Ÿå‡å°‘ä¸å¿…è¦çš„å›æº¯ï¼Œç”Ÿæˆæ›´é«˜æ•ˆçš„æ¢ç´¢è½¨è¿¹ã€‚\n- **å®é™…éƒ¨ç½²ä»·å€¼**ï¼šç¡¬ä»¶å®éªŒæˆåŠŸåœ¨ä¸€ä¸ªå¤§è§„æ¨¡ï¼ˆ200mÃ—130mï¼‰çœŸå®å»ºç­‘ä¸­å®Œæˆäº†å…¨è‡ªä¸»æ¢ç´¢ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶**ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»èƒ½åŠ›**ä»¥åŠ**åœ¨æœºè½½è®¡ç®—è®¾å¤‡ï¼ˆJetson AGX Orinï¼‰ä¸Šè¿è¡ŒLLMçš„å¯è¡Œæ€§**ã€‚",
            "timestamp": "2026-01-27T16:06:16.011933",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼ŒFAREæ¡†æ¶è™½ç„¶å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### æŠ€æœ¯å±‚é¢\n- **ç¯å¢ƒæè¿°ä¾èµ–äººå·¥è¾“å…¥**ï¼šæ…¢æ€è€ƒæ¨¡å—ï¼ˆLLMï¼‰çš„æ¨ç†ä¾èµ–äº**äººå·¥æä¾›çš„ç®€æ´æ–‡æœ¬ç¯å¢ƒæè¿°**ï¼ˆå¦‚â€œç°ä»£å®¤å†…åŠå…¬æ¥¼â€ã€â€œæˆ·å¤–æ£®æ—ç¯å¢ƒâ€ï¼‰ã€‚è¿™åœ¨å®é™…å®Œå…¨æœªçŸ¥çš„ç¯å¢ƒä¸­å¯èƒ½ä¸ç°å®æˆ–éœ€è¦é¢å¤–æ„ŸçŸ¥æ¨¡å—è‡ªåŠ¨ç”Ÿæˆæè¿°ã€‚\n- **è®¡ç®—å¼€é”€ä¸å®æ—¶æ€§**ï¼šä½¿ç”¨Qwen3-14Bç­‰å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œ**è¿­ä»£å¼å›¾æ¨ç†**å¯èƒ½å¸¦æ¥æ˜¾è‘—çš„è®¡ç®—å»¶è¿Ÿï¼Œå°½ç®¡è®ºæ–‡æåŠä¿æŒäº†ç¨³å®šè¿è¡Œæ—¶æ€§èƒ½ï¼Œä½†åœ¨èµ„æºå—é™çš„åµŒå…¥å¼å¹³å°ï¼ˆå¦‚æœºå™¨äººï¼‰ä¸Šå¤§è§„æ¨¡éƒ¨ç½²ä»å…·æŒ‘æˆ˜æ€§ã€‚\n- **è¡¨ç¤ºä¸åŠ¨ä½œç©ºé—´é™åˆ¶**ï¼š\n    - ç¯å¢ƒè¡¨ç¤ºä¸º**äºŒç»´å æ®æ …æ ¼**ï¼Œæœªå……åˆ†åˆ©ç”¨ä¸‰ç»´å‡ ä½•æˆ–è¯­ä¹‰ä¿¡æ¯ã€‚\n    - åŠ¨ä½œç©ºé—´åŸºäº**å›¾èŠ‚ç‚¹é€‰æ‹©**ï¼Œæœªæ‰©å±•åˆ°è¿ç»­æˆ–æ›´å¤æ‚çš„è¿åŠ¨æ§åˆ¶ã€‚\n- **æ³›åŒ–ä¸è‡ªé€‚åº”èƒ½åŠ›**ï¼šç­–ç•¥åœ¨è®­ç»ƒåå›ºå®šï¼Œ**æ— æ³•åœ¨çº¿é€‚åº”ç¯å¢ƒç±»å‹çš„çªå˜**ï¼ˆå¦‚ä»èµ°å»Šçªç„¶è¿›å…¥å¼€é˜”å¤§å…ï¼‰ã€‚LLMçš„ç­–ç•¥ç”ŸæˆåŸºäºåˆå§‹æè¿°ï¼Œå¯èƒ½æ— æ³•å“åº”æ¢ç´¢è¿‡ç¨‹ä¸­å‘ç°çš„æ–°ç»“æ„ç‰¹å¾ã€‚\n\n### ç®—æ³•ä¸é›†æˆå±‚é¢\n- **å¥–åŠ±è®¾è®¡çš„æ½œåœ¨çŸ­è§†æ€§**ï¼šå¿«é€Ÿæ€è€ƒæ¨¡å—çš„å¥–åŠ±åŒ…å«**æŒ‡ä»¤è·Ÿéšå¥–åŠ±**ï¼Œè™½é¼“åŠ±éµå¾ªå…¨å±€è·¯å¾„ï¼Œä½†å¯èƒ½æŠ‘åˆ¶å¯¹æ„å¤–å±€éƒ¨æœºé‡ï¼ˆå¦‚æ–°å‘ç°æ·å¾„ï¼‰çš„åˆ©ç”¨ï¼Œéœ€åœ¨â€œéµå¾ªâ€ä¸â€œçµæ´»â€é—´ç²¾ç»†å¹³è¡¡ã€‚\n- **å›¾æ„å»ºä¸ä¿®å‰ªçš„å¯å‘å¼**ï¼šå…¨å±€ä¿¡å¿µå›¾çš„æ„å»ºä¾èµ–**æ¨¡å—åº¦ä¿®å‰ª**ï¼Œä¿ç•™Top-kç¤¾åŒºã€‚è¿™ç§å¯å‘å¼å¯èƒ½åœ¨æŸäº›ç¯å¢ƒç»“æ„ä¸‹ä¸¢å¼ƒé‡è¦ä¿¡æ¯ï¼Œä¸”å‚æ•°kéœ€é¢„è®¾ã€‚\n- **å•ä¸€æœºå™¨äººè®¾å®š**ï¼šæ¡†æ¶ç›®å‰ä»…é’ˆå¯¹**å•æœºå™¨äººæ¢ç´¢**ï¼Œæœªè€ƒè™‘å¤šæœºå™¨äººååŒä¸­çš„é€šä¿¡ã€ä»»åŠ¡åˆ†é…ä¸å†²çªé¿å…é—®é¢˜ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡åœ¨ç»“è®ºä¸æœªæ¥å·¥ä½œä¸­å·²æ˜ç¡®æŒ‡å‡ºä»¥ä¸‹å‡ ä¸ªæ–¹å‘ï¼Œç»“åˆåˆ†æå¯è¿›ä¸€æ­¥æ‰©å±•ï¼š\n\n### 1. å¤šæœºå™¨äººæ¢ç´¢æ‰©å±•\n- åœ¨æ¡†æ¶ä¸­å¼•å…¥**æ˜¾å¼çš„æ™ºèƒ½ä½“é—´åè°ƒæœºåˆ¶**ï¼Œä¾‹å¦‚é€šè¿‡LLMè¿›è¡Œåˆ†å¸ƒå¼ç­–ç•¥åå•†æˆ–å…¨å±€ä»»åŠ¡åˆ†è§£ï¼Œä»¥æå‡å¤§èŒƒå›´ç¯å¢ƒçš„å¹¶è¡Œæ¢ç´¢æ•ˆç‡ã€‚\n\n### 2. æ„ŸçŸ¥ä¸æè¿°çš„è‡ªåŠ¨åŒ–\n- é›†æˆ**è§†è§‰è¯­ä¹‰æ„ŸçŸ¥æ¨¡å—**ï¼Œå®ç°**åœ¨çº¿ç¯å¢ƒæè¿°ç”Ÿæˆ**ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨VLMï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰å®æ—¶åˆ†æåœºæ™¯ï¼Œè‡ªåŠ¨ç”Ÿæˆæˆ–æ›´æ–°ç”¨äºLLMæ¨ç†çš„æ–‡æœ¬æè¿°ï¼Œå‡å°‘å¯¹å…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ã€‚\n- æ¢ç´¢**å¤šæ¨¡æ€ç¯å¢ƒè¡¨ç¤º**ï¼Œèåˆå‡ ä½•ã€çº¹ç†ä¸è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿ç­–ç•¥èƒ½ç†è§£â€œæˆ¿é—´â€ã€â€œé—¨â€ã€â€œèµ°å»Šâ€ç­‰é«˜çº§æ¦‚å¿µã€‚\n\n### 3. ä¸‰ç»´ä¸åŠ¨æ€ç¯å¢ƒé€‚åº”\n- å°†è¡¨ç¤ºæ‰©å±•è‡³**ä¸‰ç»´ä½“ç´ åœ°å›¾æˆ–ç‚¹äº‘å›¾**ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„ä¸‰ç»´å›¾æ¨ç†ä¸åŠ¨ä½œç©ºé—´ï¼Œä»¥æ”¯æŒæ— äººæœºæˆ–å¤æ‚åœ°å½¢ä¸‹çš„æ¢ç´¢ã€‚\n- å¢å¼ºæ¡†æ¶å¯¹**åŠ¨æ€å˜åŒ–ç¯å¢ƒ**ï¼ˆå¦‚ç§»åŠ¨éšœç¢ç‰©ã€ä¸´æ—¶å°é—­åŒºåŸŸï¼‰çš„åœ¨çº¿é€‚åº”èƒ½åŠ›ï¼Œå¯èƒ½é€šè¿‡åœ¨çº¿é‡è§„åˆ’æˆ–åŠ¨æ€å›¾æ›´æ–°å®ç°ã€‚\n\n### 4. ç®—æ³•æ•ˆç‡ä¸é²æ£’æ€§æå‡\n- ä¼˜åŒ–LLMæ¨ç†æ•ˆç‡ï¼Œç ”ç©¶**è½»é‡åŒ–æ¨¡å‹å¾®è°ƒ**ã€**æ¨ç†åŠ é€Ÿ**ï¼ˆå¦‚æ¨¡å‹è’¸é¦ã€ä¸“ç”¨ç¡¬ä»¶ï¼‰æˆ–**ç¼“å­˜æœºåˆ¶**ï¼Œä»¥æå‡å®æ—¶æ€§ã€‚\n- è®¾è®¡æ›´**è‡ªé€‚åº”**çš„å›¾ä¿®å‰ªä¸è¡¨ç¤ºæ–¹æ³•ï¼Œä½¿å›¾ç»“æ„èƒ½æ ¹æ®ç¯å¢ƒå¤æ‚åº¦åŠ¨æ€è°ƒæ•´ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±ã€‚\n- å¼ºåŒ–**å¥–åŠ±å¡‘é€ **ï¼Œæ¢ç´¢å¦‚ä½•è®©æŒ‡ä»¤è·Ÿéšå¥–åŠ±æ›´å¥½åœ°ä¸é•¿æœŸæ¢ç´¢æ”¶ç›Šç»“åˆï¼Œé¿å…å±€éƒ¨æœ€ä¼˜ã€‚\n\n### 5. ä»¿çœŸåˆ°å®ç‰©çš„æ³›åŒ–å¼ºåŒ–\n- åœ¨æ›´å¤šæ ·åŒ–çš„ä»¿çœŸç¯å¢ƒï¼ˆåŒ…æ‹¬æç«¯ç»“æ„ã€å…‰ç…§å˜åŒ–ã€ä¼ æ„Ÿå™¨å™ªå£°ï¼‰ä¸­è¿›è¡Œ**å¤§è§„æ¨¡è®­ç»ƒä¸æµ‹è¯•**ï¼Œæå‡ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚\n- ç ”ç©¶**åŸŸè‡ªé€‚åº”æŠ€æœ¯**ï¼Œç¼©å°ä»¿çœŸä¸çœŸå®ä¸–ç•Œåœ¨ä¼ æ„Ÿå™¨æ¨¡å‹ã€åŠ¨åŠ›å­¦ç­‰æ–¹é¢çš„å·®è·ã€‚\n\n```plaintext\næ ¸å¿ƒæ”¹è¿›é€»è¾‘é“¾ï¼š\näººå·¥æè¿° â†’ è‡ªåŠ¨è¯­ä¹‰æ„ŸçŸ¥ â†’ å¤šæ¨¡æ€è¡¨ç¤º\nå•æœºå™¨äºº â†’ å¤šæœºå™¨äººååŒ â†’ åˆ†å¸ƒå¼ä¼˜åŒ–\näºŒç»´å›ºå®šç¯å¢ƒ â†’ ä¸‰ç»´åŠ¨æ€ç¯å¢ƒ â†’ åœ¨çº¿é€‚åº”\nLLMç¦»çº¿æ¨ç† â†’ è½»é‡åŒ–/åŠ é€Ÿ â†’ å®æ—¶é—­ç¯å†³ç­–\n```\n\nè¿™äº›æ”¹è¿›æ–¹å‘æ—¨åœ¨ä½¿FAREæ¡†æ¶æ›´å…·**è‡ªä¸»æ€§ã€é€‚åº”æ€§ã€æ‰©å±•æ€§å’Œå®ç”¨æ€§**ï¼Œä»è€Œæ›´å¥½åœ°åº”å¯¹å¤æ‚ã€å¤§è§„æ¨¡ã€åŠ¨æ€çš„çœŸå®ä¸–ç•Œæ¢ç´¢ä»»åŠ¡ã€‚",
            "timestamp": "2026-01-27T16:06:51.653698",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T05:56:24Z",
    "created_at": "2026-01-27T15:53:17.636144",
    "updated_at": "2026-01-27T15:53:17.636150"
}