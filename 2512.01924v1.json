{
    "id": "2512.01924v1",
    "title": "Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model",
    "authors": [
        "Kentaro Fujii",
        "Shingo Murata"
    ],
    "abstract": "åœ¨ä¸ç¡®å®šçš„çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œæœºå™¨äººå¿…é¡»åŒæ—¶æ‰§è¡Œç›®æ ‡å¯¼å‘å’Œæ¢ç´¢æ€§åŠ¨ä½œã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„æ§åˆ¶æ–¹æ³•å¿½ç•¥äº†æ¢ç´¢è¡Œä¸ºï¼Œåœ¨ä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨æ·±åº¦ä¸»åŠ¨æ¨ç†ï¼ˆdeep active inferenceï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç»Ÿä¸€è§£é‡Šäººç±»çš„ç›®æ ‡å¯¼å‘ä¸æ¢ç´¢æ€§è¡Œä¸ºã€‚ä½†ä¼ ç»Ÿçš„æ·±åº¦ä¸»åŠ¨æ¨ç†æ–¹æ³•ç”±äºç¯å¢ƒè¡¨å¾èƒ½åŠ›æœ‰é™ä»¥åŠåŠ¨ä½œé€‰æ‹©è¿‡ç¨‹ä¸­è®¡ç®—æˆæœ¬è¿‡é«˜è€Œé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°é¢–çš„æ·±åº¦ä¸»åŠ¨æ¨ç†æ¡†æ¶ï¼ŒåŒ…å«ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ã€ä¸€ä¸ªåŠ¨ä½œæ¨¡å‹å’Œä¸€ä¸ªæŠ½è±¡ä¸–ç•Œæ¨¡å‹ã€‚ä¸–ç•Œæ¨¡å‹å°†ç¯å¢ƒåŠ¨åŠ›å­¦ç¼–ç ä¸ºå¿«æ…¢ä¸¤ç§æ—¶é—´å°ºåº¦ä¸‹çš„éšçŠ¶æ€è¡¨ç¤ºï¼›åŠ¨ä½œæ¨¡å‹é€šè¿‡å‘é‡é‡åŒ–å°†åŠ¨ä½œåºåˆ—å‹ç¼©ä¸ºæŠ½è±¡åŠ¨ä½œï¼›æŠ½è±¡ä¸–ç•Œæ¨¡å‹åˆ™åŸºäºè¿™äº›æŠ½è±¡åŠ¨ä½œé¢„æµ‹æœªæ¥çš„æ…¢é€ŸçŠ¶æ€ï¼Œä»è€Œå®ç°ä½è®¡ç®—æˆæœ¬çš„åŠ¨ä½œé€‰æ‹©ã€‚æˆ‘ä»¬åœ¨çœŸå®æœºå™¨äººä¸Šè¿›è¡Œäº†ç‰©ä½“æ“ä½œä»»åŠ¡çš„å®éªŒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸­å‡å®ç°äº†é«˜æˆåŠŸç‡ï¼Œèƒ½å¤Ÿåœ¨ä¸ç¡®å®šç¯å¢ƒä¸‹çµæ´»åˆ‡æ¢ç›®æ ‡å¯¼å‘ä¸æ¢ç´¢æ€§è¡Œä¸ºï¼ŒåŒæ—¶æ˜¾è‘—é™ä½åŠ¨ä½œé€‰æ‹©çš„è®¡ç®—å¤æ‚åº¦ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å¯¹å¤šæ—¶é—´å°ºåº¦åŠ¨æ€å»ºæ¨¡ä»¥åŠåŠ¨ä½œä¸çŠ¶æ€è½¬ç§»æŠ½è±¡åŒ–çš„é‡è¦æ€§ã€‚",
    "url": "https://arxiv.org/abs/2512.01924v1",
    "html_url": "https://arxiv.org/html/2512.01924v1",
    "html_content": "Real-World Robot Control by Deep Active Inference\nwith a Temporally Hierarchical World Model\nKentaro Fujii\n1\nand Shingo Murata\n1\n*This work was supported by JST PRESTO (JPMJPR22C9), JSPS KAKENHI (JP24K03012), Mori Manufacturing Research and Technology Foundation.\n1\nKentaro Fujii and Shingo Murata are with Graduate School of Integrated Design Engineering, Keio University\noakwood.n14.4sp@keio.jp, murata@elec.keio.ac.jp\nAbstract\nRobots in uncertain realâ€‘world environments must perform both goalâ€‘directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot.\nResults show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable.\nThese findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.\nI\nINTRODUCTION\nWith recent advances in deep learning-based robot control methods, there is growing expectation for the realization of robots capable of achieving a wide range of human-like goals\n[\nchi2023diffusion\n,\netukuru2024robot\n,\nblack2024pi\n]\n.\nIn real-world environments, the presence or arrangement of objects required for a task is often uncertain, and current robots struggle to cope with such uncertainty\n[\nlynch2023interactive\n]\n.\nIn contrast, humans can not only act toward achieving goals but also explore to resolve environmental uncertaintyâ€”e.g., by searching for the location of an objectâ€”thereby adapting effectively to uncertain situations\n[\nfriston2015epistemic\n,\nFriston2017\n]\n.\nTo realize robots capable of both goal-directed and exploratory actions, we focus on deep active inference\n[\nMillidge2020\n,\nfountas2020deep\n,\nMazzaglia2021\n,\nfujii2024real\n]\nâ€”a deep learning-based framework grounded in a computational theory that accounts for various cognitive functions\n[\nfriston2010free\n,\nfriston2015epistemic\n,\nschwartenbeck2019computational\n]\n.\nHowever, deep active inference faces two key challenges: (1) its performance heavily depends on the capability of the framework to represent environmental dynamics\n[\nsajid2021exploration\n]\n, and (2) the computational cost is prohibitively high\n[\nMazzaglia2021\n]\n, making it difficult to apply to real-world robots.\nTo address these challenges, we propose a deep active inference framework comprising a world model, an action model, and an abstract world model.\nThe world model learns hidden state transitions to represent environmental dynamics from human-collected robot action and observation data\n[\nha2018world\n,\nHafner2018\n,\nahmadi2019novel\n]\n.\nThe action model maps a sequence of actual actions to one of a learned set of abstract actions, each corresponding to a meaningful behavior (e.g., moving an object from a dish to a pan)\n[\nlee2024behavior\n]\n.\nThe abstract world model learns the relationship between the state representations learned by the world model and the abstract action representations learned by the action model\n[\ngumbsch2024learning\n]\n.\nBy leveraging the abstract world model and the abstract action representations, the framework enables efficient active inference.\nTo evaluate the proposed method, we conducted robot experiments in real-world environments with uncertainty.\nWe investigated whether the framework could reduce computational cost, enable the robot to achieve diverse goals involving the manipulation of multiple objects, and perform exploratory actions to resolve environmental uncertainty.\nFigure 1:\nThe overview of the proposed framework. The framework comprises a world model, an action model, and an abstract world model. Here, key variables are visualized: observation\no\nt\no_{t}\nand action\na\nt\na_{t}\nare processed by the world model to infer hierarchical hidden states\nz\nt\ns\nz_{t}^{\\mathrm{s}}\n,\nz\nt\nf\nz_{t}^{\\mathrm{f}}\n. The action model compresses action sequences into abstract actions\nA\nt\nA_{t}\n. The abstract world model uses\nA\nt\nA_{t}\nto predict the future slow deterministic state\nd\nt\n+\nh\ns\nd_{t+h}^{\\mathrm{s}}\n.\nII\nRELATED WORK\nII-A\nLearning from Demonstration (LfD) for Robot Control\nLfD is a method to train robots by imitating human experts, providing safe, task-relevant data for learning control policies\n[\nravichandar2020recent\n,\ncorreia2024survey\n,\nzare2024survey\n,\nflorence2022implicit\n,\nlancaster2023modem\n,\nJang2022\n]\n.\nA key advancement contributing to recent progress in LfD for robotics is the idea of generating multi-step action sequences, rather than only single-step actions\n[\nzhao2023learning\n,\nchi2023diffusion\n,\nlee2024behavior\n,\netukuru2024robot\n,\nblack2024pi\n]\n.\nHowever, a major challenge in LfD is the difficulty of generalizing to environments with uncertainty, even when trained on large amounts of expert demonstrations\n[\nlynch2023interactive\n]\n.\nIn this work, we focus on the approach that uses quantized features extracted from action sequences\n[\nlee2024behavior\n]\n, and treat the extracted features as abstract action representations.\nII-B\nWorld Model\nA world model captures the dynamics of the environment by modeling the relationship between data (observations), their latent causes (hidden states), and actions.\nThey have recently attracted significant attention in the context of model-based reinforcement learning\n[\nha2018world\n,\nHafner2018\n]\n, especially in artificial agents and robotics\n[\nTaniguchi03072023\n]\n.\nHowever, when robots learn using a world model, their performance is constrained by the modelâ€™s capability to represent environmental dynamics\n[\nCai2022\n,\ndeng2024facing\n]\n.\nIn particular, learning long-term dependencies in the environment remains a challenge.\nOne solution is to introduce temporal hierarchy into the model structure\n[\nKim2019\n,\nCai2022\n,\nSaxena2021\n,\nfujii2023hierarchical\n]\n.\nFurthermore, by incorporating abstract action representations that capture slow dynamics, the model can more efficiently predict future observations and states\n[\ngumbsch2024learning\n]\n.\nTemporal hierarchy can be introduced by differentiating state update frequencies\n[\nKim2019\n,\nSaxena2021\n,\nCai2022\n]\nor modulating time constants of state transitions\n[\nYamashita2008\n,\nahmadi2019novel\n,\nspieler2024the\n]\n.\nIn this work, we adopt the latter to better represent slow dynamics in our world model\n[\nfujii2023hierarchical\n]\n.\nIII\nTHE FORMULATION OF ACTIVE INFERENCE\nThe free-energy principle\n[\nfriston2010free\n,\nfriston2015epistemic\n,\nFriston2017\n]\nis a computational principle that accounts for various cognitive functions.\nAccording to this principle, human observations\no\no\nare generated by unobservable hidden states\nz\nz\n, which evolve in response to actions\na\na\n, following a partially observable Markov decision process\n[\nfriston2015epistemic\n]\n.\nThe brain is assumed to model this generative process with the world model.\nUnder the free-energy principle, human perception and action aim to minimize the surprise\nâˆ’\nlog\nâ¡\np\nâ€‹\n(\no\n)\n-\\log p(o)\n.\nHowever, since directly minimizing surprise is intractable, active inference instead minimizes its tractable upper bound, the variational free energy\n[\nfriston2015epistemic\n,\nFriston2017\n]\n.\nPerception can be formulated as the minimization of the following variational free energy at time step\nt\nt\n[\nMazzaglia2021\n,\nmazzaglia2022free\n,\nSmith2022\n]\n:\nâ„±\nâ€‹\n(\nt\n)\n\\displaystyle\\mathcal{F}(t)\n=\nD\nKL\nâ€‹\n[\nq\nâ€‹\n(\nz\nt\n)\nâˆ¥\np\nâ€‹\n(\nz\nt\n)\n]\nâˆ’\nğ”¼\nq\nâ€‹\n(\nz\nt\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nt\nâˆ£\nz\nt\n)\n]\n\\displaystyle=D_{\\mathrm{KL}}\\left[q\\left(z_{t}\\right)\\|p\\left(z_{t}\\right)\\right]-\\mathbb{E}_{q\\left(z_{t}\\right)}\\left[\\log p\\left(o_{t}\\mid z_{t}\\right)\\right]\n(1)\nâ‰¥\nâˆ’\nlog\nâ¡\np\nâ€‹\n(\no\nt\n)\n.\n\\displaystyle\\geq-\\log p(o_{t}).\nHere,\nq\nâ€‹\n(\nz\nt\n)\nq(z_{t})\ndenotes the approximate posterior over the hidden state\nz\nt\nz_{t}\n,\nD\nKL\n[\nq\n(\nâ‹…\n)\n|\n|\np\n(\nâ‹…\n)\n]\nD_{\\mathrm{KL}}[q(\\cdot)||p(\\cdot)]\nis the Kullbackâ€“Leibler (KL) divergence. Note that the first line of (\n1\n) is equivalent to the negative evidence lower bound\n[\nKingma2013\n,\nrezende2014stochastic\n]\n.\nAction can be formulated as the minimization of expected free energy (EFE), which extends variational free energy to account for future states and observations.\nLet\nÏ„\n>\nt\n\\tau>t\nbe a future time step, The EFE is defined as follows\n[\nSmith2022\n]\n:\nğ’¢\nâ€‹\n(\nÏ„\n)\nâ‰ˆ\n\\displaystyle\\mathcal{G}(\\tau)\\approx\nâˆ’\nğ”¼\nq\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\nq\nâ€‹\n(\nz\nÏ„\nâˆ£\no\nÏ„\n,\nÏ€\n)\nâˆ’\nlog\nâ¡\nq\nâ€‹\n(\nz\nÏ„\nâˆ£\nÏ€\n)\n]\nâŸ\nEpistemic value\n\\displaystyle-\\underbrace{\\mathbb{E}_{q(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q(z_{\\tau}\\mid o_{\\tau},\\pi)-\\log q(\\mathrm{z_{\\tau}}\\mid\\pi)]}_{\\text{Epistemic value}}\n(2)\nâˆ’\nğ”¼\nq\nâ€‹\n(\no\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\nâŸ\nExtrinsic value\n.\n\\displaystyle-\\underbrace{\\mathbb{E}_{q(o_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})]}_{\\text{Extrinsic value}}.\nHere, the expectation is over the observation\no\nÏ„\no_{\\tau}\nbecause the future observation is not yet available\n[\nSmith2022\n]\n, and\nÏ€\n\\pi\nindicates the policy (i.e. an action sequence).\nThe variable\no\npref\no_{\\text{pref}}\nis referred to as a preference, which encodes the goal, and the distribution\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\np(o_{\\tau}\\mid o_{\\text{pref}})\nis called the prior preference.\nIn (\n2\n), the first term referred to as the epistemic value is the mutual information between the state\nz\nÏ„\nz_{\\tau}\nand the observation\no\nÏ„\no_{\\tau}\n.\nThis term encourages exploratory policies that reduce the uncertainty in the prior belief\nq\nâ€‹\n(\nz\nÏ„\nâˆ£\nÏ€\n)\nq(z_{\\tau}\\mid\\pi)\n.\nOn the other hand, the second term referred to as the extrinsic value encourages goal-directed policies.\nTherefore, selecting a policy\nÏ€\n\\pi\nthat minimizes the EFE can account for both exploratory and goal-directed actions\n[\nfriston2015epistemic\n,\nFriston2017\n,\nigari2024selection\n]\n.\nConventional active inference requires calculating the EFE over all possible action sequences during task execution, which is intractable for real-world action spaces\n[\nFriston2017\n]\n.\nRecent works have addressed this by using the EFE as a loss function for training of action generation models\n[\nMillidge2020\n,\nfountas2020deep\n,\nMazzaglia2021\n]\n, but often ignored exploration capability.\nIn this work, we propose a novel framework focusing on both goal-achievement performance and exploration capability tractably calculating the EFE during task execution.\nFigure 2:\nThe world model. It consists of a dynamics model, an encoder, and a decoder. The dynamics model has two different timescales.\nIV\nMETHOD\nIV-A\nFramework\nWe propose a framework based on deep active inference that enables both goal achievement and exploration. The proposed framework consists of a world model, an action model, and an abstract world model (Fig.\n1\n).\nIV-A1\nWorld Model\nThe world model comprises a dynamics model, an encoder, and a decoder, all of which are trained simultaneously (Fig.\n2\n).\nAs the dynamics model, we utilize a hierarchical model\n[\nfujii2022hierarchical\n]\n, which consists of the slow and fast states as the hidden states\nz\nt\n=\n{\nz\nt\ns\n,\nz\nt\nf\n}\nz_{t}=\\{z^{\\mathrm{s}}_{t},z^{\\mathrm{f}}_{t}\\}\nfor time step\nt\nt\n. Both deterministic\nd\nd\nand stochastic\ns\ns\nstates are defined for each of the slow and fast states\nz\nt\ns\n=\n{\nd\nt\ns\n,\ns\nt\ns\n}\n,\nz\nt\nf\n=\n{\nd\nt\nf\n,\ns\nt\nf\n}\nz^{\\mathrm{s}}_{t}=\\{d^{\\mathrm{s}}_{t},s^{\\mathrm{s}}_{t}\\},z^{\\mathrm{f}}_{t}=\\{d^{\\mathrm{f}}_{t},s^{\\mathrm{f}}_{t}\\}\n, respectively.\nThese hidden states are calculated as follows:\nSlow dynamics\n(3)\nDeterministic state:\nd\nt\ns\n=\nf\nÎ¸\ns\nâ€‹\n(\nz\nt\nâˆ’\n1\ns\n)\n\\displaystyle\\quad d^{\\mathrm{s}}_{t}=f^{\\mathrm{s}}_{\\theta}\\left(z^{\\mathrm{s}}_{t-1}\\right)\nPrior:\ns\n^\nt\ns\nâˆ¼\np\nÎ¸\ns\nâ€‹\n(\ns\nt\ns\nâˆ£\nd\nt\ns\n)\n\\displaystyle\\quad\\hat{s}^{\\mathrm{s}}_{t}\\sim p^{\\mathrm{s}}_{\\theta}\\left(s^{\\mathrm{s}}_{t}\\mid d^{\\mathrm{s}}_{t}\\right)\nApproximate posterior:\ns\nt\ns\nâˆ¼\nq\nÎ¸\ns\nâ€‹\n(\ns\nt\ns\nâˆ£\nd\nt\ns\n,\nd\nt\nâˆ’\n1\nf\n)\n.\n\\displaystyle\\quad s^{\\mathrm{s}}_{t}\\sim q^{\\mathrm{s}}_{\\theta}\\left(s^{\\mathrm{s}}_{t}\\mid d^{\\mathrm{s}}_{t},d^{\\mathrm{f}}_{t-1}\\right).\nFast dynamics\nDeterministic State:\nd\nt\nf\n=\nf\nÎ¸\nf\nâ€‹\n(\ns\nt\ns\n,\nz\nt\nâˆ’\n1\nf\n,\na\nt\nâˆ’\n1\n)\n\\displaystyle\\quad d^{\\mathrm{f}}_{t}=f^{\\mathrm{f}}_{\\theta}\\left(s^{\\mathrm{s}}_{t},z^{\\mathrm{f}}_{t-1},a_{t-1}\\right)\nPrior:\ns\n^\nt\nf\nâˆ¼\np\nÎ¸\nf\nâ€‹\n(\ns\nt\nf\nâˆ£\nd\nt\nf\n)\n\\displaystyle\\quad\\hat{s}^{\\mathrm{f}}_{t}\\sim p^{\\mathrm{f}}_{\\theta}\\left(s^{\\mathrm{f}}_{t}\\mid d^{\\mathrm{f}}_{t}\\right)\nApproximate posterior:\ns\nt\nf\nâˆ¼\nq\nÎ¸\nf\nâ€‹\n(\ns\nt\nf\nâˆ£\nd\nt\nf\n,\no\nt\n)\n\\displaystyle\\quad s^{\\mathrm{f}}_{t}\\sim q^{\\mathrm{f}}_{\\theta}\\left(s^{\\mathrm{f}}_{t}\\mid d^{\\mathrm{f}}_{t},o_{t}\\right)\nHere,\no\nt\no_{t}\nis the observation and\na\nt\nâˆ’\n1\na_{t-1}\nis the action at the previous time step.\nThe approximate posterior of the fast dynamics\nq\nÎ¸\nf\nq^{\\mathrm{f}}_{\\theta}\nis conditioned on the observation\no\nt\no_{t}\nby receiving its features extracted by the encoder.\nThe slow and fast deterministic states\nd\nt\ns\nd^{\\mathrm{s}}_{t}\nand\nd\nt\nf\nd^{\\mathrm{f}}_{t}\nare computed by multiple timescale recurrent neural network parameterized with a time constant\n[\nYamashita2008\n]\n.\nWhen the time constant is large, the state tends to evolve slowly compared to when the time constant is small.\nTherefore, by setting the time constant for the slow layer larger than one for the fast layer, the dynamics model represent a temporal hierarchy.\nThe slow and fast stochastic states\ns\nt\ns\n,\ns\n^\nt\ns\ns^{\\mathrm{s}}_{t},\\hat{s}^{\\mathrm{s}}_{t}\nand\ns\nt\nf\n,\ns\n^\nt\nf\ns^{\\mathrm{f}}_{t},\\hat{s}^{\\mathrm{f}}_{t}\nare represented as one-hot vectors sampled from an approximate posterior or a prior,\ndefined by categorical distributions\n[\nHafner2020\n]\n.\nThe decoder is employed to reconstruct the observation\no\nt\no_{t}\nfrom the hidden state\nz\nt\nz_{t}\n, modeling likelihood\np\nÎ¸\nâ€‹\n(\no\nt\nâˆ£\nz\nt\n)\np_{\\theta}(o_{t}\\mid z_{t})\n.\nSimultaneously, a network\np\nÎ¸\nâ€‹\n(\nd\nt\nf\nâˆ£\nz\nt\ns\n)\np_{\\theta}(d^{\\mathrm{f}}_{t}\\mid z_{t}^{\\mathrm{s}})\nthat predicts the fast deterministic state\nd\n^\nt\nf\n\\hat{d}^{\\mathrm{f}}_{t}\nfrom the slow hidden state\nz\nt\ns\nz^{\\mathrm{s}}_{t}\nis also trained.\nThe predicted deterministic state\nd\n^\nt\nf\n\\hat{d}^{\\mathrm{f}}_{t}\nis then used to sample the fast stochastic state. By combining both slow and predicted fast hidden states as inputs to the decoder, the dynamics model can represent the observation likelihood\np\nÎ¸\nâ€‹\n(\no\nt\nâˆ£\nz\nt\ns\n)\np_{\\theta}(o_{t}\\mid z^{\\mathrm{s}}_{t})\n1\n1\n1\nCorrectly, this distribution is written as\np\nÎ¸\nâ€‹\n(\no\nt\nâˆ£\nz\nt\ns\n)\n=\nâˆ«\np\nÎ¸\nâ€‹\n(\no\nt\nâˆ£\nz\nt\n)\nâ€‹\np\nÎ¸\nf\nâ€‹\n(\ns\nt\nf\nâˆ£\nd\nt\nf\n)\nâ€‹\np\nÎ¸\nâ€‹\n(\nd\nt\nf\nâˆ£\nz\nt\ns\n)\nâ€‹\nd\nz\nt\nf\np_{\\theta}\\left(o_{t}\\mid z_{t}^{\\mathrm{s}}\\right)=\\int p_{\\theta}\\left(o_{t}\\mid z_{t}\\right)p_{\\theta}^{\\mathrm{f}}\\left(s_{t}^{\\mathrm{f}}\\mid d_{t}^{\\mathrm{f}}\\right)p_{\\theta}\\left(d_{t}^{\\mathrm{f}}\\mid z_{t}^{\\mathrm{s}}\\right)\\mathrm{d}z_{t}^{\\mathrm{f}}\n. We approximate the marginal over the fast states\nz\nt\nf\nz_{t}^{\\mathrm{f}}\nwith a single Monte Carlo sample.\nbased on only the slow hidden state\nz\nt\ns\nz^{\\mathrm{s}}_{t}\n.\nThe world model is trained by minimizing the variational free energy\nâ„±\nâ€‹\n(\nt\n)\n\\mathcal{F}(t)\n. Here, since the fast deterministic state\nd\nt\nf\nd^{\\mathrm{f}}_{t}\ncan be regarded as an observation for the slow dynamics, the variational free energies\nâ„±\ns\nâ€‹\n(\nt\n)\n\\mathcal{F}_{\\mathrm{s}}(t)\nand\nâ„±\nf\nâ€‹\n(\nt\n)\n\\mathcal{F}_{\\mathrm{f}}(t)\ncan be computed separately for the slow and fast layers, respectively.\nFurthermore, we also minimize, as an auxiliary task, the negative log-likelihood of observation\no\nt\no_{t}\ngiven the slow hidden state\nz\nt\ns\nz^{\\mathrm{s}}_{t}\n, denoted as\nlog\nâ¡\np\nÎ¸\nâ€‹\n(\no\nt\nâˆ£\nz\nt\ns\n)\n\\log p_{\\theta}(o_{t}\\mid z^{\\mathrm{s}}_{t})\n.\nIn summary, the variational free energy\nâ„±\nâ€‹\n(\nt\n)\n\\mathcal{F}(t)\nin this work is described as follows:\nâ„±\nâ€‹\n(\nt\n)\n=\n\\displaystyle\\mathcal{F}(t)=\nâ„±\ns\nâ€‹\n(\nt\n)\n+\nâ„±\nf\nâ€‹\n(\nt\n)\nâˆ’\nlog\nâ¡\np\nÎ¸\nâ€‹\n(\no\nt\n|\nz\nt\ns\n)\n\\displaystyle\\mathcal{F}_{\\mathrm{s}}(t)+\\mathcal{F}_{\\mathrm{f}}(t)-\\log p_{\\theta}(o_{t}|z^{\\mathrm{s}}_{t})\n(4)\nâ„±\ns\nâ€‹\n(\nt\n)\n=\n\\displaystyle\\mathcal{F}_{\\mathrm{s}}(t)=\nD\nKL\n[\nsg\n(\nq\nÎ¸\ns\n(\ns\nt\ns\nâˆ£\nd\nt\ns\n,\nd\nt\nâˆ’\n1\nf\n)\n)\nâˆ¥\np\nÎ¸\ns\n(\ns\nt\ns\nâˆ£\nd\nt\ns\n)\n]\n\\displaystyle\\quad D_{\\text{KL}}[\\mathrm{sg}(q^{\\mathrm{s}}_{\\theta}\\left(s^{\\mathrm{s}}_{t}\\mid d^{\\mathrm{s}}_{t},d^{\\mathrm{f}}_{t-1}\\right))\\|p^{\\mathrm{s}}_{\\theta}\\left(s^{\\mathrm{s}}_{t}\\mid d^{\\mathrm{s}}_{t}\\right)]\nâˆ’\nlog\nâ¡\np\nÎ¸\nâ€‹\n(\nd\nt\nf\nâˆ£\nz\nt\ns\n)\n\\displaystyle-\\log p_{\\theta}(d^{\\mathrm{f}}_{t}\\mid z_{t}^{\\mathrm{s}})\nâ„±\nf\nâ€‹\n(\nt\n)\n=\n\\displaystyle\\mathcal{F}_{\\mathrm{f}}(t)=\nD\nKL\n[\nsg\n(\nq\nÎ¸\nf\n(\ns\nt\nf\nâˆ£\nd\nt\nf\n,\no\nt\n)\n)\nâˆ¥\np\nÎ¸\nf\n(\ns\nt\nf\nâˆ£\nd\nt\nf\n)\n]\n\\displaystyle\\quad D_{\\text{KL}}[\\mathrm{sg}(q^{\\mathrm{f}}_{\\theta}\\left(s^{\\mathrm{f}}_{t}\\mid d^{\\mathrm{f}}_{t},o_{t}\\right))\\|p^{\\mathrm{f}}_{\\theta}\\left(s^{\\mathrm{f}}_{t}\\mid d^{\\mathrm{f}}_{t}\\right)]\nâˆ’\nlog\np\nÎ¸\n(\no\nt\nâˆ£\nz\nt\n)\n]\n.\n\\displaystyle-\\log p_{\\theta}(o_{t}\\mid z_{t})].\nHere, for the KL divergence calculation, we use the KL balancing technique with a weighting factor\nw\nw\n[\nHafner2020\n]\n.\nIV-A2\nAction Model\nThe action model consists of an encoder\nâ„°\nÏ•\n\\mathcal{E}_{\\phi}\nand a decoder\nğ’Ÿ\nÏ•\n\\mathcal{D}_{\\phi}\ncomposed of multilayer perceptron (MLP), as well as a residual vector quantizer\n[\nvan2017neural\n,\nzeghidour2021soundstream\n,\nlee2024behavior\n]\nğ’¬\nÏ•\n\\mathcal{Q}_{\\phi}\nwith\nN\nq\n=\n2\nN_{q}=2\nlayers.\nFirst, the encoder\nâ„°\nÏ•\n\\mathcal{E}_{\\phi}\nembeds the action sequence\na\nt\n:\nt\n+\nh\na_{t:t+h}\nof length\nh\nh\ninto a low-dimensional feature\nA\nt\nA_{t}\n. Next, the feature\nA\nt\nA_{t}\nis quantized into\nA\n^\nt\n\\hat{A}_{t}\nusing the residual vector quantizer\nğ’¬\nÏ•\n\\mathcal{Q}_{\\phi}\n. The residual vector quantizer includes codebooks\n{\nC\ni\n}\ni\n=\n1\nN\nq\n\\{C_{i}\\}_{i=1}^{N_{q}}\n, each containing\nK\nK\nlearnable codes\n{\nc\ni\n,\nj\n}\nj\n=\n1\nK\n\\{c_{i,j}\\}_{j=1}^{K}\n.\nSpecifically, the quantized vector at layer\ni\ni\nis the code\nc\ni\n,\nk\nc_{i,k}\nhaving the smallest Euclidean distance to the input at layer\ni\ni\n.\nThe quantized feature\nA\n^\nt\n\\hat{A}_{t}\nis the sum of outputs from each quantization layer\n{\nA\n^\nt\n,\ni\n}\ni\n=\n1\nN\nq\n=\nâˆ‘\ni\nN\nq\nc\ni\n,\nk\n\\{\\hat{A}_{t,i}\\}_{i=1}^{N_{q}}=\\sum_{i}^{N_{q}}c_{i,k}\n.\nFinally, the decoder\nğ’Ÿ\nÏ•\n\\mathcal{D}_{\\phi}\nreconstructs the quantized feature\nA\n^\nt\n\\hat{A}_{t}\ninto the action sequence\na\n^\nt\n:\nt\n+\nh\n\\hat{a}_{t:t+h}\n. In summary, the procedure of the action model is described as follows:\nA\nt\n\\displaystyle A_{t}\n=\nâ„°\nÏ•\nâ€‹\n(\na\nt\n:\nt\n+\nh\n)\n\\displaystyle=\\mathcal{E}_{\\phi}(a_{t:t+h})\n(5)\nA\n^\nt\n\\displaystyle\\hat{A}_{t}\n=\nğ’¬\nÏ•\nâ€‹\n(\nA\nt\n)\n\\displaystyle=\\mathcal{Q}_{\\phi}(A_{t})\na\n^\nt\n:\nt\n+\nh\n\\displaystyle\\hat{a}_{t:t+h}\n=\nğ’Ÿ\nÏ•\nâ€‹\n(\nA\n^\nt\n)\n.\n\\displaystyle=\\mathcal{D}_{\\phi}(\\hat{A}_{t}).\nWe treat the feature\nA\n^\nt\n\\hat{A}_{t}\n, obtained by the action model, as an abstract action representing the action sequence\na\nt\n:\nt\n+\nh\na_{t:t+h}\n.\nThe encoder\nâ„°\nÏ•\n\\mathcal{E}_{\\phi}\nand decoder\nğ’Ÿ\nÏ•\n\\mathcal{D}_{\\phi}\nof the action model are trained by minimizing the following objective:\nâ„’\nÏ•\n=\n\\displaystyle\\mathcal{L}_{\\phi}=\nÎ»\nMSE\nâ€‹\nâ€–\na\nt\n:\nt\n+\nh\nâˆ’\na\n^\nt\n:\nt\n+\nh\nâ€–\n2\n2\n\\displaystyle\\lambda_{\\text{MSE}}\\|a_{t:t+h}-\\hat{a}_{t:t+h}\\|_{2}^{2}\n(6)\n+\n\\displaystyle+\nÎ»\ncommit\nâ€‹\nÎ£\ni\n=\n1\nN\nq\nâ€‹\nâ€–\n(\nA\nt\nâˆ’\nÎ£\ni\nâ€‹\n(\nA\n^\nt\n,\ni\nâˆ’\n1\n)\n)\nâˆ’\nsg\nâ¡\n(\nc\ni\n,\nk\n)\nâ€–\n2\n2\n\\displaystyle\\lambda_{\\text{commit}}\\Sigma_{i=1}^{N_{q}}\\left\\|(A_{t}-\\Sigma_{i}(\\hat{A}_{t,i-1}))-\\operatorname{sg}\\left(c_{i,k}\\right)\\right\\|_{2}^{2}\nwhere we assume\nA\n^\nt\n,\n0\n=\n0\n\\hat{A}_{t,0}=0\n. Moreover,\nÎ»\nMSE\n\\lambda_{\\text{MSE}}\nand\nÎ»\ncommit\n\\lambda_{\\text{commit}}\nare coefficients for the reconstruction loss\nâ„’\nMSE\n\\mathcal{L}_{\\text{MSE}}\nand the commitment loss\nâ„’\ncommit\n\\mathcal{L}_{\\text{commit}}\n, respectively. The learning of the codebooks\n{\nC\ni\n}\ni\n=\n1\nN\nq\n\\{C_{i}\\}_{i=1}^{N_{q}}\nof the residual vector quantizer\nğ’¬\nÏ•\n\\mathcal{Q}_{\\phi}\nis performed using exponential moving averages\n[\nvan2017neural\n,\nlee2024behavior\n]\n.\nIV-A3\nAbstract World Model\nThe abstract world model\nğ’²\nÏˆ\n\\mathcal{W}_{\\psi}\nlearns a mapping from the current world model state\nz\nt\nz_{t}\nand an abstract action\nA\nt\nA_{t}\nto the future slow deterministic state\nd\nt\n+\nh\ns\nd^{\\mathrm{s}}_{t+h}\n.\nIn other words, it provides an abstract representation of state transitions.\nThe model\nğ’²\nÏˆ\n\\mathcal{W}_{\\psi}\nis composed of MLP and takes the abstract action\nA\nt\nA_{t}\nand the current world model state\nz\nt\nz_{t}\nas inputs to predict the slow deterministic state\nd\nt\n+\nh\ns\nd^{\\mathrm{s}}_{t+h}\n.\nHere, the input abstract action\nA\nt\nA_{t}\nto\nğ’²\nÏˆ\n\\mathcal{W}_{\\psi}\ncan be any of the\nK\nN\nq\nK^{N_{q}}\ncombinations of learned codes from the action model, denoted as\n{\nA\n^\nn\n}\nn\n=\n1\nK\nN\nq\n\\{\\hat{A}_{n}\\}_{n=1}^{K^{N_{q}}}\n.\nAccordingly, for a given current hidden state\nz\nt\nz_{t}\n, the abstract world model\nğ’²\nÏˆ\n\\mathcal{W}_{\\psi}\npredicts\nK\nN\nq\nK^{N_{q}}\npossible future slow deterministic states\n{\nd\nt\n+\nh\n,\nn\ns\n}\nn\n=\n1\nK\nN\nq\n\\{d^{\\mathrm{s}}_{t+h,n}\\}_{n=1}^{K^{N_{q}}}\n:\n{\nd\n^\nt\n+\nh\n,\nn\ns\n}\nn\n=\n1\nK\nN\nq\n=\nğ’²\nÏˆ\nâ€‹\n(\nz\nt\n,\n{\nA\n^\nn\n}\nn\n=\n1\nK\nN\nq\n)\n.\n\\displaystyle\\{{\\hat{d}^{\\mathrm{s}}_{t+h,n}}\\}_{n=1}^{K^{N_{q}}}=\\mathcal{W}_{\\psi}(z_{t},\\{\\hat{A}_{n}\\}_{n=1}^{K^{N_{q}}}).\n(7)\nThe abstract world mode is trained by minimizing the following objective:\nâ„’\nÏˆ\n=\n1\nK\nN\nq\nâ€‹\nâˆ‘\nn\n=\n1\nK\nN\nq\nâ€–\nd\n^\nt\n+\nh\n,\nn\ns\nâˆ’\nd\nt\n+\nh\n,\nn\ns\nâ€–\n2\n2\n.\n\\mathcal{L}_{\\psi}=\\frac{1}{K^{N_{q}}}\\sum_{n=1}^{K^{N_{q}}}\\|{\\hat{d}^{\\mathrm{s}}_{t+h,n}}-d^{\\mathrm{s}}_{t+h,n}\\|_{2}^{2}.\n(8)\nHere, to obtain the target slow deterministic states\n{\nd\nt\n+\nh\n,\nn\ns\n}\nn\n=\n1\nK\nN\nq\n\\{d^{\\mathrm{s}}_{t+h,n}\\}_{n=1}^{K^{N_{q}}}\n, we utilize latent imagination of the world model\n[\nHafner2018\n]\n. To this end, the action sequences\n{\na\n^\n0\n:\nh\n,\nn\n}\nn\n=\n1\nK\nN\nq\n\\{\\hat{a}_{0:h,n}\\}_{n=1}^{K^{N_{q}}}\nare generated from the code combinations\n{\nA\n^\nn\n}\nn\n=\n1\nK\nN\nq\n\\{\\hat{A}_{n}\\}_{n=1}^{K^{N_{q}}}\nusing the decoder\nğ’Ÿ\nÏ•\n\\mathcal{D}_{\\phi}\nof the action model. Then, by leveraging the prior distribution over the fast states, the slow deterministic states\n{\nd\nt\n+\nh\n,\nn\ns\n}\nn\n=\n1\nK\nN\nq\n\\{d^{\\mathrm{s}}_{t+h,n}\\}_{n=1}^{K^{N_{q}}}\nat\nh\nh\nsteps ahead are obtained.\nFigure 3:\nAction selection based on the minimization of EFE.\nFirst, future states are predicted for multiple abstract actions.\nThen, the EFE is calculated for each of the predicted future states.\nFinally, the robot execute action sequence reconstructed from the abstract action that yields the lowest EFE.\nIV-B\nAction Selection\nTo make the EFE\nğ’¢\nâ€‹\n(\nÏ„\n)\n\\mathcal{G}(\\tau)\ncalculation tractable, our framework leverages a learned, finite set of abstract actions\n{\nA\n^\nn\n}\nn\n=\n1\nK\nN\nq\n\\{\\hat{A}_{n}\\}_{n=1}^{K^{N_{q}}}\n, instead of considering all possible (and thus infinite) continuous action sequences.\nFirst, we reformulate (\n2\n) in accordance with our world model (for a detailed derivation, see Appendix\nA\n):\nğ’¢\nâ€‹\n(\nÏ„\n)\n=\n\\displaystyle\\mathcal{G}(\\tau)=\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\nâˆ£\no\nÏ„\n,\nÏ€\n)\nâˆ’\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\nâˆ£\nÏ€\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(z_{\\tau}\\mid o_{\\tau},\\pi)-\\log q_{\\theta}(\\mathrm{z_{\\tau}}\\mid\\pi)]\n(9)\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})]\nâ‰ˆ\n\\displaystyle\\approx\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\ns\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n,\no\nÏ„\n)\nâˆ’\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\ns\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}},o_{\\tau})-\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}})]\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\n.\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})].\nHere, the joint distribution\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nq_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)\ncan be decomposed as\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\n=\np\nÎ¸\nâ€‹\n(\no\nÏ„\nâˆ£\nz\nÏ„\n)\nâ€‹\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\nâ€‹\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\ns\nâˆ£\nÏ€\n)\nq_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)=p_{\\theta}(o_{\\tau}\\mid z_{\\tau})q_{\\theta}(z^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}})q_{\\theta}(z_{\\tau}^{\\mathrm{s}}\\mid\\pi)\nin our proposed framework.\nNote that, given the distribution\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\ns\nâˆ£\nÏ€\n)\nq_{\\theta}(z_{\\tau}^{\\mathrm{s}}\\mid\\pi)\nover the slow states, all distributions required to compute the EFE\nğ’¢\nâ€‹\n(\nÏ„\n)\n\\mathcal{G}(\\tau)\ncan be obtained using the world model, and thus\nğ’¢\nâ€‹\n(\nÏ„\n)\n\\mathcal{G}(\\tau)\nbecomes computable.\nHere, we replace the policy\nÏ€\n\\pi\nwith an abstract action\nA\n^\nâˆˆ\n{\nA\n^\nn\n}\nn\n=\n1\nK\nN\nq\n\\hat{A}\\in\\{\\hat{A}_{n}\\}_{n=1}^{K^{N_{q}}}\n, and express the distribution\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\ns\nâˆ£\nÏ€\n)\nq_{\\theta}(z_{\\tau}^{\\mathrm{s}}\\mid\\pi)\nas follows:\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\ns\nâˆ£\nÏ€\n)\n\\displaystyle q_{\\theta}(z_{\\tau}^{\\mathrm{s}}\\mid\\pi)\nâ‰ˆ\nq\nÎ¸\nâ€‹\n(\ns\nÏ„\ns\nâˆ£\nd\nÏ„\ns\n,\nA\n^\n)\nâ€‹\nq\nÏˆ\nâ€‹\n(\nd\nÏ„\ns\nâˆ£\nA\n^\n)\n.\n\\displaystyle\\approx q_{\\theta}(s_{\\tau}^{\\mathrm{s}}\\mid d^{\\mathrm{s}}_{\\tau},\\hat{A})q_{\\psi}(d^{\\mathrm{s}}_{\\tau}\\mid\\hat{A}).\n(10)\nIn this way, we can use the abstract world model\nğ’²\nÏˆ\n\\mathcal{W}_{\\psi}\nto predict the slow deterministic state\nd\nÏ„\ns\nd^{\\mathrm{s}}_{\\tau}\nat\nÏ„\n=\nt\n+\nh\n\\tau=t+h\nfrom the abstract action\nA\n^\n\\hat{A}\n.\nUsing the predicted deterministic state\nd\nÏ„\ns\nd^{\\mathrm{s}}_{\\tau}\n, we can obtain the slow prior\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\ns\nâˆ£\nÏ€\n)\nq_{\\theta}(z_{\\tau}^{\\mathrm{s}}\\mid\\pi)\nand compute the EFE.\nWhen computing the EFE, the prior preference\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\np(o_{\\tau}\\mid o_{\\text{pref}})\nis assumed to follow a Gaussian distribution\nğ’©\nâ€‹\n(\no\npref\n,\nÏƒ\n2\n)\n\\mathcal{N}(o_{\\text{pref}},\\sigma^{2})\nwith mean\no\npref\n{o_{\\text{pref}}}\nand variance\nÏƒ\n2\n\\sigma^{2}\n. Therefore, the EFE can be written as follows:\nğ’¢\nâ€‹\n(\nÏ„\n)\nâ‰ˆ\n\\displaystyle\\mathcal{G}(\\tau)\\approx\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\ns\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n,\no\nÏ„\n)\nâˆ’\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\ns\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}},o_{\\tau})-\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}})]\n(11)\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nâˆ’\nÎ³\nâ€‹\n(\no\nÏ„\nâˆ’\no\npref\n)\n2\n]\n,\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[{-\\gamma{(o_{\\tau}-o_{\\text{pref}}})^{2}}],\nwhere\nÎ³\n=\n1\n/\n2\nâ€‹\nÏƒ\n2\n\\gamma=1/2\\sigma^{2}\nis the preference precision, which balances the epistemic and extrinsic values, and the expectations in the EFE are approximated via Monte Carlo sampling\n[\nigari2024selection\n]\n.\nTo generate actual robot actions, we first use the abstract world model\nğ’²\nÏˆ\n\\mathcal{W}_{\\psi}\nto predict the slow deterministic states\n{\nd\nt\n+\nh\n,\nn\ns\n}\nn\n=\n1\nK\nN\nq\n\\{d^{\\mathrm{s}}_{t+h,n}\\}_{n=1}^{K^{N_{q}}}\nat\nh\nh\nsteps into the future for all abstract actions\n{\nA\n^\nn\n}\nn\n=\n1\nK\nN\nq\n\\{\\hat{A}_{n}\\}_{n=1}^{K^{N_{q}}}\n, given the current world model state\nz\nt\nz_{t}\n.\nNext, we predict slow hidden states\n{\nz\nt\n+\nh\n,\nn\ns\n}\nn\n=\n1\nK\nN\nq\n\\{z^{\\mathrm{s}}_{t+h,n}\\}_{n=1}^{K^{N_{q}}}\nbased on the predicted slow deterministic states by using (\n10\n).\nThen, for each predicted state, we compute the EFE and select the abstract action that yields the minimum EFE.\nThe selected abstract action is then decoded into an action sequence\na\n^\nt\n:\nt\n+\nh\n\\hat{a}_{t:t+h}\nby the action model, and the robot executes this sequence.\nV\nEXPERIMENTS\nV-A\nEnvironment Setup\nFigure 4:\nExperimental environment (left) and policy patterns included in the collected dataset (right).\nThe environment contains either a blue ball, a red ball, or both.\nThe dataset includes demonstrations of eight different policy patterns involving the movement of the lid and the balls.\nTo investigate whether the proposed framework enables both goal achievement and exploration in real-world environmentsâ€”where multiple objects can be manipulated and uncertainty arises from their placementâ€”we conducted an experiment using a robot shown in Fig.\n4\n(left)\n[\nkoch2024lowcost\n,\ncadene2024lerobot\n]\n.\nThe robot had six degrees of freedom, one of which is the gripper.\nA camera (RealSense Depth Camera D435; Intel) was mounted opposite to the robot to capture a view of both the robot and its environment.\nFrom the viewpoint of the camera, a simple dish, a pot, and a pan were placed on the right, center, and left, respectively, and a pot lid was placed closer to the camera than the center pot.\nAdditionally, the environment was configured such that a blue ball, a red ball, or both could be present.\nNote that, therefore, uncertainty arose when the lid was closed, as the pot might or might not contain a blue or red ball in this environment.\nAs training data, we collected object manipulation data by demonstrating the predetermined eight patterns of policies (Fig.\n4\n(right)).\nEach demonstration consists of a sequence of two patterns of policies. For all valid combinationsâ€”excluding those in which the policy would result in no movement (e.g., performing action 3 twice in a row)â€”we collected five demonstrations per combination by teleoperating the robot in a leaderâ€“follower manner.\nThere are\n36\n36\nvalid action combinations for environments containing either a blue ball or a red ball, and\n72\n72\ncombinations for environments containing both.\nEach sequence contains\n100\n100\ntime steps of joint angles and camera images recorded at\n5\n5\nHz.\nTherefore, each pattern of policies had roughly\n50\n50\ntime steps.\nThe original RGB images were captured, resized and clipped to\n64\nÃ—\n80\n64\\times 80\n. In this experiment, the robot action\na\nt\na_{t}\nis defined as the absolute joint angle positions, and the observation\no\nt\no_{t}\nis defined as the camera image.\nV-B\nInterpretation of the Model Components\nIn this experiment, we expected the slow hidden states\nz\nt\ns\nz_{t}^{\\mathrm{s}}\nto represent the overarching progress of the task, such as where the balls and the lid were placed.\nIn contrast, we expected the fast hidden states\nz\nt\nf\nz_{t}^{\\mathrm{f}}\nrepresents more immediate, transient information.\nOn the other hand, we expected abstract actions\nA\nt\nA_{t}\nto represent a meaningful behavior learned from the demonstration data. In an ideal case, an abstract action corresponds to one of the eight policy patterns in Fig.\n4\n(right), such as moving the ball from the dish to the pan.\nV-C\nExperimental Criteria\nCapability of abstract world model:\nWe evaluated the capability of the abstract world model.\nFirst, we compared the computation time of our proposed framework against that of conventional deep active inference approaches\n[\nMazzaglia2021\n,\nsajid2021exploration\n,\nigari2024selection\n]\n, which predicts future states with the world model by sequentially inputting the action sequence\na\n^\n0\n:\nh\n\\hat{a}_{0:h}\nreconstructed from an abstract action\nA\n^\n\\hat{A}\nvia the action model.\nSecond, we evaluated whether different predictions can be generated from the same initial state for each abstract action learned by the action model. We also examined whether the observed outcomes resulting from executing actual actions generated from a specific abstract action are consistent with the predictions made by the abstract world model.\nGoal achievement performance:\nWe evaluated the success rate on ball- (140 trials) and lid-manipulation (24 trials) tasks with varying object configurations, such as moving a particular ball or manipulating a lid.\nA trial was considered successful if the target object was placed in its specified goal position within\n50\n50\ntime steps.\nEnvironment exploration:\nWe evaluated whether the proposed framework can generate not only goal-directed actions but also exploratory actions from an uncertain initial situation.\nTo this end, we set up a scenario in which the blue ball is initially placed in the pan and the lid is closed, creating uncertainty about whether the red ball is present inside the pot.\nIn this scenario, when taking an exploratory action, it was expected that the robot would open the lid to resolve the uncertainty.\nV-D\nBaseline and Ablation\nIn the goal-achievement performance experiment, we compared our proposed framework with a baseline and two ablations described as follows:\nâ€¢\nGoal-conditioned diffusion policy (GC-DP).\nAs a baseline, we implemented a diffusion policy with a U-Net backbone\n[\nronneberger2015u\n,\nchi2023diffusion\n]\n. In our implementation, this policy predicted a\n48\n48\n-step future actions based on the two most recent observations and a goal observation. To stabilize actions, we apply an exponential moving average of weight\n0.7\n0.7\nto the generated actions.\nâ€¢\nNon-hierarchical. As an ablation study, the world model is replaced by a non-hierarchical dynamics model\n[\nHafner2020\n]\n. In this variant, the hidden state\nz\nt\nz_{t}\nconsists of a single-level deterministic state\nd\nt\nd_{t}\nand a stochastic state\ns\nt\ns_{t}\n, where the deterministic state is computed using a gated recurrent unit\n[\nchung2014empirical\n]\n.\nâ€¢\nNo abstract world model (AWM).\nAs an ablation study, the robot does not use the abstract world model for planning. Instead, it calculates the EFE directly over actual action sequences decoded by the action model.\nWe did not perform an ablation on the action model itself, as our framework relies on it to generate the set of candidate actions (either abstract or actual) for evaluation, making it a core, indispensable component.\nVI\nRESULTS\nVI-A\nCapability of abstract world model\nFigure 5:\nExample of predicted observations using the abstract world model and actual robot actions.\n(A) Predicted observations for each abstract action.\nHere, each\nc\ni\n,\nj\nc_{i,j}\ndenotes the\nj\nj\nâ€‘th code in the\ni\ni\nâ€‘th layer of the action model.\nThe yellow box highlights an example prediction that is consistent with the initial observation, while the red box indicates an inconsistent prediction.\n(B) Actual observations corresponding to the action sequence generated from the abstract action\nA\n^\n\\hat{A}\nrepresented by\nA\n^\n=\nc\n1\n,\n2\n+\nc\n2\n,\n7\n\\hat{A}=c_{1,2}+c_{2,7}\nat each time step.\nOur proposed framework required only\n2.37\nâ€‹\nms\n2.37\\text{ ms}\nto evaluate all candidate abstract actions, in contrast to\n71.8\nâ€‹\nms\n71.8\\text{ ms}\nfor a sequential evaluation of conventional deep active inference approaches. This demonstrates the higher computational tractability of our proposed framework.\nAs shown in Fig.\n5\n, different abstract actions lead to distinct predictions.\nMoreover, for example, by using an action sequence generated from the abstract action represented by\nc\n1\n,\n2\n+\nc\n2\n,\n7\nc_{1,2}+c_{2,7}\n, the ball was successfully moved from the dish to the pan, consistent with the predicted observation (Fig.\n5\n).\nThese results suggest that the abstract world model has learned the dependency between abstract actions and the resulting state transitions, even without directly referring to actual action sequences. However, the prediction associated with the abstract action\nA\n^\n\\hat{A}\nrepresented by\nA\n^\n=\nc\n1\n,\n8\n+\nc\n2\n,\n8\n\\hat{A}=c_{1,8}+c_{2,8}\nin Fig.\n5\nshows red balls placed on both the dish and the pan, which is inconsistent with the initial condition in which only a blue ball was present. This abstract action corresponded to moving a ball from the center pot to the pan. Since this action was not demonstrated when the pot was empty, the abstract world model may have learned incorrect dependencies for unlearned actionâ€“environment combinations.\nVI-B\nGoal achievement performance\nTABLE I:\nSuccess rate (%).\nManipulation target\nBall\nLid\nTotal\nRed\nBlue\nOpening\nClosing\nProposed\n61.4\n\\bm{61.4}\n74.3\n\\bm{74.3}\n75.0\n75.0\n100.0\n\\bm{100.0}\n70.7\n\\bm{70.7}\nGC-DP\n18.6\n18.6\n25.7\n25.7\n25.0\n25.0\n50.0\n50.0\n24.4\n24.4\nNon-hierarchical\n41.4\n41.4\n51.4\n51.4\n75.0\n75.0\n58.3\n58.3\n51.2\n51.2\nNo AWM\n40.0\n40.0\n21.4\n21.4\n83.3\n\\bm{83.3}\n66.7\n66.7\n37.2\n37.2\nFigure 6:\nExample of EFE computed for each abstract action.\nTop\n: EFE values computed for all 64 abstract actions. The action with the lowest EFE is highlighted as a yellow bar.\nBottom\n: From left to right, the images show the initial observation, goal, and predicted observation resulting from the abstract action with the lowest EFE.\nTable\nI\nshows the success rates of our proposed framework on goal-directed action generation, evaluated on tasks involving specific ball and lid manipulations. The proposed method outperformed the baseline and the ablations across all goal conditions except the Lid-Opening goal, achieving a total success rate of over 70%. As a qualitative example, Fig.\n6\nillustrates the EFE calculation for a scenario where the goal is to move a ball from a dish to a pan. The abstract action with the lowest EFE correctly predicts the desired outcome, and executing the actual actions derived from this abstract action led to successful task completion. This overall result confirms that selecting abstract actions by minimizing the EFE is effective for goal achievement.\nThe failures in our framework were mainly due to inconsistent world model predictions, which misled the robot into believing an inappropriate action would succeed. For example, the proposed framework selected actions to grasp nothing but place the (non-grasped) target object at the appropriate location. In contrast, the GC-DP, Non-hierarchical, and No AWM all exhibited lower success rates. The GC-DP frequently failed in grasping and placing objects. Both ablations suffered from more prediction inconsistencies than our full model, highlighting the importance of temporal hierarchy and action/state abstraction. The lower performance of the No AWM ablation suggests that action abstraction was a particularly critical component for success.\nVI-C\nEnvironment exploration\nTABLE II:\nEFE values for two representative abstract actions (goal-directed and exploratory) in the uncertain scenario.\nPreference precision\nGoal-directed\nExploratory\nÎ³\n=\n10\n2\n\\gamma=10^{2}\n4.21\nÃ—\nğŸğŸ\nğŸ’\n\\bm{4.21\\times 10^{4}}\n14.5\nÃ—\n10\n4\n14.5\\times 10^{4}\nÎ³\n=\n10\nâˆ’\n4\n\\gamma=10^{-4}\nâˆ’\n4.67\nÃ—\n10\n0\n-4.67\\times 10^{0}\nâˆ’\n6.11\nÃ—\nğŸğŸ\nğŸ\n\\bm{-6.11\\times 10^{0}}\nFor simplicity, we computed the EFE for two abstract actions: moving the blue ball from the pan to the dish (goal-directed), and opening the lid (exploratory), as summarized in Table\nII\n.\nWhen preference precision\nÎ³\n\\gamma\nwas set to\n10\n2\n10^{2}\n, the EFE for the goal-directed action became lower, and thus the robot moved the blue ball from the pan to the dish.\nIn contrast, when preference precision\nÎ³\n\\gamma\nwas set to\n10\nâˆ’\n4\n10^{-4}\n, the EFE for the exploratory action became lower, and thus the robot opened the lid.\nThese results indicate that the proposed framework can assign high epistemic value to exploratory actions that provide new information, and that exploratory actions can be induced by appropriately adjusting the preference precision\nÎ³\n\\gamma\n.\nVII\nCONCLUSIONS\nIn this work, we introduced a deep activeâ€‘inference framework that combines a temporallyâ€‘hierarchical world model, an action model utilizing vector quantization, and an abstract world model.\nBy capturing dynamics in a temporal hierarchy and encoding action sequences as abstract actions, the framework makes the action selection based on active inference computationally tractable.\nReal-world experiments on object-manipulation tasks demonstrated that the proposed framework outperformed the baseline in various goal-directed settings, as well as the ability to switch from goal-directed to exploratory actions in uncertain environments.\nDespite these promising results, several challenges remain: 1) The action model used a fixed sequence length, which may not be optimal. 2) The modelâ€™s predictive capability decreases for action-environment combinations not present in the dataset. 3) While we validated the capability to take exploratory actions, we did not evaluate their effectiveness in solving tasks and the switching to exploratory behavior still relies on a manually tuned hyperparameter.\nFuture work will focus on extending the framework to address these limitations. An immediate step is to evaluate our framework in environments that require multi-step action selection and where exploration is necessary to solve the task. Other promising directions include developing a mechanism for adaptive switching between goal-directed and exploratory modes, and extending the action model to represent variable-length action sequences. Ultimately, this work represents a significant step toward the long-term goal of creating more capable robots that can operate effectively in uncertain real-world environments such as household tasks by leveraging both goal-directed and exploratory behaviors.\nAppendix A\nEFE DERIVATION\nWe show the detailed derivation of EFE in our framework:\nğ’¢\nâ€‹\n(\nÏ„\n)\n=\n\\displaystyle\\mathcal{G}(\\tau)=\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\nâˆ£\no\nÏ„\n,\nÏ€\n)\nâˆ’\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\nâˆ£\nÏ€\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(z_{\\tau}\\mid o_{\\tau},\\pi)-\\log q_{\\theta}(\\mathrm{z_{\\tau}}\\mid\\pi)]\n(12)\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})]\n=\n\\displaystyle=\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\n[\nlog\nq\nÎ¸\n(\nz\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n,\no\nÏ„\n)\nq\nÎ¸\n(\nz\nÏ„\ns\nâˆ£\nÏ€\n)\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(z_{\\tau}^{\\mathrm{f}}\\mid z^{\\mathrm{s}}_{\\tau},o_{\\tau})q_{\\theta}(z^{\\mathrm{s}}_{\\tau}\\mid\\pi)\nâˆ’\nlog\nq\nÎ¸\n(\nz\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\nq\nÎ¸\n(\nz\nÏ„\ns\nâˆ£\nÏ€\n)\n]\n\\displaystyle-\\log q_{\\theta}(z_{\\tau}^{\\mathrm{f}}\\mid z^{\\mathrm{s}}_{\\tau})q_{\\theta}(z^{\\mathrm{s}}_{\\tau}\\mid\\pi)]\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})]\n=\n\\displaystyle=\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n,\no\nÏ„\n)\nâˆ’\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\nz\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(z_{\\tau}^{\\mathrm{f}}\\mid z^{\\mathrm{s}}_{\\tau},o_{\\tau})-\\log q_{\\theta}(z_{\\tau}^{\\mathrm{f}}\\mid z^{\\mathrm{s}}_{\\tau})]\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})]\n=\n\\displaystyle=\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\n[\nlog\nq\nÎ¸\n(\ns\nÏ„\nf\nâˆ£\nd\nÏ„\nf\n,\no\nÏ„\n)\nq\nÎ¸\n(\nd\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid d^{\\mathrm{f}}_{\\tau},o_{\\tau})q_{\\theta}(d^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}})\nâˆ’\nlog\nq\nÎ¸\n(\ns\nÏ„\nf\nâˆ£\nd\nÏ„\nf\n)\nq\nÎ¸\n(\nd\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\n]\n\\displaystyle-\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid d^{\\mathrm{f}}_{\\tau})q_{\\theta}(d^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}})]\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})]\nâ‰ˆ\n\\displaystyle\\approx\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\ns\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n,\no\nÏ„\n)\nâˆ’\nlog\nâ¡\nq\nÎ¸\nâ€‹\n(\ns\nÏ„\nf\nâˆ£\nz\nÏ„\ns\n)\n]\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}},o_{\\tau})-\\log q_{\\theta}(s^{\\mathrm{f}}_{\\tau}\\mid z_{\\tau}^{\\mathrm{s}})]\nâˆ’\nğ”¼\nq\nÎ¸\nâ€‹\n(\no\nÏ„\n,\nz\nÏ„\nâˆ£\nÏ€\n)\nâ€‹\n[\nlog\nâ¡\np\nâ€‹\n(\no\nÏ„\nâˆ£\no\npref\n)\n]\n.\n\\displaystyle-\\mathbb{E}_{q_{\\theta}(o_{\\tau},z_{\\tau}\\mid\\pi)}[\\log p(o_{\\tau}\\mid o_{\\text{pref}})].\nAppendix B\nADDITIONAL EXPERIMENTS\nTo validate the scalability of our framework, we further evaluated our framework on the CALVIN D benchmark\n[\nmees2022calvin\n]\n, which provides various unstructured human data.\nAlthough this environment can serve language goal conditioning, we used only image-based goal conditioning.\nFor this environment, we compared our proposed framework with the GC-DP. The evaluation was conducted on eight tasks: move_slider_left/right (Slider), open/close_drawer (Drawer), turn_on/off_lightbulb (Lightbulb), and turn_on/off_led (LED). A trial was considered successful if the task was completed within\n150\n150\ntimesteps. Our proposed framework used the same hyperparameters as in our primary experiments, but the GC-DP was trained to predict a\n28\n28\n-step future action sequence from a four-step observation history and re-planned every\n16\n16\nsteps.\nAs shown in Table\nIII\n, our proposed method consistently outperformed GC-DP on the Slider and Drawer tasks, as well as on the average success rate across all tasks.\nThese results suggest that our approach, which leverages a temporally hierarchical world model and abstract actions, is robust and effective not only in our primary setup but also in more complex, long-horizon manipulation scenarios.\nTABLE III:\nSuccess rate in CALVIN Environmet (%).\ntask\nSlider\nDrawer\nLightbulb\nLED\nTotal\nProposed\n43.8\n\\bm{43.8}\n93.8\n\\bm{93.8}\n0.0\n0.0\n11.8\n11.8\n37.5\n\\bm{37.5}\nGC-DP\n1.6\n1.6\n68.3\n68.3\n52.6\n52.6\n16.7\n16.7\n34.8\n34.8\nAppendix C\nHYPER PARAMETERS\nWe show hyperparameters in our experiments in Table\nIV\n.\nTABLE IV:\nHyperparameters of our proposed framework\nName\nSymbol\nValue\nWorld Model\nTraining data sequence length\nâ€”\n75\nSlow dynamics\nDeterministic state dimensions\nâ€”\n32\nStocahstic state dimensions\nÃ—\n\\times\nclasses\nâ€”\n4\nÃ—\n4\n4\\times 4\nTime constant\nâ€”\n32\nFast dynamics\nDeterministic state dimensions\nâ€”\n128\nStocahstic state dimensions\nÃ—\n\\times\nclasses\nâ€”\n8\nÃ—\n8\n8\\times 8\nTime constant\nâ€”\n4\nKL balancing\nw\nw\n0.8\nAction Model\nLayers of MLP\nâ€”\n2\nHidden dimensions of MLP\nâ€”\n128\nAction sequence length\nh\nh\n50\nCodebook size\nK\nK\n8\nAbstract action dimensions\nâ€”\n32\nLearning coefficients\nÎ»\nMSE\n,\nÎ»\ncommit\n\\lambda_{\\text{MSE}},\\lambda_{\\text{commit}}\n1.0\n,\n5.0\n1.0,5.0\nAbstract World Model\nLayers of MLP\nâ€”\n2\nHidden dimensions of MLP\nâ€”\n512",
    "preview_text": "Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.\n\nReal-World Robot Control by Deep Active Inference\nwith a Temporally Hierarchical World Model\nKentaro Fujii\n1\nand Shingo Murata\n1\n*This work was supported by JST PRESTO (JPMJPR22C9), JSPS KAKENHI (JP24K03012), Mori Manufacturing Research and Technology Foundation.\n1\nKentaro Fujii and Shingo Murata are with Graduate School of Integrated Design Engineering, Keio University\noakwood.n14.4sp@keio.jp, murata@elec.keio.ac.jp\nAbstract\nRobots in uncertain realâ€‘world environments must perform both goalâ€‘directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggl",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "temporally hierarchical world model",
        "deep active inference",
        "action abstraction",
        "robot control",
        "efficient action selection"
    ],
    "one_line_summary": "æå‡ºä¸€ç§åŸºäºæ·±åº¦ä¸»åŠ¨æ¨ç†å’Œå¤šæ—¶é—´å°ºåº¦ä¸–ç•Œæ¨¡å‹çš„æœºå™¨äººæ§åˆ¶æ¡†æ¶ï¼Œå¼ºè°ƒåœ¨ä¸ç¡®å®šæ€§ä¸‹çš„ç›®æ ‡å¯¼å‘ä¸æ¢ç´¢è¡Œä¸ºå¹³è¡¡ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T17:41:01Z",
    "created_at": "2026-01-09T11:31:37.258467",
    "updated_at": "2026-01-09T11:31:37.258475",
    "flag": true
}