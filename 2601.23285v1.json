{
    "id": "2601.23285v1",
    "title": "End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms",
    "authors": [
        "MH Farhadi",
        "Ali Rabiee",
        "Sima Ghafoori",
        "Anna Cetera",
        "Andrew Fisher",
        "Reza Abiri"
    ],
    "abstract": "å…±äº«è‡ªä¸»ç³»ç»Ÿéœ€è¦å…·å¤‡æ¨æ–­ç”¨æˆ·æ„å›¾å’Œç¡®å®šé€‚å½“è¾…åŠ©æ°´å¹³çš„åŸç†æ€§æ–¹æ³•ã€‚è¿™æ˜¯äººæœºäº¤äº’é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç³»ç»Ÿå¿…é¡»åœ¨æˆåŠŸå®Œæˆä»»åŠ¡çš„åŒæ—¶å……åˆ†è€ƒè™‘ç”¨æˆ·çš„è‡ªä¸»æ€§ã€‚ä»¥å¾€æ–¹æ³•ä¾èµ–é™æ€æ··åˆæ¯”ä¾‹æˆ–å°†ç›®æ ‡æ¨æ–­ä¸è¾…åŠ©ä»²è£åˆ†ç¦»ï¼Œå¯¼è‡´åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­è¡¨ç°æ¬ ä½³ã€‚æˆ‘ä»¬æå‡ºBRACEï¼ˆåŸºäºä¸Šä¸‹æ–‡ç¼–ç çš„è´å¶æ–¯å¼ºåŒ–è¾…åŠ©ï¼‰æ¡†æ¶ï¼Œè¯¥åˆ›æ–°æ¶æ„é€šè¿‡å®ç°æ„å›¾æ¨æ–­ä¸è¾…åŠ©ä»²è£é—´çš„ç«¯åˆ°ç«¯æ¢¯åº¦æµï¼Œå¯¹è´å¶æ–¯æ„å›¾æ¨æ–­å’Œä¸Šä¸‹æ–‡è‡ªé€‚åº”è¾…åŠ©è¿›è¡Œè”åˆå¾®è°ƒã€‚æˆ‘ä»¬çš„æµç¨‹å°†åä½œæ§åˆ¶ç­–ç•¥ä¸ç¯å¢ƒä¸Šä¸‹æ–‡åŠå®Œæ•´ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œæ¡ä»¶åŒ–å¤„ç†ã€‚åˆ†æè¡¨æ˜ï¼šï¼ˆ1ï¼‰æœ€ä¼˜è¾…åŠ©æ°´å¹³åº”éšç›®æ ‡ä¸ç¡®å®šæ€§é™ä½ï¼Œéšç¯å¢ƒçº¦æŸå¼ºåº¦å¢åŠ ï¼›ï¼ˆ2ï¼‰å°†ä¿¡å¿µä¿¡æ¯æ•´åˆåˆ°ç­–ç•¥å­¦ä¹ ä¸­ï¼Œç›¸æ¯”é¡ºåºå¤„ç†æ–¹æ³•å¯è·å¾—äºŒæ¬¡æœŸæœ›é—æ†¾ä¼˜åŠ¿ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰é˜¶æ®µè¯„ä¼°å°†ç®—æ³•ä¸å‰æ²¿æ–¹æ³•ï¼ˆIDAã€DQNï¼‰è¿›è¡Œå¯¹æ¯”ï¼Œé€æ­¥åˆ†ç¦»æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶çš„ä¸åŒæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰äºŒç»´äººæœºäº¤äº’å…‰æ ‡ä»»åŠ¡ä¸­çš„æ ¸å¿ƒäº¤äº’åŠ¨æ€ï¼›ï¼ˆ2ï¼‰æœºæ¢°è‡‚éçº¿æ€§åŠ¨åŠ›å­¦ï¼›ï¼ˆ3ï¼‰ç›®æ ‡æ¨¡ç³Šä¸ç¯å¢ƒçº¦æŸä¸‹çš„ç»¼åˆæ“æ§ã€‚å®éªŒæ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•è¾ƒå‰æ²¿æŠ€æœ¯å–å¾—6.3%çš„æˆåŠŸç‡æå‡å’Œ41%çš„è·¯å¾„æ•ˆç‡æé«˜ï¼Œç›¸è¾ƒäºæ— è¾…åŠ©æ§åˆ¶åˆ™åˆ†åˆ«å®ç°36.3%çš„æˆåŠŸç‡æå‡å’Œ87%çš„è·¯å¾„æ•ˆç‡æ”¹è¿›ã€‚ç ”ç©¶ç»“æœè¯å®ï¼Œé›†æˆä¼˜åŒ–åœ¨å¤æ‚ç›®æ ‡æ¨¡ç³Šåœºæ™¯ä¸­æ•ˆç›Šæœ€ä¸ºæ˜¾è‘—ï¼Œä¸”å¯æ¨å¹¿è‡³éœ€è¦ç›®æ ‡å¯¼å‘è¾…åŠ©çš„å„ç±»æœºå™¨äººé¢†åŸŸï¼Œæ¨åŠ¨äº†è‡ªé€‚åº”å…±äº«è‡ªä¸»æŠ€æœ¯çš„å‰æ²¿å‘å±•ã€‚",
    "url": "https://arxiv.org/abs/2601.23285v1",
    "html_url": "https://arxiv.org/html/2601.23285v1",
    "html_content": "\\DeclareBibliographyAlias\npreprintunpublished\nEnd-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms\nMH Farhadi, Ali Rabiee, Sima Ghafoori,\nAnna Cetera, Andrew Fisher,\nReza Abiri\nUniversity of Rhode Island, Kingston, Rhode Island, USA\nShared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful, while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline processes the full Bayesian goal distribution rather than MAP estimates, conditioning collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear physical dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Results confirmed the advantage of integrated optimization is most pronounced in complex scenarios with goal ambiguity. Our method outperformed sequential approaches by 23% in completion time in scenarios with high goal uncertainty, with success rate improvements of 13.1% in complex multi-target environments. The demonstrated benefits of our integrated approach are generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.\nKeywords:\nShared Autonomy, Human-Robot Interaction (HRI), Assistive Robotics, Collaborative Control, User Agency, Goal Uncertainty, Intent Inference, Belief-Conditioned Policy\n1\nIntroduction\nShared autonomy is a control paradigm in which a human and an automated system jointly operate a device, with the system adapting its assistance to the situation. For example, In assistive robotics for users with motor impairments, the system must infer whether the user intends to grasp a large water glass or a small pill bottle. It may provide minimal assistance during the reaching phase to preserve user agency, then increase support when aligning the gripper for a precision-critical grasp on the small bottle.\nThe core challenge in shared autonomy often comes from the inherent tension between two critical processes: inferring the userâ€™s goal (often a probabilistic inference problem) and determining the appropriate level of assistance (an optimization problem)\n[\nundefr\n]\n. Prior approaches typically addressed this challenge as separate or sequential problems, either using fixed blending ratios regardless of context\n[\nundefe\n]\n, separating goal inference from assistance arbitration\n[\nundefq\n]\n, or solving them with model-based planning that requires known dynamics models\n[\nundefaj\n]\n. Recent belief-conditioned methods such as SARI\n[\nundefs\n]\n, mutual-adaptation\n[\nundefw\n]\n, and IDA\n[\nundefv\n]\nhighlighted the need for an end-to-end integration, however, their findings either relied on Maximum a-posteriori (MAP) goals, binary interventions or failed to adapt optimally to the uncertainty in the userâ€™s goals.\nClassical methods usually first estimate the userâ€™s intent, then control the system using either a single best guess of that intent or a belief-space planner whose inference is fixed and does not adapt. BRACE instead learns assistance as\nÎ³\n=\nf\nÎ¸\nâ€‹\n(\ns\n,\nb\n,\nc\n)\n\\gamma=f_{\\theta}(s,b,c)\nusing the goal belief\nb\nb\n, and it links inference and control end-to-end so that the resulting performance (the downstream gradients) also shapes and updates the belief itself.\nThis work makes the following contributions:\n1.\nFull-belief conditioning:\nAssistance is a function of the entire intent posterior, not the MAP goal, allowing a more nuanced reaction to user uncertainty.\n2.\nEnd-to-end coupling:\nThe intent inference module and assistance arbitration policy are optimized jointly from a single control objective; gradients from task returns reshape the belief to be decision-useful, avoiding the estimatorâ€“controller mismatch of sequential pipelines.\n3.\nProof of advantage over regret bounds:\nWhen different goals require different assistance (because intent is unclear and the environment is constrained), conditioning on belief and jointly optimizing inference and control reduces expected regret quadratically compared with MAP schemes. The benefit grows as constraints tighten and shrinks as belief entropy increases. Thatâ€™s exactly when BRACE helps most.\nRecent belief-aware approaches such as SARI\n[\nundefs\n]\n, Mutual-Adaptation\n[\nundefw\n]\n, IDA\n[\nundefv\n]\n, and STREAMS\n[\nundefab\n]\nhave begun to couple intent inference with assistance. Our work differs from them in three main ways.\n(i)\nBRACE leverages the complete belief distribution\nb\nâˆˆ\nâ„\n|\nG\n|\nb\\in\\mathbb{R}^{|G|}\nrather than a single goal estimate or binary signal, which is critical in scenarios with high goal uncertainty.\n(ii)\nTheir assistance logic is either huristic thresholds or secondary networks trained with the belief module frozen; BRACE performs joint gradient-based optimization of inference and control, giving provably lower regret (Theorem 2).\n(iii)\nNone of the prior systems provide task-agnostic monotonicity guarantees or regret bounds.\nThe paper is organized as follows: Section 2 reviews prior work on intent inference and assistance arbitration. Section 3 introduces the BRACE framework, detailing its methodology, theoretical properties, and system components. Section 4 presents empirical results from multiple end-effector based control experiment scenarios, simulation benchmarks, and ablation studies. Section 5 discusses the findings and concludes with limitations and future work. The Appendices provide supplementary materials, including mathematical proofs, reward function details, and additional analyses.\n2\nRelated Works\n2.1\nPlanning-based Approaches\nJavdani et al.\n[\nundefq\n]\nmodeled the problem as a Partially Observable Markov Decision Process (POMDP) with the humanâ€™s goal as a hidden state. Their hindsight optimization algorithm provided computational tractability by approximating the POMDP solution, enabling assistance even when goal confidence was low. Jain and Argall\n[\nundefo\n]\nfurther extended probabilistic approaches for assistive robotics through recursive Bayesian filtering that fused observations like human behavior with different rationality levels. While these approaches offer principled formulations, they require simplifying assumptions about goal structures and rely on discrete optimization rather than learning adaptive strategies from interaction data.\nIn the planning literature, Monte Carlo Tree Search (MCTS) methods have been effectively applied to scenarios involving uncertainty in user preferences. Aronson et al.\n[\nundefa\n]\nleveraged MCTSâ€™s statistical sampling approach to handle complex decision spaces without requiring exhaustive search. Nikolaidis et al.\n[\nundefw\n]\nadvanced this field by formalizing human-robot mutual adaptation through Bounded-Memory Models embedded in partially observable decision processes, demonstrating performance improvements over one-way adaptive approaches. Complementing these works, Panagopoulos et al.\n[\nundefy\n]\ndeveloped a Bayesian framework for intent recognition in remote navigation that fuses multiple observation sources, further extending probabilistic approaches to human-robot collaboration.\n2.2\nIntention Inference and Reward Shaping\nRecent advances in reinforcement learning have enabled new approaches to shared autonomy that learn assistance policies. Reddy et al.\n[\nundefac\n]\nimplemented deep Q-learning to learn end-to-end assistive policies without explicit goal models, implicitly inferring intent through reward signals during human-in-the-loop training. Xie et al.\n[\nundefaf\n]\nextended this work by developing a probabilistic policy blending approach that combines deep reinforcement learning with explicit uncertainty handling, demonstrating how different arbitration functions can blend human and agent actions for variable assistance levels.\nKnox et al.\n[\nundeft\n]\nexplored the interplay between reinforcement learning and human feedback, developing TAMER (Training an Agent Manually via Evaluative Reinforcement), which provides a framework for shaping agent behavior through human evaluative feedback. While not directly addressing shared autonomy, their work highlights how human input can guide learning systems, informing approaches to human-in-the-loop assistance.\nSchaff and Walter\n[\nundefad\n]\ndeveloped residual policy learning that applies minimal corrections to human inputs to satisfy environmental constraints. This approach ensures humans remain mostly in control, which maintains userâ€™s sense of agency, but lacks a principled way to scale assistance when users struggle significantly. Liu et al.\n[\nundefu\n]\nproposed a complementary approach that blends imitation and reinforcement learning for robust policy improvement, addressing some of these limitations.\nAccurate prediction of human intent forms a critical component of effective shared autonomy systems. Recent approaches to shared autonomy have employed Bayesian inference frameworks for probabilistic goal prediction, which have shown effectiveness in modeling uncertainty over user intentions\n[\nundefp\n,\nundefo\n]\n. However, many current systems, including multimodal approaches, still rely on mapping certainty to assistance through predefined rules or thresholds rather than through learned policies that optimize for overall performance across varying contexts and user capabilities\n[\nundefv\n,\nundefs\n]\n. This separation between intent inference and assistance determination often limits the systemâ€™s adaptability to complex scenarios with high goal uncertainty\n[\nundefac\n]\n. Similar integration challenges exist in other robotic domains,\n[\nundefj\n]\nrecently demonstrated that integrating AI with tactile sensing can significantly improve predictive accuracy by jointly processing multimodal sensory inputs, rather than treating sensing and interpretation as separate stages.\nTable 1\n:\nComparison of BRACE with notable prior works\nFramework\nBelief input to controller\nAssistance policy\nTraining\nValidation\nReddy et al. (2018)\nImplicit (end-to-end)\nDeep Q-learning\nHuman-in-the-loop RL\nLunar Lander, Quadrotor perching\nJavdani et al. (2015)\nFull POMDP belief\nHindsight optimization\nModel-based\nPR2 feeding + grasp tasks\nMcMahan et al. (2024)\nBinary intervene flag\nDiffusion residual\nOff-policy RL\nReacher, Lunar Lander\nOh et al. (2021)\nHeuristic confidence\nPolicy switch\nSupervised\nreach-and-grasp\nBRACE (ours)\nFull belief vector\nContinuous\nÎ³\n\\gamma\nvia actor-critic\nJoint end-to-end\nReacher-2D; Planar cursor control; pick and place\n2.3\nSequential vs. Integrated Approaches\nMost existing systems employ sequential or parallel architectures that separate goal inference from assistance determination. Dragan and Srinivasa\n[\nundefe\n]\nintroduced policy-blending with linear interpolation between human and autonomous control, providing mathematical justification for the blending formulation but employing a fixed blending parameter.\nSeveral actor-critic approaches treat goal probabilities as fixed inputs and learn only the blending policy. The most prominent example is The Disagreement method of\n[\nundefx\n]\n, implementing autonomy switching based on a predefined modality test over von Mises distributions. Our work addresses this gap in two ways: (i) we integrate intent inference within the same unified architecture, allowing for joint optimization of inference and control parameters, and (ii) we provide theoretical regret bounds for the integrated formulation, demonstrating quantifiable advantages over sequential approaches. Recent work by McMahan et al.\n[\nundefv\n]\nintroduced interventional diffusion assistance (IDA) that employs a diffusion model copilot and only intervenes when the expertâ€™s action is deemed superior to the humanâ€™s.\nWe chose IDA and Reddyâ€™s DQN because they are strong, widely used model-free baselines for shared autonomy and thus provide informative tests of BRACE. IDA is a recent method with demonstrated gains in simulation and user studies; it explicitly targets the autonomyâ€“assistance trade-off and serves as a benchmark for learned arbitration. Reddyâ€™s DQN is the standard end-to-end model-free RL baseline in the community; using it tests whether BRACEâ€™s explicit belief conditioning offers benefits beyond an implicit intent learner.\n3\nMethodology\nWe model assistance allocation as a policy over the blend parameter.\nFor state\ns\ns\n, belief\nb\nb\nover goals, and context\nc\nc\n, BRACE outputs\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\n\\gamma\\in[0,1]\n. Fig.\n1\nshows the data flow: a Bayesian module updates\nb\nb\n, while an actorâ€“critic uses\n(\ns\n,\nb\n,\nc\n)\n(s,b,c)\nto set\nÎ³\n\\gamma\n.\nJoint training allows performance gradients to shape both the belief and the arbitration policy.\nFigure 1\n:\nA general formulation of our shared autonomy framework: adaptive blending of human kinematic input (\nK\nu\nâ€‹\ns\nâ€‹\ne\nâ€‹\nr\nK_{user}\n) and expert policyâ€™s kinematic input (\nK\ne\nâ€‹\nx\nâ€‹\np\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nK_{expert}\n) via\nÎ³\n\\gamma\nin Shared Autonomy frameworks.\nThe humanâ€™s goal\ng\nâˆ—\nâˆˆ\nG\ng^{*}\\in G\nis unknown to the system and must be inferred from observed inputs. We maintain a belief state\nb\nt\nâ€‹\n(\ng\n)\nb_{t}(g)\nrepresenting the probability distribution over potential goals at time\nt\nt\n. The key challenge is effectively using this belief distribution to determine the optimal assistance level\nÎ³\n\\gamma\nthat maximizes expected performance across goal uncertainty and environmental context.\nWe define\nÎ³\n\\gamma\nas\nf\nÎ¸\nâ€‹\n(\ns\n,\nb\nt\n,\nc\nâ€‹\n(\ns\n)\n)\nf_{\\theta}(s,b_{t},c(s))\nwhere\ns\ns\nis the current environment state,\nb\nt\nb_{t}\nis the belief over goals,\nc\nâ€‹\n(\ns\n)\nc(s)\nrepresents environmental constraints (e.g., obstacle proximity), and\nÎ¸\n\\theta\nare the parameters of function\nf\nf\n. Recent works\n[\nundefah\n]\nhave similarly explored adaptive reinforcement learning-based control using proximal policy optimization for shared autonomy systems, though without the explicit Bayesian belief integration we propose. This formulation can be viewed as a special case of a Partially Observable Markov Decision Process (POMDP) where the hidden state is the true goal, but we relax assumptions about known dynamics or predefined goal sets that characterized earlier approaches\n[\nundefq\n]\n.\nWe implement our approach using a dual-head neural network architecture that processes state observations with specialized modules for goal inference and assistance policy optimization. As shown in Figure\n2\n, the network consists of two main components: a Bayesian inference module and a policy optimization Actor Critic network.\nFigure 2\n:\nArchitectural overview of our dual-head shared autonomy system. The network integrates a Bayesian goal inference module with a context-adaptive assistance policy learned through actor-critic reinforcement learning. The Bayesian inference module calculates belief states which are passed to the policy module to condition assistance decisions.\nThis architecture enables the policy to utilize the complete belief distributionâ€”including entropy and multi-modal probability structuresâ€”rather than relying solely on MAP estimates. Empirical validation in Section 4.3 confirms this advantage, particularly in high-uncertainty scenarios.\n3.1\nTraining Procedure\nTo leverage the complete belief distribution and enable joint optimization of both inference and control modules, we employ an integrated training procedure. Algorithm\n1\ndetails our approach, which combines Bayesian inference with reinforcement learning through a curriculum-based training strategy.\nAlgorithm 1\nBRACE Training Algorithm\n1:\nInitialize parameters for Bayesian inference module and policy module\n2:\nPretrain Bayesian inference module\n3:\nInitialize training mode (baseline or end-to-end)\n4:\nfor\neach curriculum stage\ndo\n5:\nInitialize environment seed with appropriate difficulty\n6:\nfor\nepisode = 1 to N\ndo\n7:\nSample goal configuration and reset environment\n8:\nInitialize belief state\nb\n0\nb_{0}\nwith uniform prior over goals\n9:\nfor\nt = 0 to T\ndo\n10:\nObserve state\ns\nt\ns_{t}\nand simluated human action\nh\nt\nh_{t}\n11:\nUpdate belief\nb\nt\nb_{t}\nusing Bayesian inference module\n12:\nCompute assistance level\nÎ³\nt\n\\gamma_{t}\nand value estimate\nV\nâ€‹\n(\ns\nt\n)\nV(s_{t})\nfrom policy module\n13:\nExecute action\na\nt\n=\n(\n1\nâˆ’\nÎ³\nt\n)\nâ€‹\nh\nt\n+\nÎ³\nt\nâ€‹\nw\nt\na_{t}=(1-\\gamma_{t})h_{t}+\\gamma_{t}w_{t}\n14:\nObserve reward\nr\nt\nr_{t}\nand next state\ns\nt\n+\n1\ns_{t+1}\n15:\nStore transition\n(\ns\nt\n,\nb\nt\n,\nÎ³\nt\n,\nr\nt\n,\ns\nt\n+\n1\n,\nb\nt\n+\n1\n)\n(s_{t},b_{t},\\gamma_{t},r_{t},s_{t+1},b_{t+1})\n16:\nend\nfor\n17:\nCompute advantages and returns\n18:\nUpdate policy parameters using PPO objective\n19:\nCompute combined supervised and reinforcement loss\n20:\nUpdate inference module parameters through REINFORCE\n21:\nApply training stability mechanisms\n22:\nend\nfor\n23:\nAdvance to next curriculum setting\n24:\nend\nfor\nThe training procedure integrates both the inference and control aspects of our system. Lines 10-15 represent the core interaction loop, where the belief state is updated based on human actions and the assistance policy is executed. Gradients flow from the policy outputs back to the inference module parameters (line 19-20), enabling the Bayesian module to adapt based on downstream control performance. Finally, curriculum learning (lines 4-23) progressively increases task difficulty, improving learning efficiency for complex scenarios.\nUnlike sequential approaches that separate goal inference from assistance arbitration, our joint optimization enables the full belief distribution to influence control decisions while simultaneously adapting inference parameters based on control outcomes.\n3.2\nTheoretical Properties of Assistance Policy\nWe denote\nb\nb\nfor the intent belief,\nc\nc\nfor constraint severity,\nU\ng\nâ€‹\n(\nÎ³\n;\ns\n,\nc\n)\nU_{g}(\\gamma;s,c)\nfor the goal-conditioned utility,\nU\nÂ¯\nâ€‹\n(\nÎ³\n;\ns\n,\nb\n,\nc\n)\n=\nğ”¼\ng\nâˆ¼\nb\nâ€‹\n[\nU\ng\nâ€‹\n(\nÎ³\n;\ns\n,\nc\n)\n]\n\\overline{U}(\\gamma;s,b,c)=\\mathbb{E}_{g\\sim b}[U_{g}(\\gamma;s,c)]\n, and\nÎ³\nâ‹†\nâ€‹\n(\ns\n,\nb\n,\nc\n)\n=\narg\nâ¡\nmax\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\nâ¡\nU\nÂ¯\nâ€‹\n(\nÎ³\n;\ns\n,\nb\n,\nc\n)\n.\n\\gamma^{\\star}(s,b,c)\\;\\;=\\;\\;\\arg\\max_{\\gamma\\in[0,1]}\\overline{U}(\\gamma;s,b,c).\nAssumptions: (A1)\nU\ng\nâ€‹\n(\nÎ³\n)\nU_{g}(\\gamma)\nis twice continuously differentiable and strongly concave in\nÎ³\n\\gamma\nnear\nÎ³\nâ‹†\n\\gamma^{\\star}\n; (A2) mixed partials\nâˆ‚\n2\nU\ng\n/\nâˆ‚\nÎ³\nâ€‹\nâˆ‚\nc\nâ‰¥\n0\n\\partial^{2}U_{g}/\\partial\\gamma\\partial c\\geq 0\n(assistance becomes more valuable as constraints tighten); (A3) we can reparameterize belief concentration by\nÎ»\n\\lambda\nwith\nd\nâ€‹\nH\nâ€‹\n(\nb\nÎ»\n)\n/\nd\nâ€‹\nÎ»\n<\n0\ndH(b_{\\lambda})/d\\lambda<0\n.\nTheorem 1\n(Uncertainty and constraint monotonicity)\n.\nUnder (A1)â€“(A3), the optimal assistance satisfies\nâˆ‚\nÎ³\nâ‹†\nâˆ‚\nH\nâ€‹\n(\nb\n)\n<\n0\nand\nâˆ‚\nÎ³\nâ‹†\nâˆ‚\nc\n>\n0\n.\n\\frac{\\partial\\gamma^{\\star}}{\\partial H(b)}\\;<\\;0\\qquad\\text{and}\\qquad\\frac{\\partial\\gamma^{\\star}}{\\partial c}\\;>\\;0.\nProof sketch.\nLet\nF\nâ€‹\n(\nÎ³\n,\nÎ»\n,\nc\n)\n=\nâˆ‚\nÎ³\nU\nÂ¯\nâ€‹\n(\nÎ³\n;\nb\nÎ»\n,\nc\n)\nF(\\gamma,\\lambda,c)=\\partial_{\\gamma}\\overline{U}(\\gamma;b_{\\lambda},c)\nwith\nF\nâ€‹\n(\nÎ³\nâ‹†\n,\nÎ»\n,\nc\n)\n=\n0\nF(\\gamma^{\\star},\\lambda,c)=0\n. By the implicit function theorem,\nd\nâ€‹\nÎ³\nâ‹†\nd\nâ€‹\nÎ»\n=\nâˆ’\nâˆ‚\nÎ»\nâˆ‚\nÎ³\nU\nÂ¯\nâ€‹\n(\nÎ³\nâ‹†\n;\nb\nÎ»\n,\nc\n)\nâˆ‚\nÎ³\n2\nU\nÂ¯\nâ€‹\n(\nÎ³\nâ‹†\n;\nb\nÎ»\n,\nc\n)\n.\n\\frac{d\\gamma^{\\star}}{d\\lambda}=-\\frac{\\partial_{\\lambda}\\partial_{\\gamma}\\overline{U}(\\gamma^{\\star};b_{\\lambda},c)}{\\partial_{\\gamma}^{2}\\overline{U}(\\gamma^{\\star};b_{\\lambda},c)}.\nThe denominator is negative by strong concavity. Increasing\nÎ»\n\\lambda\nconcentrates\nb\nÎ»\nb_{\\lambda}\n, aligning mass with goalwise optima and increasing\nâˆ‚\nÎ»\nâˆ‚\nÎ³\nU\nÂ¯\n>\n0\n\\partial_{\\lambda}\\partial_{\\gamma}\\overline{U}>0\n, hence\nd\nâ€‹\nÎ³\nâ‹†\n/\nd\nâ€‹\nÎ»\n>\n0\nd\\gamma^{\\star}/d\\lambda>0\nand\nd\nâ€‹\nÎ³\nâ‹†\n/\nd\nâ€‹\nH\n<\n0\nd\\gamma^{\\star}/dH<0\n. For\nc\nc\n, monotone comparative statics with\nâˆ‚\n2\nU\ng\n/\nâˆ‚\nÎ³\nâ€‹\nâˆ‚\nc\nâ‰¥\n0\n\\partial^{2}U_{g}/\\partial\\gamma\\partial c\\geq 0\nyields\nâˆ‚\nÎ³\nâ‹†\n/\nâˆ‚\nc\n>\n0\n\\partial\\gamma^{\\star}/\\partial c>0\n. Full details in App.Â A.\nâ–¡\n\\square\nTheorem 2\n(Integrated optimization advantage)\n.\nLet\nÎ³\ng\nâ‹†\n=\narg\nâ¡\nmax\nÎ³\nâ¡\nU\ng\nâ€‹\n(\nÎ³\n;\ns\n,\nc\n)\n\\gamma_{g}^{\\star}=\\arg\\max_{\\gamma}U_{g}(\\gamma;s,c)\n. For a sequential scheme that chooses\ng\n^\n=\narg\nâ¡\nmax\nâ¡\nb\n\\hat{g}=\\arg\\max b\nand uses\nÎ³\ng\n^\nâ‹†\n\\gamma_{\\hat{g}}^{\\star}\n, the expected regret gap versus belief-conditioned\nÎ³\nâ‹†\n\\gamma^{\\star}\nsatisfies\nğ”¼\ng\nâˆ¼\nb\nâ€‹\n[\nU\ng\nâ€‹\n(\nÎ³\ng\nâ‹†\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\nâ‹†\n)\n]\n=\n1\n2\nâ€‹\nğ”¼\ng\nâˆ¼\nb\nâ€‹\n[\n|\nU\ng\nâ€²â€²\nâ€‹\n(\nÎ³\n~\n)\n|\nâ€‹\n(\nÎ³\ng\nâ‹†\nâˆ’\nÎ³\nâ‹†\n)\n2\n]\n,\n\\mathbb{E}_{g\\sim b}\\!\\big[U_{g}(\\gamma_{g}^{\\star})-U_{g}(\\gamma^{\\star})\\big]\\;=\\;\\tfrac{1}{2}\\,\\mathbb{E}_{g\\sim b}\\!\\big[\\,|U_{g}^{\\prime\\prime}(\\tilde{\\gamma})|\\,(\\gamma_{g}^{\\star}-\\gamma^{\\star})^{2}\\big],\nfor some\nÎ³\n~\n\\tilde{\\gamma}\nbetween\nÎ³\nâ‹†\n\\gamma^{\\star}\nand\nÎ³\ng\nâ‹†\n\\gamma_{g}^{\\star}\n. The gap vanishes if all goals share the same optimal assistance\nÎ³\ng\nâ‹†\n\\gamma_{g}^{\\star}\n.\nProof sketch.\nSecond-order Taylor expansion of\nU\ng\nâ€‹\n(\nÎ³\n)\nU_{g}(\\gamma)\naround\nÎ³\nâ‹†\n\\gamma^{\\star}\n(or\nÎ³\ng\nâ‹†\n\\gamma_{g}^{\\star}\n) gives\nU\ng\nâ€‹\n(\nÎ³\ng\nâ‹†\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\nâ‹†\n)\n=\n1\n2\nâ€‹\n|\nU\ng\nâ€²â€²\nâ€‹\n(\nÎ³\n~\n)\n|\nâ€‹\n(\nÎ³\ng\nâ‹†\nâˆ’\nÎ³\nâ‹†\n)\n2\n.\nU_{g}(\\gamma_{g}^{\\star})-U_{g}(\\gamma^{\\star})=\\tfrac{1}{2}|U_{g}^{\\prime\\prime}(\\tilde{\\gamma})|\\,(\\gamma_{g}^{\\star}-\\gamma^{\\star})^{2}.\nTaking\nğ”¼\ng\nâˆ¼\nb\n\\mathbb{E}_{g\\sim b}\nyields the stated identity, showing a\nquadratic\nadvantage that grows with dispersion of\n{\nÎ³\ng\nâ‹†\n}\n\\{\\gamma_{g}^{\\star}\\}\nand curvature\n|\nU\ng\nâ€²â€²\n|\n|U_{g}^{\\prime\\prime}|\n. If\nÎ³\ng\nâ‹†\n\\gamma_{g}^{\\star}\nis constant in\ng\ng\n, the term is zero. Full proof and regularity conditions in App.Â A.\nâ–¡\n\\square\nClassical Bayesian adaptive control treats inference as an external estimator and optimizes control on a point estimate or a fixed belief update. Here intent inference is jointly optimized with control; Theorems above formalize the resulting uncertainty/constraint monotonicity and the quadratic regret gap. AppendixÂ A contains extended proofs for the theorems.\n3.3\nBayesian Inference Module\nThe Bayesian inference module implements recursive Bayesian filtering to maintain and update a probability distribution over potential goals. For a set of potential goals\nG\n=\n{\ng\n1\n,\ng\n2\n,\nâ€¦\n,\ng\nN\n}\nG=\\{g_{1},g_{2},...,g_{N}\\}\n, the model updates the posterior probability of each goal\ng\ni\ng_{i}\ngiven the trajectory of agent positions\nX\n=\n{\nx\n1\n,\nx\n2\n,\nâ€¦\n,\nx\nt\n}\nX=\\{x_{1},x_{2},...,x_{t}\\}\nand human inputs\nH\n=\n{\nh\n1\n,\nh\n2\n,\nâ€¦\n,\nh\nt\n}\nH=\\{h_{1},h_{2},...,h_{t}\\}\n:\nP\nâ€‹\n(\ng\ni\n|\nX\n,\nH\n)\nâˆ\nP\nâ€‹\n(\ng\ni\n)\nâ€‹\nâˆ\nj\n=\n1\nt\nP\nâ€‹\n(\nh\nj\n|\nx\nj\n,\ng\ni\n)\nP(g_{i}|X,H)\\propto P(g_{i})\\prod_{j=1}^{t}P(h_{j}|x_{j},g_{i})\n(1)\nThe likelihood function\nP\nâ€‹\n(\nh\nj\n|\nx\nj\n,\ng\ni\n)\nP(h_{j}|x_{j},g_{i})\nmodels the human as a noisy-rational agent, with higher probabilities assigned to inputs that move the agent toward the goal:\nP\nâ€‹\n(\nh\nj\n|\nx\nj\n,\ng\ni\n)\nâˆ\nexp\nâ¡\n(\nâˆ’\nÎ²\nâ‹…\ncost\nâ€‹\n(\nh\nj\n|\nx\nj\n,\ng\ni\n)\n)\nP(h_{j}|x_{j},g_{i})\\propto\\exp(-\\beta\\cdot\\text{cost}(h_{j}|x_{j},g_{i}))\n(2)\nwhere\ncost\nâ€‹\n(\nh\nj\n|\nx\nj\n,\ng\ni\n)\n\\text{cost}(h_{j}|x_{j},g_{i})\nmeasures the deviation of the humanâ€™s input from the optimal action for reaching goal\ng\ni\ng_{i}\nfrom position\nx\nj\nx_{j}\n, and\nÎ²\n\\beta\nis a rationality parameter determining how closely the human follows the optimal policy. The cost function combines angular deviation and distance deviation as detailed in the supplementary materials.\nThe angular deviation\nÎ¸\nd\nâ€‹\ne\nâ€‹\nv\n\\theta_{dev}\nin our cost function inside the Bayesian module is calculated as the absolute angle between the human input vector\nh\nâ†’\nj\n\\vec{h}_{j}\nand the optimal direction vector from position\nx\nj\nx_{j}\nto goal\ng\ni\ng_{i}\n:\nÎ¸\nd\nâ€‹\ne\nâ€‹\nv\n=\n|\narccos\nâ¡\n(\nh\nâ†’\nj\nâ‹…\nv\nâ†’\nx\nj\nâ†’\ng\ni\n|\nh\nâ†’\nj\n|\nâ‹…\n|\nv\nâ†’\nx\nj\nâ†’\ng\ni\n|\n)\n|\n\\theta_{dev}=\\left|\\arccos\\left(\\frac{\\vec{h}_{j}\\cdot\\vec{v}_{x_{j}\\rightarrow g_{i}}}{|\\vec{h}_{j}|\\cdot|\\vec{v}_{x_{j}\\rightarrow g_{i}}|}\\right)\\right|\n(3)\nwhere\nv\nâ†’\nx\nj\nâ†’\ng\ni\n\\vec{v}_{x_{j}\\rightarrow g_{i}}\nis the unit vector pointing from position\nx\nj\nx_{j}\nto goal\ng\ni\ng_{i}\n.\nThe distance deviation\nd\nd\nâ€‹\ne\nâ€‹\nv\nd_{dev}\nmeasures input magnitude deviation from optimal magnitude:\nd\nd\nâ€‹\ne\nâ€‹\nv\n=\n|\n1\nâˆ’\n|\nh\nâ†’\nj\n|\nh\no\nâ€‹\np\nâ€‹\nt\n|\nd_{dev}=\\left|1-\\frac{|\\vec{h}_{j}|}{h_{opt}}\\right|\n(4)\nwhere\nh\no\nâ€‹\np\nâ€‹\nt\nh_{opt}\nis the optimal input magnitude given the distance to the goal, modeled as a function that decreases as the agent approaches the goal.\nThe cost function is a weighted combination of these deviations:\ncost\nâ€‹\n(\nh\nj\n|\nx\nj\n,\ng\ni\n)\n=\nw\nÎ¸\nâ‹…\nÎ¸\nd\nâ€‹\ne\nâ€‹\nv\n+\nw\nd\nâ‹…\nd\nd\nâ€‹\ne\nâ€‹\nv\n\\text{cost}(h_{j}|x_{j},g_{i})=w_{\\theta}\\cdot\\theta_{dev}+w_{d}\\cdot d_{dev}\n(5)\nwhere\nw\nÎ¸\n=\n0.7\nw_{\\theta}=0.7\nand\nw\nd\n=\n0.3\nw_{d}=0.3\nare weighting parameters.\nThe Bayesian goal inference model was pre-calibrated using a dataset of 1968 cursor trajectories with known goal labels. We employed two-stage parameter estimation: a coarse grid search followed by Bayesian optimization. Optimal parameters were determined as:\nÎ²\n=\n10.0\n\\beta=10.0\n,\nw\nÎ¸\n=\n0.7\nw_{\\theta}=0.7\n,\nw\nd\n=\n0.3\nw_{d}=0.3\n, with uniform priors over goals. To enhance robustness, we implemented temporal smoothing (goal probabilities smoothed using exponential moving average with decay rate\nÎ±\n=\n0.85\n\\alpha=0.85\n) and confidence calibration (isotonic regression on a validation set).\nThe warm started Bayesian model achieved 74.6% accuracy at 25% path completion, 91.2% accuracy at 50% path completion, and 98.7% accuracy at 75% path completion on a held-out test set of 194 trajectories.\n3.4\nAssistance arbitration policy\nThe actor-critic network consists of two 256-neuron hidden layers using ReLU activationsâ€”an actor network that outputs the continuous blending parameter and a parallel critic network for value estimation. This structure processes a 10-dimensional vector comprising state, human input, goal positions, and contextual distances, enabling context-sensitive assistance modulation. The policy is trained using Proximal Policy Optimization (PPO)\n[\nundefae\n]\nto maximize a reward function that balances several key components:\nR\n=\nw\nc\nâ€‹\nR\nc\n+\nw\np\nâ€‹\nR\np\n+\nw\ng\nâ€‹\nR\ng\n+\nw\na\nâ€‹\nR\na\n+\nw\ng\nâ€‹\nl\nâ€‹\nR\ng\nâ€‹\nl\nR=w_{c}R_{c}+w_{p}R_{p}+w_{g}R_{g}+w_{a}R_{a}+w_{gl}R_{gl}\n(6)\nwhere each term represents a specific aspect of the shared control objective. Reward weights and network hyper-parameters are tuned via random search; The complete reward function with all terms and weights is detailed in Appendix\nB\n.\nFor assistance optimization, our PPO component includes an input layer combining features from the observation space and current goal probability distribution. The architecture uses two hidden layers with 256 neurons each and ReLU activation, followed by an output layer with a single neuron with tanh activation, mapping to\n[\nâˆ’\n1\n,\n1\n]\n[-1,1]\n(rescaled to\n[\n0\n,\n1\n]\n[0,1]\nto represent\nÎ³\n\\gamma\n). A separate value network with identical structure outputs a value estimate, The architecture of this PPO module is shown in Fig.\n3\nFigure 3\n:\nArchitecture of the assistance arbitration PPO. The policy module consists of an Actor Head generating the blending parameter\nÎ³\n\\gamma\nand a Critic Head estimating state value. Both share a Feature Fusion Block that processes combined state and belief information.\nKey hyperparameters include learning rate of\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\nwith cosine annealing, batch size of 1024 samples, 4 epochs per batch, discount factor of 0.99, GAE parameter of 0.95, and clip ratio of 0.2.\n3.5\nExpert Policy\nOur BRACE framework relies on an expert policy that acts as an oracle and provides perfect-path guidance to the assistance module. While the assistance arbitration module learns to blend human and AI control, the expert policy focuses on optimal goal-directed trajectories regardless of human input.\nFor the Planar cursor control task, the expert architecture consisted of a 4-layer network with [256, 256, 128, 64] neurons using ReLU activations and layer normalization. We sampled 128 configurations across the hyperparameter space, optimizing learning rate (\n3.2\nÃ—\n10\nâˆ’\n4\n3.2\\times 10^{-4}\n), entropy coefficient (0.01), GAE parameter (0.92), and discount factor (0.99). To ensure reproducible optimization, we employed fixed seeds and evaluated each candidate configuration across 10 randomized environments, selecting the configuration that achieved the highest mean reward with lowest variance. As shown in Table\n2\n, BRACE maintained consistently high performance (within 8% of optimal) even when paired with experts whose performance degraded by up to 37%.\nThe expertâ€™s reward function balances goal progress, smoothness, and safety through:\nR\ne\nâ€‹\nx\nâ€‹\np\nâ€‹\ne\nâ€‹\nr\nâ€‹\nt\nâ€‹\n(\ns\n,\na\n)\n=\n3.0\nâ‹…\nd\nt\nâˆ’\n1\nâˆ’\nd\nt\nd\nm\nâ€‹\na\nâ€‹\nx\nâˆ’\n0.8\nâ‹…\nâ€–\nÎ”\nâ€‹\nÎ¸\nâ€–\n2\nâˆ’\n2.5\nâ‹…\nexp\nâ¡\n(\nâˆ’\nmin\no\nâˆˆ\nğ’ª\nâ¡\nâ€–\ns\nâˆ’\no\nâ€–\nd\ns\nâ€‹\na\nâ€‹\nf\nâ€‹\ne\n)\nR_{expert}(s,a)=3.0\\cdot\\frac{d_{t-1}-d_{t}}{d_{max}}-0.8\\cdot\\|\\Delta\\theta\\|^{2}-2.5\\cdot\\exp\\left(-\\frac{\\min_{o\\in\\mathcal{O}}\\|s-o\\|}{d_{safe}}\\right)\n(7)\nwhere\nd\nt\nd_{t}\nis distance to the nearest goal,\nâ€–\nÎ”\nâ€‹\nÎ¸\nâ€–\n2\n\\|\\Delta\\theta\\|^{2}\npenalizes angular acceleration for smooth trajectories, and the exponential term creates repulsive force around obstacles\nğ’ª\n\\mathcal{O}\n. This formulation ensures optimal path generation while maintaining safety.\nDuring online operation, the expertâ€™s policy outputs represent the optimal action at each state, providing a perfect benchmark against which human inputs can be compared. The quality of this expert policy directly impacts BRACEâ€™s ability to provide appropriate assistance, particularly in high-gamma scenarios where the system relies heavily on the expertâ€™s guidance.\nTo demonstrate that BRACE genuinely adapts to user input rather than simply copying the expert policy, we conducted a series of controlled experiments using a hindsight optimization expert\n[\nundefq\n]\nas a benchmark. We varied this expertâ€™s optimality by introducing artificial constraints that limited its planning horizon and adding simulated delays to its reactions, creating a spectrum of expert policies ranging from optimal to significantly sub-optimal. Our results revealed that BRACE maintained consistently high performance (within 8% of optimal) even when paired with experts whose performance degraded by up to 37%. This resilience indicates true adaptation rather than imitation, as a simple copying approach would show proportional performance degradation.\nTable 2\n:\nBRACE Performance with Degraded Expert Policies. The Expert Performance column shows the standalone success rate of expert policies that were artificially constrained with limited planning horizons or simulated delays. BRACE Performance measures the final systemâ€™s success rate when paired with that specific expert.\nExpert Policy\nExpert Performance\nBRACE Performance\nPerformance Delta\nFull\n100%\n98.2%\n-1.8%\nHorizon-Limited\n82.4%\n94.1%\n+11.7%\nDelayed\n73.6%\n92.8%\n+19.2%\nRandom-Perturbed\n63.1%\n90.5%\n+27.4%\n4\nResults\nWe designed a three-part evaluation to validate BRACE against various challenges of human-in-the-loop robotic end-effector control, with each experiment isolating a different aspect of the end-effector assistance problem.\n(1) The Human-Interaction Challenge (2D Cursor Control): First, we use a simulated 2D cursor-control task as a direct analogy for 2D end-effector positioning (e.g., in screen-based assistive tasks or simplified pick-and-place). This experimentâ€™s primary purpose is to isolate and test the core human-robot interaction dynamics. It validates BRACE against the noisy, variable, and unpredictable trajectories of real human users, which is especially critical for modeling assistive control from imperfect biosignals.\n(2) The Physical-Dynamics Challenge (Reacher-2D): Then, we advance from simple point-mass physics to address the challenge of more complex physical dynamics. A real-world end-effector is part of a multi-joint kinematic chain. The Reacher-2D benchmark allows us to isolate this challenge, proving that BRACEâ€™s arbitration framework is robust enough to handle the non-linear, coupled dynamics common in robotic arm control.\n(3) The Integrated 3D Task-Context Challenge (Fetch Pick-and-Place): Finally, we integrate these challenges into a high-dimensional manipulation task. A modified FetchPickAndPlace-v3 environment serves as our testbed for scaling BRACE to an end-effector operating in 3D space. Its purpose is to validate that our framework can jointly reason over the two most critical components of a realistic assistive task: high goal ambiguity (e.g., multiple similar target bins) and localized safety demands (e.g., navigating nearby obstacles during grasp and release).\n4.1\nPlanar Human-in-the-loop Cursor Control\nOur experiment employed a within-subjects design (\nN\n=\n12\nN=12\n) where participants performed a goal-directed cursor control task using a DualSense controller. This study compared five different assistance conditions. All participants were required to complete informed consent forms prior to participation, screened for prior neurological diseases that could affect motor performance, and were compensated with a gift card. The study protocol received full approval from our Institutional Review Board (IRB) prior to participant recruitment and data collection. Participants completed all conditions to enable direct comparison of performance metrics across different assistance modes. An a priori power analysis using G*Power (v3.1) with parameters\nÎ±\n=\n0.05\n\\alpha=0.05\n, power\n(\n1\nâˆ’\nÎ²\n)\n=\n0.8\n(1-\\beta)=0.8\n, and an expected effect size of\nf\n=\n0.35\nf=0.35\n(medium-large) based on pilot testing indicated a minimum sample of 10 participants would be sufficient to detect significant differences.\nFigure 4\n:\nStructure of the human-in-the-loop cursor control user study, showing the distribution of trials across three test environments and five assistance conditions.\nTo create controlled testing environments, we first implemented a procedural environment generation approach, then chose three fixed random seeds that produced consistent scenarios across all participants and conditions. In each environment, goals were placed randomly within margins while maintaining minimum separation. Obstacles were strategically positioned along paths between the start position and goals with perpendicular offsets. This approach created three distinct environments that systematically varied in goal ambiguity (multiple targets in similar directions), constraint severity (narrow passages between obstacles), and path complexity (indirect routes to targets). Each participant completed 24 trials per environment (72 total) across all five assistance conditions. This trial count was determined through Monte Carlo power analysis to achieve 95% confidence at 80% power while minimizing fatigue, as individual trials lasted 5-15 seconds.\nWe used a balanced Latin square design to counterbalance condition and environment presentation order, minimizing potential sequence effects. Performance was measured through success rate, completion time, path efficiency, and throughput. Statistical analysis revealed strong effect sizes with 95% confidence intervals across all primary metrics: success rate (\nd\n=\n1.28\nd=1.28\n, 95% CI [0.94, 1.62]), completion time (\nd\n=\n1.41\nd=1.41\n, 95% CI [1.07, 1.75]), and path efficiency (\nd\n=\n1.35\nd=1.35\n, 95% CI [1.01, 1.69]). These values indicate large practical significance in the improvements conferred by our approach. To account for individual differences, we employed stratified sampling across participant skill levels and conducted subgroup analyses. While inter-subject variance was noticeable, particularly in the manual condition, the improvement pattern remained consistent across all skill quartiles, with even the lowest-performing participants showing significant benefits from context-adaptive assistance (12.3% improvement,\np\n<\n0.01\np<0.01\n).\nParticipants completed target acquisition tasks under five control conditions across three environments with varied obstacle and goal configurations, performing 24 trials per environment for a total of 4,320 collected trajectories. We compared five control conditions: manual control (no assistance,\nÎ³\n=\n0\n\\gamma=0\n), Reddy et al.â€™s DQN approach\n[\nundefac\n]\n, McMahan et al â€™s IDA approach\n[\nundefv\n]\n, user-controlled gamma (where participants manually adjusted assistance level), and our approach BRACE, as illustrated in Figure\n4\n. Performance was evaluated using success rate, completion time, path efficiency, and throughput.\n4.1.1\nQuantitative Performance Results\nTable\n3\nsummarizes the key performance metrics across all conditions.\nBRACE outperformed all baseline methods across all metrics. Compared to manual control, BRACE achieved 36.3% higher success rate (F(4,48) = 27.3, p Â¡ 0.001) and 60.9% reduction in completion time (F(4,48) = 19.8, p Â¡ 0.001). BRACE improved over state-of-the-art with 6% higher success rate, 42% better path efficiency and 32% faster completion times. Even compared to user-controlled assistance levels, which performed well due to explicit human modulation of assistance levels, our automatic approach achieved 8.5% better path efficiency and 26% faster completion times.\nFigure\n5\nconfirms our theoretical prediction that optimal assistance varies with environmental context and goal certainty. High assistance values (yellow regions,\nÎ³\n>\n0.8\n\\gamma>0.8\n) appear near obstacles and in narrow passages, while low assistance (blue,\nÎ³\n<\n0.3\n\\gamma<0.3\n) occurs in open spaces and areas with goal ambiguity. Temporal analysis shows that assistance starts at 0.28\nÂ±\n\\pm\n0.12 during trajectory initiation when uncertainty is highest, then increases to 0.74\nÂ±\n\\pm\n0.09 as goal inference confidence improves. This dynamic modulation provides advantages over both DQN approaches that use embedded policies and binary intervention methods like IDA, while achieving better consistency than manual user-controlled assistance.\nTable 3\n:\nPerformance metrics across control conditions (Mean\nÂ±\n\\pm\n(SD across participants))\nNo assist\nDQN\nBRACE\nIDA\nManual gamma level\nSuccess (%)\n72.1\nÂ±\n\\pm\n3.2\n89.8\nÂ±\n\\pm\n2.5\n98.3\nÂ±\n\\pm\n1.7\n92.5\nÂ±\n\\pm\n2.4\n86.8\nÂ±\n\\pm\n1.9\nTime (s)\n8.44\nÂ±\n\\pm\n0.31\n5.62\nÂ±\n\\pm\n0.26\n3.30\nÂ±\n\\pm\n0.22\n5.07\nÂ±\n\\pm\n0.27\n4.5\nÂ±\n\\pm\n0.23\nPath Eff.\n0.43\nÂ±\n\\pm\n0.05\n0.75\nÂ±\n\\pm\n0.05\n0.89\nÂ±\n\\pm\n0.06\n0.63\nÂ±\n\\pm\n0.06\n0.82\nÂ±\n\\pm\n0.06\nThroughput\n1.14\nÂ±\n\\pm\n0.10\n1.2\nÂ±\n\\pm\n0.10\n1.27\nÂ±\n\\pm\n0.08\n1.19\nÂ±\n\\pm\n0.10\n1.25\nÂ±\n\\pm\n0.09\n(a)\nManual gamma control\n(b)\nBRACE adaptive control\nFigure 5\n:\nComparison between manual gamma (left) and BRACE (right). Color scale shows assistance level from 0 (purple, human control) to 1 (red, AI control). Green circles are targets, gray circles are obstacles. BRACE dynamically adjusts assistance based on context and goal certainty.\n4.1.2\nQualitative Performance Results\nPost-experiment user feedback, collected via NASA-TLX surveys (visualized in Fig.\n6\n), revealed high satisfaction, with BRACE earning the strongest overall subjective profile across ease of use, perceived assistance quality, flexibility, and confidence. This pattern matches the mechanism observed quantitatively: assistance never overrides user input when intent inference confidence is low, and it rises gradually as the intention becomes clearer or environmental constraints increase (\nÎ³\nâ‰ˆ\n\\gamma\\approx\n0.28 to 0.74 over a trial). This behavior ensures user success and reduces cognitive workload without taking over user agency.\nWhile a few participants expressed a preference for unassisted control (a familiar HRI trade-off where manual control feels more predictable\n[\nundefc\n,\nundeff\n]\n) satisfaction with BRACE was notably high. This was largely attributed to measures implemented to enhance transparency. To show the inner workings of BRACE and build user trust, the interface provided real-time feedback by highlighting the active goal perceived by the system, shadowing the recorded trajectory and user history, and showing the real-time amount of assistance provided. These features were designed to enhance transparency without overwhelming the user with extra information.\n(a)\n(b)\nFigure 6\n:\nUser interaction qualitative evaluation on the planar cursor control task. Comparing BRACEâ€™s performance compared to four alternative control scenarios. (a) subjective ratings across seven dimensions (higher is better) for five modes: Unassisted, BRACE, IDA, DQN, and Manual-\nÎ³\n\\gamma\n. over N=12 participants (within-subject). * pÂ¡0.05, ** pÂ¡0.01 using a one-way repeated-measures ANOVA with Bonferroniâ€“corrected pairwise t-tests, (b) Average profile of the same ratings by mode (higher is better; Effort reverse-coded).\nWe also see why IDA scores competitively on â€œconfidenceâ€. By design, IDA intervenes only when the expert action is judged superior to the humanâ€™s, which biases the experience toward safety and collision avoidance. Users read this conservatism as reliability, though it can trade off flexibility and speed. In contrast, BRACE blends continuously with both the expert and the user input rather than switching between them, yielding better end-to-end task metrics while maintaining high subjective ratingsâ€”especially in ambiguous, constrained layouts where its belief-conditioned policy has a theoretical and empirical advantage.\n4.2\nReacher-2D\nWe also evaluated BRACE in the Reacher 2D environment, a physics-based robotic arm control task where a two-joint arm must reach target positions. To isolate and test the frameworkâ€™s robustness to non-linear dynamics and high-frequency control noise, we modified the environment to include three uniformly distributed goals. To ensure the task required non-trivial path planning, a cylindrical obstacle was placed along the direct line from the start position to each of the three goals.\nFor the expert policy, we trained a SAC network, similar to the architecture used in\n[\nundefv\n]\n. For a fair comparison, the IDA baseline was provided with the same SAC-trained expert policy as BRACE. Its intervention threshold was optimized via grid search on 20% of the trials to maximize its goals-per-minute, ensuring a robust baseline. Our framework performed arbitration over the control allocation between the simulated human pilot and the expert policy.\n(a)\nReacher-2D environment\n(b)\nPerformance across control methods.\nFigure 7\n:\nEvaluation on the Reacher-2D benchmark (left). Experimental results (right) Comparing BRACEâ€™s performance compared to three alternative control scenarios.\nTo simulate a control signal with significant execution noise (rather than planning-level ambiguity), the pilotâ€™s actions were generated from the expert policy with 30% random Gaussian noise injected at each timestep. This setup is not designed to model human goal ambiguity, but rather to create a high-disturbance scenario to evaluate the stability and smoothing capabilities of the arbitration policies.\nWe conducted 120 trials per condition (determined using G*Power analysis with\nÎ±\n=\n0.05\n\\alpha=0.05\nand power\n=\n0.8\n=0.8\n). BRACE outperformed both the unassisted pilot and IDA across all test conditions, with particularly significant improvements for noisy pilots (106% improvement over pilot-only, 17.9% over IDA,\np\n<\n0.001\np<0.001\n)[cite: 412].\nFigure 7b quantifies BRACEâ€™s performance advantage in the Reacher-2D environment, achieving 4.8 goals/minute compared to 3.7 for IDA and 2.4 for the unassisted noisy pilot\n(\nF\n(\n2\n,\n357\n)\n=\n18.2\n,\n(F(2,357)=18.2,\np\n<\n0.001\n)\n<0.001)\n. This 29.7% boost over IDA stems from BRACEâ€™s continuous blending mechanism, which is more effective at smoothing high-frequency noise than binary policies. BRACE also exhibits significantly lower performance variance (\nÏƒ\n2\n=\n0.46\n\\sigma^{2}=0.46\nvs.\nÏƒ\n2\n=\n0.89\n\\sigma^{2}=0.89\nfor IDA) due to the blending mechanism maintaining consistency across environmental conditions. This suggests that the continuous arbitration policy leads to more stable and predictable control, whereas binary switching might struggle to find a consistent policy when faced with a noisy input signal. While this experiment does not test the goal-ambiguity claims of Theorem 2, it confirms the frameworkâ€™s robustness in handling physical dynamics under noisy control conditions.\n4.3\nPick and Place with a Robotic Arm\nTo validate our frameworkâ€™s performance on a high-dimensional robotic task that combines the challenges of goal ambiguity and environmental constraints, we evaluated it in a modified version of the FetchPickAndPlace-v3 environment to move beyond 2D benchmarks to more realistic assistive manipulation scenarios.\n[\nundefaa\n]\nWe configured the environment with one graspable cube, three visually similar target bins, and obstacles constraining the path (creating localized safety demands). At the start of each episode, one bin is randomly sampled as the hidden target. This setup is designed to create a dual challenge that directly tests the paperâ€™s central hypothesis. The visually similar, randomly selected target bins induce high goal ambiguity, forcing the system to rely on Bayesian intent inference rather than simple visual cues. Simultaneously, the obstacles create localized safety demands, requiring the policy to be context-aware. This forces the framework to prove it can jointly reason over both goal uncertainty and constraint severity.\nControl is managed via Cartesian end-effector velocity. We compared the performance of our framework against two key baselines: IDA\n[\nundefv\n]\nand DQN\n[\nundefac\n]\n. The results are summarized in Table 4 and visualized in Figure 8.\nBRACE demonstrated superior performance, achieving the highest success rate (86% Â± 2.2%), the fastest completion time (9.8s Â± 0.6s), and the fewest collisions (0.22 Â± 0.04). In comparison, DQN was less successful (74% Â± 2.6%) and slower (12.3s Â± 0.7s), while IDA struggled significantly with the taskâ€™s ambiguity, resulting in a much lower success rate (68% Â± 2.9%) and longer task time (14.7s Â± 0.8s).\nFigure 8 visually explains this advantage. The trajectory overlays (d-f) show that BRACEâ€™s paths are smoother and more stable, which is a direct result of its belief-conditioned policy. Assistance dynamically peaks during high-constraint phases like Grasp and Release (Figure 8(b)). The system also intelligently adapts to belief entropy (Figure 8(c)): it provides minimal assistance at the start when the goal is unknown (high entropy) and delivers confident, precise help as the userâ€™s intent becomes clear (low entropy). This belief-conditioned blending allows for smooth, decisive guidance, preventing the hesitant trajectories seen with IDAâ€™s binary interventions and the aberrant movements resulting from DQNâ€™s implicit policy.\nTable 4\n:\nFetch Pick & Place (tri-bin, obstacles). Mean\nÂ±\n\\pm\nSEM with 95% CIs in brackets. CIs computed as\nmean\nÂ±\n1.96\nÃ—\nSEM\n\\text{mean}\\pm 1.96\\times\\text{SEM}\n. Lower is better for time, collisions, and error.\nMethod\nSuccess (%)\nTime to\nplace (s)\nCollisions\nPlacement\nerror (cm)\nIDA\n68\nÂ±\n2.9\nâ€‹\n[\n62.3\n,\n73.7\n]\n68\\pm 2.9\\;[62.3,\\;73.7]\n14.7\nÂ±\n0.8\nâ€‹\n[\n13.1\n,\n16.3\n]\n14.7\\pm 0.8\\;[13.1,\\;16.3]\n0.58\nÂ±\n0.07\nâ€‹\n[\n0.44\n,\n0.72\n]\n0.58\\pm 0.07\\;[0.44,\\;0.72]\n2.50\nÂ±\n0.20\nâ€‹\n[\n2.11\n,\n2.89\n]\n2.50\\pm 0.20\\;[2.11,\\;2.89]\nDQN\n74\nÂ±\n2.6\nâ€‹\n[\n68.9\n,\n79.1\n]\n74\\pm 2.6\\;[68.9,\\;79.1]\n12.3\nÂ±\n0.7\nâ€‹\n[\n10.9\n,\n13.7\n]\n12.3\\pm 0.7\\;[10.9,\\;13.7]\n0.41\nÂ±\n0.06\nâ€‹\n[\n0.29\n,\n0.53\n]\n0.41\\pm 0.06\\;[0.29,\\;0.53]\n1.90\nÂ±\n0.16\nâ€‹\n[\n1.58\n,\n2.22\n]\n1.90\\pm 0.16\\;[1.58,\\;2.22]\nBRACE (full belief & context)\n86\nÂ±\n\\pm\n2.2\n[81.7,â€„90.3]\n9.8\nÂ±\n\\pm\n0.6\n[8.6,â€„11.0]\n0.22\nÂ±\n\\pm\n0.04\n[0.14,â€„0.30]\n1.30\nÂ±\n\\pm\n0.12\n[1.07,â€„1.54]\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 8\n:\nBRACE evaluation in the Fetch Pick & Place task, demonstrating adaptive assistance based on goal uncertainty and environmental context. (a) The experimental environment, featuring a robotic arm, three potential target bins to induce goal ambiguity, and obstacles to create localized safety demands. (b) Assistance dynamically peaks during critical, constrained phases Grasp and during completing the task. (c) Belief entropy over the trajectory. Uncertainty is high at the start and drops sharply as the userâ€™s intent becomes clear, allowing assistance to confidently engage. (d-f) Trajectory path overlays for IDA (d), DQN (e), and BRACE (f). BRACEâ€™s assistance policy (f) produces visibly smoother, more direct paths with less hesitation and fewer aberrant movement patterns, which translates to the faster and safer task completion.\n4.4\nAblation Studies\n4.4.1\nIs joint optimization necessary?\nDuring the training process of BRACE we observed that if intent inference was trained in isolation merely to fit trajectories, its beliefs could be statistically well calibrated yet still allocate probability in ways that cause the controller to intervene at the wrong times (for example, backing off in narrow passages where assistance should increase, or helping aggressively and prematurely in stages where user intention wasnâ€™t clear enough). The assistance policy should depend on both how uncertain the goal is and how costly an error would be. Joint training closes this loop: when a belief allocation would increase task regret, policy gradients from the controller push the inference module to reshape its probabilities to reduce that regret, and the optimal assistance becomes a learned monotone function of goal belief uncertainty. we conducted an ablations showing that omitting this procedure degrades success rates and slows task completion. Table\n5\nquantifies this effect, showing that a warm-started joint optimization outperforms both a frozen belief model and joint training from scratch.\nThe pre-trained Bayesian module was trained in a supervised manner, we used a dataset of 1,968 human-controlled cursor trajectories, where the ground-truth goals were known. The modelâ€™s key hyperparameters, such as the user rationality parameter, were tuned using a two-stage process of grid search followed by Bayesian optimization to find the best fit. To ensure the resulting beliefs were reliable, we also implemented temporal smoothing on the probability updates and also calibrated the confidence with isotonic regression.\nVariant\nSuccess (%)\nâ†‘\n\\uparrow\nTime to complete (s)\nâ†“\n\\downarrow\nPath efficiency\nâ†‘\n\\uparrow\nFrozen pretrained goal belief\n81.5\n4.13\n0.64\nJoint training from scratch\n90.7\n3.49\n0.71\nWarm-start (5 ep)\n93.4\n3.25\n0.75\nWarm-start (15 ep)\n94.6\n3.12\n0.76\nWarm-start (30 ep)\n94.9\n3.08\n0.77\nTable 5\n:\nWarm-starting the Bayesian head\n4.4.2\nIs Full Belief Conditioning necessary?\nTo isolate the contribution of belief information, we compared our full approach to a variant where the belief distribution input to the policy module was replaced with a uniform prior.\nAs shown in Table\n6\n, removing belief conditioning led to a notable decrease in performance across all metrics. Without the fine-grained probability distribution over goals, the system defaulted to more conservative assistance strategies, unable to modulate support based on inference confidence. This manifested in a 7.4% drop in success rate as the system failed to provide targeted assistance during critical maneuvers near obstacles. The 26.9% increase in completion time resulted from hesitant assistance during high-confidence scenarios, while the 18.4% reduction in path efficiency reflected suboptimal trajectories when navigating ambiguous regions with multiple potential targets. These results validate our theoretical prediction in Theorem 2, demonstrating that the quadratic regret advantage of full belief distribution integration becomes most pronounced in scenarios with high goal uncertainty or closely positioned targets.\nFigure 9\n:\nLearning performance during curriculum training. Integrated Optimization (blue) achieves higher final performance and 2.3\nÃ—\n\\times\nfaster progression through curriculum stages than baseline methods. Vertical lines indicate curriculum stage transitions: Basic goal-directed behavior, Basic collision avoidance, Challenging obstacle configurations, Goal ambiguity, and Full complexity.\nTable 6\n:\nAblation study: Uniform Prior vs. Belief Conditioning\nMethod\nSuccess\nTime\nPath\nRate (%)\n(s)\nEfficiency\nUniform Prior\n87.2\nÂ±\n\\pm\n3.0\n3.96\nÂ±\n\\pm\n0.32\n0.62\nÂ±\n\\pm\n0.07\nFull Belief Conditioning\n94.6\nÂ±\n\\pm\n2.3\n3.12\nÂ±\n\\pm\n0.27\n0.76\nÂ±\n\\pm\n0.05\nWhile the end-to-end approach required careful hyperparameter tuning, it provided measurable improvements over the baseline approach, particularly in complex scenarios with high goal uncertainty. These results demonstrate that allowing bidirectional information flow between inference and control components leads to more robust shared autonomy systems that can better handle challenging environments with high levels of uncertainty. As shown in Figure\n9\n, our integrated optimization approach achieves higher final performance and faster progression through curriculum stages than baseline methods.\n4.4.3\nHow much does curriculum learning help?\nTo evaluate the contribution of curriculum learning, we compared our full approach to a variant trained without curriculum progression (random sampling of environments throughout training).\nOur curriculum progressed through five stages with specific completion criteria:\n1. Basic goal-directed behavior: 100 episodes with Â¿80% success rate\n2. Basic collision avoidance: 200 episodes with Â¿75% success rate and Â¡15% collision rate\n3. Challenging obstacle configurations: 300 episodes with Â¿70% success rate\n4. Goal ambiguity (2-3 potential targets): 400 episodes with Â¿65% success rate\n5. Full complexity: Trained until convergence (reward plateau for 200 episodes)\nTable 7\n:\nAblation study: With vs. Without Curriculum Learning on Reacher 2D\nMethod\nSuccess\nTime\nPath\nRate (%)\n(s)\nEfficiency\nWithout Curriculum\n90.2\nÂ±\n\\pm\n2.7\n3.58\nÂ±\n\\pm\n0.30\n0.68\nÂ±\n\\pm\n0.06\nWith Curriculum\n94.6\nÂ±\n\\pm\n2.3\n3.12\nÂ±\n\\pm\n0.27\n0.76\nÂ±\n\\pm\n0.05\nAs shown in Table\n7\n, removing curriculum learning led to a decrease in performance across all metrics. The success rate dropped by 4.4%, completion time increased by 14.7%, and path efficiency decreased by 10.5%. This confirms the value of progressive difficulty in training, particularly for complex multi-stage tasks like shared autonomy with goal ambiguity and environmental constraints.\n5\nDiscussion\nThis work introduced BRACE, an integrated framework designed to effectively balance assistive performance with user agency. Our results validate our claim that moving beyond MAP estimates to leverage the full belief distribution is not just an algorithmic improvement, but a mechanism that yields direct, positive consequences for the human user.\nUnlike prior approaches, BRACEâ€™s belief-conditioned policy avoids overriding the user prematurely. This was empirically validated in our user studies: assistance levels started low (\nÎ³\nâ‰ˆ\n0.28\nÂ±\n0.12\n\\gamma\\approx 0.28\\pm 0.12\n) when intent was ambiguous and rose (\nÎ³\nâ‰ˆ\n0.74\nÂ±\n0.09\n\\gamma\\approx 0.74\\pm 0.09\n) only as goal confidence improved. This behavior is central to preserving user agency, allowing the user to explore their decision space without fighting the system. This modulation also directly translates to measurable performance gains, with BRACE achieving up to 13.1% higher success rates in high-ambiguity multi-target scenarios.\nFurthermore, the frameworkâ€™s assistance policy\nÎ³\n\\gamma\nis conditioned not only on belief but also on environmental context (e.g., obstacle proximity). The system intelligently provides more help during the most difficult parts of the task, such as navigating narrow passages or performing the final Grasp and Release maneuvers. This offloading of difficulty corresponds to the significantly lower Effort scores reported by participants in our NASA-TLX surveys. This design also enhances trust and predictability. BRACE arbitrates control via a smooth, continuous blend, rather than the binary interventions. This predictability is less jarring for the user, builds trust, and is reflected in the high subjective ratings for â€Confidenceâ€ and â€Satisfactionâ€.\nOur validation strategy was structured to evaluate BRACE across different aspects of end-effector control. We first isolated the human-interaction challenge using the 12-participant 2D cursor control study. This allowed us to validate the core subjective HRI benefits (agency, effort, and satisfaction) against real, unpredictable human control. Having validated the core HRI benefits, we then isolated the physical-dynamics challenge in the Reacher-2D task. This demonstrated robustness to the non-linear, coupled kinematics inherent in robotic arms. Finally, we integrated these components into the 3D task-context challenge with the Fetch environment. This confirmed that our joint belief-and-context optimization scales to a functional, high-dimensional manipulation task, successfully navigating both goal ambiguity and localized safety demands. For these latter two domains, we used a high-fidelity simulated human agent, which was developed from an analysis of real human trajectory data to exhibit realistic planning and motor variability. This progressive three-part approach confirms both the subjective HRI benefits and the objective algorithmic performance across the key challenges of end-effector control.\nThe algorithmic advantage of our coupled, fine-tuning approach (as opposed to a â€frozenâ€ sequential pipeline) is most compelling in these complex scenarios. The end-to-end gradient flow between the control policy and the belief module creates a feedback loop, allowing the system to learn what to be uncertain about. Our transfer learning experiments and robustness tests with sub-optimal experts further confirm the adaptability of this architecture.\nOur findings establish BRACE as a significant advancement for human-AI collaboration and suggest several concrete guidelines for designing future shared-control systems. First, assistance must scale with certainty. To protect user agency, systems should provide minimal assistance when intent is ambiguous (high belief entropy) and only increase support as intent becomes clear. Our work proves this is possible by conditioning policy on the full belief distribution, not a single MAP estimate. Second, blend, donâ€™t just switch. For tasks requiring fluid motion, a continuous blending of human and expert control is less jarring, enhances predictability, and builds user trust more effectively than binary, all-or-nothing interventions. Third, offload contextual difficulty. Assistance should not only be a function of the userâ€™s goal but also of the environmentâ€™s difficulty. By increasing support in high-constraint areas (e.g., near obstacles), the system can demonstrably reduce user cognitive and motor load. Finally, transparency builds trust. A known HRI trade-off is that users sometimes prefer predictable manual control even if it leads to worse performance. Our high qualitative feedback was aided by an interface that provided real-time feedback on the systemâ€™s perceived goal and assistance level. This transparency is not an add-on but a core feature for building user trust and acceptance.\nThis work provides a foundation for future research on adaptive assistance that respects user autonomy while ensuring successful task completion, with clear pathways to extension in high-dimensional and multi-modal settings.\n5.1\nLimitations and Future Work\nWhile BRACE demonstrates significant advantages, its current implementation has boundaries that motivate clear avenues for future research. The systemâ€™s performance is bounded by the quality and availability of an expert policy. While our robustness tests showed high resilience, the system is designed to arbitrate, not to overcome the limitations of a fundamentally flawed expert. Future work should investigate online adaptation of the expert policy itself, using the userâ€™s inputs as a corrective signal. This would move the paradigm from simple arbitration to true human-AI co-adaptation, where the expert learns from the user over time. This motivates extending BRACE to support longitudinal adaptation, where the systemâ€™s model of the human is personalized over time by learning a specific userâ€™s unique behavioral patterns, especially those with different motor impairments.\nAlso, while this work focused on kinematic inputs, the framework is well-suited for multi-modal data streams. Future iterations will Also explore integrating inputs such as gaze, EMG, and EEG decoders to enrich the belief state and provide more robust intent inference, especially for users with severe motor impairments.\n6\nAcknowledgment\nThis research study was supported by the NSF CAREER under award ID 2441496 and the NSF grant under award ID 2245558.\nReferences\n[undef]\nR. Abiri et al.\nâ€œA usability study of low-cost wireless brain-computer interface for cursor control using online linear modelâ€\nIn\nIEEE Transactions on Human-Machine Systems\n50.4\n, 2020, pp. 287â€“297\n[undefa]\nR. Aronson, A. Feng and A.. Dragan\nâ€œRobot motion planning with observation uncertainty in human preferencesâ€\nIn\nProceedings of the 3rd Conference on Robot Learning (CoRL)\n, 2019, pp. 289â€“298\n[undefb]\nA. Cetera et al.\nâ€œMacroscopic EEG reveals discriminative low-frequency oscillations in plan-to-grasp visuomotor tasksâ€, 2025\narXiv:\n2510.19057\n[undefc]\nM.. Collier, R. Narayan and H. Admoni\nâ€œThe sense of agency in assistive robotics using shared autonomyâ€\nIn\n2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI)\n, 2025, pp. 880â€“888\n[undefd]\nK. Ding et al.\nâ€œQuantitative Evaluation System of Wrist Motor Function for Stroke Patients Based on Force Feedbackâ€ PMID: 35591058\nIn\nSensors\n22.9\n, 2022, pp. 3368\nDOI:\n10.3390/s22093368\n[undefe]\nA.. Dragan and S.. Srinivasa\nâ€œA policy-blending formalism for shared controlâ€\nIn\nInternational Journal of Robotics Research\n32.7\n, 2013, pp. 790â€“805\n[undeff]\nMH. Farhadi et al.\nâ€œHuman-Centered Shared Autonomy for Motor Planning, Learning, and Control Applicationsâ€, 2025\narXiv:\n2506.16044\n[undefg]\nT. Flash and N. Hogan\nâ€œThe coordination of arm movements: an experimentally confirmed mathematical modelâ€\nIn\nJournal of Neuroscience\n5.7\n, 1985, pp. 1688â€“1703\n[undefh]\nS. Ghafoori et al.\nâ€œA novel seamless magnetic-based actuating mechanism for end-effector-based robotic rehabilitation platformsâ€\nIn\nIEEE Robotics and Automation Letters\n10.5\n, 2025, pp. 4516â€“4523\n[undefi]\nD.. Gilden, T. Thornton and M.. Mallon\nâ€œ1/f noise in human cognitionâ€\nIn\nScience\n267.5205\n, 1995, pp. 1837â€“1839\n[undefj]\nA. Goshtasbi et al.\nâ€œAI-Infused Soft Fluidic Tactile Sensingâ€\nIn\n2024 IEEE 7th International Conference on Soft Robotics, RoboSoft 2024\nIEEE, 2024, pp. 1095â€“1100\n[undefk]\nE. Greensmith, P.. Bartlett and J. Baxter\nâ€œVariance Reduction Techniques for Gradient Estimates in Reinforcement Learningâ€\nIn\nJournal of Machine Learning Research\n5\n, 2004, pp. 1471â€“1530\n[undefl]\nK. Hauser\nâ€œRecognition, prediction, and planning for assisted teleoperation of freeform tasksâ€\nIn\nAutonomous Robots\n35.4\n, 2013, pp. 241â€“254\n[undefm]\nV. Hernandez-Cruz, X. Zhang and K. Youcef-Toumi\nâ€œBayesian intention for enhanced humanâ€“robot collaborationâ€, 2024\narXiv:\n2410.00302\n[undefn]\nJ. Hu, H. Xu, J.. Liu and W. Shen\nâ€œREINFORCE++: Stabilizing Critic-Free Policy Optimization with Global Normalizationâ€\nIn\narXiv preprint arXiv:2501.03262\n, 2025\n[undefo]\nS. Jain and B. Argall\nâ€œProbabilistic human intent recognition for shared autonomy in assistive roboticsâ€\nIn\nACM Transactions on Human-Robot Interaction\n9.1\n, 2019, pp. 1â€“23\n[undefp]\nS. Jain and B. Argall\nâ€œRecursive Bayesian human intent recognition in shared-control roboticsâ€\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, 2018, pp. 3905â€“3912\n[undefq]\nS. Javdani, S.. Srinivasa and J.. Bagnell\nâ€œShared autonomy via hindsight optimizationâ€\nIn\nRobotics: Science and Systems (RSS)\n, 2015\n[undefr]\nH.. Jeon, D.. Losey and D. Sadigh\nâ€œShared autonomy with learned latent actionsâ€\nIn\nRobotics: Science and Systems (RSS)\n, 2020\n[undefs]\nA. Jonnavittula, S.. Mehta and D.. Losey\nâ€œSARI: Shared autonomy across repeated interactionâ€\nIn\nACM Transactions on Humanâ€“Robot Interaction\n13.2\n, 2022, pp. 1â€“36\n[undeft]\nW.. Knox, P. Stone and C. Breazeal\nâ€œTraining a robot via human feedback: A case studyâ€\nIn\nProceedings of the 5th International Conference on Social Robotics (ICSR)\n, 2012, pp. 460â€“470\n[undefu]\nX. Liu et al.\nâ€œBlending imitation and reinforcement learning for robust policy improvementâ€\nIn\nProceedings of the 12th International Conference on Learning Representations (ICLR)\n, 2024\narXiv:\n2310.01737\n[undefv]\nB.. McMahan, Z. Peng, B. Zhou and J.. Kao\nâ€œShared autonomy with IDA: Interventional diffusion assistanceâ€\nIn\nProceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS)\n, 2024\narXiv:\n2409.15317\n[undefw]\nS. Nikolaidis, Y. Zhu, D. Hsu and S. Srinivasa\nâ€œHuman-robot mutual adaptation in shared autonomyâ€\nIn\nProceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction\n, 2017\n[undefx]\nY. Oh, M. Toussaint and J. Mainprice\nâ€œLearning to arbitrate human and robot control using disagreement between sub-policiesâ€\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, 2021, pp. 5305â€“5311\n[undefy]\nD. Panagopoulos et al.\nâ€œA Bayesian-based approach to human operator intent recognition in remote mobile robot navigationâ€\nIn\nIEEE International Conference on Systems, Man, and Cybernetics (SMC)\n, 2021, pp. 3168â€“3175\n[undefz]\nA. Rabiee et al.\nâ€œA Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systemsâ€, 2024\narXiv:\n2402.09448\n[undefaa]\nA. Rabiee et al.\nâ€œLearning multimodal AI algorithms for amplifying limited user input into high-dimensional control spaceâ€, 2025\narXiv:\n2505.11366\n[undefab]\nA. Rabiee et al.\nâ€œSTREAMS: An Assistive Multimodal AI Framework for Empowering Biosignal Based Robotic Controlsâ€, 2024\narXiv:\n2410.03486\n[undefac]\nS. Reddy, A.. Dragan and S. Levine\nâ€œShared autonomy via deep reinforcement learningâ€\nIn\nRobotics: Science and Systems (RSS)\n, 2018\n[undefad]\nC. Schaff and M.. Walter\nâ€œResidual policy learning for shared autonomyâ€\nIn\nRobotics: Science and Systems (RSS)\n, 2020\n[undefae]\nJ. Schulman et al.\nâ€œProximal policy optimization algorithmsâ€, 2017\narXiv:\n1707.06347\n[undefaf]\nS. Singh and J. Heard\nâ€œProbabilistic policy blending for shared autonomy using deep reinforcement learningâ€\nIn\n32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)\n, 2023, pp. 1537â€“1544\n[undefag]\nR.. Williams\nâ€œSimple statistical gradient-following algorithms for connectionist reinforcement learningâ€\nIn\nMachine Learning\n8.3\n, 1992, pp. 229â€“256\n[undefah]\nE. Yousefi, M. Chen and I. Sharf\nâ€œShared autonomy policy fine-tuning and alignment for robotic tasksâ€ advance online publication\nIn\nThe International Journal of Robotics Research\n, 2025\nDOI:\n10.1177/02783649241312699\n[undefai]\nW. Yu, G. Turk and C.. Liu\nâ€œLearning to assist physical human-robot collaboration with reinforcement learningâ€\nIn\nIEEE Transactions on Robotics\n38.3\n, 2022, pp. 1537â€“1553\n[undefaj]\nR. Zhang et al.\nâ€œLeveraging human guidance for deep reinforcement learning tasksâ€\nIn\nProceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)\n, 2019, pp. 6339â€“6346\nAppendix A\nProofs\nA.1\nUncertainty Principle Proof\nWe define the expected utility function for a given state\ns\ns\n, a\ndistribution over potential goals\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\np(g|h_{1:t})\n, and a blending\nparameter\nÎ³\n\\gamma\n:\nğ”¼\nâ€‹\n[\nU\nâ€‹\n(\na\nâ†’\n,\np\n)\n]\n=\nâˆ‘\ng\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\nâ€‹\nU\ng\nâ€‹\n(\n(\n1\nâˆ’\nÎ³\n)\nâ€‹\nh\nâ†’\n+\nÎ³\nâ€‹\nw\nâ†’\ng\n)\n\\displaystyle\\mathbb{E}[U(\\vec{a},p)]=\\sum_{g}p(g|h_{1:t})U_{g}((1-\\gamma)\\vec{h}+\\gamma\\vec{w}_{g})\n(8)\nwhere\nU\ng\nU_{g}\nis the utility function for goal\ng\ng\n, measuring action\nquality in terms of goal progress and constraint satisfaction.\nFor typical control tasks, the utility function for a specific goal\ng\ng\nis:\nU\ng\nâ€‹\n(\na\nâ†’\n)\n=\nÎ±\ng\nâ‹…\nprogress\ng\nâ€‹\n(\na\nâ†’\n)\nâˆ’\nÎ²\ng\nâ‹…\nconstraint_violation\nâ€‹\n(\na\nâ†’\n)\n\\displaystyle U_{g}(\\vec{a})=\\alpha_{g}\\cdot\\text{progress}_{g}(\\vec{a})-\\beta_{g}\\cdot\\text{constraint\\_violation}(\\vec{a})\n(9)\nwhere\nprogress\ng\nâ€‹\n(\na\nâ†’\n)\n\\text{progress}_{g}(\\vec{a})\nmeasures progress toward goal\ng\ng\ngiven\naction\na\nâ†’\n\\vec{a}\n,\nconstraint_violation\nâ€‹\n(\na\nâ†’\n)\n\\text{constraint\\_violation}(\\vec{a})\nmeasures\nconstraint violation severity, and\nÎ±\ng\n,\nÎ²\ng\n>\n0\n\\alpha_{g},\\beta_{g}>0\nare weighting parameters.\nTo analyze the relationship between goal uncertainty and optimal assistance,\nwe define the optimal blending parameter as:\nÎ³\nâˆ—\nâ€‹\n(\np\n)\n=\narg\nâ€‹\nmax\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\nâ¡\nğ”¼\nâ€‹\n[\nU\nâ€‹\n(\na\nâ†’\n,\np\n)\n]\n\\displaystyle\\gamma^{*}(p)=\\operatorname*{arg\\,max}_{\\gamma\\in[0,1]}\\mathbb{E}[U(\\vec{a},p)]\n(10)\nWe parameterize uncertainty through a family of distributions\n{\np\nÎ»\n}\n\\{p_{\\lambda}\\}\nwhere\nÎ»\nâˆˆ\n[\n0\n,\n1\n]\n\\lambda\\in[0,1]\ncontrols the certainty level. For a true goal\ng\nâˆ—\ng^{*}\n:\np\nÎ»\nâ€‹\n(\ng\n)\n=\n{\nÎ»\n+\n(\n1\nâˆ’\nÎ»\n)\n/\n|\nG\n|\nif\nâ€‹\ng\n=\ng\nâˆ—\n(\n1\nâˆ’\nÎ»\n)\n/\n|\nG\n|\nif\nâ€‹\ng\nâ‰ \ng\nâˆ—\n\\displaystyle p_{\\lambda}(g)=\\begin{cases}\\lambda+(1-\\lambda)/|G|&\\text{if }g=g^{*}\\\\\n(1-\\lambda)/|G|&\\text{if }g\\neq g^{*}\\end{cases}\n(11)\nHere,\nÎ»\n=\n1\n\\lambda=1\nrepresents complete certainty, and\nÎ»\n=\n0\n\\lambda=0\nrepresents\ncomplete uncertainty. The entropy\nH\nâ€‹\n(\np\nÎ»\n)\nH(p_{\\lambda})\nis monotonically decreasing\nwith\nÎ»\n\\lambda\n, so we analyze how\nÎ³\nâˆ—\nâ€‹\n(\np\nÎ»\n)\n\\gamma^{*}(p_{\\lambda})\nvaries with\nÎ»\n\\lambda\n.\nThe first-order optimality condition for\nÎ³\nâˆ—\nâ€‹\n(\np\nÎ»\n)\n\\gamma^{*}(p_{\\lambda})\ngives:\nâˆ‚\nğ”¼\nâ€‹\n[\nU\nâ€‹\n(\na\nâ†’\n,\np\nÎ»\n)\n]\nâˆ‚\nÎ³\n=\n0\n\\displaystyle\\frac{\\partial\\mathbb{E}[U(\\vec{a},p_{\\lambda})]}{\\partial\\gamma}=0\n(12)\nThis expands to:\nâˆ‘\ng\np\nÎ»\nâ€‹\n(\ng\n)\nâ€‹\nâˆ‚\nU\ng\nâ€‹\n(\n(\n1\nâˆ’\nÎ³\n)\nâ€‹\nh\nâ†’\n+\nÎ³\nâ€‹\nw\nâ†’\ng\n)\nâˆ‚\nÎ³\n=\n0\n\\displaystyle\\sum_{g}p_{\\lambda}(g)\\frac{\\partial U_{g}((1-\\gamma)\\vec{h}+\\gamma\\vec{w}_{g})}{\\partial\\gamma}=0\n(13)\nLet\nÎ”\nâ€‹\nU\ng\n=\nâˆ‚\nU\ng\nâ€‹\n(\n(\n1\nâˆ’\nÎ³\n)\nâ€‹\nh\nâ†’\n+\nÎ³\nâ€‹\nw\nâ†’\ng\n)\nâˆ‚\nÎ³\n\\Delta U_{g}=\\frac{\\partial U_{g}((1-\\gamma)\\vec{h}+\\gamma\\vec{w}_{g})}{\\partial\\gamma}\n,\nrepresenting the marginal utility gain from increasing assistance for goal\ng\ng\n.\nFor the true goal\ng\nâˆ—\ng^{*}\n, we expect\nÎ”\nâ€‹\nU\ng\nâˆ—\n>\n0\n\\Delta U_{g^{*}}>0\nsince expert action\nw\nâ†’\ng\nâˆ—\n\\vec{w}_{g^{*}}\nis designed to be optimal for that goal. For incorrect goals\ng\nâ‰ \ng\nâˆ—\ng\\neq g^{*}\n, we may have\nÎ”\nâ€‹\nU\ng\n<\n0\n\\Delta U_{g}<0\n.\nThe optimality condition becomes:\n(\nÎ»\n+\n(\n1\nâˆ’\nÎ»\n)\n/\n|\nG\n|\n)\nâ‹…\nÎ”\nâ€‹\nU\ng\nâˆ—\n+\nâˆ‘\ng\nâ‰ \ng\nâˆ—\n(\n1\nâˆ’\nÎ»\n)\n/\n|\nG\n|\nâ‹…\nÎ”\nâ€‹\nU\ng\n=\n0\n\\displaystyle(\\lambda+(1-\\lambda)/|G|)\\cdot\\Delta U_{g^{*}}+\\sum_{g\\neq g^{*}}(1-\\lambda)/|G|\\cdot\\Delta U_{g}=0\n(14)\nTo analyze how\nÎ³\nâˆ—\n\\gamma^{*}\nchanges with\nÎ»\n\\lambda\n, we apply the implicit function theorem:\nd\nâ€‹\nÎ³\nâˆ—\nd\nâ€‹\nÎ»\n=\nâˆ’\nâˆ‚\n2\nğ”¼\nâ€‹\n[\nU\n]\nâˆ‚\nÎ»\nâ€‹\nâˆ‚\nÎ³\nâˆ‚\n2\nğ”¼\nâ€‹\n[\nU\n]\nâˆ‚\nÎ³\n2\n\\displaystyle\\frac{d\\gamma^{*}}{d\\lambda}=-\\frac{\\frac{\\partial^{2}\\mathbb{E}[U]}{\\partial\\lambda\\partial\\gamma}}{\\frac{\\partial^{2}\\mathbb{E}[U]}{\\partial\\gamma^{2}}}\n(15)\nComputing the mixed partial derivative in the numerator:\nâˆ‚\n2\nğ”¼\nâ€‹\n[\nU\n]\nâˆ‚\nÎ»\nâ€‹\nâˆ‚\nÎ³\n\\displaystyle\\frac{\\partial^{2}\\mathbb{E}[U]}{\\partial\\lambda\\partial\\gamma}\n=\nâˆ‚\nâˆ‚\nÎ»\nâ€‹\n[\nâˆ‘\ng\np\nÎ»\nâ€‹\n(\ng\n)\nâ€‹\nÎ”\nâ€‹\nU\ng\n]\n\\displaystyle=\\frac{\\partial}{\\partial\\lambda}\\left[\\sum_{g}p_{\\lambda}(g)\\Delta U_{g}\\right]\n(16)\n=\nâˆ‚\np\nÎ»\nâ€‹\n(\ng\nâˆ—\n)\nâˆ‚\nÎ»\nâ€‹\nÎ”\nâ€‹\nU\ng\nâˆ—\n+\nâˆ‘\ng\nâ‰ \ng\nâˆ—\nâˆ‚\np\nÎ»\nâ€‹\n(\ng\n)\nâˆ‚\nÎ»\nâ€‹\nÎ”\nâ€‹\nU\ng\n\\displaystyle=\\frac{\\partial p_{\\lambda}(g^{*})}{\\partial\\lambda}\\Delta U_{g^{*}}+\\sum_{g\\neq g^{*}}\\frac{\\partial p_{\\lambda}(g)}{\\partial\\lambda}\\Delta U_{g}\n(17)\n=\n(\n1\nâˆ’\n1\n|\nG\n|\n)\nâ€‹\nÎ”\nâ€‹\nU\ng\nâˆ—\nâˆ’\nâˆ‘\ng\nâ‰ \ng\nâˆ—\n1\n|\nG\n|\nâ€‹\nÎ”\nâ€‹\nU\ng\n\\displaystyle=\\left(1-\\frac{1}{|G|}\\right)\\Delta U_{g^{*}}-\\sum_{g\\neq g^{*}}\\frac{1}{|G|}\\Delta U_{g}\n(18)\n=\nÎ”\nâ€‹\nU\ng\nâˆ—\nâˆ’\n1\n|\nG\n|\nâ€‹\nâˆ‘\ng\nÎ”\nâ€‹\nU\ng\n\\displaystyle=\\Delta U_{g^{*}}-\\frac{1}{|G|}\\sum_{g}\\Delta U_{g}\n(19)\nAssumption (Expert Efficiency):\nUnder rational design, the marginal utility\ngain\nÎ”\nâ€‹\nU\ng\nâˆ—\n\\Delta U_{g^{*}}\nfor the true goal exceeds the average marginal utility\ngain across all goals:\nÎ”\nâ€‹\nU\ng\nâˆ—\n>\n1\n|\nG\n|\nâ€‹\nâˆ‘\ng\nÎ”\nâ€‹\nU\ng\n\\Delta U_{g^{*}}>\\frac{1}{|G|}\\sum_{g}\\Delta U_{g}\n.\nUnder this assumption, the mixed partial derivative\nâˆ‚\n2\nğ”¼\nâ€‹\n[\nU\n]\nâˆ‚\nÎ»\nâ€‹\nâˆ‚\nÎ³\n>\n0\n\\frac{\\partial^{2}\\mathbb{E}[U]}{\\partial\\lambda\\partial\\gamma}>0\n.\nFor the denominator, we have:\nâˆ‚\n2\nğ”¼\nâ€‹\n[\nU\n]\nâˆ‚\nÎ³\n2\n=\nâˆ‘\ng\np\nÎ»\nâ€‹\n(\ng\n)\nâ€‹\nâˆ‚\n2\nU\ng\nâˆ‚\nÎ³\n2\n\\displaystyle\\frac{\\partial^{2}\\mathbb{E}[U]}{\\partial\\gamma^{2}}=\\sum_{g}p_{\\lambda}(g)\\frac{\\partial^{2}U_{g}}{\\partial\\gamma^{2}}\n(20)\nIn practical control scenarios, utility functions are typically concave\nwith respect to\nÎ³\n\\gamma\nat optimal operating points, making\nâˆ‚\n2\nğ”¼\nâ€‹\n[\nU\n]\nâˆ‚\nÎ³\n2\n<\n0\n\\frac{\\partial^{2}\\mathbb{E}[U]}{\\partial\\gamma^{2}}<0\n. This can be verified\nfor common progress terms like\nprogress\ng\nâ€‹\n(\na\nâ†’\n)\n=\nâˆ’\nâ€–\nx\nâ†’\n+\na\nâ†’\nâˆ’\ng\nâ†’\nâ€–\n2\n\\text{progress}_{g}(\\vec{a})=-\\|\\vec{x}+\\vec{a}-\\vec{g}\\|^{2}\n,\nwhich yield concave utility with respect to\nÎ³\n\\gamma\n.\nTherefore:\nd\nâ€‹\nÎ³\nâˆ—\nd\nâ€‹\nÎ»\n=\nâˆ’\nÎ”\nâ€‹\nU\ng\nâˆ—\nâˆ’\n1\n|\nG\n|\nâ€‹\nâˆ‘\ng\nÎ”\nâ€‹\nU\ng\nâˆ‘\ng\np\nÎ»\nâ€‹\n(\ng\n)\nâ€‹\nâˆ‚\n2\nU\ng\nâˆ‚\nÎ³\n2\n>\n0\n\\displaystyle\\frac{d\\gamma^{*}}{d\\lambda}=-\\frac{\\Delta U_{g^{*}}-\\frac{1}{|G|}\\sum_{g}\\Delta U_{g}}{\\sum_{g}p_{\\lambda}(g)\\frac{\\partial^{2}U_{g}}{\\partial\\gamma^{2}}}>0\n(21)\nSince\nÎ»\n\\lambda\nis inversely related to uncertainty (higher\nÎ»\n\\lambda\nmeans\nlower entropy), this implies\nd\nâ€‹\nÎ³\nâˆ—\nd\nâ€‹\nH\n<\n0\n\\frac{d\\gamma^{*}}{dH}<0\n, establishing that\nthe optimal blending parameter decreases as goal uncertainty increases.\nAs environmental constraint severity increases, the weight\nÎ²\ng\n\\beta_{g}\non the constraint violation term effectively increases. This amplifies\nthe utility gain from the expertâ€™s constraint-satisfying actions,\nresulting in an increase in the optimal\nÎ³\nâˆ—\n\\gamma^{*}\n.\nThrough similar application of the implicit function theorem, we can show\nd\nâ€‹\nÎ³\nâˆ—\nd\nâ€‹\nÎ²\n>\n0\n\\frac{d\\gamma^{*}}{d\\beta}>0\n, establishing that the optimal blending parameter\nincreases with environmental constraint severity.\nA.2\nIntegrated Optimization Advantage Proof\nWe formalize the comparison between belief-aware and MAP-only approaches using policy\nregret, defined as the utility loss relative to the optimal policy for each possible goal.\nFor a specific goal\ng\ng\n, the optimal blending parameter is:\nÎ³\ng\nâˆ—\n=\narg\nâ€‹\nmax\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\nâ¡\nU\ng\nâ€‹\n(\nÎ³\n)\n\\displaystyle\\gamma^{*}_{g}=\\operatorname*{arg\\,max}_{\\gamma\\in[0,1]}U_{g}(\\gamma)\n(22)\nThe MAP approach identifies the most likely goal\ng\n^\n=\narg\nâ€‹\nmax\ng\nâ¡\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\n\\hat{g}=\\operatorname*{arg\\,max}_{g}p(g|h_{1:t})\nand optimizes for it:\nÎ³\nMAP\nâˆ—\n=\nÎ³\ng\n^\nâˆ—\n=\narg\nâ€‹\nmax\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\nâ¡\nU\ng\n^\nâ€‹\n(\nÎ³\n)\n\\displaystyle\\gamma^{*}_{\\text{MAP}}=\\gamma^{*}_{\\hat{g}}=\\operatorname*{arg\\,max}_{\\gamma\\in[0,1]}U_{\\hat{g}}(\\gamma)\n(23)\nThe integrated approach maximizes expected utility across all potential goals:\nÎ³\nintegrated\nâˆ—\n=\narg\nâ€‹\nmax\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\nâ€‹\nâˆ‘\ng\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\nâ€‹\nU\ng\nâ€‹\n(\nÎ³\n)\n\\displaystyle\\gamma^{*}_{\\text{integrated}}=\\operatorname*{arg\\,max}_{\\gamma\\in[0,1]}\\sum_{g}p(g|h_{1:t})U_{g}(\\gamma)\n(24)\nThe expected regret of a policy with blending parameter\nÎ³\n\\gamma\nis:\nR\nâ€‹\n(\nÎ³\n)\n=\nğ”¼\ng\nâ€‹\n[\nU\ng\nâ€‹\n(\nÎ³\ng\nâˆ—\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\n)\n]\n\\displaystyle R(\\gamma)=\\mathbb{E}_{g}\\left[U_{g}(\\gamma^{*}_{g})-U_{g}(\\gamma)\\right]\n(25)\nThe expected regret of the MAP approach is:\nR\nMAP\n=\nğ”¼\ng\nâ€‹\n[\nU\ng\nâ€‹\n(\nÎ³\ng\nâˆ—\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\n]\n\\displaystyle R_{\\text{MAP}}=\\mathbb{E}_{g}\\left[U_{g}(\\gamma^{*}_{g})-U_{g}(\\gamma^{*}_{\\hat{g}})\\right]\n(26)\nThe expected regret of the integrated approach is:\nR\nintegrated\n=\nğ”¼\ng\nâ€‹\n[\nU\ng\nâ€‹\n(\nÎ³\ng\nâˆ—\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\n)\n]\n\\displaystyle R_{\\text{integrated}}=\\mathbb{E}_{g}\\left[U_{g}(\\gamma^{*}_{g})-U_{g}(\\gamma^{*}_{\\text{integrated}})\\right]\n(27)\nTheorem 2.\nUnder locally concave utility functions, the integrated approach\nachieves lower expected regret than the MAP approach:\nR\nMAP\nâ‰¥\nR\nintegrated\n\\displaystyle R_{\\text{MAP}}\\geq R_{\\text{integrated}}\n(28)\nProof.\nWe need to show:\nğ”¼\ng\nâ€‹\n[\nU\ng\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\n]\nâ‰¥\n0\n\\displaystyle\\mathbb{E}_{g}\\left[U_{g}(\\gamma^{*}_{\\text{integrated}})-U_{g}(\\gamma^{*}_{\\hat{g}})\\right]\\geq 0\n(29)\nBy definition of\nÎ³\nintegrated\nâˆ—\n\\gamma^{*}_{\\text{integrated}}\nas the maximizer of expected utility:\nâˆ‘\ng\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\nâ€‹\nU\ng\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\n)\nâ‰¥\nâˆ‘\ng\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\nâ€‹\nU\ng\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\n\\displaystyle\\sum_{g}p(g|h_{1:t})U_{g}(\\gamma^{*}_{\\text{integrated}})\\geq\\sum_{g}p(g|h_{1:t})U_{g}(\\gamma^{*}_{\\hat{g}})\n(30)\nFor each goal\ng\ng\n, using a second-order Taylor expansion around\nÎ³\ng\n^\nâˆ—\n\\gamma^{*}_{\\hat{g}}\n:\nU\ng\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\nâ‰ˆ\nd\nâ€‹\nU\ng\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\nd\nâ€‹\nÎ³\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\nâˆ’\nÎ³\ng\n^\nâˆ—\n)\nâˆ’\n1\n2\nâ€‹\n|\nU\ng\nâ€²â€²\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\n|\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\nâˆ’\nÎ³\ng\n^\nâˆ—\n)\n2\n\\displaystyle U_{g}(\\gamma^{*}_{\\text{integrated}})-U_{g}(\\gamma^{*}_{\\hat{g}})\\approx\\frac{dU_{g}(\\gamma^{*}_{\\hat{g}})}{d\\gamma}(\\gamma^{*}_{\\text{integrated}}-\\gamma^{*}_{\\hat{g}})-\\frac{1}{2}|U^{\\prime\\prime}_{g}(\\gamma^{*}_{\\hat{g}})|(\\gamma^{*}_{\\text{integrated}}-\\gamma^{*}_{\\hat{g}})^{2}\n(31)\nTaking the expectation and using the first-order optimality condition for\nÎ³\ng\n^\nâˆ—\n\\gamma^{*}_{\\hat{g}}\n:\nğ”¼\ng\nâ€‹\n[\nU\ng\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\n)\nâˆ’\nU\ng\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\n]\nâ‰ˆ\nâˆ‘\ng\nâ‰ \ng\n^\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\nâ€‹\nd\nâ€‹\nU\ng\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\nd\nâ€‹\nÎ³\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\nâˆ’\nÎ³\ng\n^\nâˆ—\n)\nâˆ’\n\\displaystyle\\mathbb{E}_{g}\\left[U_{g}(\\gamma^{*}_{\\text{integrated}})-U_{g}(\\gamma^{*}_{\\hat{g}})\\right]\\approx\\sum_{g\\neq\\hat{g}}p(g|h_{1:t})\\frac{dU_{g}(\\gamma^{*}_{\\hat{g}})}{d\\gamma}(\\gamma^{*}_{\\text{integrated}}-\\gamma^{*}_{\\hat{g}})-\n(32)\n1\n2\nâ€‹\nâˆ‘\ng\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\nâ€‹\n|\nU\ng\nâ€²â€²\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\n|\nâ€‹\n(\nÎ³\nintegrated\nâˆ—\nâˆ’\nÎ³\ng\n^\nâˆ—\n)\n2\n\\displaystyle\\frac{1}{2}\\sum_{g}p(g|h_{1:t})|U^{\\prime\\prime}_{g}(\\gamma^{*}_{\\hat{g}})|(\\gamma^{*}_{\\text{integrated}}-\\gamma^{*}_{\\hat{g}})^{2}\n(33)\nThe first-order optimality condition for\nÎ³\nintegrated\nâˆ—\n\\gamma^{*}_{\\text{integrated}}\nimplies a balance of\nmarginal utilities across all goals, weighted by their probabilities. This leads to a positive\nexpected utility gain for non-MAP goals that outweighs the small negative utility impact on\nthe MAP goal.\nThe regret difference can be quantified as:\nR\nMAP\nâˆ’\nR\nintegrated\nâ‰ˆ\n1\n2\nâ€‹\nâˆ‘\ng\np\nâ€‹\n(\ng\n|\nh\n1\n:\nt\n)\nâ€‹\n|\nU\ng\nâ€²â€²\nâ€‹\n(\nÎ³\ng\n^\nâˆ—\n)\n|\nâ‹…\n(\nÎ³\ng\nâˆ—\nâˆ’\nÎ³\ng\n^\nâˆ—\n)\n2\n\\displaystyle R_{\\text{MAP}}-R_{\\text{integrated}}\\approx\\frac{1}{2}\\sum_{g}p(g|h_{1:t})|U^{\\prime\\prime}_{g}(\\gamma^{*}_{\\hat{g}})|\\cdot(\\gamma^{*}_{g}-\\gamma^{*}_{\\hat{g}})^{2}\n(34)\nThis demonstrates that integrated optimization provides the greatest advantage in scenarios with\nhigh goal uncertainty and where different goals require substantially different assistance strategies.\nThe advantage scales quadratically with the divergence between goal-specific optimal assistance levels,\naligning with the\nO\n~\nâ€‹\n(\n(\nÎ”\nâ€‹\nÎ³\n)\n2\n)\n\\tilde{O}((\\Delta\\gamma)^{2})\nregret gap claimed in the main paper.\nAppendix B\nDetailed Reward Function\nThe complete reward function used in BRACE is:\nR\n=\nâˆ’\nw\nc\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\nâ‹…\nğŸ™\nc\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\nâ€‹\ni\nâ€‹\ns\nâ€‹\ni\nâ€‹\no\nâ€‹\nn\n+\nw\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nx\nâ‹…\nÎ³\nâ‹…\np\nm\nâ€‹\na\nâ€‹\nx\nâ‹…\nğŸ™\nn\nâ€‹\ne\nâ€‹\na\nâ€‹\nr\nâˆ’\nw\nf\nâ€‹\na\nâ€‹\nr\nâ‹…\nÎ³\nâ‹…\nğŸ™\nf\nâ€‹\na\nâ€‹\nr\n+\nw\np\nâ€‹\nr\nâ€‹\no\nâ€‹\ng\nâ‹…\np\nm\nâ€‹\na\nâ€‹\nx\nâ‹…\n(\nd\nt\nâˆ’\n1\nâˆ’\nd\nt\n)\nâˆ’\nw\na\nâ€‹\nu\nâ€‹\nt\nâ€‹\no\nâ‹…\nÎ³\n2\n+\nw\ng\nâ€‹\no\nâ€‹\na\nâ€‹\nl\nâ‹…\nlog\nâ¡\n(\np\nt\nâ€‹\nr\nâ€‹\nu\nâ€‹\ne\n)\n\\begin{split}R=&-w_{coll}\\cdot\\mathds{1}_{collision}+w_{prox}\\cdot\\gamma\\cdot p_{max}\\cdot\\mathds{1}_{near}\\\\\n&-w_{far}\\cdot\\gamma\\cdot\\mathds{1}_{far}+w_{prog}\\cdot p_{max}\\cdot(d_{t-1}-d_{t})\\\\\n&-w_{auto}\\cdot\\gamma^{2}+w_{goal}\\cdot\\log(p_{true})\\end{split}\n(35)\nWhere:\nâ€¢\nw\nc\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\n=\n10.0\nw_{coll}=10.0\npenalizes collisions with obstacles\nâ€¢\nw\np\nâ€‹\nr\nâ€‹\no\nâ€‹\nx\n=\n2.5\nw_{prox}=2.5\nrewards assistance when near the predicted target, weighted by the maximum goal probability\np\nm\nâ€‹\na\nâ€‹\nx\np_{max}\nâ€¢\nw\nf\nâ€‹\na\nâ€‹\nr\n=\n1.5\nw_{far}=1.5\npenalizes assistance when far from targets, discouraging premature assistance\nâ€¢\nw\np\nâ€‹\nr\nâ€‹\no\nâ€‹\ng\n=\n3.0\nw_{prog}=3.0\nrewards progress toward goals, measured as the reduction in distance from timestep\nt\nâˆ’\n1\nt-1\nto\nt\nt\n, weighted by the maximum goal probability\nâ€¢\nw\na\nâ€‹\nu\nâ€‹\nt\nâ€‹\no\n=\n1.5\nw_{auto}=1.5\ncreates a quadratic penalty for intervention, ensuring assistance is provided only when necessary\nâ€¢\nw\ng\nâ€‹\no\nâ€‹\na\nâ€‹\nl\n=\n2.0\nw_{goal}=2.0\nrewards correct goal identification, measured by the log probability of the true goal\nFigure 10\n:\nReward Ablation Study comparing performance with different reward components removed. The full reward function (blue) achieves the highest performance, while removing the progress term (green) has the most detrimental effect. Removing the collision term (orange) significantly impacts early learning, while removing the gamma penalty (red) leads to overassistance and reduced final performance.\nThe indicator functions\nğŸ™\nc\nâ€‹\no\nâ€‹\nl\nâ€‹\nl\nâ€‹\ni\nâ€‹\ns\nâ€‹\ni\nâ€‹\no\nâ€‹\nn\n\\mathds{1}_{collision}\n,\nğŸ™\nn\nâ€‹\ne\nâ€‹\na\nâ€‹\nr\n\\mathds{1}_{near}\n, and\nğŸ™\nf\nâ€‹\na\nâ€‹\nr\n\\mathds{1}_{far}\nare binary flags that activate their respective terms based on the agentâ€™s position relative to obstacles and targets. The policy module encourages minimal assistance through a regularization term (\nw\na\nâ€‹\nu\nâ€‹\nt\nâ€‹\no\nw_{auto}\n), ensuring the system provides support only when necessary. This promotes user agency while still offering effective assistance. We kept the reward function design domain-agnostic: make progress to the goal, avoid hazards, preserve user agency and avoid unnecessary assistance. Porting to another task mainly requires setting two main weights: a safety weight (how conservative to be near obstacles) and an agency weight (how readily assistance should engage).\nFigure\n10\nshows our reward function ablation study, where we systematically removed individual components to assess their impact on learning performance. The full reward function (blue) achieves the highest cumulative reward, while removing the progress term (green) has the most detrimental effect on performance. Removing the collision term (orange) significantly impacts early learning but eventually recovers, while removing the gamma penalty (red) leads to over-assistance and reduced final performance. These results validate the importance of each reward component in our formulation.\nAppendix C\nIntegration Optimization\nOur approach implements two specific methods for integrating the inference and policy components in shared autonomy. The first method, which we call the Maximum a posteriori (MAP) baseline approach, uses a frozen pre-trained Bayesian inference module that provides belief estimates to the policy network but does not receive gradient updates from downstream performance. The second method, our end-to-end approach, allows bidirectional information flow where gradients from the policy outputs propagate back to the inference module parameters. This architectural distinction enables us to quantitatively evaluate whether adaptive inference parameters improve overall system performance.\nC.1\nEnd-to-End Optimization and Policy Network Stability Measures\nWe implemented a mixed objective function that balances the original supervised inference accuracy with the integrated reinforcement learning objective. We leverage principles from\n[\nundefag\n]\nto estimate gradients through the non-differentiable sampling process in the Bayesian inference module, while extending their approach for our pipeline. Our extended version functions as follows:\n1.\nDuring initial supervised pretraining, the Bayesian inference module parameters\nÏ•\n\\phi\nare optimized to maximize the log-likelihood of the true goals:\nâ„’\ns\nâ€‹\nu\nâ€‹\np\nâ€‹\ne\nâ€‹\nr\nâ€‹\nv\nâ€‹\ni\nâ€‹\ns\nâ€‹\ne\nâ€‹\nd\nâ€‹\n(\nÏ•\n)\n=\nâˆ’\nğ”¼\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\nj\nâ€‹\ne\nâ€‹\nc\nâ€‹\nt\nâ€‹\no\nâ€‹\nr\nâ€‹\ni\nâ€‹\ne\nâ€‹\ns\nâ€‹\n[\nlog\nâ¡\nP\nÏ•\nâ€‹\n(\ng\nâˆ—\n|\nX\n,\nH\n)\n]\n\\mathcal{L}_{supervised}(\\phi)=-\\mathbb{E}_{trajectories}\\left[\\log P_{\\phi}(g^{*}|X,H)\\right]\n(36)\n2.\nDuring end-to-end training, we use REINFORCE to estimate gradients of the reinforcement learning objective\nâ„’\nR\nâ€‹\nL\nâ€‹\n(\nÎ¸\n,\nÏ•\n)\n\\mathcal{L}_{RL}(\\theta,\\phi)\nwith respect to the inference module parameters\nÏ•\n\\phi\n:\nâˆ‡\nÏ•\nâ„’\nR\nâ€‹\nL\nâ€‹\n(\nÎ¸\n,\nÏ•\n)\nâ‰ˆ\nğ”¼\nt\nâ€‹\nr\nâ€‹\na\nâ€‹\nj\nâ€‹\ne\nâ€‹\nc\nâ€‹\nt\nâ€‹\no\nâ€‹\nr\nâ€‹\ni\nâ€‹\ne\nâ€‹\ns\nâ€‹\n[\nâˆ‘\nt\n=\n0\nT\nR\nt\nâ€‹\nâˆ‡\nÏ•\nlog\nâ¡\nP\nÏ•\nâ€‹\n(\nb\nt\n|\nX\n1\n:\nt\n,\nH\n1\n:\nt\n)\n]\n\\nabla_{\\phi}\\mathcal{L}_{RL}(\\theta,\\phi)\\approx\\mathbb{E}_{trajectories}\\left[\\sum_{t=0}^{T}R_{t}\\nabla_{\\phi}\\log P_{\\phi}(b_{t}|X_{1:t},H_{1:t})\\right]\n(37)\n3.\nWe employ a mixed objective that gradually shifts from supervised to reinforcement objectives:\nâ„’\nt\nâ€‹\no\nâ€‹\nt\nâ€‹\na\nâ€‹\nl\nâ€‹\n(\nÎ¸\n,\nÏ•\n)\n=\nÎ±\nâ€‹\nâ„’\nR\nâ€‹\nL\nâ€‹\n(\nÎ¸\n,\nÏ•\n)\n+\n(\n1\nâˆ’\nÎ±\n)\nâ€‹\nâ„’\ns\nâ€‹\nu\nâ€‹\np\nâ€‹\ne\nâ€‹\nr\nâ€‹\nv\nâ€‹\ni\nâ€‹\ns\nâ€‹\ne\nâ€‹\nd\nâ€‹\n(\nÏ•\n)\n\\mathcal{L}_{total}(\\theta,\\phi)=\\alpha\\mathcal{L}_{RL}(\\theta,\\phi)+(1-\\alpha)\\mathcal{L}_{supervised}(\\phi)\n(38)\nwhere\nÎ±\n\\alpha\nis a weighting hyperparameter that we anneal during training to gradually shift the Bayesian module from supervised learning to reinforcement-driven optimization.\nTo reduce variance in the REINFORCE estimator, we employ a number of stabilization techniques that are standard in modern actor-critic implementations. First, we use a learned, state-dependent baseline; While simpler baselines, such as a moving average of rewards, can provide some variance reduction\n[\nundefk\n]\n, we utilize the output of the critic network,\nV\nâ€‹\n(\ns\nt\n)\nV(s_{t})\n, as our baseline, as the critic is trained to be an unbiased estimator of the expected return from state\ns\nt\ns_{t}\n. By subtracting this value from the empirical episodic return\nR\nt\nR_{t}\n, we compute the advantage function,\nA\nâ€‹\n(\ns\nt\n)\n=\nR\nt\nâˆ’\nV\nâ€‹\n(\ns\nt\n)\nA(s_{t})=R_{t}-V(s_{t})\n, which provides a lower-variance, but still unbiased gradient vector. This way, we center the returns, ensuring that actions performing better than expected receive positive updates, while those performing worse receive negative updates.\nTo manage the optimization landscape and prevent catastrophic policy shifts from high-magnitude gradients, we implement gradient normalization. Rather than clipping gradients element-wise, which would alter the gradientâ€™s direction, we scaled the entire gradient vector\nâˆ‡\nÏ•\nâ„’\nR\nâ€‹\nL\n\\nabla_{\\phi}\\mathcal{L}_{RL}\nif its L2 norm exceeds\nğ’\nc\nâ€‹\nl\nâ€‹\ni\nâ€‹\np\n\\mathcal{C}_{clip}\n. This scaling, which is defined as\nğ \nâ†\nğ \nâ‹…\nğ’\nc\nâ€‹\nl\nâ€‹\ni\nâ€‹\np\nâ€–\nğ \nâ€–\n2\n\\mathbf{g}\\leftarrow\\mathbf{g}\\cdot\\frac{\\mathcal{C}_{clip}}{||\\mathbf{g}||_{2}}\nwhere\nğ \n\\mathbf{g}\nis\nâˆ‡\nÏ•\nâ„’\nR\nâ€‹\nL\n\\nabla_{\\phi}\\mathcal{L}_{RL}\n, preserves the direction of the update while limiting the magnitude. This process is important for maintaining a stable learning trajectory, especially when the reward landscape is noisy.\nFinally, to find a trade-off between exploration and exploitation within the belief module itself, we apply a temperature parameter\nÏ„\n\\tau\nto the final softmax function of the inference network. This temperature is controlled via an annealing schedule (e.g., exponential decay) over the course of training. Initially, a high\nÏ„\n\\tau\n(e.g.,\nÏ„\nâ‰¥\n1.0\n\\tau\\geq 1.0\n) smooths the belief distribution, encouraging exploration by preventing the module from becoming prematurely overconfident. As training progresses,\nÏ„\n\\tau\nis gradually reduced, allowing the distribution to sharpen so we get higher probability values for the inferred goals, which allows the assistance policy to also learn from more decisive signals.\nTo improve the training stability and sample efficiency of the dual-head architecture, especially given the observed 3.2\nÃ—\n\\times\nincrease in gradient variance, we implemented two key mechanisms. First, we implement an epistemic regularizer in the form of confidence-based update scaling. We observed that periods of high goal uncertainty (corresponding to a low maximum goal probability, led to significant parameter oscillations. This is not because the belief state is noisy or unreliable, but because the policy network struggles to converge when its input belief vector is high-entropy. We therefore dynamically dampen the update magnitude by a factor of\nÎ±\n=\nmin\nâ¡\n(\n1.0\n,\np\nm\nâ€‹\na\nâ€‹\nx\n/\nc\n)\n\\alpha=\\min(1.0,p_{max}/c)\n, where\nc\n=\n0.8\nc=0.8\nis our confidence threshold. This mechanism acts as a state-dependent learning rate dampener. When the system is uncertain (e.g.,\np\nm\nâ€‹\na\nâ€‹\nx\n=\n0.3\np_{max}=0.3\n), the update step-size is reduced (\nÎ±\n=\n0.375\n\\alpha=0.375\n). This prevents the policy parameters from oscillating wildly in response to ambiguous inputs, allowing the network to gradually converge on the correct policy that is appropriate for high-uncertainty states.\nAdditionally, we address the high variance in the reward signal itself by applying advantage normalization. This is a standard technique in actor-critic implementations where, within each minibatch, we normalize the computed advantage estimates to have a zero mean and unit standard deviation. This normalization prevents reward-scaling issues across different tasks or curriculum stages and shifts the learning objective from predicting absolute returns to predicting relative returns within a batch, which provides a more consistent gradient signal\n[\nundefn\n]\n. These mechanisms work to create a robust training pipeline and allow for smoother convergence.\nAppendix D\nSimulated Human Agent in BRACE\nAccurate noise modeling in human behavior is critical for Bayesian inference-based shared autonomy systems for three primary reasons\n[\nundefz\n]\n: (1) it ensures inference robustness by helping distinguish intentional directional changes from natural variability; (2) it enables properly calibrated assistance thresholds; and (3) it improves generalization to diverse real-world environments with different input devices and user skill levels.\nWe developed a data-driven EEG decoder noise behavior model based on spectral and trajectory analysis of 798 human-controlled cursor paths from\n[\nundef\n]\n. These trajectories were collected from cursor control trajectories across 23 participants (age 19-34, 14 male, 9 female) with varying skill levels (novice to experienced). While EEG-based control differs from joystick or mouse input,\n[\nundefm\n,\nundefb\n]\ndemonstrates that the fundamental motor planning patterns, particularly the temporal correlation structure and goal-directed behavior, remain consistent across input modalities. We validated this generalization by comparing velocity profiles from our EEG dataset with published joystick and force-sensor trajectories from Hauser\n[\nundefl\n]\n, finding comparable normalized jerk statistics (RMSE Â¡ 0.14).\nTo avoid overfitting and ensure robust cross-modal applicability, we employed stratified k-fold cross-validation (k=5) during model calibration, with participants and task conditions separated between training and validation. This participant-wise splitting prevented subject-specific idiosyncrasies from leaking between calibration and evaluation phases.\nThe model incorporates a deterministic trajectory component using minimal-jerk polynomial regression that captures the characteristic curved paths of goal-directed human movement\n[\nundefg\n]\n. For environments with obstacles, we extend the basic minimum-jerk model with intermediate via-points generated using a potential field approach, which better approximates the non-smooth decelerations humans exhibit around constraints. While a fully dynamic obstacle-aware optimal control model would be more principled, our approach provides a computationally efficient approximation that captures key trajectory features. Figure\n11\nillustrates the comparison between expert paths and actual human trajectories along with the spectral characteristics of control variability observed in our data.\nFigure 11\n:\nA snapshot of an actual trajectory translated from an EEG decoder\n[\nundef\n]\n(A) Comparison between expert path and actual human trajectory (B) Power spectral density analysis of trajectory deviations\nTo model control variability, we implemented a stochastic noise component using a first-order auto-regressive filter that transforms white Gaussian noise into 1/f (pink) noise:\ny\nâ€‹\n[\nn\n]\n=\n0.5\nâ‹…\ny\nâ€‹\n[\nn\nâˆ’\n1\n]\n+\n0.5\nâ‹…\nx\nâ€‹\n[\nn\n]\ny[n]=0.5\\cdot y[n-1]+0.5\\cdot x[n]\n(39)\nThe spectral slope of -1.59 was validated against multiple studies\n[\nundefai\n,\nundefi\n,\nundef\n]\n(slopes ranging from -1.42 to -1.67) and our own dataset analysis (mean slope -1.55, SD=0.21). The noise amplitude of 3.2% (SD=2.7%) relative to trajectory length was determined through systematic parameter sweeps measuring KL-divergence between simulated and real trajectory distributions, selecting the configuration that minimized this distance across validation folds. For sensitivity analysis, we tested BRACEâ€™s performance across a range of noise model configurations, varying both spectral slope (-1.2 to -1.8) and amplitude (1.5% to 5.0%). BRACE maintained above 92% of optimal performance across this ranges.\nAppendix E\nTransfer learning and Adaptation to New Settings\nWe examined layer-specific transfer learning setups by freezing varying network portions and fine-tuning specific layers for adaptation to new environments. The most effective approach was retaining the shared encoderâ€™s knowledge while fine-tuning the value head, achieving over 87% of full retraining performance across all environments with only 28% of the trainable parameters.\nFigure 12\n:\nLayer-Selective Transfer Learning Efficiency Across Training Phases\nAs shown in Figure\n12\n, freezing the encoder and retraining value heads achieved 90% of final performance with 27% fewer training steps compared to full retraining. This approach demonstrates particularly strong advantages during early training phases while maintaining comparable asymptotic performance. The different curriculum stages (Basic, Collision Avoidance, Complex Obstacles, and Goal Ambiguity) reveal how transfer learning provides the greatest benefits during more complex learning phases.\nThe data reveals that freezing the encoder and the actor head while retraining critic head with smaller learning rate achieved 90% of its final performance after only 32% of the training steps required for full retraining to reach the same level. The optimization trajectory in this scenario confirms that the model encodes generalizable latent structures that effectively capture the fundamental dynamics of shared autonomy tasks, enabling efficient knowledge transfer across environments without catastrophic forgetting or representation collapse.\nTo rigorously evaluate these transfer learning capabilities in complex scenarios, we developed a challenging maze environment with higher complexity than our standard test environments. This environment, shown in Figure\n13\n, features 15 goals organized as 3 clusters of 3 goals each plus 6 scattered goals, along with 12 obstacles. By arranging goals in clusters with varying densities (cluster radius of 80 units) and placing obstacles along goal paths, we created scenarios requiring fine-grained distinctions between closely positioned targetsâ€”precisely the challenging situations where representation transfer is most difficult.\nFigure 13\n:\nMaze environment used for transfer learning evaluation. Red circles represent potential goals (organized in clusters and scattered positions), gray circles represent obstacles, and the white circle shows the agent position. This complex environment tests BRACEâ€™s ability to transfer learned representations to novel scenarios with dense decision spaces.\nThis environment constitutes a rigorous assessment paradigm that systematically evaluates the generalization capacity of BRACE through topologically complex decision spaces, non-convex navigation constraints, and high-dimensional goal distributions. The frameworkâ€™s robust performance across this domain substantiates that its learned latent manifold effectively disentangles the underlying probabilistic factors governing intent inference and constraint satisfaction processes, rather than overfitting to environment-specific policy mappings encountered during training. The high-entropy goal distribution and constrained navigation paths represent conditions commonly encountered in rehabilitative and assistive technology applications where goals are rarely isolated or equally distributed\n[\nundefh\n]\n.\nThe advantage of integrated belief-conditioned optimization increased with goal uncertainty, as predicted by Theorem 2. Our experiments showed a clear pattern of increasing performance improvement as uncertainty levels increased, summarized in Table\n8\n.\nTable 8\n:\nPerformance advantage of BRACE over MAP-based approaches across uncertainty levels in Maze environment\nUncertainty Level\nSuccess Rate\nCompletion Time\nPath Efficiency\nImprovement (%)\nReduction (%)\nImprovement (%)\nLow (Entropy Â¡ 0.5)\n1.1\nÂ±\n\\pm\n0.4\n3.2\nÂ±\n\\pm\n0.7\n2.3\nÂ±\n\\pm\n0.6\nMedium (Entropy 0.5-1.0)\n2.4\nÂ±\n\\pm\n0.5\n8.7\nÂ±\n\\pm\n1.1\n5.8\nÂ±\n\\pm\n0.9\nHigh (Entropy Â¿ 1.0)\n4.3\nÂ±\n\\pm\n0.6\n16.2\nÂ±\n\\pm\n1.8\n9.1\nÂ±\n\\pm\n1.2\nMulti-target Scenarios\n13.1\nÂ±\n\\pm\n1.2\n24.5\nÂ±\n\\pm\n2.3\n18.6\nÂ±\n\\pm\n1.7\nIn multi-target scenarios, our End-to-End BRACE variant achieved 13.1% higher success rate and 24.5% faster completion time compared to MAP selection. This substantial improvement in high-uncertainty conditions empirically validates our theoretical result that belief-conditioned optimization provides the greatest advantage when different goals require different assistance strategies. These performance differences were particularly pronounced in the maze environment, where goal clustering created naturally ambiguous intent scenarios that challenged baseline approaches but were effectively handled by BRACEâ€™s belief-conditioned policy.\nAppendix F\nComputational Resources\nThe BRACE framework is computationally efficient in both training and real-time inference. The complete training curriculum requires approximately 7 hours on a single GPU, with the Bayesian inference module pre-training in 45 minutes. At runtime, the system achieves an end-to-end inference latency of 36 ms per cycle (27 Hz) on a desktop with an Intel i9-12900KF CPU and an NVIDIA RTX 3080 GPU. Bayesian inference accounts for approximately 3 ms and the assistance arbitration policy for 12 ms, with the remainder used for input delay and environment overhead. The frameworkâ€™s performance remains robust even without GPU acceleration; on a laptop using a 9th-generation Intel Core i5 CPU, the latency was 49 ms (20.4 Hz). This speed meets the 20 Hz operational frequency of common robotic platforms like the Kinova arm, so BRACE is unlikely to introduce a computational bottleneck in conventional implementations.\nThe architecture also scales efficiently. The per-step computational cost grows linearly with the number of goals since the Bayesian inference module evaluates each goalâ€™s likelihood independently. Also, to facilitate adaptation to new domains during transfer learning, we incorporate gradient normalization and confidence-scaled updates.\nAppendix G\nRobustness to Input Modalities\nA key design goal of BRACE was to be input-modality agnostic, allowing the core assistance logic to operate independently of the physical user interface. Since the majority of results were obtained using a DualSense controller, we questioned whether the frameworkâ€™s performance was dependent on this specific device, so we evaluated BRACEâ€™s robustness by comparing the baseline controller against a custom-built, force-sensing (isometric) joystick, which is more representative of assistive technology used in rehabilitation settings\n[\nundefd\n]\n.\nThis interface consisted of four 20Â kg load cells arranged in L-bracket assemblies with integrated Wheatstone bridge circuits and HX711 amplifiers for signal processing. The differential measurements from paired load cells enable precise detection of force magnitude and direction. These signals were normalized to match the expected input format of the DualSense controller to ensure that the BRACE framework receives a standardized kinematic signal, regardless of the physical mechanism generating it.\nTable 9\n:\nComparison of performance metrics between DualSense and Force-based interfaces\nInterface\nSuccess Rate (%)\nCompletion Time (s)\nPath Efficiency\nDualSense\n98.1\nÂ±\n\\pm\n1.8\n3.32\nÂ±\n\\pm\n0.23\n0.88\nÂ±\n\\pm\n0.06\nForce-based\n97.6\nÂ±\n\\pm\n2.1\n3.41\nÂ±\n\\pm\n0.29\n0.85\nÂ±\n\\pm\n0.07\nWe conducted a follow-up study with N=5 participants performing the target acquisition task in the cursor control environment using this force-based interface. Comparing performance between the two interfaces, both using BRACE assistance, revealed statistically indistinguishable results across all metrics, as shown in Table\n9\n. Statistical analysis confirmed no significant differences between interfaces for success rate (\nF\nâ€‹\n(\n1\n,\n8\n)\n=\n0.05\n,\np\n=\n0.83\nF(1,8)=0.05,p=0.83\n), completion time (\nF\nâ€‹\n(\n1\n,\n8\n)\n=\n0.09\n,\np\n=\n0.76\nF(1,8)=0.09,p=0.76\n), or path efficiency (\nF\nâ€‹\n(\n1\n,\n8\n)\n=\n0.14\n,\np\n=\n0.71\nF(1,8)=0.14,p=0.71\n). This finding validates that BRACEâ€™s assistance framework is not related to a specific deviceâ€™s characteristics, but effectively generalizes as long as the input signal is normalized, demonstrating input-modality agnosticism.",
    "preview_text": "Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.\n\n\\DeclareBibliographyAlias\npreprintunpublished\nEnd-to-end Optimization of Belief and Policy Learnin",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "fine tune",
        "constrained reinforcement learning",
        "goal conditioned reinforcement learning",
        "world model"
    ],
    "one_line_summary": "BRACEæ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯ä¼˜åŒ–è´å¶æ–¯æ„å›¾æ¨æ–­å’Œä¸Šä¸‹æ–‡è‡ªé€‚åº”è¾…åŠ©ç­–ç•¥ï¼Œæå‡å…±äº«è‡ªä¸»ç³»ç»Ÿä¸­äººæœºåä½œçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ç›®æ ‡æ¨¡ç³Šå’Œå¤æ‚ç¯å¢ƒçº¦æŸä¸‹è¡¨ç°ä¼˜å¼‚ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šå…±äº«è‡ªä¸»èŒƒå¼ä¸­ä¿¡å¿µä¸ç­–ç•¥å­¦ä¹ çš„ç«¯åˆ°ç«¯ä¼˜åŒ–\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nå…±äº«è‡ªä¸»ç³»ç»Ÿéœ€è¦åœ¨æ¨æ–­ç”¨æˆ·æ„å›¾ä¸ç¡®å®šé€‚å½“è¾…åŠ©æ°´å¹³ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œè¿™æ˜¯äººæœºäº¤äº’ï¼ˆHRIï¼‰çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–é™æ€æ··åˆæ¯”ä¾‹æˆ–å°†ç›®æ ‡æ¨æ–­ä¸è¾…åŠ©ä»²è£åˆ†ç¦»ï¼Œå¯¼è‡´åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ€§èƒ½æ¬ ä½³ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€åˆ†ç¦»é—®é¢˜ï¼Œæå‡ºä¸€ç§èƒ½å¤Ÿ**è”åˆä¼˜åŒ–**æ„å›¾æ¨æ–­å’Œè¾…åŠ©ç­–ç•¥çš„æ¡†æ¶ï¼Œä»¥åœ¨ä¿è¯ä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ç»´æŠ¤ç”¨æˆ·è‡ªä¸»æ€§ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº† **BRACE**ï¼ˆåŸºäºä¸Šä¸‹æ–‡ç¼–ç çš„è´å¶æ–¯å¼ºåŒ–è¾…åŠ©ï¼‰æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **å…¨ä¿¡å¿µæ¡ä»¶åŒ–**ï¼šè¾…åŠ©ç­–ç•¥åŸºäºå®Œæ•´çš„è´å¶æ–¯ç›®æ ‡åéªŒåˆ†å¸ƒï¼Œè€Œéå•ä¸€çš„æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡ï¼Œä»è€Œèƒ½æ›´ç»†è‡´åœ°å“åº”ç”¨æˆ·çš„ä¸ç¡®å®šæ€§ã€‚\n- **ç«¯åˆ°ç«¯è€¦åˆ**ï¼šé€šè¿‡ä¸€ä¸ªåŒå¤´ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå®ç°äº†æ„å›¾æ¨æ–­æ¨¡å—ä¸è¾…åŠ©ä»²è£ç­–ç•¥çš„**è”åˆæ¢¯åº¦ä¼˜åŒ–**ï¼Œä½¿ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æ¢¯åº¦èƒ½å¤Ÿåå‘å¡‘é€ ä¿¡å¿µæœ¬èº«ï¼Œä½¿å…¶æ›´å…·å†³ç­–ä»·å€¼ã€‚\n- **ç†è®ºä¼˜åŠ¿è¯æ˜**ï¼šä»ç†è®ºä¸Šè¯æ˜äº†åœ¨ç›®æ ‡ä¸æ˜ç¡®ä¸”ç¯å¢ƒå—é™æ—¶ï¼ŒåŸºäºå®Œæ•´ä¿¡å¿µçš„è”åˆä¼˜åŒ–ç›¸æ¯”ä¼ ç»Ÿçš„åºåˆ—å¼æ–¹æ³•ï¼Œèƒ½å¸¦æ¥**äºŒæ¬¡æ–¹çº§åˆ«çš„æœŸæœ›é—æ†¾ä¼˜åŠ¿**ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\né€šè¿‡ä¸‰ä¸ªé€æ­¥å¤æ‚çš„å®éªŒéªŒè¯äº†BRACEçš„æœ‰æ•ˆæ€§ï¼š\n- **2Då…‰æ ‡æ§åˆ¶ä»»åŠ¡**ï¼šåœ¨12åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼ŒBRACEç›¸æ¯”æ— è¾…åŠ©æ§åˆ¶ï¼ŒæˆåŠŸç‡æå‡36.3%ï¼Œè·¯å¾„æ•ˆç‡æå‡87%ï¼›ç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•ï¼ˆå¦‚IDAã€DQNï¼‰ï¼ŒæˆåŠŸç‡æå‡6.3%ï¼Œè·¯å¾„æ•ˆç‡æå‡41%ã€‚\n- **Reacher-2Dæœºæ¢°è‡‚ä»»åŠ¡**ï¼šåœ¨å­˜åœ¨æ§åˆ¶å™ªå£°çš„åœºæ™¯ä¸‹ï¼ŒBRACEç›¸æ¯”äºŒè¿›åˆ¶å¹²é¢„æ–¹æ³•ï¼Œå®ç°äº†æ›´å¹³æ»‘çš„æ§åˆ¶å’Œæ›´ä½çš„æ€§èƒ½æ–¹å·®ã€‚\n- **Fetchæœºæ¢°è‡‚æŠ“å–æ”¾ç½®ä»»åŠ¡**ï¼šåœ¨ç›®æ ‡æ¨¡ç³Šï¼ˆä¸‰ä¸ªç›¸ä¼¼ç›®æ ‡ç®±ï¼‰å’Œç¯å¢ƒçº¦æŸå¹¶å­˜çš„é«˜ç»´ä»»åŠ¡ä¸­ï¼ŒBRACEå–å¾—äº†æœ€é«˜çš„æˆåŠŸç‡ï¼ˆ86%ï¼‰å’Œæœ€å°‘çš„ç¢°æ’æ¬¡æ•°ï¼Œå…¶è½¨è¿¹æ›´å¹³æ»‘ã€å†³ç­–æ›´æœæ–­ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nBRACEä¸ºè‡ªé€‚åº”å…±äº«è‡ªä¸»ç³»ç»Ÿæä¾›äº†ä¸€ä¸ª**åŸåˆ™æ€§ä¸”é«˜æ•ˆçš„æ¡†æ¶**ã€‚å®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–ï¼Œå®ç°äº†è¾…åŠ©æ°´å¹³éšç›®æ ‡ä¸ç¡®å®šæ€§é™ä½ã€éšç¯å¢ƒçº¦æŸä¸¥é‡æ€§å¢åŠ çš„è‡ªé€‚åº”è°ƒèŠ‚ï¼Œåœ¨æå‡ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆç»´æŠ¤äº†ç”¨æˆ·è‡ªä¸»æ€§ã€‚è¯¥å·¥ä½œæ¨åŠ¨äº†äººæœºåä½œçš„èŒƒå¼å‘å±•ï¼Œå¯¹è¾…åŠ©æœºå™¨äººã€åº·å¤æŠ€æœ¯ç­‰éœ€è¦ç›®æ ‡å¯¼å‘è¾…åŠ©çš„é¢†åŸŸå…·æœ‰é‡è¦çš„ç†è®ºå’Œåº”ç”¨ä»·å€¼ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šBRACEæ¡†æ¶\n\n### **è®ºæ–‡æ‹Ÿè§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**å…±äº«è‡ªä¸»æƒï¼ˆShared Autonomyï¼‰** ä¸­çš„ä¸€ä¸ªæ ¹æœ¬æ€§çŸ›ç›¾ï¼šå¦‚ä½•åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…ï¼Œ**åŒæ—¶ä¼˜åŒ–ç”¨æˆ·æ„å›¾æ¨æ–­å’Œè¾…åŠ©å†³ç­–**ï¼Œä»¥åœ¨ç¡®ä¿ä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶ï¼Œæœ€å¤§é™åº¦åœ°å°Šé‡ç”¨æˆ·çš„è‡ªä¸»æƒï¼ˆUser Agencyï¼‰ã€‚å…·ä½“æŒ‘æˆ˜åŒ…æ‹¬ï¼š\n1.  **ç›®æ ‡ä¸ç¡®å®šæ€§**ï¼šåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œç”¨æˆ·çš„çœŸå®æ„å›¾ï¼ˆç›®æ ‡ï¼‰æ˜¯æ¨¡ç³Šä¸”éšæ—¶é—´å˜åŒ–çš„ã€‚\n2.  **ç¯å¢ƒçº¦æŸ**ï¼šå­˜åœ¨éšœç¢ç‰©ç­‰çº¦æŸï¼Œéœ€è¦ç³»ç»ŸåŠ¨æ€è°ƒæ•´è¾…åŠ©åŠ›åº¦ã€‚\n3.  **æ¬¡ä¼˜çš„ç°æœ‰æ–¹æ¡ˆ**ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸å°†æ„å›¾æ¨æ–­å’Œè¾…åŠ©å†³ç­–**åˆ†ç¦»æˆ–é¡ºåºæ‰§è¡Œ**ï¼ˆå¦‚å…ˆæ¨æ–­æœ€å¯èƒ½ç›®æ ‡ï¼Œå†å†³å®šè¾…åŠ©åŠ›åº¦ï¼‰ï¼Œæˆ–ä½¿ç”¨**å›ºå®šçš„æ··åˆæ¯”ä¾‹**ï¼Œå¯¼è‡´åœ¨å¤æ‚ã€ä¸ç¡®å®šåœºæ™¯ä¸‹æ€§èƒ½ä¸ä½³ã€‚\n\n### **æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†åä¸º **BRACEï¼ˆBayesian Reinforcement Assistance with Context Encodingï¼‰** çš„æ–°å‹æ¡†æ¶ï¼Œå…¶åˆ›æ–°æ€§ä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸‰ä¸ªç´§å¯†å…³è”çš„æ–¹é¢ï¼š\n\n1.  **å…¨ä¿¡å¿µæ¡ä»¶åŒ–ç­–ç•¥**\n    *   **åˆ›æ–°**ï¼šè¾…åŠ©ç­–ç•¥ `Î³` çš„ç”Ÿæˆ**ä¾èµ–äºå®Œæ•´çš„è´å¶æ–¯åéªŒæ¦‚ç‡åˆ†å¸ƒ `b`**ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡ã€‚\n    *   **ä»·å€¼**ï¼šä½¿ç³»ç»Ÿèƒ½å¯¹ç›®æ ‡ä¸ç¡®å®šæ€§åšå‡ºæ›´ç²¾ç»†ã€æ›´çµæ´»çš„ååº”ã€‚ä¾‹å¦‚ï¼Œå½“ä¿¡å¿µåˆ†å¸ƒå¹³å¦ï¼ˆé«˜ç†µï¼‰æ—¶ï¼Œç³»ç»ŸçŸ¥é“â€œè‡ªå·±ä¸ç¡®å®šâ€ï¼Œä»è€Œå‡å°‘å¹²é¢„ï¼Œä¿æŠ¤ç”¨æˆ·è‡ªä¸»æƒã€‚\n\n2.  **ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**\n    *   **åˆ›æ–°**ï¼šå°†**è´å¶æ–¯æ„å›¾æ¨æ–­æ¨¡å—**å’Œ**åŸºäºActor-Criticçš„è¾…åŠ©ä»²è£ç­–ç•¥**ç½®äºä¸€ä¸ªå¯å¾®æ¶æ„ä¸­ï¼Œ**é€šè¿‡å•ä¸€æ§åˆ¶ç›®æ ‡è¿›è¡Œè”åˆä¼˜åŒ–**ã€‚\n    *   **ä»·å€¼**ï¼šå®ç°äº†ä»ç­–ç•¥æ€§èƒ½åˆ°ä¿¡å¿µæ›´æ–°çš„**æ¢¯åº¦åå‘ä¼ æ’­**ã€‚è¿™æ„å‘³ç€ï¼Œæ¨æ–­æ¨¡å—ä¸ä»…å­¦ä¹ â€œå‡†ç¡®â€çš„ä¿¡å¿µï¼Œæ›´å­¦ä¹ å¯¹ä¸‹æ¸¸æ§åˆ¶â€œå†³ç­–æœ‰ç”¨â€çš„ä¿¡å¿µï¼Œä»æ ¹æœ¬ä¸Šé¿å…äº†ä¼ ç»Ÿé¡ºåºç®¡é“ä¸­â€œä¼°è®¡å™¨-æ§åˆ¶å™¨ä¸åŒ¹é…â€çš„é—®é¢˜ã€‚\n\n3.  **ç†è®ºä¼˜åŠ¿è¯æ˜**\n    *   **åˆ›æ–°**ï¼šé€šè¿‡**å®šç†1å’Œå®šç†2**ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†BRACEæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚\n        *   **å®šç†1ï¼ˆå•è°ƒæ€§ï¼‰**ï¼šæœ€ä¼˜è¾…åŠ©æ°´å¹³åº”**éšç›®æ ‡ä¸ç¡®å®šæ€§çš„å¢åŠ è€Œé™ä½**ï¼Œ**éšç¯å¢ƒçº¦æŸä¸¥é‡æ€§çš„å¢åŠ è€Œå¢åŠ **ã€‚è¿™ä¸ºç³»ç»Ÿçš„è‡ªé€‚åº”è¡Œä¸ºæä¾›äº†æ•°å­¦ä¾æ®ã€‚\n        *   **å®šç†2ï¼ˆé—æ†¾ä¼˜åŠ¿ï¼‰**ï¼šä¸åŸºäºMAPä¼°è®¡çš„é¡ºåºæ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºå®Œæ•´ä¿¡å¿µçš„æ¡ä»¶åŒ–ç­–ç•¥èƒ½å°†**æœŸæœ›é—æ†¾é™ä½ä¸€ä¸ªäºŒæ¬¡é¡¹çš„é‡çº§**ã€‚ä¼˜åŠ¿åœ¨ç›®æ ‡æ¨¡ç³Šã€ä¸åŒç›®æ ‡éœ€è¦ä¸åŒè¾…åŠ©ç­–ç•¥çš„åœºæ™¯ä¸­æœ€ä¸ºæ˜æ˜¾ã€‚\n\n### **è§£å†³æ–¹æ¡ˆï¼šBRACEæ¡†æ¶å¦‚ä½•å·¥ä½œ**\nBRACEé€šè¿‡ä¸€ä¸ª**åŒå¤´ç¥ç»ç½‘ç»œæ¶æ„**å®ç°ä¸Šè¿°åˆ›æ–°ï¼š\n\n1.  **è´å¶æ–¯æ¨æ–­æ¨¡å—**ï¼š\n    *   ä½¿ç”¨é€’å½’è´å¶æ–¯æ»¤æ³¢ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å†å²è½¨è¿¹ï¼ŒæŒç»­æ›´æ–°å¯¹æ‰€æœ‰æ½œåœ¨ç›®æ ‡çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆä¿¡å¿µ `b`ï¼‰ã€‚\n    *   å°†ç”¨æˆ·å»ºæ¨¡ä¸ºâ€œå™ªå£°ç†æ€§ä½“â€ï¼Œå…¶è¾“å…¥æœå‘ç›®æ ‡çš„ä¼¼ç„¶æ€§æ›´é«˜ã€‚\n\n2.  **ç­–ç•¥ä¼˜åŒ–æ¨¡å—ï¼ˆActor-Criticï¼‰**ï¼š\n    *   è¾“å…¥ä¸º**çŠ¶æ€ `s`ã€å®Œæ•´ä¿¡å¿µåˆ†å¸ƒ `b`ã€ç¯å¢ƒä¸Šä¸‹æ–‡ `c`**ã€‚\n    *   è¾“å‡ºä¸ºä¸€ä¸ªè¿ç»­çš„**æ··åˆå‚æ•° `Î³ âˆˆ [0, 1]`**ï¼Œç”¨äºçº¿æ€§èåˆç”¨æˆ·è¾“å…¥å’Œä¸“å®¶ç­–ç•¥è¾“å…¥ï¼š`a = (1-Î³)*h + Î³*w`ã€‚\n    *   ä½¿ç”¨**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–**è¿›è¡Œè®­ç»ƒï¼Œå¥–åŠ±å‡½æ•°å¹³è¡¡äº†ä»»åŠ¡è¿›åº¦ã€é¿éšœã€ç”¨æˆ·è‡ªä¸»æƒç­‰å¤šä¸ªç›®æ ‡ã€‚\n\n3.  **è”åˆè®­ç»ƒæµç¨‹**ï¼š\n    *   é‡‡ç”¨**è¯¾ç¨‹å­¦ä¹ **ï¼Œä»ç®€å•åœºæ™¯é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚åœºæ™¯ï¼ˆå¤šç›®æ ‡ã€å¤šéšœç¢ï¼‰ã€‚\n    *   è®­ç»ƒæ—¶ï¼Œç­–ç•¥æ¨¡å—çš„æ¢¯åº¦ä¼šé€šè¿‡REINFORCEç­‰æ–¹æ³•**åå‘ä¼ æ’­è‡³è´å¶æ–¯æ¨æ–­æ¨¡å—**ï¼Œå®ç°ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚\n\n### **å®é™…ä»·å€¼ä¸éªŒè¯**\nè®ºæ–‡é€šè¿‡**ä¸‰å±‚é€’è¿›çš„å®éªŒ**éªŒè¯äº†BRACEçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼š\n\n1.  **2Då…‰æ ‡æ§åˆ¶ï¼ˆäººæœºäº¤äº’éªŒè¯ï¼‰**ï¼š12åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒBRACEåœ¨**æˆåŠŸç‡ï¼ˆ+6.3%ï¼‰ã€è·¯å¾„æ•ˆç‡ï¼ˆ+41%ï¼‰ã€å®Œæˆæ—¶é—´ï¼ˆ-32%ï¼‰** ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•ï¼ˆIDAï¼Œ DQNï¼‰ï¼ŒåŒæ—¶è·å¾—äº†æœ€é«˜çš„ç”¨æˆ·ä¸»è§‚è¯„åˆ†ï¼ˆæ˜“ç”¨æ€§ã€æ»¡æ„åº¦ç­‰ï¼‰ã€‚\n\n2.  **Reacher-2Dï¼ˆç‰©ç†åŠ¨åŠ›å­¦éªŒè¯ï¼‰**ï¼šåœ¨æ¨¡æ‹Ÿæœºæ¢°è‡‚ç¯å¢ƒä¸­ï¼ŒBRACEçš„è¿ç»­æ··åˆç­–ç•¥åœ¨**å¤„ç†é«˜é¢‘æ§åˆ¶å™ªå£°æ–¹é¢**ä¼˜äºäºŒè¿›åˆ¶å¹²é¢„æ–¹æ³•ï¼Œå®ç°äº†æ›´å¹³æ»‘ã€æ›´ç¨³å®šçš„æ§åˆ¶ã€‚\n\n3.  **Fetchæœºæ¢°è‡‚æ‹¾æ”¾ï¼ˆç»¼åˆä»»åŠ¡éªŒè¯ï¼‰**ï¼šåœ¨åŒ…å«**ç›®æ ‡æ¨¡ç³Šæ€§ï¼ˆä¸‰ä¸ªç›¸ä¼¼ç›®æ ‡ç®±ï¼‰å’Œå±€éƒ¨å®‰å…¨çº¦æŸï¼ˆéšœç¢ç‰©ï¼‰** çš„å¤æ‚3Dæ“ä½œä»»åŠ¡ä¸­ï¼ŒBRACEçš„ä¼˜åŠ¿æœ€ä¸ºçªå‡ºï¼Œ**æˆåŠŸç‡æ¯”IDAé«˜18%**ï¼Œä¸”èƒ½æ ¹æ®ä¿¡å¿µç†µåŠ¨æ€è°ƒæ•´è¾…åŠ©åŠ›åº¦ã€‚\n\n**ç»“è®º**ï¼šBRACEé€šè¿‡**å°†å®Œæ•´ä¿¡å¿µæ¡ä»¶åŒ–ä¸ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ç›¸ç»“åˆ**ï¼Œä¸ºè§£å†³å…±äº«è‡ªä¸»æƒä¸­æ„å›¾æ¨æ–­ä¸è¾…åŠ©å†³ç­–çš„è€¦åˆé—®é¢˜æä¾›äº†ä¸€ä¸ª**åŸåˆ™æ€§å¼ºã€ç†è®ºæœ‰ä¿éšœã€å®è·µæœ‰æ•ˆ**çš„æ¡†æ¶ã€‚å®ƒä¸ä»…åœ¨æ€§èƒ½æŒ‡æ ‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ›´é‡è¦çš„æ˜¯é€šè¿‡å…¶è‡ªé€‚åº”æœºåˆ¶ï¼Œåœ¨æå‡ä»»åŠ¡æ•ˆç‡çš„åŒæ—¶ï¼Œæ›´å¥½åœ°ç»´æŠ¤äº†ç”¨æˆ·ä½“éªŒå’Œè‡ªä¸»æƒã€‚",
            "timestamp": "2026-02-03T16:02:14.419841",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## å…±äº«è‡ªä¸»æ€§å‘å±•è„‰ç»œæ¢³ç†\n\nåŸºäºè®ºæ–‡å†…å®¹åŠå…¶å¼•ç”¨çš„ç›¸å…³ç ”ç©¶ï¼Œè¯¥é¢†åŸŸï¼ˆå…±äº«è‡ªä¸»æ€§ä¸­çš„æ„å›¾æ¨æ–­ä¸è¾…åŠ©ä»²è£ï¼‰çš„å‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºå‡ ä¸ªå…³é”®é˜¶æ®µå’Œä¸åŒçš„å‘å±•æ–¹å‘ã€‚å…¶æ ¸å¿ƒæŒ‘æˆ˜å§‹ç»ˆæ˜¯**å¹³è¡¡æ„å›¾æ¨æ–­çš„å‡†ç¡®æ€§ä¸è¾…åŠ©å†³ç­–çš„ä¼˜åŒ–**ï¼Œä»¥åœ¨æå‡ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ç»´æŠ¤ç”¨æˆ·èƒ½åŠ¨æ€§ã€‚\n\nä»¥ä¸‹æ˜¯æŒ‰æ—¶é—´å’ŒæŠ€æœ¯èŒƒå¼æ¼”è¿›çš„ä¸»è¦é˜¶æ®µï¼š\n\n### ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºæ¨¡å‹çš„è§„åˆ’ä¸é™æ€æ··åˆï¼ˆ2013-2015ï¼‰\n**æ ¸å¿ƒæ€æƒ³**ï¼šå°†é—®é¢˜å½¢å¼åŒ–ä¸ºå·²çŸ¥æ¨¡å‹çš„è§„åˆ’é—®é¢˜ï¼Œä½¿ç”¨å›ºå®šè§„åˆ™æˆ–ç®€å•ä¼˜åŒ–æ¥æ··åˆäººæœºæ§åˆ¶ã€‚\n*   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šDraganå’ŒSrinivasa (2013) çš„**æ”¿ç­–æ··åˆå½¢å¼åŒ–**ã€‚ä»–ä»¬æå‡ºäº†äººæœºè¾“å…¥çº¿æ€§æ’å€¼çš„æ•°å­¦æ¡†æ¶ï¼ˆ`a = (1-Î³)*h + Î³*w`ï¼‰ï¼Œä½†æ··åˆå‚æ•°Î³é€šå¸¸æ˜¯**é™æ€çš„**æˆ–åŸºäºç®€å•å¯å‘å¼è§„åˆ™ï¼Œæ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡æˆ–ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼šä¸ºå…±äº«æ§åˆ¶æä¾›äº†é¦–ä¸ªä¸¥è°¨çš„æ•°å­¦è¡¨è¿°ï¼Œå¥ å®šäº†â€œæ··åˆâ€è¿™ä¸€æ ¸å¿ƒæ“ä½œçš„åŸºç¡€ã€‚\n    *   **å±€é™**ï¼šç¼ºä¹å¯¹ç”¨æˆ·æ„å›¾çš„**æ¦‚ç‡åŒ–æ¨æ–­**ï¼Œè¾…åŠ©æ°´å¹³æ— æ³•é€‚åº”ä»»åŠ¡å¤æ‚åº¦æˆ–ç”¨æˆ·æ„å›¾çš„ä¸ç¡®å®šæ€§ã€‚\n\n### ç¬¬äºŒé˜¶æ®µï¼šæ¦‚ç‡æ¨æ–­ä¸åºåˆ—åŒ–æ¶æ„ï¼ˆ2015-2018ï¼‰\n**æ ¸å¿ƒæ€æƒ³**ï¼šå¼•å…¥æ¦‚ç‡æ¨¡å‹ï¼ˆå¦‚è´å¶æ–¯æ»¤æ³¢ï¼‰æ¥æ˜¾å¼æ¨æ–­ç”¨æˆ·ç›®æ ‡ï¼Œä½†æ¨æ–­æ¨¡å—å’Œè¾…åŠ©æ§åˆ¶æ¨¡å—æ˜¯**åˆ†ç¦»æˆ–é¡ºåºæ‰§è¡Œ**çš„ã€‚\n*   **ä»£è¡¨æ€§å·¥ä½œ**ï¼š\n    1.  **Javdaniç­‰äºº (2015) çš„â€œäº‹åä¼˜åŒ–â€**ï¼šå°†é—®é¢˜å»ºæ¨¡ä¸º**éƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰**ï¼Œç”¨æˆ·çš„çœŸå®ç›®æ ‡æ˜¯éšçŠ¶æ€ã€‚ä»–ä»¬é‡‡ç”¨ hindsight optimization æ¥è¿‘ä¼¼æ±‚è§£ï¼Œèƒ½åœ¨ç½®ä¿¡åº¦ä½æ—¶ä»æä¾›è¾…åŠ©ã€‚\n    2.  **Jainå’ŒArgall (2018, 2019) çš„é€’å½’è´å¶æ–¯æ„å›¾è¯†åˆ«**ï¼šä¸ºè¾…åŠ©æœºå™¨äººå¼€å‘äº†ç³»ç»Ÿçš„æ¦‚ç‡æ¡†æ¶ï¼Œèåˆä¸åŒç†æ€§æ°´å¹³çš„äººç±»è¡Œä¸ºè§‚æµ‹ã€‚\n*   **æ”¹è¿›ä¸å±€é™**ï¼š\n    *   **æ”¹è¿›**ï¼š\n        *   é¦–æ¬¡ç³»ç»Ÿåœ°å°†**æ„å›¾ä¸ç¡®å®šæ€§**çº³å…¥æ¨¡å‹ï¼Œä½¿ç”¨å®Œæ•´ä¿¡å¿µåˆ†å¸ƒ `b(g)`ã€‚\n        *   æä¾›äº†åº”å¯¹ç›®æ ‡æ¨¡ç³Šæ€§çš„åŸåˆ™æ€§æ–¹æ³•ã€‚\n    *   **å±€é™**ï¼š\n        *   **åºåˆ—åŒ–ç®¡é“**ï¼šå…ˆè¿›è¡Œæ„å›¾æ¨æ–­ï¼ˆäº§ç”Ÿä¸€ä¸ªä¿¡å¿µåˆ†å¸ƒï¼‰ï¼Œå†åŸºäºæ­¤ä¿¡å¿µè¿›è¡Œè§„åˆ’æˆ–æ§åˆ¶ã€‚ä¸¤ä¸ªæ¨¡å—**ç‹¬ç«‹ä¼˜åŒ–**ï¼ˆä¾‹å¦‚ï¼Œæ¨æ–­æ¨¡å—æœ€å°åŒ–é¢„æµ‹è¯¯å·®ï¼Œæ§åˆ¶æ¨¡å—æœ€å¤§åŒ–ä»»åŠ¡å¥–åŠ±ï¼‰ï¼Œå¯¼è‡´â€œä¼°è®¡å™¨-æ§åˆ¶å™¨ä¸åŒ¹é…â€â€”â€”æ¨æ–­å‡ºçš„ä¿¡å¿µå¯èƒ½å¯¹ä¸‹æ¸¸æ§åˆ¶å†³ç­–ä¸æ˜¯æœ€ä¼˜çš„ã€‚\n        *   **ä¾èµ–æ¨¡å‹ä¸ç®€åŒ–å‡è®¾**ï¼šé€šå¸¸éœ€è¦å¯¹ç›®æ ‡é›†ã€åŠ¨æ€æ¨¡å‹æˆ–äººç±»è¡Œä¸ºæ¨¡å‹åšå‡ºè¾ƒå¼ºå‡è®¾ï¼Œé™åˆ¶äº†åœ¨å¤æ‚ã€éç»“æ„åŒ–ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚\n\n### ç¬¬ä¸‰é˜¶æ®µï¼šå­¦ä¹ å‹æ–¹æ³•ä¸åˆæ­¥è€¦åˆï¼ˆ2018-2022ï¼‰\n**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨æœºå™¨å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ ï¼‰ç›´æ¥ä»äº¤äº’æ•°æ®ä¸­å­¦ä¹ è¾…åŠ©ç­–ç•¥ï¼Œå¹¶å¼€å§‹å°è¯•å°†æ¨æ–­ä¸æ§åˆ¶è¿›è¡Œä¸€å®šç¨‹åº¦çš„è€¦åˆã€‚\n*   **å‘å±•æ–¹å‘1ï¼šç«¯åˆ°ç«¯éšå¼æ„å›¾å­¦ä¹ **\n    *   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šReddyç­‰äºº (2018) çš„**æ·±åº¦Qå­¦ä¹ ï¼ˆDQNï¼‰æ–¹æ³•**ã€‚ä½¿ç”¨æ·±åº¦RLç›´æ¥ä»çŠ¶æ€-åŠ¨ä½œ-å¥–åŠ±åºåˆ—ä¸­å­¦ä¹ è¾…åŠ©ç­–ç•¥ï¼Œ**éšå¼åœ°**é€šè¿‡å¥–åŠ±ä¿¡å·æ¨æ–­æ„å›¾ï¼Œæ²¡æœ‰æ˜¾å¼çš„æ¦‚ç‡ç›®æ ‡æ¨¡å‹ã€‚\n    *   **æ”¹è¿›**ï¼š**æ¨¡å‹æ— å…³**ï¼Œèƒ½é€‚åº”å¤æ‚åŠ¨æ€å’Œå¥–åŠ±å‡½æ•°ï¼Œåœ¨è¯¸å¦‚ Lunar Lander ç­‰ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚\n    *   **å±€é™**ï¼šæ„å›¾è¡¨ç¤ºæ˜¯éšå¼çš„ã€é»‘ç›’çš„ï¼Œç¼ºä¹å¯¹ä¸ç¡®å®šæ€§çš„æ˜¾å¼å»ºæ¨¡å’Œè§£é‡Šï¼Œåœ¨ç›®æ ‡æ¨¡ç³Šçš„é«˜ä¸ç¡®å®šæ€§åœºæ™¯ä¸­å¯èƒ½è¡¨ç°ä¸ç¨³å®šã€‚\n*   **å‘å±•æ–¹å‘2ï¼šä¿¡å¿µæ„ŸçŸ¥ä¸ç®€å•è€¦åˆ**\n    *   **ä»£è¡¨æ€§å·¥ä½œ**ï¼š\n        1.  **SARI (Jonnavittula et al., 2022)** å’Œ **Mutual-Adaptation (Nikolaidis et al., 2017)**ï¼šå¼ºè°ƒåœ¨é‡å¤äº¤äº’æˆ–åŒå‘é€‚åº”ä¸­è€¦åˆæ„å›¾æ¨æ–­ä¸è¾…åŠ©ã€‚\n        2.  **IDA (McMahan et al., 2024)**ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºå‰¯é©¾é©¶ï¼Œä»…åœ¨åˆ¤æ–­ä¸“å®¶åŠ¨ä½œä¼˜äºäººç±»åŠ¨ä½œæ—¶è¿›è¡Œ**äºŒå…ƒå¹²é¢„**ï¼ˆåˆ‡æ¢æ§åˆ¶æƒï¼‰ã€‚\n        3.  **Ohç­‰äºº (2021) çš„â€œåˆ†æ­§æ–¹æ³•â€**ï¼šåŸºäºå†¯Â·ç±³å¡æ–¯åˆ†å¸ƒè¿›è¡Œæ¨¡æ€æµ‹è¯•ï¼Œå®ç°è‡ªä¸»æƒåˆ‡æ¢ã€‚\n    *   **æ”¹è¿›**ï¼š\n        *   è®¤è¯†åˆ°åˆ†ç¦»æ¶æ„çš„ä¸è¶³ï¼Œå¼€å§‹æ¢ç´¢æ¨æ–­ä¸æ§åˆ¶çš„**äº¤äº’**ã€‚\n        *   IDA ç­‰æ–¹æ³•å¼•å…¥äº†å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰æ¥æä¾›é«˜è´¨é‡çš„ä¸“å®¶å»ºè®®ã€‚\n    *   **å±€é™**ï¼š\n        *   **ä¿¡æ¯åˆ©ç”¨ä¸å……åˆ†**ï¼šå¤§å¤šä»ä¾èµ–**æœ€å¤§åéªŒï¼ˆMAPï¼‰ä¼°è®¡**ï¼ˆåªå–æ¦‚ç‡æœ€é«˜çš„ç›®æ ‡ï¼‰æˆ–**äºŒå…ƒä¿¡å·**ï¼ˆå¹²é¢„/ä¸å¹²é¢„ï¼‰ï¼Œä¸¢å¼ƒäº†å®Œæ•´ä¿¡å¿µåˆ†å¸ƒä¸­åŒ…å«çš„å®è´µä¸ç¡®å®šæ€§ä¿¡æ¯ã€‚\n        *   **è€¦åˆç¨‹åº¦æœ‰é™**ï¼šé€šå¸¸æ˜¯å¯å‘å¼é˜ˆå€¼æˆ–ä½¿ç”¨**å†»ç»“çš„**ä¿¡å¿µæ¨¡å—è®­ç»ƒè¾…åŠ©ç½‘ç»œï¼Œå¹¶éçœŸæ­£çš„**è”åˆä¼˜åŒ–**ã€‚æ¨æ–­æ¨¡å—çš„å‚æ•°ä¸éšæ§åˆ¶æ€§èƒ½çš„æ¢¯åº¦è€Œæ›´æ–°ã€‚\n\n### ç¬¬å››é˜¶æ®µï¼šå®Œå…¨ä¿¡å¿µæ¡ä»¶åŒ–ä¸ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼ˆæœ¬è®ºæ–‡ BRACEï¼‰\n**æ ¸å¿ƒæ€æƒ³**ï¼šå®ç°**æ„å›¾æ¨æ–­**ä¸**è¾…åŠ©ä»²è£**çš„**æ·±åº¦ã€åŒå‘ã€æ¢¯åº¦é©±åŠ¨çš„è”åˆä¼˜åŒ–**ï¼Œå¹¶ä»¥**å®Œæ•´ä¿¡å¿µåˆ†å¸ƒ**ä½œä¸ºæ¡ä»¶è¾“å…¥çš„æ ¸å¿ƒã€‚\n*   **ä»£è¡¨æ€§å·¥ä½œ**ï¼šæœ¬è®ºæ–‡æå‡ºçš„ **BRACE (Bayesian Reinforcement Assistance with Context Encoding)** æ¡†æ¶ã€‚\n*   **æ ¸å¿ƒæ”¹è¿›**ï¼ˆé’ˆå¯¹å‰è¿°æ‰€æœ‰é˜¶æ®µçš„å±€é™ï¼‰ï¼š\n    1.  **å…¨ä¿¡å¿µæ¡ä»¶åŒ–**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šè¾…åŠ©ç­–ç•¥ `Î³ = f_Î¸(s, b, c)` çš„è¾“å…¥æ˜¯å®Œæ•´çš„ä¿¡å¿µå‘é‡ `b âˆˆ â„^|G|`ï¼Œè€Œéå•ä¸€çš„MAPä¼°è®¡ã€‚\n        *   **ä»·å€¼**ï¼šä½¿ç³»ç»Ÿèƒ½å¯¹ä¸ç¡®å®šæ€§åšå‡º**ç²¾ç»†ã€è¿ç»­çš„ååº”**ï¼ˆä¾‹å¦‚ï¼Œåœ¨å¤šä¸ªç›®æ ‡æ¦‚ç‡ç›¸å½“æ—¶å¤§å¹…é™ä½è¾…åŠ©æ°´å¹³ä»¥ä¿æŠ¤ç”¨æˆ·èƒ½åŠ¨æ€§ï¼‰ã€‚\n    2.  **ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šé€šè¿‡åŒå¤´ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä½¿ä»»åŠ¡å›æŠ¥äº§ç”Ÿçš„æ¢¯åº¦å¯ä»¥**åå‘ä¼ æ’­è‡³è´å¶æ–¯æ¨æ–­æ¨¡å—**ï¼Œä»è€Œæ ¹æ®**å†³ç­–æœ‰ç”¨æ€§**æ¥å¡‘é€ ä¿¡å¿µã€‚\n        *   **ä»·å€¼**ï¼šä»æ ¹æœ¬ä¸Šè§£å†³äº†â€œä¼°è®¡å™¨-æ§åˆ¶å™¨ä¸åŒ¹é…â€é—®é¢˜ã€‚æ¨æ–­æ¨¡å—å­¦ä¹ äº§ç”Ÿé‚£äº›èƒ½**æœ€å¤§åŒ–æ•´ä½“ä»»åŠ¡æ€§èƒ½**çš„ä¿¡å¿µï¼Œè€Œä¸ä»…ä»…æ˜¯é¢„æµ‹æœ€å‡†ç¡®çš„ä¿¡å¿µã€‚\n    3.  **ç†è®ºä¿éšœä¸å¯è§£é‡Šæ€§**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šæä¾›äº†**å®šç†è¯æ˜**ï¼ˆTheorem 1 & 2ï¼‰ï¼š\n            *   **å®šç†1**ï¼šæœ€ä¼˜è¾…åŠ©æ°´å¹³åº”éš**ç›®æ ‡ä¸ç¡®å®šæ€§ï¼ˆç†µï¼‰å¢åŠ è€Œé™ä½**ï¼Œéš**ç¯å¢ƒçº¦æŸä¸¥é‡æ€§å¢åŠ è€Œå¢åŠ **ã€‚è¿™ä¸ºç³»ç»Ÿè¡Œä¸ºæä¾›äº†å¯è§£é‡Šçš„ã€ç¬¦åˆç›´è§‰çš„åŸåˆ™ã€‚\n            *   **å®šç†2**ï¼šç›¸æ¯”åŸºäºMAPçš„åºåˆ—åŒ–æ–¹æ³•ï¼Œè¿™ç§ä¿¡å¿µæ¡ä»¶åŒ–è”åˆä¼˜åŒ–èƒ½å¸¦æ¥**äºŒæ¬¡æ–¹æœŸæœ›é—æ†¾ä¼˜åŠ¿**ã€‚ä¼˜åŠ¿åœ¨ç›®æ ‡æ¨¡ç³Šã€ä¸åŒç›®æ ‡éœ€è¦ä¸åŒè¾…åŠ©ç­–ç•¥çš„åœºæ™¯ä¸­æœ€æ˜æ˜¾ã€‚\n    4.  **è¿ç»­ã€ä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„ä»²è£**ï¼š\n        *   **æ”¹è¿›ç‚¹**ï¼šè¾“å‡ºè¿ç»­çš„æ··åˆå‚æ•° `Î³ âˆˆ [0,1]`ï¼Œå¹¶åŒæ—¶ä»¥ç¯å¢ƒä¸Šä¸‹æ–‡ `c`ï¼ˆå¦‚éšœç¢ç‰©è·ç¦»ï¼‰ä¸ºæ¡ä»¶ã€‚\n        *   **ä»·å€¼**ï¼šå®ç°äº†å¹³æ»‘ã€åŠ¨æ€çš„è¾…åŠ©è°ƒæ•´ï¼Œé¿å…äº†äºŒå…ƒåˆ‡æ¢å¸¦æ¥çš„çªå…€æ„Ÿï¼Œå¹¶èƒ½æ™ºèƒ½åœ°åœ¨ä»»åŠ¡å›°éš¾é˜¶æ®µï¼ˆå¦‚ç©¿è¶Šç‹­çª„é€šé“ï¼‰æä¾›æ›´å¤šå¸®åŠ©ã€‚\n\n### æ€»ç»“ï¼šå‡ ä¸ªå¹³è¡Œçš„å‘å±•æ–¹å‘\n1.  **ä»é™æ€è§„åˆ™åˆ°åŠ¨æ€å­¦ä¹ **ï¼šä»å›ºå®šÎ³æˆ–å¯å‘å¼è§„åˆ™ï¼Œå‘å±•åˆ°åŸºäºæ¨¡å‹ä¼˜åŒ–ï¼Œå†åˆ°å®Œå…¨ä»æ•°æ®ä¸­å­¦ä¹ çš„RLç­–ç•¥ã€‚\n2.  **ä»ç¡®å®šæ„å›¾åˆ°æ¦‚ç‡ä¿¡å¿µ**ï¼šä»å‡è®¾å·²çŸ¥æˆ–ç‚¹ä¼°è®¡ç›®æ ‡ï¼Œå‘å±•åˆ°æ˜¾å¼å»ºæ¨¡å’Œç»´æŠ¤å®Œæ•´çš„æ¦‚ç‡ä¿¡å¿µåˆ†å¸ƒã€‚\n3.  **ä»åºåˆ—åˆ†ç¦»åˆ°ç«¯åˆ°ç«¯è”åˆ**ï¼šä»â€œå…ˆæ¨æ–­ï¼Œåæ§åˆ¶â€çš„ç®¡é“ï¼Œå‘å±•åˆ°åŒå‘æ¢¯åº¦æµè€¦åˆçš„é›†æˆç³»ç»Ÿã€‚\n4.  **ä»äºŒå…ƒå¹²é¢„åˆ°è¿ç»­æ··åˆ**ï¼šä»â€œå…¨æ‰‹åŠ¨â€æˆ–â€œå…¨è‡ªåŠ¨â€çš„åˆ‡æ¢ï¼Œå‘å±•åˆ°ç²¾ç»†ã€è¿ç»­çš„äººæœºæ§åˆ¶èåˆã€‚\n5.  **ä»å¿½ç•¥ä¸Šä¸‹æ–‡åˆ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šè¾…åŠ©å†³ç­–ä¸ä»…è€ƒè™‘ç”¨æˆ·æ„å›¾ï¼Œè¿˜ç»¼åˆè€ƒè™‘ç¯å¢ƒçº¦æŸã€ä»»åŠ¡é˜¶æ®µç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n\n**BRACE æ¡†æ¶çš„è´¡çŒ®**åœ¨äºï¼Œå®ƒ**æ±‡èšå¹¶æ·±åŒ–äº†è¿™äº›å‘å±•æ–¹å‘**ï¼šå®ƒé‡‡ç”¨äº†**å­¦ä¹ å‹ç­–ç•¥ï¼ˆæ–¹å‘1ï¼‰**ï¼Œä»¥**å®Œæ•´æ¦‚ç‡ä¿¡å¿µï¼ˆæ–¹å‘2ï¼‰** å’Œ**ç¯å¢ƒä¸Šä¸‹æ–‡**ä¸ºæ¡ä»¶ï¼Œé€šè¿‡**ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ï¼ˆæ–¹å‘3ï¼‰** æ¥äº§ç”Ÿ**è¿ç»­çš„è‡ªé€‚åº”è¾…åŠ©ï¼ˆæ–¹å‘4ï¼‰**ï¼Œå¹¶æä¾›äº†æ”¯æ’‘å…¶ä¼˜è¶Šæ€§çš„**ç†è®ºè§£é‡Šï¼ˆæ–¹å‘5çš„æ·±åŒ–ï¼‰**ã€‚å…¶å®éªŒéªŒè¯ä¹Ÿç³»ç»Ÿæ€§åœ°æ¶µç›–äº†ä»åŸºç¡€äººæœºäº¤äº’ã€å¤æ‚ç‰©ç†åŠ¨åŠ›å­¦åˆ°é›†æˆä¸‰ç»´æ“ä½œçš„å…¨é“¾æ¡æŒ‘æˆ˜ã€‚",
            "timestamp": "2026-02-03T16:03:24.531935",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆç‰¹åˆ«æ˜¯MH Farhadi, Ali Rabiee, Reza Abiriç­‰äººï¼‰åœ¨å…±äº«è‡ªæ²»å’Œäººæœºäº¤äº’é¢†åŸŸæœ‰ä¸€ç³»åˆ—ç›¸å…³ç ”ç©¶ã€‚ä»¥ä¸‹æ˜¯å…¶ä¸»è¦å‰ä½œçš„æ¢³ç†ï¼Œä»¥åŠä¸æœ¬æ–‡BRACEæ¡†æ¶çš„æ ¸å¿ƒåŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n### å‰ä½œæ¦‚è§ˆè¡¨\n\n| è®ºæ–‡æ ‡é¢˜ / å¼•ç”¨æ ‡è¯† | å‘è¡¨å¹´ä»½/çŠ¶æ€ | æ ¸å¿ƒæ–¹æ³•/æ¡†æ¶ | ä¸»è¦è´¡çŒ®ä¸ç‰¹ç‚¹ | ä¸æœ¬æ–‡BRACEçš„ä¸»è¦åŒºåˆ«åŠBRACEçš„æ”¹è¿› |\n| :--- | :--- | :--- | :--- | :--- |\n| **STREAMS** <br> (Rabiee et al., 2024, `[undefab]`) | 2024 (arXiv) | å¤šæ¨¡æ€AIè¾…åŠ©æ¡†æ¶ï¼Œç”¨äºå¢å¼ºåŸºäºç”Ÿç‰©ä¿¡å·çš„æœºå™¨äººæ§åˆ¶ã€‚ | 1. **å¤šæ¨¡æ€èåˆ**ï¼šæ•´åˆEEGã€çœ¼åŠ¨ç­‰ç”Ÿç‰©ä¿¡å·è¿›è¡Œæ„å›¾æ¨æ–­å’Œæ§åˆ¶ã€‚<br>2. **è¾…åŠ©æ¡†æ¶**ï¼šæ—¨åœ¨å°†æœ‰é™çš„ç”¨æˆ·è¾“å…¥æ”¾å¤§åˆ°é«˜ç»´æ§åˆ¶ç©ºé—´ã€‚<br>3. **åº”ç”¨äºæœºå™¨äººæ§åˆ¶**ï¼šä¸“æ³¨äºä»ç”Ÿç‰©ä¿¡å·åˆ°æœºå™¨äººåŠ¨ä½œçš„æ˜ å°„ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **æ„å›¾æ¨æ–­ä¸ç­–ç•¥è€¦åˆæ–¹å¼**ï¼šSTREAMS ä¾§é‡äº**å¤šæ¨¡æ€ä¿¡å·èåˆ**ä»¥å¢å¼ºè¾“å…¥ï¼Œè€ŒBRACE çš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**æ„å›¾æ¨æ–­æ¨¡å—ä¸è¾…åŠ©ä»²è£ç­–ç•¥çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**ã€‚<br>2. **ä¼˜åŒ–ç›®æ ‡**ï¼šSTREAMS å¯èƒ½æ›´å…³æ³¨ä¿¡å·è§£ç çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼›BRACE æ˜ç¡®æå‡ºäº†ä¸€ä¸ª**å•ä¸€æ§åˆ¶ç›®æ ‡**ï¼Œé€šè¿‡æ¢¯åº¦æµåŒæ—¶ä¼˜åŒ–æ¨æ–­å’Œç­–ç•¥ï¼Œä½¿ä¿¡å¿µåˆ†å¸ƒå˜å¾—å¯¹å†³ç­–æ›´æœ‰ç”¨ã€‚<br>3. **ç†è®ºæ·±åº¦**ï¼šBRACE æä¾›äº†**ä¸¥æ ¼çš„ç†è®ºåˆ†æ**ï¼ˆå¦‚å®šç†1ã€2ï¼‰ï¼Œè¯æ˜äº†åœ¨ç›®æ ‡ä¸ç¡®å®šæ€§å’Œç¯å¢ƒçº¦æŸä¸‹æœ€ä¼˜è¾…åŠ©æ°´å¹³çš„å•è°ƒæ€§ï¼Œä»¥åŠç›¸æ¯”MAPæ–¹æ¡ˆçš„äºŒæ¬¡é—æ†¾ä¼˜åŠ¿ï¼Œè¿™æ˜¯STREAMSæœªæ¶‰åŠçš„ã€‚ |\n| **Learning multimodal AI algorithms...** <br> (Rabiee et al., 2025, `[undefaa]`) | 2025 (arXiv) | å­¦ä¹ å¤šæ¨¡æ€AIç®—æ³•ï¼Œå°†æœ‰é™ç”¨æˆ·è¾“å…¥æ”¾å¤§åˆ°é«˜ç»´æ§åˆ¶ç©ºé—´ã€‚ | 1. **è¾“å…¥æ”¾å¤§**ï¼šä¸“æ³¨äºå¤„ç†ä½ç»´ã€æœ‰å™ªå£°æˆ–ä¸å®Œæ•´çš„ç”¨æˆ·è¾“å…¥ï¼ˆå¦‚æ¥è‡ªå—æŸè¿åŠ¨åŠŸèƒ½ï¼‰ã€‚<br>2. **å¤šæ¨¡æ€å­¦ä¹ **ï¼šå¯èƒ½åˆ©ç”¨å¤šç§æ•°æ®æ¨¡å¼æ¥è¡¥å¿å•ä¸€æ¨¡æ€çš„ä¸å¯é æ€§ã€‚<br>3. **åº”ç”¨äºé«˜ç»´æ§åˆ¶**ï¼šæ—¨åœ¨å®ç°å¤æ‚æœºå™¨äººä»»åŠ¡çš„æ§åˆ¶ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **é—®é¢˜ç„¦ç‚¹**ï¼šè¯¥å‰ä½œè§£å†³çš„æ˜¯**â€œè¾“å…¥ä¸è¶³â€** çš„é—®é¢˜ï¼Œå³å¦‚ä½•ä»å¼±ä¿¡å·ä¸­æå–æœ‰æ•ˆå‘½ä»¤ã€‚BRACE è§£å†³çš„æ˜¯**â€œå†³ç­–ä¼˜åŒ–â€** é—®é¢˜ï¼Œå³åœ¨ç»™å®šç”¨æˆ·è¾“å…¥ï¼ˆæ— è®ºæ¥æºï¼‰åï¼Œå¦‚ä½•æœ€ä¼˜åœ°æ··åˆäººç±»å’Œä¸“å®¶æ§åˆ¶ã€‚<br>2. **ç³»ç»Ÿæ¶æ„**ï¼šå‰ä½œå¯èƒ½æ˜¯ä¸€ä¸ªæ›´å‰ç«¯çš„ä¿¡å·å¤„ç†ä¸ç‰¹å¾æå–ç®¡é“ã€‚BRACE æ˜¯ä¸€ä¸ª**ä¸­åç«¯å†³ç­–æ¡†æ¶**ï¼Œå®ƒæ¥æ”¶ï¼ˆå¯èƒ½å·²ç»é¢„å¤„ç†è¿‡çš„ï¼‰ç”¨æˆ·åŠ¨ä½œå’ŒçŠ¶æ€ï¼Œè¾“å‡ºæ··åˆç³»æ•°Î³ã€‚ä¸¤è€…å¯ä»¥äº’è¡¥ã€‚<br>3. **è”åˆä¼˜åŒ–**ï¼šBRACE çš„å…³é”®æ”¹è¿›åœ¨äºå…¶**åŒå¤´æ¶æ„çš„ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼Œä½¿æ„å›¾æ¨æ–­ä¸å†æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€å†»ç»“çš„æ¨¡å—ï¼Œè€Œæ˜¯èƒ½æ ¹æ®ä¸‹æ¸¸æ§åˆ¶æ€§èƒ½è¿›è¡Œå¾®è°ƒã€‚ |\n| **Human-Centered Shared Autonomy...** <br> (Farhadi et al., 2025, `[undeff]`) | 2025 (arXiv) | é¢å‘è¿åŠ¨è§„åˆ’ã€å­¦ä¹ ä¸æ§åˆ¶åº”ç”¨çš„äººæœ¬å…±äº«è‡ªæ²»ã€‚ | 1. **ä»¥äººä¸ºä¸­å¿ƒ**ï¼šå¼ºè°ƒç”¨æˆ·ä½“éªŒã€ä»£ç†æ„Ÿå’Œé€æ˜åº¦ã€‚<br>2. **åº”ç”¨äºè¿åŠ¨è§„åˆ’ä¸æ§åˆ¶**ï¼šå¯èƒ½æ¢è®¨äº†åœ¨è¿åŠ¨å­¦ä¹ èƒŒæ™¯ä¸‹å…±äº«è‡ªæ²»çš„è®¾è®¡åŸåˆ™ã€‚<br>3. **ç»¼åˆæ€§è§†è§’**ï¼šå¯èƒ½æ¶µç›–äº†ä»ç†è®ºåˆ°ç”¨æˆ·ç ”ç©¶çš„å¤šä¸ªæ–¹é¢ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **æ–¹æ³•è®ºå…·ä½“åŒ–**ï¼šè¯¥å‰ä½œå¯èƒ½æå‡ºäº†**æ›´å¹¿æ³›çš„è®¾è®¡ç†å¿µå’ŒåŸåˆ™**ã€‚BRACE æ˜¯è¿™äº›åŸåˆ™çš„ä¸€ä¸ª**å…·ä½“ç®—æ³•å®ä¾‹åŒ–å’Œå®ç°**ï¼Œæå‡ºäº†ä¸€ä¸ªå¯è®­ç»ƒã€å¯ç†è®ºåˆ†æçš„æ¡†æ¶ï¼ˆBRACEï¼‰ã€‚<br>2. **æŠ€æœ¯åˆ›æ–°**ï¼šBRACE æ˜ç¡®äº†**ä¸‰ä¸ªå…·ä½“æŠ€æœ¯è´¡çŒ®**ï¼ˆå…¨ä¿¡å¿µæ¡ä»¶åŒ–ã€ç«¯åˆ°ç«¯è€¦åˆã€é—æ†¾ç•Œè¯æ˜ï¼‰ï¼Œè¿™æ˜¯å¯¹å‰ä½œä¸­å¯èƒ½æå‡ºçš„æ¦‚å¿µçš„å…·ä½“æŠ€æœ¯å’Œç†è®ºè½å®ã€‚<br>3. **éªŒè¯é‡ç‚¹**ï¼šBRACE é€šè¿‡**ä¸¥æ ¼çš„ä¸‰éƒ¨åˆ†å®éªŒ**ï¼ˆ2Då…‰æ ‡ã€Reacher-2Dã€Fetchæœºæ¢°è‡‚ï¼‰ç³»ç»Ÿæ€§åœ°éªŒè¯äº†å…¶ç®—æ³•ä¼˜åŠ¿ï¼Œè€Œå‰ä½œå¯èƒ½æ›´ä¾§é‡äºæ¦‚å¿µè®ºè¯æˆ–åˆæ­¥å®éªŒã€‚ |\n| **A usability study of low-cost wireless BCI for cursor control...** <br> (Abiri et al., 2020, `[undef]`) | 2020 (IEEE Trans.) | åŸºäºåœ¨çº¿çº¿æ€§æ¨¡å‹çš„ä½æˆæœ¬æ— çº¿è„‘æœºæ¥å£ï¼ˆBCIï¼‰å…‰æ ‡æ§åˆ¶å¯ç”¨æ€§ç ”ç©¶ã€‚ | 1. **BCIè§£ç **ï¼šå¼€å‘å¹¶æµ‹è¯•äº†ç”¨äºå…‰æ ‡æ§åˆ¶çš„EEGè§£ç ç®—æ³•ã€‚<br>2. **ç”¨æˆ·ç ”ç©¶**ï¼šæä¾›äº†çœŸå®äººç±»EEGæ§åˆ¶è½¨è¿¹çš„æ•°æ®é›†å’Œåˆ†æã€‚<br>3. **åŸºç¡€æŠ€æœ¯**ï¼šä¸ºåŸºäºç”Ÿç‰©ä¿¡å·çš„äº¤äº’æä¾›äº†åº•å±‚è¾“å…¥æŠ€æœ¯ã€‚ | **åŒºåˆ«ä¸æ”¹è¿›**ï¼š<br>1. **ç ”ç©¶å±‚çº§**ï¼šè¯¥å·¥ä½œæ˜¯**åº•å±‚è¾“å…¥æŠ€æœ¯**ç ”ç©¶ï¼Œæä¾›æ§åˆ¶ä¿¡å·ã€‚BRACE æ˜¯**é«˜å±‚å†³ç­–ä¸è¾…åŠ©**æ¡†æ¶ï¼Œå®ƒä¸å…³å¿ƒä¿¡å·å¦‚ä½•äº§ç”Ÿï¼Œè€Œæ˜¯å…³å¿ƒå¦‚ä½•åˆ©ç”¨è¿™äº›ä¿¡å·è¿›è¡Œæœ€ä¼˜è¾…åŠ©ã€‚<br>2. **ç›´æ¥è”ç³»**ï¼šBRACE è®ºæ–‡ä¸­çš„**æ¨¡æ‹Ÿäººç±»ä»£ç†æ¨¡å‹**ï¼ˆé™„å½•Dï¼‰æ­£æ˜¯åŸºäºæ­¤é¡¹ç ”ç©¶ï¼ˆåŠå…¶ä»–ï¼‰æ”¶é›†çš„çœŸå®äººç±»è½¨è¿¹æ•°æ®æ„å»ºçš„ï¼Œä»¥ç¡®ä¿æ¨¡æ‹Ÿçš„å™ªå£°å’Œè¡Œä¸ºæ¨¡å¼ç¬¦åˆç°å®ã€‚å› æ­¤ï¼Œè¯¥å‰ä½œä¸ºBRACEçš„éªŒè¯æä¾›äº†é‡è¦çš„**ç°å®åŸºç¡€å’Œæ•°æ®**ã€‚<br>3. **æ ¸å¿ƒæ”¹è¿›**ï¼šBRACE å°†æ­¤ç±»åº•å±‚æ§åˆ¶é—®é¢˜æå‡åˆ°äº†**å¦‚ä½•ä¸AIæ™ºèƒ½ä½“æœ€ä¼˜åä½œ**çš„å±‚é¢ï¼Œå¼•å…¥äº†ä¿¡å¿µæ¨ç†å’Œç­–ç•¥å­¦ä¹ çš„è”åˆä¼˜åŒ–ï¼Œè¿™æ˜¯å•çº¯çš„BCIè§£ç ç ”ç©¶æœªæ¶µç›–çš„ã€‚ |\n\n### æ€»ç»“ï¼šBRACEç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒæ”¹åŠ¨ä¸æ”¹å–„\n\n1.  **ä»æ¨¡å—åŒ–åˆ°ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**ï¼š\n    *   **å‰ä½œ**ï¼šå¤šä¸º**é¡ºåºæˆ–å¹¶è¡Œæ¶æ„**ã€‚ä¾‹å¦‚ï¼ŒSTREAMSå¯èƒ½å…ˆè¿›è¡Œå¤šæ¨¡æ€èåˆè§£ç ï¼Œå†å°†è§£ç ç»“æœé€ç»™æ§åˆ¶å™¨ï¼›BCIç ”ç©¶ä¸“æ³¨äºç”Ÿæˆå¹²å‡€çš„è¾“å…¥ä¿¡å·ã€‚æ„å›¾æ¨æ–­å’Œè¾…åŠ©å†³ç­–æ˜¯åˆ†ç¦»çš„ã€‚\n    *   **BRACE**ï¼šæå‡º**åŒå¤´ç¥ç»ç½‘ç»œæ¶æ„**ï¼Œå®ç°**ç«¯åˆ°ç«¯æ¢¯åº¦æµ**ã€‚æ§åˆ¶ç­–ç•¥çš„æ€§èƒ½æ¢¯åº¦å¯ä»¥åå‘ä¼ æ’­å¹¶**å¾®è°ƒè´å¶æ–¯æ„å›¾æ¨æ–­æ¨¡å—**çš„å‚æ•°ã€‚è¿™ä½¿å¾—ä¿¡å¿µåˆ†å¸ƒèƒ½å¤Ÿæ ¹æ®â€œä»€ä¹ˆå¯¹å†³ç­–æœ€æœ‰ç”¨â€è€Œè°ƒæ•´ï¼Œé¿å…äº†ä¼°è®¡å™¨-æ§åˆ¶å™¨ä¸åŒ¹é…ã€‚\n\n2.  **ä»ç‚¹ä¼°è®¡åˆ°å…¨ä¿¡å¿µåˆ†å¸ƒåˆ©ç”¨**ï¼š\n    *   **å‰ä½œ/å¸¸è§åŸºçº¿**ï¼šå¦‚IDAã€SARIç­‰æ–¹æ³•ï¼Œé€šå¸¸ä¾èµ–**æœ€å¤§åéªŒæ¦‚ç‡ï¼ˆMAPï¼‰ä¼°è®¡**ï¼ˆå³å•ä¸€æœ€å¯èƒ½ç›®æ ‡ï¼‰ï¼Œæˆ–ä½¿ç”¨å¯å‘å¼é˜ˆå€¼/äºŒå…ƒå¹²é¢„ã€‚\n    *   **BRACE**ï¼š**æ¡ä»¶åŒ–äºå®Œæ•´çš„ä¿¡å¿µåˆ†å¸ƒå‘é‡ `b âˆˆ â„^{|G|}`**ã€‚è¾…åŠ©æ°´å¹³Î³æ˜¯çŠ¶æ€ã€å®Œæ•´ä¿¡å¿µå’Œä¸Šä¸‹æ–‡çš„å‡½æ•°ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¯¹ä¸ç¡®å®šæ€§åšå‡ºæ›´ç²¾ç»†çš„ååº”ï¼ˆä¾‹å¦‚ï¼Œåœ¨å¤šä¸ªç›®æ ‡æ¦‚ç‡ç›¸å½“æ—¶é™ä½è¾…åŠ©å¼ºåº¦ä»¥ä¿æŒç”¨æˆ·ä»£ç†æ„Ÿï¼‰ã€‚\n\n3.  **ä»ç»éªŒéªŒè¯åˆ°ç†è®ºä¿éšœ**ï¼š\n    *   **å‰ä½œ**ï¼šæ”¹å–„å¤šåŸºäº**ç»éªŒæ€§èƒ½æå‡**ï¼ˆå¦‚æˆåŠŸç‡ã€ä»»åŠ¡æ—¶é—´ï¼‰ã€‚\n    *   **BRACE**ï¼šæä¾›äº†**ä¸¥æ ¼çš„ç†è®ºåˆ†æ**ï¼ˆ**å®šç†1ã€2**ï¼‰ï¼š\n        *   è¯æ˜äº†æœ€ä¼˜è¾…åŠ©æ°´å¹³åº”éš**ç›®æ ‡ä¸ç¡®å®šæ€§ï¼ˆç†µï¼‰å¢åŠ è€Œå‡å°‘**ï¼Œéš**ç¯å¢ƒçº¦æŸä¸¥é‡æ€§å¢åŠ è€Œå¢åŠ **ã€‚\n        *   è¯æ˜äº†ç›¸æ¯”åŸºäºMAPçš„é¡ºåºæ–¹æ³•ï¼Œé›†æˆä¼˜åŒ–èƒ½å¸¦æ¥**äºŒæ¬¡æœŸæœ›é—æ†¾ä¼˜åŠ¿**ã€‚è¿™ä¸ºæ€§èƒ½æå‡æä¾›äº†ç†è®ºè§£é‡Šï¼Œå¹¶æŒ‡æ˜äº†ä¼˜åŠ¿æœ€æ˜æ˜¾çš„åœºæ™¯ï¼ˆé«˜ç›®æ ‡ä¸ç¡®å®šæ€§ã€ä¸åŒç›®æ ‡éœ€è¦ä¸åŒè¾…åŠ©ç­–ç•¥æ—¶ï¼‰ã€‚\n\n4.  **ç³»ç»ŸåŒ–çš„è¯„ä¼°ä¸æ³›åŒ–éªŒè¯**ï¼š\n    *   **å‰ä½œ**ï¼šè¯„ä¼°å¯èƒ½é›†ä¸­äºç‰¹å®šä»»åŠ¡æˆ–è¾“å…¥æ¨¡æ€ï¼ˆå¦‚BCIå…‰æ ‡æ§åˆ¶ï¼‰ã€‚\n    *   **BRACE**ï¼šè®¾è®¡äº†**ä¸‰å±‚é€’è¿›çš„è¯„ä¼°ä½“ç³»**ï¼Œé€æ­¥åˆ†ç¦»å¹¶éªŒè¯æ ¸å¿ƒæŒ‘æˆ˜ï¼šäººæœºäº¤äº’åŠ¨æ€ï¼ˆ2Då…‰æ ‡ç”¨æˆ·ç ”ç©¶ï¼‰ã€éçº¿æ€§ç‰©ç†åŠ¨åŠ›å­¦ï¼ˆReacher-2Dï¼‰ã€ç»¼åˆä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆFetchæœºæ¢°è‡‚æ‹¾æ”¾ï¼‰ã€‚è¿™è¯æ˜äº†BRACEçš„**æ³›åŒ–èƒ½åŠ›**è·¨è¶Šä¸åŒæœºå™¨äººé¢†åŸŸã€‚é™„å½•è¿˜è¯¦ç»†éªŒè¯äº†å…¶å¯¹ä¸åŒè¾“å…¥æ¨¡æ€ï¼ˆåŠ›ä¼ æ„Ÿ vs. æ¸¸æˆæ‰‹æŸ„ï¼‰çš„é²æ£’æ€§ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒBRACEçš„æ ¸å¿ƒæ”¹è¿›åœ¨äºå°†å…±äº«è‡ªæ²»ä¸­çš„æ„å›¾æ¨æ–­å’Œè¾…åŠ©å†³ç­–è¿™ä¸¤ä¸ªä¼ ç»Ÿä¸Šåˆ†ç¦»çš„å…³é”®è¿‡ç¨‹ï¼Œé€šè¿‡ä¸€ä¸ªå¯ç«¯åˆ°ç«¯è®­ç»ƒçš„ã€ä¿¡å¿µæ¡ä»¶åŒ–çš„æ¡†æ¶æ·±åº¦è€¦åˆèµ·æ¥ï¼Œå¹¶è¾…ä»¥ç†è®ºè¯æ˜å’Œç³»ç»ŸåŒ–å®éªŒï¼Œå®ç°äº†åœ¨å¤æ‚ã€ä¸ç¡®å®šç¯å¢ƒä¸­æ€§èƒ½ä¸ç”¨æˆ·ä»£ç†æ„Ÿä¹‹é—´æ›´ä¼˜çš„å¹³è¡¡ã€‚** å®ƒæ˜¯å¯¹å›¢é˜Ÿå‰æœŸåœ¨BCIã€å¤šæ¨¡æ€èåˆå’Œäººæœ¬è®¾è®¡ç­‰æ–¹é¢å·¥ä½œçš„ä¸€ä¸ªé«˜å±‚æ¬¡é›†æˆä¸ç®—æ³•æ·±åŒ–ã€‚",
            "timestamp": "2026-02-03T16:04:36.243876",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºçš„æ ¸å¿ƒæ¡†æ¶æ˜¯ **BRACE**ï¼Œå…¶å…³é”®æŠ€æœ¯æ–¹æ³•å¯å½’çº³ä¸ºä»¥ä¸‹å››ä¸ªæ–¹é¢ï¼Œå…·ä½“ç»†èŠ‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| å…³é”®æŠ€æœ¯æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…·ä½“å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. åŸºäºå®Œæ•´ä¿¡å¿µåˆ†å¸ƒçš„æ¡ä»¶åŒ–ç­–ç•¥** | æ‘’å¼ƒä¼ ç»Ÿçš„æœ€å¤§åéªŒä¼°è®¡ï¼Œå°†**æ•´ä¸ªç›®æ ‡åéªŒæ¦‚ç‡åˆ†å¸ƒ**ä½œä¸ºè¾…åŠ©ç­–ç•¥çš„è¾“å…¥ï¼Œå®ç°å¯¹ç”¨æˆ·æ„å›¾ä¸ç¡®å®šæ€§çš„ç²¾ç»†åŒ–å“åº”ã€‚ | **æ•°å­¦åŸç†**ï¼š<br>è¾…åŠ©æ°´å¹³ `Î³` æ˜¯çŠ¶æ€ `s`ã€ä¿¡å¿µ `b` å’Œç¯å¢ƒçº¦æŸ `c` çš„å‡½æ•°ï¼š<br>`Î³ = f_Î¸(s, b, c)`<br>å…¶ä¸­ `b âˆˆ â„^|G|` æ˜¯ç›®æ ‡é›†åˆ `G` ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒå‘é‡ã€‚ç­–ç•¥ä¼˜åŒ–åŸºäºæœŸæœ›æ•ˆç”¨ï¼š<br>`Åª(Î³; s, b, c) = ğ”¼_(gâˆ¼b)[U_g(Î³; s, c)]`<br>æœ€ä¼˜è¾…åŠ© `Î³*` é€šè¿‡æœ€å¤§åŒ–æ­¤æœŸæœ›æ•ˆç”¨è·å¾—ï¼š<br>`Î³*(s, b, c) = arg max_(Î³âˆˆ[0,1]) Åª(Î³; s, b, c)`<br>**ç†è®ºè´¡çŒ®**ï¼ˆå®šç†1ï¼‰ï¼šè¯æ˜äº†æœ€ä¼˜è¾…åŠ©æ°´å¹³ `Î³*` éšä¿¡å¿µç†µ `H(b)` å¢åŠ è€Œ**å‡å°**ï¼Œéšç¯å¢ƒçº¦æŸä¸¥é‡æ€§ `c` å¢åŠ è€Œ**å¢åŠ **ã€‚ | **ä¿¡å¿µå‘é‡ç»´åº¦**ï¼š`|G|`ï¼Œå³æ½œåœ¨ç›®æ ‡çš„æ•°é‡ã€‚<br>**ç­–ç•¥ç½‘ç»œè¾“å…¥**ï¼šçŠ¶æ€ `s` (å¦‚ä½ç½®ã€é€Ÿåº¦) + ä¿¡å¿µå‘é‡ `b` + ä¸Šä¸‹æ–‡ç‰¹å¾ `c` (å¦‚åˆ°æœ€è¿‘éšœç¢ç‰©çš„è·ç¦»)ã€‚<br>**è¾“å‡º**ï¼šè¿ç»­è¾…åŠ©å‚æ•° `Î³ âˆˆ [0, 1]`ã€‚ |\n| **2. æ„å›¾æ¨æ–­ä¸è¾…åŠ©ä»²è£çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–** | é€šè¿‡å…è®¸æ¢¯åº¦åœ¨è´å¶æ–¯æ¨æ–­æ¨¡å—å’Œç­–ç•¥ç½‘ç»œä¹‹é—´åŒå‘æµåŠ¨ï¼Œå®ç°æ¨æ–­ä¸æ§åˆ¶å‚æ•°çš„**è”åˆè®­ç»ƒ**ï¼Œä½¿ä¿¡å¿µæ›´æ–°ä¸ºä¸‹æ¸¸æ§åˆ¶ä»»åŠ¡æä¾›â€œå†³ç­–æœ‰ç”¨â€çš„ä¿¡æ¯ã€‚ | **æ•°å­¦åŸç†**ï¼š<br>é‡‡ç”¨æ··åˆæŸå¤±å‡½æ•°è¿›è¡Œè”åˆä¼˜åŒ–ï¼š<br>`â„’_total(Î¸, Ï•) = Î± * â„’_RL(Î¸, Ï•) + (1-Î±) * â„’_supervised(Ï•)`<br>1. **ç›‘ç£æŸå¤±**ï¼šé¢„è®­ç»ƒé˜¶æ®µï¼Œæœ€å¤§åŒ–çœŸå®ç›®æ ‡çš„å¯¹æ•°ä¼¼ç„¶ï¼š<br>`â„’_supervised(Ï•) = -ğ”¼[log P_Ï•(g*|X, H)]`<br>2. **å¼ºåŒ–å­¦ä¹ æŸå¤±**ï¼šä½¿ç”¨REINFORCEç®—æ³•ä¼°è®¡æ¨æ–­æ¨¡å—å‚æ•° `Ï•` çš„æ¢¯åº¦ï¼š<br>`âˆ‡_Ï• â„’_RL(Î¸, Ï•) â‰ˆ ğ”¼[ Î£_(t=0)^T R_t âˆ‡_Ï• log P_Ï•(b_t|X_(1:t), H_(1:t)) ]`<br>**ç†è®ºè´¡çŒ®**ï¼ˆå®šç†2ï¼‰ï¼šè¯æ˜äº†ä¸ä½¿ç”¨MAPä¼°è®¡çš„åºåˆ—æ–¹æ³•ç›¸æ¯”ï¼Œè”åˆä¼˜åŒ–æ–¹æ³•èƒ½è·å¾—**äºŒæ¬¡æ–¹**çš„æœŸæœ›é—æ†¾ä¼˜åŠ¿ï¼š<br>`ğ”¼_(gâˆ¼b)[U_g(Î³_g*) - U_g(Î³*)] = 1/2 ğ”¼_(gâˆ¼b)[ |U_g''(Î³Ìƒ)| (Î³_g* - Î³*)^2 ]` | **è®­ç»ƒç®—æ³•**ï¼šç®—æ³•1ï¼Œç»“åˆè¯¾ç¨‹å­¦ä¹ ã€‚<br>**ä¼˜åŒ–å™¨**ï¼šPPO (è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–)ã€‚<br>**å…³é”®è¶…å‚æ•°**ï¼š<br>- å­¦ä¹ ç‡ï¼š`3 Ã— 10^-4` (å¸¦ä½™å¼¦é€€ç«)<br>- æŠ˜æ‰£å› å­ï¼š`0.99`<br>- GAEå‚æ•°ï¼š`0.95`<br>- è£å‰ªæ¯”ç‡ï¼š`0.2`<br>- æ··åˆæƒé‡ `Î±` åœ¨è®­ç»ƒä¸­é€€ç«ã€‚ |\n| **3. é€’å½’è´å¶æ–¯æ„å›¾æ¨æ–­æ¨¡å—** | å°†ç”¨æˆ·å»ºæ¨¡ä¸ºå™ªå£°ç†æ€§ä¸»ä½“ï¼Œé€šè¿‡é€’å½’è´å¶æ–¯æ»¤æ³¢ï¼Œæ ¹æ®è§‚å¯Ÿåˆ°çš„ç”¨æˆ·è¾“å…¥å’Œæ™ºèƒ½ä½“è½¨è¿¹ï¼Œå®æ—¶æ›´æ–°å¯¹å„ä¸ªæ½œåœ¨ç›®æ ‡çš„ä¿¡å¿µã€‚ | **æ•°å­¦åŸç†**ï¼š<br>å¯¹äºç›®æ ‡ `g_i`ï¼Œå…¶åéªŒæ¦‚ç‡æ›´æ–°å…¬å¼ä¸ºï¼š<br>`P(g_i | X, H) âˆ P(g_i) Î _(j=1)^t P(h_j | x_j, g_i)`<br>**ä¼¼ç„¶å‡½æ•°**åŸºäºæˆæœ¬æ¨¡å‹ï¼š<br>`P(h_j | x_j, g_i) âˆ exp(-Î² Â· cost(h_j | x_j, g_i))`<br>**æˆæœ¬å‡½æ•°**ç»“åˆè§’åº¦åå·®å’Œè·ç¦»åå·®ï¼š<br>`cost(h_j | x_j, g_i) = w_Î¸ Â· Î¸_dev + w_d Â· d_dev`<br>- **è§’åº¦åå·®**ï¼š`Î¸_dev = | arccos( (hâƒ—_j Â· vâƒ—_(x_jâ†’g_i)) / (|hâƒ—_j| Â· |vâƒ—_(x_jâ†’g_i)|) ) |`<br>- **è·ç¦»åå·®**ï¼š`d_dev = |1 - |hâƒ—_j| / h_opt |`ï¼Œå…¶ä¸­ `h_opt` ä¸ºåˆ°ç›®æ ‡çš„æœ€ä¼˜è¾“å…¥å¹…åº¦ã€‚ | **æ¨¡å‹å‚æ•°**ï¼š<br>- ç†æ€§å‚æ•° `Î² = 10.0`<br>- æƒé‡ `w_Î¸ = 0.7`, `w_d = 0.3`<br>- å…ˆéªŒï¼šç›®æ ‡å‡åŒ€åˆ†å¸ƒ<br>- æ—¶é—´å¹³æ»‘ï¼šæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œè¡°å‡ç‡ `Î± = 0.85`<br>**æ ¡å‡†æ•°æ®**ï¼šä½¿ç”¨1968æ¡å·²çŸ¥ç›®æ ‡çš„äººç±»å…‰æ ‡è½¨è¿¹è¿›è¡Œé¢„è®­ç»ƒå’Œå‚æ•°è°ƒä¼˜ã€‚ |\n| **4. ä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„è¾…åŠ©ä»²è£ç­–ç•¥** | ç­–ç•¥ç½‘ç»œï¼ˆActor-Criticï¼‰æ¥æ”¶èåˆåçš„çŠ¶æ€ã€ä¿¡å¿µå’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¾“å‡ºè¿ç»­çš„è¾…åŠ©æ°´å¹³ `Î³`ï¼Œå…¶è®­ç»ƒå¥–åŠ±å‡½æ•°å¹³è¡¡äº†ä»»åŠ¡å®Œæˆã€å®‰å…¨æ€§ã€ç”¨æˆ·è‡ªä¸»æ€§ç­‰å¤šé‡ç›®æ ‡ã€‚ | **æ•°å­¦åŸç†**ï¼š<br>**å¥–åŠ±å‡½æ•°**ï¼ˆå…¬å¼6, 35ï¼‰æ˜¯å¤šé¡¹çš„åŠ æƒå’Œï¼š<br>`R = w_coll * R_coll + w_prox * R_prox + w_prog * R_prog + w_auto * R_auto + w_goal * R_goal`<br>å…·ä½“å½¢å¼ï¼š<br>`R = -w_collÂ·ğŸ™_collision + w_proxÂ·Î³Â·p_maxÂ·ğŸ™_near - w_farÂ·Î³Â·ğŸ™_far + w_progÂ·p_maxÂ·(d_(t-1)-d_t) - w_autoÂ·Î³^2 + w_goalÂ·log(p_true)`<br>**ç½‘ç»œæ¶æ„**ï¼šåŒå¤´Actor-Criticç½‘ç»œã€‚<br>- **å…±äº«ç‰¹å¾èåˆå±‚**ï¼šå¤„ç† `(s, b, c)`ã€‚<br>- **Actorå¤´**ï¼šè¾“å‡º `Î³` (ç»è¿‡tanhæ¿€æ´»å¹¶ç¼©æ”¾åˆ°[0,1])ã€‚<br>- **Criticå¤´**ï¼šè¾“å‡ºçŠ¶æ€ä»·å€¼ä¼°è®¡ `V(s)`ã€‚ | **ç½‘ç»œç»“æ„**ï¼š<br>- éšè—å±‚ï¼š2å±‚ï¼Œæ¯å±‚256ä¸ªç¥ç»å…ƒï¼Œä½¿ç”¨ReLUæ¿€æ´»ã€‚<br>**å¥–åŠ±æƒé‡**ï¼š<br>- `w_coll = 10.0` (ç¢°æ’æƒ©ç½š)<br>- `w_prox = 2.5` (æ¥è¿‘ç›®æ ‡æ—¶è¾…åŠ©å¥–åŠ±)<br>- `w_far = 1.5` (è¿œç¦»ç›®æ ‡æ—¶è¾…åŠ©æƒ©ç½š)<br>- `w_prog = 3.0` (å‘ç›®æ ‡è¿›å±•å¥–åŠ±)<br>- `w_auto = 1.5` (è¾…åŠ©å¹³æ–¹æƒ©ç½šï¼Œé¼“åŠ±æœ€å°å¹²é¢„)<br>- `w_goal = 2.0` (æ­£ç¡®è¯†åˆ«ç›®æ ‡å¥–åŠ±) |\n\n**æ€»ç»“**ï¼šBRACE çš„æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°åœ¨äºå°† **å®Œæ•´ä¿¡å¿µæ¡ä»¶åŒ–** ä¸ **ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–** ç›¸ç»“åˆï¼Œé€šè¿‡ä¸¥è°¨çš„è´å¶æ–¯æ¨æ–­æ¡†æ¶å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ç½‘ç»œï¼Œå®ç°äº†å¯¹ç”¨æˆ·æ„å›¾ä¸ç¡®å®šæ€§å’Œç¯å¢ƒçº¦æŸçš„è‡ªé€‚åº”å“åº”ã€‚å…¶æ•°å­¦åŸç†ï¼ˆå®šç†1å’Œå®šç†2ï¼‰ä¸ä»…ä¸ºæ–¹æ³•æä¾›äº†ç†è®ºä¿è¯ï¼Œä¹Ÿæ¸…æ™°åœ°æŒ‡å¯¼äº†ç³»ç»Ÿè®¾è®¡ï¼Œä½¿å…¶åœ¨å¤æ‚ã€é«˜ä¸ç¡®å®šæ€§çš„åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åºåˆ—åŒ–æˆ–åŸºäºMAPä¼°è®¡çš„æ–¹æ³•ã€‚",
            "timestamp": "2026-02-03T16:05:38.089513",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\n\nè®ºæ–‡åœ¨ä¸‰ä¸ªä¸»è¦å®éªŒåœºæ™¯ä¸­ä½¿ç”¨äº†å¤šç»´åº¦çš„å®šé‡å’Œå®šæ€§è¯„ä»·æŒ‡æ ‡ï¼Œä»¥å…¨é¢è¯„ä¼°BRACEæ¡†æ¶çš„æ€§èƒ½ã€‚\n\n#### **1. æ ¸å¿ƒå®šé‡æŒ‡æ ‡**\n\n| æŒ‡æ ‡åç§° | å®šä¹‰/è®¡ç®—æ–¹å¼ | è¯„ä¼°ç›®çš„ | è®ºæ–‡ä¸­å…¸å‹ç»“æœ (BRACE vs. åŸºçº¿) |\n| :--- | :--- | :--- | :--- |\n| **æˆåŠŸç‡** | ä»»åŠ¡æˆåŠŸå®Œæˆçš„è¯•éªŒæ¯”ä¾‹ã€‚ä¾‹å¦‚ï¼Œåœ¨æ‹¾æ”¾ä»»åŠ¡ä¸­æˆåŠŸå°†ç‰©ä½“æ”¾å…¥æ­£ç¡®ç›®æ ‡ç®±ã€‚ | è¡¡é‡ç³»ç»Ÿå®Œæˆæ ¸å¿ƒä»»åŠ¡çš„æ ¹æœ¬èƒ½åŠ›ã€‚ | **å¹³é¢å…‰æ ‡æ§åˆ¶**: 98.3% vs. æ— è¾…åŠ© 72.1% <br> **æ‹¾æ”¾ä»»åŠ¡**: 86% vs. IDA 68% |\n| **å®Œæˆæ—¶é—´** | ä»ä»»åŠ¡å¼€å§‹åˆ°æˆåŠŸå®Œæˆæ‰€èŠ±è´¹çš„æ—¶é—´ï¼ˆç§’ï¼‰ã€‚ | è¡¡é‡ä»»åŠ¡æ‰§è¡Œæ•ˆç‡ã€‚æ—¶é—´è¶ŠçŸ­ï¼Œè¾…åŠ©è¶Šæœ‰æ•ˆã€‚ | **å¹³é¢å…‰æ ‡æ§åˆ¶**: 3.30s vs. æ— è¾…åŠ© 8.44s <br> **æ‹¾æ”¾ä»»åŠ¡**: 9.8s vs. IDA 14.7s |\n| **è·¯å¾„æ•ˆç‡** | å®šä¹‰ä¸ºç†æƒ³æœ€çŸ­è·¯å¾„é•¿åº¦ä¸å®é™…è·¯å¾„é•¿åº¦çš„æ¯”å€¼ã€‚å€¼è¶Šæ¥è¿‘1ï¼Œè·¯å¾„è¶Šä¼˜ã€‚ | è¡¡é‡è¿åŠ¨è½¨è¿¹çš„ä¼˜åŒ–ç¨‹åº¦ï¼Œåæ˜ è¾…åŠ©ç³»ç»Ÿåœ¨å¯¼èˆªå’Œé¿éšœæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ | **å¹³é¢å…‰æ ‡æ§åˆ¶**: 0.89 vs. æ— è¾…åŠ© 0.43 <br> **æ‹¾æ”¾ä»»åŠ¡**: æ›´é«˜ï¼ˆå…·ä½“æ•°å€¼æœªç›´æ¥ç»™å‡ºï¼Œä½†è½¨è¿¹æ›´å¹³æ»‘ï¼‰ |\n| **ååé‡** | åœ¨å…‰æ ‡æ§åˆ¶ä»»åŠ¡ä¸­ä½¿ç”¨ï¼Œç»¼åˆäº†é€Ÿåº¦ä¸ç²¾åº¦ï¼ˆå¦‚Fittså®šå¾‹è¡ç”ŸæŒ‡æ ‡ï¼‰ã€‚ | è¯„ä¼°äººæœºäº¤äº’çš„ç»¼åˆæ•ˆç‡ã€‚ | **å¹³é¢å…‰æ ‡æ§åˆ¶**: 1.27 vs. æ— è¾…åŠ© 1.14 |\n| **ç¢°æ’æ¬¡æ•°** | åœ¨æ‰§è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­ä¸ç¯å¢ƒéšœç¢ç‰©å‘ç”Ÿç¢°æ’çš„æ¬¡æ•°ã€‚ | è¡¡é‡ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œé¿éšœèƒ½åŠ›ã€‚ | **æ‹¾æ”¾ä»»åŠ¡**: 0.22 vs. IDA 0.58 |\n| **æ”¾ç½®è¯¯å·®** | åœ¨æ‹¾æ”¾ä»»åŠ¡ä¸­ï¼Œç‰©ä½“æœ€ç»ˆä½ç½®ä¸ç›®æ ‡ä½ç½®ä¸­å¿ƒçš„è·ç¦»ï¼ˆå˜ç±³ï¼‰ã€‚ | è¡¡é‡æ“ä½œç²¾åº¦ã€‚ | **æ‹¾æ”¾ä»»åŠ¡**: 1.30cm vs. IDA 2.50cm |\n| **ç›®æ ‡/åˆ†é’Ÿ** | åœ¨Reacher-2Dä»»åŠ¡ä¸­ä½¿ç”¨ï¼Œå•ä½æ—¶é—´å†…æˆåŠŸåˆ°è¾¾çš„ç›®æ ‡æ•°é‡ã€‚ | è¡¡é‡åœ¨è¿ç»­ã€åŠ¨æ€ä»»åŠ¡ä¸­çš„æŒç»­æ•ˆç‡ã€‚ | **Reacher-2D**: 4.8 goals/min vs. IDA 3.7 goals/min |\n\n#### **2. æ ¸å¿ƒå®šæ€§æŒ‡æ ‡**\n*   **NASA-TLXå·¥ä½œé‡æŒ‡æ•°**ï¼šé€šè¿‡é—®å·è°ƒæŸ¥è¯„ä¼°ç”¨æˆ·çš„ä¸»è§‚ä½“éªŒï¼ŒåŒ…æ‹¬ï¼š\n    *   **å¿ƒæ™ºéœ€æ±‚**ã€**ä½“åŠ›éœ€æ±‚**ã€**æ—¶é—´éœ€æ±‚**ã€**ç»©æ•ˆ**ã€**åŠªåŠ›ç¨‹åº¦**ã€**æŒ«æŠ˜æ„Ÿ**ã€‚\n    *   **é™„åŠ ç»´åº¦**ï¼šè®ºæ–‡è¿˜æ‰©å±•è¯„ä¼°äº†**æ˜“ç”¨æ€§**ã€**æ„ŸçŸ¥è¾…åŠ©è´¨é‡**ã€**çµæ´»æ€§**ã€**ä¿¡å¿ƒ**å’Œ**æ»¡æ„åº¦**ã€‚\n*   **ç»“æœ**ï¼šBRACEåœ¨**æ»¡æ„åº¦**ã€**ä¿¡å¿ƒ**å’Œ**é™ä½åŠªåŠ›ç¨‹åº¦**æ–¹é¢è·å¾—æœ€é«˜åˆ†ï¼Œè¯æ˜äº†å…¶åœ¨æå‡ç”¨æˆ·ä½“éªŒå’Œä¿æŒç”¨æˆ·ä¸»ä½“æ„Ÿæ–¹é¢çš„ä¼˜åŠ¿ã€‚\n\n#### **3. ç³»ç»Ÿå†…éƒ¨æŒ‡æ ‡ï¼ˆç”¨äºåˆ†æä¸éªŒè¯ï¼‰**\n*   **ä¿¡å¿µç†µ**ï¼šç›®æ ‡æ¦‚ç‡åˆ†å¸ƒçš„ä¿¡æ¯ç†µã€‚ç”¨äºé‡åŒ–ç³»ç»Ÿå¯¹ç”¨æˆ·æ„å›¾çš„ä¸ç¡®å®šæ€§ã€‚è®ºæ–‡é€šè¿‡å¯è§†åŒ–å±•ç¤ºç†µéšæ—¶é—´ä¸‹é™çš„è¿‡ç¨‹ï¼ŒéªŒè¯äº†è¾…åŠ©æ°´å¹³ä¸ä¸ç¡®å®šæ€§è´Ÿç›¸å…³çš„ç†è®ºï¼ˆå®šç†1ï¼‰ã€‚\n*   **è¾…åŠ©æ°´å¹³ Î³**ï¼šæ··åˆå‚æ•°ï¼ˆ0ä¸ºå…¨äººå·¥ï¼Œ1ä¸ºå…¨è‡ªä¸»ï¼‰ã€‚è®ºæ–‡åˆ†æäº†Î³éš**æ—¶é—´**ï¼ˆå¯åŠ¨æ—¶ä½ï¼ŒåæœŸé«˜ï¼‰ã€**ç©ºé—´**ï¼ˆéšœç¢ç‰©é™„è¿‘é«˜ï¼Œå¼€æ”¾åŒºåŸŸä½ï¼‰å’Œ**ä¿¡å¿µç¡®å®šæ€§**ï¼ˆç†µä½æ—¶é«˜ï¼‰çš„åŠ¨æ€å˜åŒ–ï¼Œè¿™æ˜¯å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹çš„ç›´æ¥ä½“ç°ã€‚\n\n---\n\n### äºŒã€ æ•°æ®é›†\n\nè®ºæ–‡ä½¿ç”¨äº†**çœŸå®äººç±»è½¨è¿¹æ•°æ®é›†**æ¥å»ºæ¨¡å’ŒéªŒè¯ç³»ç»Ÿï¼Œå¹¶åœ¨**æ¨¡æ‹Ÿç¯å¢ƒ**ä¸­è¿›è¡Œäº†å¤§è§„æ¨¡è®­ç»ƒä¸è¯„ä¼°ã€‚\n\n#### **1. ç”¨äºå»ºæ¨¡â€œæ¨¡æ‹Ÿäººç±»â€çš„æ•°æ®é›†**\n*   **æ¥æº**ï¼šåŸºäºä¹‹å‰çš„å·¥ä½œ `[undef]`ï¼ˆAbiri et al., 2020ï¼‰ï¼Œè¯¥ç ”ç©¶æ¶‰åŠåŸºäºEEGçš„è„‘æœºæ¥å£å…‰æ ‡æ§åˆ¶ã€‚\n*   **å†…å®¹**ï¼š\n    *   **æ•°é‡**ï¼š798æ¡äººç±»æ§åˆ¶çš„å…‰æ ‡è½¨è¿¹ã€‚\n    *   **å‚ä¸è€…**ï¼š23äººï¼ˆå¹´é¾„19-34ï¼Œ14ç”·ï¼Œ9å¥³ï¼‰ï¼ŒæŠ€èƒ½æ°´å¹³ä»æ–°æ‰‹åˆ°ç†Ÿç»ƒã€‚\n    *   **ç”¨é€”**ï¼š\n        1.  **æ ¡å‡†è´å¶æ–¯æ¨æ–­æ¨¡å—**ï¼šç”¨äºç¡®å®šä¼¼ç„¶å‡½æ•°ä¸­çš„å…³é”®å‚æ•°ï¼ˆå¦‚ç†æ€§ç³»æ•° Î²=10.0ï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ä»å™ªå£°è¾“å…¥ä¸­æ¨æ–­æ„å›¾ã€‚\n        2.  **æ„å»ºâ€œæ¨¡æ‹Ÿäººç±»ä»£ç†â€**ï¼šåˆ†æè½¨è¿¹çš„é¢‘è°±ç‰¹æ€§ï¼ˆå‘ç°å…·æœ‰1/få™ªå£°ç‰¹å¾ï¼‰å’Œè¿åŠ¨å­¦ç‰¹å¾ï¼ˆå¦‚éµå¾ªæœ€å°æ€¥åŠ¨åº¦åŸåˆ™ï¼‰ï¼Œä»è€Œåœ¨ä»¿çœŸä¸­ç”Ÿæˆé€¼çœŸã€å¸¦æœ‰å™ªå£°çš„äººç±»æ§åˆ¶ä¿¡å·ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°BRACEã€‚\n*   **éªŒè¯**ï¼šä½œè€…é€šè¿‡äº¤å‰éªŒè¯ï¼ˆk=5ï¼‰ç¡®ä¿æ¨¡å‹æœªè¿‡æ‹Ÿåˆï¼Œå¹¶ä¸å…¶ä»–è¾“å…¥æ¨¡æ€ï¼ˆå¦‚æ“çºµæ†ï¼‰çš„å…¬å¼€æ•°æ®å¯¹æ¯”ï¼ŒéªŒè¯äº†è¿åŠ¨è§„åˆ’æ¨¡å¼çš„ä¸€è‡´æ€§ã€‚\n\n#### **2. ç”¨äºè®­ç»ƒä¸è¯„ä¼°çš„æ¨¡æ‹Ÿç¯å¢ƒï¼ˆæ•°æ®é›†ï¼‰**\nè®ºæ–‡é‡‡ç”¨äº†**æ¸è¿›å¼ä¸‰å±‚è¯„ä¼°æ¡†æ¶**ï¼Œæ¯ä¸ªç¯å¢ƒéƒ½æ„æˆäº†ä¸€ä¸ªç‰¹å®šçš„æµ‹è¯•æ•°æ®é›†ã€‚\n\n| å®éªŒåœºæ™¯ | ç¯å¢ƒ/å¹³å° | æ ¸å¿ƒæŒ‘æˆ˜ | æ•°æ®é›†/è¯•éªŒæ„æˆç»†èŠ‚ |\n| :--- | :--- | :--- | :--- |\n| **1. äººæœºäº¤äº’æŒ‘æˆ˜** | è‡ªå®šä¹‰2Då¹³é¢å…‰æ ‡æ§åˆ¶ | çœŸå®ã€ä¸å¯é¢„æµ‹çš„äººç±»è¾“å…¥ï¼›æ ¸å¿ƒäº¤äº’åŠ¨æ€ã€‚ | **ç”¨æˆ·ç ”ç©¶**ï¼šN=12åå‚ä¸è€…ï¼Œæ¯äººå®Œæˆ5ç§æ¡ä»¶ Ã— 3ç§ç¯å¢ƒ Ã— 24æ¬¡è¯•éªŒ = **4320æ¡è½¨è¿¹**ã€‚ç¯å¢ƒé€šè¿‡å›ºå®šéšæœºç§å­ç”Ÿæˆï¼ŒåŒ…å«ä¸åŒç›®æ ‡æ¨¡ç³Šåº¦å’Œéšœç¢ç‰©é…ç½®ã€‚ |\n| **2. ç‰©ç†åŠ¨æ€æŒ‘æˆ˜** | Reacher-2D (MuJoCo) | éçº¿æ€§ã€è€¦åˆçš„æœºå™¨äººè‡‚åŠ¨åŠ›å­¦ã€‚ | **æ¨¡æ‹Ÿè¯•éªŒ**ï¼šæ¯ä¸ªå¯¹æ¯”æ¡ä»¶è¿›è¡Œ120æ¬¡è¯•éªŒï¼ˆé€šè¿‡ç»Ÿè®¡åŠŸæ•ˆåˆ†æç¡®å®šï¼‰ã€‚äººç±»åŠ¨ä½œä¸ºä¸“å®¶ç­–ç•¥å åŠ 30%é«˜æ–¯å™ªå£°ï¼Œæ¨¡æ‹Ÿé«˜æ‰°åŠ¨æ§åˆ¶ä¿¡å·ã€‚ |\n| **3. ç»¼åˆä»»åŠ¡ä¸Šä¸‹æ–‡æŒ‘æˆ˜** | ä¿®æ”¹çš„FetchPickAndPlace-v3 (Gym) | é«˜ç»´ç©ºé—´ä¸­çš„ç›®æ ‡æ¨¡ç³Šæ€§ + ç¯å¢ƒçº¦æŸã€‚ | **æ¨¡æ‹Ÿè¯•éªŒ**ï¼šé…ç½®ä¸º1ä¸ªå¯æŠ“å–ç«‹æ–¹ä½“ï¼Œ**3ä¸ªè§†è§‰ç›¸ä¼¼çš„ç›®æ ‡ç®±**ï¼ˆéšæœºæŒ‡å®šå…¶ä¸€ä¸ºçœŸå®ç›®æ ‡ï¼‰ï¼Œä»¥åŠè·¯å¾„éšœç¢ç‰©ã€‚è¿›è¡Œäº†å¤§é‡è¯•éªŒä»¥è®¡ç®—å‡å€¼å’Œç½®ä¿¡åŒºé—´ï¼ˆå¦‚è¡¨4ï¼‰ã€‚ |\n\n#### **3. ç”¨äºé¢„è®­ç»ƒå’Œæ¶ˆèç ”ç©¶çš„æ•°æ®é›†**\n*   **è´å¶æ–¯æ¨¡å—é¢„è®­ç»ƒé›†**ï¼š\n    *   **æ¥æº**ï¼šç‹¬ç«‹æ”¶é›†çš„å…‰æ ‡è½¨è¿¹ã€‚\n    *   **å†…å®¹**ï¼š1968æ¡å¸¦æœ‰å·²çŸ¥ç›®æ ‡æ ‡ç­¾çš„è½¨è¿¹ã€‚\n    *   **ç”¨é€”**ï¼šä»¥ç›‘ç£å­¦ä¹ æ–¹å¼é¢„è®­ç»ƒè´å¶æ–¯æ„å›¾æ¨æ–­æ¨¡å—ï¼Œä½¿å…¶åœ¨é›†æˆåˆ°ç«¯åˆ°ç«¯è®­ç»ƒå‰å°±å·²å…·å¤‡è¾ƒå¥½çš„æ¨æ–­èƒ½åŠ›ï¼ˆ**çƒ­å¯åŠ¨**ï¼‰ã€‚è¯¥æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°ï¼šè·¯å¾„å®Œæˆ25%æ—¶å‡†ç¡®ç‡74.6%ï¼Œ50%æ—¶91.2%ï¼Œ75%æ—¶98.7%ã€‚\n*   **æ¶ˆèç ”ç©¶ç¯å¢ƒ**ï¼š\n    *   **å¤æ‚è¿·å®«ç¯å¢ƒ**ï¼šç”¨äºæµ‹è¯•æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…å«15ä¸ªç›®æ ‡ï¼ˆ3ä¸ªé›†ç¾¤+6ä¸ªåˆ†æ•£ç›®æ ‡ï¼‰å’Œ12ä¸ªéšœç¢ç‰©ï¼Œä¸“é—¨åˆ¶é€ é«˜ç›®æ ‡æ¨¡ç³Šæ€§åœºæ™¯ã€‚\n    *   **è¯¾ç¨‹å­¦ä¹ å„é˜¶æ®µç¯å¢ƒ**ï¼šç”Ÿæˆäº†ä»ç®€å•ï¼ˆåŸºç¡€æŒ‡å‘ï¼‰åˆ°å¤æ‚ï¼ˆå¤šç›®æ ‡ã€éšœç¢ç‰©ï¼‰çš„ä¸€ç³»åˆ—ç¯å¢ƒï¼Œæ„æˆè®­ç»ƒç”¨çš„â€œè¯¾ç¨‹æ•°æ®é›†â€ã€‚\n\n### æ€»ç»“\nè¯¥è®ºæ–‡çš„è¯„ä»·ä½“ç³»**å…¨é¢ä¸”æ·±å…¥**ï¼Œæ—¢åŒ…å«è¡¡é‡ä»»åŠ¡æ€§èƒ½çš„å®¢è§‚æŒ‡æ ‡ï¼ˆæˆåŠŸç‡ã€æ•ˆç‡ï¼‰ï¼Œä¹ŸåŒ…å«è¡¡é‡äººæœºäº¤äº’è´¨é‡çš„ä¸»è§‚æŒ‡æ ‡ï¼ˆNASA-TLXï¼‰ï¼ŒåŒæ—¶è¿˜é€šè¿‡ç³»ç»Ÿå†…éƒ¨æŒ‡æ ‡ï¼ˆÎ³ï¼Œ ç†µï¼‰éªŒè¯äº†å…¶ç†è®ºåˆ›æ–°ã€‚æ•°æ®é›†çš„ä½¿ç”¨**å±‚æ¬¡åˆ†æ˜**ï¼Œç”¨çœŸå®äººç±»æ•°æ®ç¡®ä¿æ¨¡å‹åŸºç¡€çš„çœŸå®æ€§ï¼Œç”¨ç²¾å¿ƒè®¾è®¡çš„æ¨¡æ‹Ÿç¯å¢ƒè¿›è¡Œå¯æ§ã€å¯æ‰©å±•çš„ç®—æ³•éªŒè¯ä¸æ¯”è¾ƒï¼Œä½“ç°äº†ä¸¥è°¨çš„è¯„ä¼°æ–¹æ³•ã€‚",
            "timestamp": "2026-02-03T16:06:41.180791",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†ä¸ä»»åŠ¡åœºæ™¯\nè®ºæ–‡é€šè¿‡ä¸‰ä¸ªé€æ­¥é€’è¿›çš„å®éªŒåœºæ™¯è¿›è¡ŒéªŒè¯ï¼Œæ¯ä¸ªåœºæ™¯æ—¨åœ¨éš”ç¦»å’Œæµ‹è¯•å…±äº«è‡ªä¸»æ€§ä¸­çš„ä¸åŒæ ¸å¿ƒæŒ‘æˆ˜ï¼š\n\n| å®éªŒåœºæ™¯ | æ ¸å¿ƒæŒ‘æˆ˜ | ç¯å¢ƒ/æ•°æ®é›†æè¿° |\n| :--- | :--- | :--- |\n| **1. å¹³é¢å…‰æ ‡æ§åˆ¶ (Planar Cursor Control)** | **äººæœºäº¤äº’åŠ¨æ€** | **çœŸå®ç”¨æˆ·ç ”ç©¶**ï¼š12åå‚ä¸è€…ä½¿ç”¨DualSenseæ§åˆ¶å™¨ã€‚ç”Ÿæˆ3ä¸ªå›ºå®šç¯å¢ƒï¼ˆé€šè¿‡éšæœºç§å­ï¼‰ï¼Œæ¯ä¸ªç¯å¢ƒåŒ…å«éšæœºæ”¾ç½®çš„ç›®æ ‡å’Œéšœç¢ç‰©ã€‚å…±æ”¶é›† **4,320æ¡è½¨è¿¹**ï¼ˆæ¯äººæ¯ç¯å¢ƒ24æ¬¡è¯•éªŒï¼‰ã€‚ |\n| **2. Reacher-2D** | **éçº¿æ€§ç‰©ç†åŠ¨åŠ›å­¦** | **æ¨¡æ‹Ÿç¯å¢ƒ**ï¼šä¿®æ”¹çš„Reacher-2Dç¯å¢ƒï¼ŒåŒ…å«ä¸‰ä¸ªå‡åŒ€åˆ†å¸ƒçš„ç›®æ ‡å’Œä¸€ä¸ªåœ†æŸ±å½¢éšœç¢ç‰©ã€‚æ¨¡æ‹Ÿäººç±»æ“ä½œå‘˜çš„åŠ¨ä½œä¸ºä¸“å®¶ç­–ç•¥è¾“å‡ºå åŠ 30%é«˜æ–¯å™ªå£°ã€‚è¿›è¡Œäº† **æ¯æ¡ä»¶120æ¬¡è¯•éªŒ**ã€‚ |\n| **3. Fetchæœºæ¢°è‡‚æ‹¾å–æ”¾ç½®** | **é›†æˆä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆç›®æ ‡æ¨¡ç³Šæ€§+ç¯å¢ƒçº¦æŸï¼‰** | **æ¨¡æ‹Ÿç¯å¢ƒ**ï¼šä¿®æ”¹çš„FetchPickAndPlace-v3ç¯å¢ƒã€‚è®¾ç½®ï¼š1ä¸ªå¯æŠ“å–ç«‹æ–¹ä½“ã€**3ä¸ªè§†è§‰ç›¸ä¼¼çš„ç›®æ ‡ç®±**ã€è·¯å¾„ä¸Šçš„éšœç¢ç‰©ã€‚éšè—ç›®æ ‡åœ¨æ¯è½®å¼€å§‹æ—¶éšæœºæŒ‡å®šã€‚ |\n\n**è¡¥å……è¯´æ˜**ï¼š\n*   **æ¨¡æ‹Ÿäººç±»ä»£ç†**ï¼šåœ¨Reacher-2Då’ŒFetchå®éªŒä¸­ï¼Œä½¿ç”¨äº†ä¸€ä¸ª**æ•°æ®é©±åŠ¨çš„å™ªå£°è¡Œä¸ºæ¨¡å‹**ã€‚è¯¥æ¨¡å‹åŸºäºå¯¹798æ¡çœŸå®äººç±»å…‰æ ‡è½¨è¿¹ï¼ˆæ¥è‡ªEEGè§£ç å™¨ç ”ç©¶ï¼‰çš„åˆ†æï¼Œç»“åˆäº†ç¡®å®šæ€§æœ€å°æ€¥åŠ¨è½¨è¿¹å’Œéšæœº1/fï¼ˆç²‰çº¢ï¼‰å™ªå£°ã€‚\n*   **ä¸“å®¶ç­–ç•¥æ•°æ®**ï¼šä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒäº†ä¸“é—¨çš„ä¸“å®¶ç­–ç•¥ï¼ˆå¦‚ç”¨äºReacher-2Dçš„SACç½‘ç»œï¼Œç”¨äºå…‰æ ‡ä»»åŠ¡çš„4å±‚ç½‘ç»œï¼‰ï¼Œä½œä¸ºè¾…åŠ©çš„â€œå®Œç¾è·¯å¾„â€å‚è€ƒã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨äº†å¤šç»´åº¦çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ¶µç›–ä»»åŠ¡æ€§èƒ½ã€æ•ˆç‡å’Œä¸»è§‚ä½“éªŒï¼š\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | è¯´æ˜ |\n| :--- | :--- | :--- |\n| **ä»»åŠ¡æ€§èƒ½** | **æˆåŠŸç‡** | æˆåŠŸå®Œæˆç›®æ ‡è·å–æˆ–æ”¾ç½®ä»»åŠ¡çš„è¯•éªŒæ¯”ä¾‹ã€‚ |\n| **æ•ˆç‡** | **å®Œæˆæ—¶é—´** | ä»ä»»åŠ¡å¼€å§‹åˆ°æˆåŠŸå®Œæˆæ‰€èŠ±è´¹çš„æ—¶é—´ï¼ˆç§’ï¼‰ã€‚ |\n| | **è·¯å¾„æ•ˆç‡** | å®é™…è·¯å¾„é•¿åº¦ä¸èµ·å§‹ç‚¹åˆ°ç›®æ ‡ç‚¹ç›´çº¿è·ç¦»çš„æ¯”å€¼ï¼ˆå€¼è¶Šæ¥è¿‘1è¶Šå¥½ï¼‰ã€‚ |\n| | **ååé‡** | å•ä½æ—¶é—´å†…å®Œæˆçš„ç›®æ ‡æ•°é‡ï¼ˆå¦‚ goals/minï¼‰ã€‚ |\n| **å®‰å…¨æ€§/ç²¾ç¡®åº¦** | **ç¢°æ’æ¬¡æ•°** | æ‰§è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­ä¸éšœç¢ç‰©å‘ç”Ÿç¢°æ’çš„æ¬¡æ•°ã€‚ |\n| | **æ”¾ç½®è¯¯å·®** | ï¼ˆFetchä»»åŠ¡ï¼‰ç‰©ä½“è¢«æ”¾ç½®æ—¶ä¸ç›®æ ‡ä½ç½®çš„è·ç¦»ï¼ˆå˜ç±³ï¼‰ã€‚ |\n| **ä¸»è§‚ä½“éªŒ** | **NASA-TLXè¯„åˆ†** | é€šè¿‡é—®å·è°ƒæŸ¥è¯„ä¼°ç”¨æˆ·çš„å·¥ä½œè´Ÿè·ã€æ»¡æ„åº¦ã€ä¿¡å¿ƒã€æ˜“ç”¨æ€§ç­‰ä¸»è§‚æ„Ÿå—ï¼ˆ7ä¸ªç»´åº¦ï¼‰ã€‚ |\n\n### ä¸‰ã€ ä¸»è¦æ€§èƒ½æå‡ï¼ˆBRACE vs. åŸºçº¿æ–¹æ³•ï¼‰\nBRACEæ¡†æ¶åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚åœºæ™¯ä¸­ä¼˜åŠ¿æ˜æ˜¾ã€‚\n\n#### 1. ç»¼åˆæ€§èƒ½å¯¹æ¯”ï¼ˆå¹³é¢å…‰æ ‡æ§åˆ¶ä»»åŠ¡ï¼‰\nä¸æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•å’Œæ— è¾…åŠ©æ§åˆ¶ç›¸æ¯”ï¼š\n\n| å¯¹æ¯”åŸºå‡† | æˆåŠŸç‡æå‡ | è·¯å¾„æ•ˆç‡æå‡ | å®Œæˆæ—¶é—´å‡å°‘ |\n| :--- | :--- | :--- | :--- |\n| **æ— è¾…åŠ©æ§åˆ¶** | **+36.3%** (98.3% vs. 72.1%) | **+87%** (0.89 vs. 0.43) | **-60.9%** (3.30s vs. 8.44s) |\n| **SOTAæ–¹æ³• (IDA)** | **+6.3%** (98.3% vs. 92.5%) | **+41%** (0.89 vs. 0.63) | **-35%** (3.30s vs. 5.07s) |\n| **SOTAæ–¹æ³• (DQN)** | **+9.5%** (98.3% vs. 89.8%) | **+19%** (0.89 vs. 0.75) | **-41%** (3.30s vs. 5.62s) |\n\n#### 2. å…³é”®åœºæ™¯ä¸‹çš„ä¼˜åŠ¿\n*   **é«˜ç›®æ ‡ä¸ç¡®å®šæ€§åœºæ™¯**ï¼šåœ¨ç›®æ ‡æ¨¡ç³Šæ€§é«˜çš„å¤æ‚å¤šç›®æ ‡ç¯å¢ƒä¸­ï¼ŒBRACEç›¸æ¯”é¡ºåºå¼ï¼ˆMAPï¼‰æ–¹æ³•ï¼Œ**æˆåŠŸç‡æå‡13.1%**ï¼Œ**å®Œæˆæ—¶é—´å‡å°‘23%**ã€‚\n*   **å¤æ‚ç‰©ç†åŠ¨åŠ›å­¦åœºæ™¯ (Reacher-2D)**ï¼šç›¸æ¯”IDAï¼ŒBRACEå®ç°äº†**29.7%çš„ååé‡æå‡**ï¼ˆ4.8 vs. 3.7 goals/minï¼‰ï¼Œä¸”æ€§èƒ½æ–¹å·®æ˜¾è‘—æ›´ä½ã€‚\n*   **é›†æˆ3Dæ“ä½œä»»åŠ¡ (Fetch)**ï¼šåœ¨åŒæ—¶å…·æœ‰ç›®æ ‡æ¨¡ç³Šæ€§å’Œç¯å¢ƒçº¦æŸçš„ä»»åŠ¡ä¸­ï¼ŒBRACEå–å¾—æœ€ä½³æ€§èƒ½ï¼š\n    *   **æˆåŠŸç‡**ï¼š86% (BRACE) vs. 68% (IDA) vs. 74% (DQN)\n    *   **å®Œæˆæ—¶é—´**ï¼š9.8s (BRACE) vs. 14.7s (IDA) vs. 12.3s (DQN)\n    *   **ç¢°æ’æ¬¡æ•°**ï¼š0.22 (BRACE) vs. 0.58 (IDA) vs. 0.41 (DQN)\n\n#### 3. ç”¨æˆ·ä¸»è§‚è¯„ä»·\nåœ¨NASA-TLXè°ƒæŸ¥ä¸­ï¼ŒBRACEåœ¨**æ˜“ç”¨æ€§ã€æ„ŸçŸ¥è¾…åŠ©è´¨é‡ã€çµæ´»æ€§å’Œä¿¡å¿ƒ**ç­‰å¤šä¸ªç»´åº¦è·å¾—äº†æœ€é«˜çš„ä¸»è§‚è¯„åˆ†ï¼ŒåŒæ—¶**åŠªåŠ›ç¨‹åº¦è¯„åˆ†æœ€ä½**ï¼Œè¡¨æ˜å…¶åœ¨æå‡æ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆé™ä½äº†ç”¨æˆ·çš„è®¤çŸ¥ä¸æ“ä½œè´Ÿè·ã€‚\n\n#### 4. ç†è®ºéªŒè¯\nå®éªŒæ•°æ®éªŒè¯äº†è®ºæ–‡çš„ä¸¤ä¸ªæ ¸å¿ƒç†è®ºï¼š\n*   **å®šç†1ï¼ˆä¸ç¡®å®šæ€§/çº¦æŸå•è°ƒæ€§ï¼‰**ï¼šè¾…åŠ©æ°´å¹³ `Î³` éšç›®æ ‡ä¸ç¡®å®šæ€§ï¼ˆç†µï¼‰**å¢åŠ è€Œé™ä½**ï¼Œéšç¯å¢ƒçº¦æŸä¸¥é‡æ€§**å¢åŠ è€Œå¢åŠ **ã€‚ä¾‹å¦‚ï¼Œåœ¨è½¨è¿¹åˆæœŸä¸ç¡®å®šæ€§é«˜æ—¶ï¼Œ`Î³ â‰ˆ 0.28`ï¼Œéšç€æ„å›¾æ˜ç¡®å’Œæ¥è¿‘éšœç¢ç‰©ï¼Œ`Î³` ä¸Šå‡è‡³ `â‰ˆ 0.74`ã€‚\n*   **å®šç†2ï¼ˆé›†æˆä¼˜åŒ–ä¼˜åŠ¿ï¼‰**ï¼šåœ¨ç›®æ ‡ä¸ç¡®å®šæ€§é«˜ã€ä¸”ä¸åŒç›®æ ‡æ‰€éœ€è¾…åŠ©ç­–ç•¥å·®å¼‚å¤§æ—¶ï¼ŒåŸºäºå®Œæ•´ä¿¡å¿µåˆ†å¸ƒçš„é›†æˆä¼˜åŒ–ç›¸æ¯”åŸºäºMAPä¼°è®¡çš„é¡ºåºæ–¹æ³•ï¼Œå¸¦æ¥äº†**äºŒæ¬¡æ–¹çš„æœŸæœ›é—æ†¾ä¼˜åŠ¿**ï¼Œè¿™åœ¨å¤šç›®æ ‡è¿·å®«ç¯å¢ƒç­‰å®éªŒä¸­å¾—åˆ°äº†è¯å®ã€‚\n\n**ç»“è®º**ï¼šBRACEé€šè¿‡åœ¨**å®Œæ•´ä¿¡å¿µåˆ†å¸ƒ**ä¸Šçš„**ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**ï¼Œåœ¨ä¿æŒç”¨æˆ·è‡ªä¸»æ€§çš„åŒæ—¶ï¼Œåœ¨æˆåŠŸç‡ã€æ•ˆç‡ã€å®‰å…¨æ€§åŠç”¨æˆ·ä½“éªŒæ–¹é¢å‡å®ç°äº†æ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œå…¶ä¼˜åŠ¿åœ¨**ç›®æ ‡æ¨¡ç³Šã€ç¯å¢ƒçº¦æŸå¤æ‚çš„çœŸå®åœºæ™¯ä¸­æœ€ä¸ºçªå‡º**ã€‚",
            "timestamp": "2026-02-03T16:07:33.348181",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡çš„ä¸»è¦å±€é™æ€§\n\næ ¹æ®è®ºæ–‡ç¬¬5.1èŠ‚ï¼ˆLimitations and Future Workï¼‰åŠå…¨æ–‡åˆ†æï¼ŒBRACEæ¡†æ¶å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n- **ä¸“å®¶ç­–ç•¥çš„ä¾èµ–æ€§**ï¼šç³»ç»Ÿçš„æ€§èƒ½**å—é™äºä¸“å®¶ç­–ç•¥çš„è´¨é‡å’Œå¯ç”¨æ€§**ã€‚è™½ç„¶é²æ£’æ€§æµ‹è¯•è¡¨æ˜ç³»ç»Ÿå…·æœ‰è¾ƒé«˜çš„éŸ§æ€§ï¼Œä½†å…¶è®¾è®¡æ ¸å¿ƒæ˜¯â€œä»²è£â€è€Œéâ€œè¶…è¶Šâ€ã€‚å¦‚æœä¸“å®¶ç­–ç•¥æœ¬èº«å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ï¼ˆä¾‹å¦‚ï¼Œè§„åˆ’è§†é‡æœ‰é™ã€ååº”å»¶è¿Ÿï¼‰ï¼Œç³»ç»Ÿçš„æ€§èƒ½ä¸Šé™å°†å—åˆ°å½±å“ã€‚\n- **ç¼ºä¹é•¿æœŸä¸ªæ€§åŒ–é€‚åº”**ï¼šå½“å‰çš„æ¡†æ¶**æœªå®ç°é•¿æœŸçš„ã€ä¸ªæ€§åŒ–çš„ç”¨æˆ·æ¨¡å‹å­¦ä¹ **ã€‚ç³»ç»Ÿåœ¨å•æ¬¡ä»»åŠ¡ä¸­æ ¹æ®å®æ—¶è¾“å…¥æ¨æ–­æ„å›¾å¹¶è°ƒæ•´è¾…åŠ©ï¼Œä½†ä¸ä¼šè·¨ä¼šè¯å­¦ä¹ ç‰¹å®šç”¨æˆ·ï¼ˆå°¤å…¶æ˜¯å…·æœ‰ä¸åŒè¿åŠ¨éšœç¢çš„ç”¨æˆ·ï¼‰ç‹¬ç‰¹çš„è¡Œä¸ºæ¨¡å¼ï¼Œä»è€Œè¿›è¡Œä¸ªæ€§åŒ–ä¼˜åŒ–ã€‚\n- **è¾“å…¥æ¨¡æ€çš„å±€é™æ€§**ï¼šå½“å‰å·¥ä½œ**ä¸»è¦å…³æ³¨è¿åŠ¨å­¦ï¼ˆå¦‚æ“çºµæ†ã€åŠ›ä¼ æ„Ÿå™¨ï¼‰è¾“å…¥**ã€‚è™½ç„¶è®ºæ–‡åœ¨é™„å½•Gä¸­éªŒè¯äº†å¯¹ä¸åŒç‰©ç†æ¥å£çš„é²æ£’æ€§ï¼Œä½†å°šæœªæ•´åˆæ›´ä¸°å¯Œçš„å¤šæ¨¡æ€ç”Ÿç‰©ä¿¡å·ï¼ˆå¦‚çœ¼åŠ¨ã€è‚Œç”µã€è„‘ç”µï¼‰æ¥å¢å¼ºæ„å›¾æ¨æ–­ï¼Œè¿™å¯¹äºé‡åº¦è¿åŠ¨éšœç¢ç”¨æˆ·å°¤ä¸ºé‡è¦ã€‚\n- **ä¸“å®¶ç­–ç•¥çš„é™æ€æ€§**ï¼šæ¡†æ¶ä¸­çš„**ä¸“å®¶ç­–ç•¥æ˜¯é¢„å…ˆè®­ç»ƒå¥½ä¸”å›ºå®šçš„**ã€‚ç³»ç»Ÿå­¦ä¹ å¦‚ä½•æ··åˆç”¨æˆ·è¾“å…¥å’Œä¸“å®¶åŠ¨ä½œï¼Œä½†ä¸“å®¶ç­–ç•¥æœ¬èº«ä¸ä¼šæ ¹æ®ç”¨æˆ·çš„çº æ­£æ€§è¾“å…¥è¿›è¡Œåœ¨çº¿è°ƒæ•´æˆ–å…±åŒé€‚åº”ã€‚\n- **è®¡ç®—ä¸å®éªŒèŒƒå›´çš„é™åˆ¶**ï¼š\n    - è®­ç»ƒéœ€è¦çº¦7å°æ—¶çš„GPUæ—¶é—´ï¼Œè™½ç„¶è¿è¡Œæ—¶å»¶è¿Ÿï¼ˆ36msï¼‰æ»¡è¶³å®æ—¶æ€§è¦æ±‚ï¼Œä½†**è®­ç»ƒæˆæœ¬è¾ƒé«˜**ã€‚\n    - éªŒè¯å®éªŒè™½ç„¶åœ¨2Då…‰æ ‡æ§åˆ¶ã€2Dæœºæ¢°è‡‚å’Œ3Då–æ”¾ä»»åŠ¡ä¸Šç³»ç»Ÿè¿›è¡Œï¼Œä½†**ç¯å¢ƒå¤æ‚åº¦å’Œä»»åŠ¡å¤šæ ·æ€§ä»æœ‰æ‰©å±•ç©ºé—´**ï¼ˆä¾‹å¦‚ï¼Œæ›´åŠ¨æ€çš„ç¯å¢ƒã€æ›´å¤šæ ·åŒ–çš„ç›®æ ‡é›†ï¼‰ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡é’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæå‡ºäº†æ¸…æ™°çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼š\n\n1.  **ä»ä»²è£åˆ°å…±åŒé€‚åº”**ï¼š\n    - **ç ”ç©¶æ–¹å‘**ï¼šç ”ç©¶**åœ¨çº¿è°ƒæ•´ä¸“å®¶ç­–ç•¥æœ¬èº«**ï¼Œåˆ©ç”¨ç”¨æˆ·çš„è¾“å…¥ä½œä¸ºçº æ­£ä¿¡å·ã€‚è¿™å°†ä½¿èŒƒå¼ä»ç®€å•çš„æ··åˆä»²è£ï¼Œè½¬å˜ä¸ºçœŸæ­£çš„äººæœº**å…±åŒé€‚åº”**ï¼Œè®©ä¸“å®¶ç­–ç•¥èƒ½å¤Ÿéšæ—¶é—´å‘ç”¨æˆ·å­¦ä¹ ã€‚\n\n2.  **çºµå‘ä¸ªæ€§åŒ–ä¸å¤šæ¨¡æ€èåˆ**ï¼š\n    - **ç ”ç©¶æ–¹å‘**ï¼šæ‰©å±•BRACEä»¥æ”¯æŒ**çºµå‘é€‚åº”**ï¼Œç³»ç»Ÿé€šè¿‡å­¦ä¹ ç‰¹å®šç”¨æˆ·çš„ç‹¬ç‰¹è¡Œä¸ºæ¨¡å¼æ¥ä¸ªæ€§åŒ–å…¶äººç±»æ¨¡å‹ã€‚\n    - **ç ”ç©¶æ–¹å‘**ï¼š**é›†æˆå¤šæ¨¡æ€æ•°æ®æµ**ï¼Œå¦‚æ³¨è§†ç‚¹ã€è‚Œç”µå’Œè„‘ç”µè§£ç å™¨ï¼Œä»¥ä¸°å¯Œä¿¡å¿µçŠ¶æ€ï¼Œåœ¨æ„å›¾æ¨¡ç³Šæ—¶æä¾›æ›´é²æ£’çš„æ¨æ–­ã€‚\n\n3.  **å¤„ç†æ¬¡ä¼˜ä¸“å®¶ä¸æ¨¡å‹åŸºç¡€æ‰©å±•**ï¼š\n    - **ç ”ç©¶æ–¹å‘**ï¼šè¿›ä¸€æ­¥ç ”ç©¶åœ¨**ä¸“å®¶ç­–ç•¥æ˜¾è‘—æ¬¡ä¼˜æˆ–ä¸å¯ç”¨**çš„åœºæ™¯ä¸‹çš„é²æ£’æ€§ã€‚æ¢ç´¢å¦‚ä½•ä»äº¤äº’æ•°æ®ä¸­è”åˆå­¦ä¹ æˆ–åœ¨çº¿æ”¹è¿›ä¸“å®¶ç­–ç•¥ã€‚\n    - **ç ”ç©¶æ–¹å‘**ï¼šå°†æ¡†æ¶æ‰©å±•åˆ°**æ›´å¤æ‚çš„æ¨¡å‹åŸºç¡€è§„åˆ’**æˆ–**å±‚æ¬¡åŒ–ç­–ç•¥**ä¸­ï¼Œä»¥å¤„ç†æ›´é•¿è§†é‡çš„ä»»åŠ¡å’Œæ›´å¤æ‚çš„åŠ¨æ€ã€‚\n\n4.  **å®éªŒéªŒè¯ä¸é¢†åŸŸæ‹“å±•**ï¼š\n    - **ç ”ç©¶æ–¹å‘**ï¼šåœ¨**æ›´å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡å’Œå¤šæ ·åŒ–ç”¨æˆ·ç¾¤ä½“**ï¼ˆåŒ…æ‹¬ä¸´åºŠäººç¾¤ï¼‰ä¸­è¿›è¡Œå¹¿æ³›çš„**ç”¨æˆ·ç ”ç©¶**ï¼Œä»¥éªŒè¯ä¸»è§‚ä½“éªŒï¼ˆå¦‚ä»£ç†æ„Ÿã€ä¿¡ä»»åº¦ï¼‰å’Œå®¢è§‚æ€§èƒ½ã€‚\n    - **ç ”ç©¶æ–¹å‘**ï¼šå°†æ¡†æ¶åº”ç”¨äº**å…±äº«è‡ªæ²»ä¹‹å¤–çš„å…¶ä»–é¢†åŸŸ**ï¼Œå¦‚äººæœºåä½œåˆ¶é€ ã€è¿œç¨‹æ“ä½œæˆ–è‡ªåŠ¨é©¾é©¶ï¼Œè¿™äº›é¢†åŸŸåŒæ ·éœ€è¦å¹³è¡¡è‡ªä¸»ååŠ©ä¸äººç±»æ„å›¾ã€‚\n\n5.  **ç®—æ³•ä¸æ•ˆç‡ä¼˜åŒ–**ï¼š\n    - **ç ”ç©¶æ–¹å‘**ï¼šæ¢ç´¢**æ›´é«˜æ•ˆçš„è®­ç»ƒç®—æ³•**å’Œ**ç½‘ç»œæ¶æ„**ï¼Œä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼Œå¹¶æå‡åœ¨èµ„æºå—é™å¹³å°ä¸Šçš„éƒ¨ç½²èƒ½åŠ›ã€‚\n    - **ç ”ç©¶æ–¹å‘**ï¼šç ”ç©¶**æ›´å…ˆè¿›çš„ä¿¡å¿µè¡¨ç¤ºä¸æ›´æ–°æ–¹æ³•**ï¼ˆå¦‚ç¥ç»ä¿¡å¿µç½‘ç»œï¼‰ï¼Œä»¥å¤„ç†è¿ç»­æˆ–é«˜ç»´çš„ç›®æ ‡ç©ºé—´ã€‚\n\n**æ€»ç»“**ï¼šBRACEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**ä¿¡å¿µä¸ç­–ç•¥çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–**ï¼Œå…¶å½“å‰å±€é™æ€§ä¸»è¦å›´ç»•**ç³»ç»Ÿçš„é€‚åº”è¾¹ç•Œã€è¾“å…¥ä¸°å¯Œæ€§ä»¥åŠé•¿æœŸä¸ªæ€§åŒ–èƒ½åŠ›**ã€‚æœªæ¥çš„å·¥ä½œæ—¨åœ¨çªç ´è¿™äº›è¾¹ç•Œï¼Œæ¨åŠ¨å…±äº«è‡ªæ²»ç³»ç»Ÿå‘æ›´è‡ªé€‚åº”ã€æ›´ä¸ªæ€§åŒ–ã€æ›´èåˆçš„æ–¹å‘å‘å±•ã€‚",
            "timestamp": "2026-02-03T16:08:06.500080",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T18:59:16Z",
    "created_at": "2026-02-03T15:53:11.612476",
    "updated_at": "2026-02-03T15:53:11.612484"
}