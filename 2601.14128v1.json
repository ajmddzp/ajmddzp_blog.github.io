{
    "id": "2601.14128v1",
    "title": "SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media",
    "authors": [
        "Shoujie Li",
        "Changqing Guo",
        "Junhao Gong",
        "Chenxin Liang",
        "Wenhua Ding",
        "Wenbo Ding"
    ],
    "abstract": "颗粒介质中的感知因粒子动力学难以预测而极具挑战。为应对此问题，我们提出SandWorm——一种通过蠕动运动增强移动能力的仿生螺旋驱动机器人，以及SWTac——一种采用主动振动弹性体的新型事件驱动视觉触觉传感器。该传感器通过弹簧隔振机制使事件相机与振动机械解耦，实现了对动态与静态物体的高质量触觉成像。在算法设计方面，我们提出IMU引导的时序滤波器以增强成像一致性，将MSNR指标提升24%。此外，我们通过振动参数、事件相机设置和弹性体特性对SWTac进行了系统优化。受非对称边缘特征启发，我们还基于U-Net实现了接触面估计。实验验证表明：SWTac具备0.2毫米的纹理分辨率、98%的岩石分类准确率及0.15牛的力估计误差；SandWorm在挑战性地形中展现出多模式运动能力（最高速度达12.5毫米/秒），并在复杂颗粒介质中成功执行管道疏通与地下勘探任务（观测成功率达90%）。野外实验进一步证实了该系统的实际性能。",
    "url": "https://arxiv.org/abs/2601.14128v1",
    "html_url": "https://arxiv.org/html/2601.14128v1",
    "html_content": "SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media\nShoujie Li\n∗\n, Changqing Guo\n∗\n, Junhao Gong, Chenxin Liang, Wenhua Ding, Wenbo Ding\n*These authors contributed equally to this\nwork.This work was supported by the National Key R&D Program of China grant (2024YFB3816000), Guangdong Innovative and Entrepreneurial Research Team Program (2021ZT09L197), Tsinghua Shenzhen International Graduate School-Shenzhen Pengrui Young Faculty Program of Shenzhen Pengrui Foundation (No. SZPR2023005) and Meituan.\n(Corresponding author: Wenbo Ding, ding.wenbo@sz.tsinghua.edu.cn) Shoujie Li, Changqing Guo, Junhao Gong, Chenxin Liang, Wenhua Ding, Wenbo Ding are with Tsinghua Shenzhen International Graduate School, Shenzhen 518055, China. (email: {lsj20, gcq24, gongjh24, liangcx23, dingwh24}@mails.tsinghua.edu.cn, ding.wenbo@sz.tsinghua.edu.cn))Shoujie Li is also with the School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore 639956, Singapore.\nAbstract\nPerception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw‑actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event‑based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac’s 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system’s practical performance.\nI\nIntroduction\nGranular materials, such as sand, gravel, and powder, exhibit complex behaviors characterized by particle flowability, frictional interactions, and heterogeneous porosity\n[\n28\n,\n1\n,\n24\n]\n. These materials are commonly encountered in underground infrastructure inspection\n[\n31\n]\n, mineral exploration\n[\n43\n]\n, disaster rescue\n[\n47\n]\n, and planetary missions\n[\n60\n]\n. In such applications, robotic systems often employ active techniques including drilling and vibration to facilitate penetration\n[\n45\n,\n61\n,\n42\n,\n23\n]\n. Due to the low visibility and unpredictable nature of these environments, tactile feedback is critical. It provides real-time information on contact forces and material properties, thereby enabling effective perception and adaptive control\n[\n35\n,\n10\n]\n.\nFigure 1:\nOverview of SandWorm. (a) The snakelike robot is navigating an outdoor trench with the conical elastomer. (b) Manually operated drilling with the frustoconical elastomer on a sandy beach, showing the texture of an ammonoid fossil. (c) Schematic of SandWorm, featuring an integrated active vibration mechanism and hybrid locomotion system. An IMU-guided temporal filter is introduced to enhance imaging quality.\nConventional electronic tactile sensors (e.g., strain gauges, piezoresistive arrays) would suffer performance degradation due to strong vibrational resonance in the drilling process\n[\n14\n,\n52\n]\n.\nVisuotactile sensors reduce these mechanical drawbacks by separating the electronics from their sensing skin\n[\n34\n]\n.\nHowever, their reliance on the camera introduces new limitations, as high-frequency vibration would induce motion blur, image trailing, and feature‑tracking errors\n[\n11\n,\n44\n]\n. With the event-based imaging mechanism, event cameras can capture environmental changes in extremely short time intervals, effectively avoiding the blurring issues associated with traditional cameras due to long exposure times\n[\n13\n]\n. Moreover, with higher dynamic range, event cameras can detect subtle changes, making them ideal for precise visuotactile perception in granular media under vibration. However, event cameras intrinsically respond only to dynamic changes. To achieve static imaging, an imaging medium should be employed to generate detectable events\n[\n18\n]\n.\nInspired by benthic polychaetes such as\nalitta virens\n, which use sensory-rich heads and muscle-driven peristalsis combined with spiral locomotion to traverse granular sediments\n[\n59\n,\n16\n]\n, we present SandWorm, a biomimetic robotic system. SandWorm mimics the worm’s locomotion to explore granular media and confined environments, and integrates SWTac, an actively vibrated event-based visuotactile sensor for tactile perception.\nThe main contributions are as follows:\n•\nActive Vibration for Static Tactile Imaging:\nWe introduce a vibration-isolation mechanism in the SWTac visuotactile sensor that generates controlled elastomer vibration, enabling static tactile perception by the mechanically isolated event camera. This approach not only achieves pixel-level tactile imaging at 1000 Hz by overcoming the inherent limitation of event cameras in capturing stationary objects, but also leverages vibration-induced granular fluidization to reduce subsurface penetration resistance.\n•\nIMU-Guided Perception Algorithm :\nTo enhance event stream consistency, we propose an IMU-guided temporal filter based on a vibration-imaging model, achieving 24% improvement in masked signal-to-noise ratio (MSNR). Furthermore, leveraging asymmetric edge features verified by finite element analysis, we employ a U-Net model for contact surface estimation, achieving a structural similarity index measure (SSIM) about 0.97. We also systematically characterize the effects of vibration frequency, amplitude, direction, event camera sensitivity, and elastomer viscoelasticity in SWTac.\nTABLE I:\nComparison of invasive robotic perception methods\nReference\nSensor\nResolution\nForce & Contact\nRichter et al.\n[\n48\n]\nCamera\nPixel Level,\n∼\n\\sim\n30 Hz\n✗\nXue et al.\n[\n62\n]\nCamera\nPixel Level,\n∼\n\\sim\n30 Hz\n✗\nKwon et al.\n[\n30\n]\nCamera\nPixel Level,\n∼\n\\sim\n30 Hz\n✗\nKakogawa et al.\n[\n22\n]\nCamera\nPixel Level,\n∼\n\\sim\n30 Hz\n✗\nNourizadeh et al.\n[\n40\n]\nIMU\nSingle Point, 5 Hz\n✗\nKolvenbach et al.\n[\n27\n]\nForce\nSingle Point, 2 Hz\n✓\nRuppert et al.\n[\n50\n]\nForce\nSingle Point, 330 Hz\n✓\nRussell et al.\n[\n51\n]\nChemistry\nSingle Point, 0.003 Hz\n✗\nDigger Finger\n[\n45\n]\nVisuotactile\nPixel Level,\n∼\n\\sim\n30 Hz\n✓\nSandWorm (Ours)\nVisuotactile\nPixel Level, 1000 Hz\n✓\nTABLE II:\nComparison of event-based sensors for robotic applications\nSensor\nLatency\nSpatial Resolution\nForce Sensing\nSensing Type\nCamera\nTexture\nCharacteristics\nNoise2Image\n[\n7\n]\n100 ms\nN/A\nN/A\nVision\nEV (Dynamic + Static)\nN/A\nStatic Imaging for EV, Software\nYousefzadeh et al.\n[\n64\n]\n100 ms\n1 mm\n✗\nVision\nEV (Dynamic + Static)\n✓\nStatic Imaging for EV, Hardware\nAMI-EV\n[\n18\n]\n0.8 ms\n0.04 mm\n✗\nVision\nEV (Dynamic + Static)\n✓\nStatic Imaging for EV, Hardware\nNeuTouch\n[\n56\n]\n10 ms\n1 mm\n✓\nVision, Tactile\nRGB + EV (Dynamic)\n✓\nMultimodal Processing\nBaghaei et al.\n[\n2\n]\n21 ms\n0.2 mm\n0.16 N\nVision, Tactile\nEV (Dynamic)\n✓\nForce Estimation\nGelEvent\n[\n63\n]\n5.5 ms\n225 markers (3 mm)\n0.8 N\nVisuotactile\nEV (Dynamic)\n✗\nContact Area Estimation\nE-BTS\n[\n39\n]\n2 ms\n5 markers\nShear 0.27 N\nVisuotactile\nEV (Dynamic)\n✗\nTeleoperation\nEvetac\n[\n12\n]\n1 ms\n63 markers\nShear 0.22 N\nVisuotactile\nEV (Dynamic)\n✗\nSlip Detection\nDigger Finger\n[\n45\n]\n33 ms\n0.3 mm\n✓\nVisuotactile\nRGB (Static)\n✓\nGelSight\n[\n65\n]\nin Granular Media\nSWTac (\nOurs\n)\n1 ms\n0.2 mm\nShear 0.15 N\nVisuotactile\nEV (Dynamic + Static)\n✓\nGranular Media\nStatic Imaging for EV, Hardware\n•\nHybrid Locomotion Mechanism:\nWe introduce a hybrid locomotion mechanism that synergistically combines screw-driven rotation with pushrod-actuated peristalsis for enhanced propulsion in confined environments. Implemented in SandWorm, a snakelike robot integrated with SWTac, this mechanism improves propulsion efficiency while enabling adaptable application.\n•\nReal-World Field Validation:\nExtensive experiments demonstrate SWTac’s capabilities in precise force estimation (0.15 N accuracy), high-resolution texture sensing (0.2 mm resolution), and robust stone classification (98% accuracy). SandWorm demonstrates robust performance across diverse field scenarios, successfully executing high-curvature movements, steering and multi-terrain traversal (dense grass, tangled bushes, cement surfaces) at speeds up to 12.5 mm/s; performing complex tasks such as pipeline dredging and subsurface exploration in non-uniform media (beach sand, garden soil, four industrial granular media); and achieving an observed 90% success rate in detecting subsurface objects.\nThe remainder of this paper is organized as follows. Section II reviews related works. Section III describes the design and framework of SWTac and SandWorm. Section IV presents the algorithms for enhancing event-based imaging, and characterization of SWTac. Section V details the experimental evaluations. Section VI discusses insights and future work. Finally, Section VII concludes this article.\nII\nRelated Works\nII-A\nPerception in Granular Media\nPerception in granular media poses unique challenges that have motivated a variety of non‐invasive and invasive solutions. Current non-invasive methods, such as LiDAR\n[\n66\n]\n, ultrasonic wave propagation\n[\n15\n]\n, and Rayleigh waves\n[\n32\n]\nstruggle in granular environments due to factors like clutter, material heterogeneity, and deformation under stress. Techniques like MRI\n[\n55\n]\nand dynamic X-ray radiography\n[\n17\n]\n, are useful for inferring media properties rather than detecting subsurface objects.\nInvasive sensing is therefore essential for subsurface exploration. Table\nI\nsummarizes existing invasive robotic perception approaches. Robots operating in granular media\n[\n62\n,\n48\n]\nand confined environments\n[\n22\n,\n30\n]\ntypically rely on cameras for perception, but these systems can only capture surface features and fail to image once the camera is in contact with the medium due to blocked illumination. Specialized invasive sensors are designed with penetration ability, but may suffer from prohibitively long detection times\n[\n51\n]\n. Force sensor\n[\n27\n,\n50\n]\nand IMU\n[\n40\n]\nare commonly used for locomotion control, yet they measure only localized contact information and cannot provide the broad coverage like vision-based methods. In contrast, visuotactile sensors integrate illumination, imaging, and contact skin into a single module\n[\n34\n]\n, making them ideal for exploration in granular media. For example, DiggerFinger\n[\n45\n]\nperforms exploration by GelSight\n[\n65\n]\nin a sandy environment, but its imaging performance would degrade under the strong vibrations required for medium penetration.\nII-B\nVisuotactile Sensors with Event Camera\nEvent cameras effectively mitigate motion blur, which is a common issue for traditional RGB cameras in dynamic environments\n[\n46\n,\n13\n]\n, such as those arising from vibration-driven exploration.\nAs shown in Table\nII\n, existing event-based visuotactile sensors generally based on markers\n[\n12\n,\n39\n,\n63\n]\n. They follow the design of conventional visuotactile sensors (e.g. GelSight\n[\n65\n]\n), only replacing the RGB camera with event camera.\nBy employing advanced elastomer designs, they achieve higher spatial resolution for tasks including force and contact surface estimation\n[\n63\n]\n.\nAlthough these sensors are particularly useful in teleoperation\n[\n39\n]\nand sliding detection\n[\n12\n]\n, their reliance on markers obstructs the field of view and limits imaging to taxel‐level resolution rather than pixel‐level\n[\n45\n]\n, resulting in poor texture perception and therefore unsuitable for exploration tasks.\nFigure 2:\nOverview of SandWorm’s mechanical structure. (a) Rendered views of SandWorm: an autonomous snakelike robot and a manually operated device. Cross-sectional views of (b) SWTac, the perception module, (c) the rotational driving module and (d) the snakelike tail module. (e) Exploded view of SWTac.\nIf not relying on markers, existing methods fix the event camera externally to the tactile sensor to capture the dynamic deformations of the sensing skin during contact\n[\n56\n,\n2\n]\n. Although effective for texture extraction, these approaches produce experimental setups to make the event camera remaining stationary, rather than proposing a compact, integrated visuotactile sensor. The primary challenge lies in the fact that event cameras only capture dynamic changes\n[\n13\n]\n. In integrated visuotactile systems, the sensing skin and event camera remain stationary relative to each other, so events are generated only during transient deformations\n[\n63\n]\n, resulting in sparse and inconsistent imaging. Markers have predetermined positions, enabling localization of taxels despite these gaps. But in visuotactile sensors without markers, the absence of predictable features makes it difficult to acquire high‐quality and consistent event‐based tactile images.\nSeveral vision-based solutions have been proposed to address the intrinsic limitation on static imaging of event cameras. Software-based noise sampling recovery methods\n[\n7\n]\ndemand high processing time. For hardware-based solutions, directly vibrating the event camera carries the risk of mechanical damage\n[\n64\n]\n. A better approach is to add an imaging medium outside the sensor to trigger events, such as using a rotating prism to redirect the light path while the camera remains static\n[\n18\n]\n. Particularly, in visuotactile sensors, the elastomer could serve as a natural medium between the event camera and the object. Therefore, without requiring additional imaging medium, introducing active vibration to the elastomer would enable the event‐based visuotactile sensor to capture a high‐quality and consistent tactile event stream.\nIn this article, we present SandWorm, a robot designed for navigation and exploration in both granular media and confined environments (Fig.\n2\n(a)). SandWorm comprises two main modules: an actuation module featuring a screw-actuated body enhanced with peristaltic motion for improved mobility, and a perception module, SWTac. SWTac is an active event-based visuotactile sensor that applies controlled vibration to the elastomer while mechanically isolating the event camera (Fig.\n2\n(b)). This active vibration effectively integrates perception and actuation, as it guarantees consistent tactile perception and simultaneously facilitates invasive exploration by loosening the granular material.\nIII\nHardware Structure\nThis section first details the hardware structure of SWTac, including the vibration-isolation mechanism, elastomer, and illumination design. We then introduce the actuation module, followed by an analysis of the rotational–peristaltic locomotion mechanics. Finally, we summarize SandWorm’s perception-actuation framework to highlight our core innovative modules and their interconnections.\nIII-A\nPerception Module Design\nThe structure of SWTac is designed to efficiently transmit active vibrations to the elastomer while simultaneously isolating the event camera. Fig.\n2\n(b) shows the cross-sectional view of SWTac, and Fig.\n2\n(e) presents a detailed schematic breakdown of its components. The core of this visuotactile sensor is the DVXplorer Mini event camera, selected for its compact size. The peripheral structure is designed around this camera and features two functionally distinct sections: a vibrational part and an isolated part.\nFor the vibrational part, the design aims to generate and guide multi-axis vibration using a compact assembly. This is achieved by mounting both the primary vertical actuator (electromagnetic valve, P15) and the supplementary horizontal actuators (eccentric motors, P11) onto a shared vibration connector (P9). This entire dynamic assembly is then coupled to the stationary inner shell (P14) via sliding rails (P10, P12). These rails function to constrain the motion primarily to the vertical axis while accommodating assembly tolerances, ensuring stable operation.\nFor the isolated part, the core design is to mechanically decouple the event camera (P4) from the high-vibration assembly. This is accomplished by soft-coupling the camera (mounted on P6) to the stationary inner shell using a dedicated multi-axis isolation system: eight springs (P5) provide horizontal isolation, and two flexible shaft connectors (P7) provide vertical isolation. This design effectively protects the sensitive electronics without obstructing the camera’s field of view. Finally, the LED board (P13) and diffuser plate (P8) are integrated for illumination. The whole assembly is secured by the primary load-bearing (Assembly VIII) and enclosed within the spiral outer shell (P16) to enhance drilling performance.\nThe elastomer serves as a fundamental component in visuotactile sensors. In granular media, elevated abrasive forces necessitate a thicker elastomer layer than those used in conventional designs to enhance durability. Therefore, polydimethylsiloxane (PDMS; Sylgard™ 184, Dow Corning) is adopted to guarantee optical transparency. The assembled elastomer tip (P1-P3) is installed at the end of the vibration connector. The vibration-isolation structure enables dual-axis vibrational control of the elastomer tip, both horizontally and vertically.\nThe illumination system is crucial to event cameras, as sufficient lighting is essential for enhancing imaging quality. In our design, we utilize a circular array of 14 LEDs (WS2812B 5V) with a diffuser plate.\nAs illustrated in Fig.\n3\n, with direct lighting, a peripheral reflective ring is observed. Direct dark‐field illumination would reduce the quality of the primary signal by diverting the light path from a direct frontal configuration to a lateral one.\nFig.\n3\n(b)iv shows the capture result by diffuse bright‐field illumination. The diffuser plate homogenizes the light distribution while attenuating its intensity, thereby reducing the severity of specular reflections. Additionally, the central opening of the diffuser plate is also optimized to provide a hardware feature mask that delineates the region of interest for subsequent software processing.\nFigure 3:\nIllumination setup and conditions.\n(a) Hardware setup and optical‑path layout.\n(b) Reconstructed images of event camera with\ni. No illumination;\nii. Direct dark‐field illumination;\niii. Direct bright‐field illumination;\niv. Diffuse bright‐field illumination with a diffuser plate.\nIII-B\nActuation Module Design\nSnakelike robots are commonly used for exploration in granular environments. These robots utilize different actuating methods, including lateral wave motion\n[\n4\n]\n, peristaltic motion\n[\n53\n,\n8\n]\nand screw mechanisms\n[\n48\n,\n60\n]\n. Screw mechanisms provide both propulsion and steering forces, making them particularly effective for navigating in confined spaces\n[\n9\n]\nand pipelines\n[\n48\n]\n. Therefore, we adopt a rotational‐peristaltic driving scheme for locomotion, which requires fewer actuators compared to lateral wave methods and enhances traction in granular media. As depicted in Fig.\n2\n(a), SandWorm has a reconfigurable design, and can work either as a autonomous snakelike robot or as a manually operated device. Both configurations differ only at the tail end, while the perception module (SWTac, Fig.\n2\n(b)) and the rotational driving module (Fig.\n2\n(c)) are identical.\nThe rotational driving module (Fig.\n2\n(c)) provides both data processing and screw actuation. An onboard computer (P17) executes event–stream processing and high-level controlling. A brushless motor (P20) is employed to continuously rotating the SWTac sensor. The drivetrain incorporates a coaxial bearing stack and a dedicated mechanical interface, ensuring clean separation between the rotating and non-rotating modules. A spiral outer shell surrounds the assembly, implemented as two semi-circular housings (P19, Assembly VI) that enclose the periphery of the module. The shell is machined from an Al–Mg alloy to improve structural robustness and wear resistance during contact with granular media.\nThe tail module (Fig.\n2\n(d)) hosts the remaining actuation. It embeds electronics within a collapsible bellows conduit (P34). Steering actuation is provided by a servo motor (P26), while peristaltic motion is generated by a pushrod mechanism (P33). An ESP32 microcontroller (P27) and a compact router (P28) serve as the low-level controller and wireless communication backbone. Linear guides (P32) and reinforced metal connector (P30) increase stiffness under load. At the distal end, a compliant, non-extendable support hose (P37) with cable (P36) inside is employed. For extended applications, a pushrod (P39) of larger size will be mounted to the rotational driving module, forming a hand‑held (P40) drilling assembly.\nFigure 4:\nOverview of the SandWorm robot’s DOFs. Locomotion is achieved by combining rotation, peristaltic translation, and vertical oscillation. The vibrational elastomer tip for perception is also illustrated.\nIII-C\nLocomotion Mechanics\nSandWorm provides three degrees of freedom (DOFs) of locomotion, as shown in Fig.\n4\n: a rotation of the spiral shell driven by a brushless motor; a servo-driven vertical oscillation for steering; and a peristalsis motion via internal pushrods for linear translation. The front rotational axis is rigidly coupled to SWTac enclosed by the spiral shell. The rear of SandWorm is guided by a flexible hose which supplies reaction force. Consequently, with its tail serving as a pivot, SandWorm leverages frictional contact to realize forward motion.\nTo realize robust locomotion under frictional contact, SandWorm employs a peristaltic drive made of two alternating phases that combine screw traction with pushrod actuation. In confined environments (e.g., pipelines), gravity together with the pipe support provides roll stability so that a single-chirality screw can generate forward thrust without requiring paired counter-rotating screws. Let\nF\np\nF_{\\text{p}}\ndenote the axial force applied by the pushrod and\nF\nfriction\nF_{\\text{friction}}\nthe lumped friction opposing motion. The effective gravitationally resolved driving component along the axis is denoted\nF\npropel\nF_{\\text{propel}}\n(cf. Appendix\nA\n). During the extension phase, the pushrod assists propulsion, leading to the net force:\nF\nextension\n=\nF\npropel\n+\nF\np\n−\nF\nfriction\n,\nF_{\\text{extension}}=F_{\\text{propel}}+F_{\\text{p}}-F_{\\text{friction}},\n(1)\nwhich produces a rapid forward stroke. During retraction, the pushrod opposes motion, and the net force becomes:\nF\nretraction\n=\nF\npropel\n−\nF\np\n−\nF\nfriction\n,\nF_{\\text{retraction}}=F_{\\text{propel}}-F_{\\text{p}}-F_{\\text{friction}},\n(2)\nyielding little displacement. This asymmetric two-phase actuation, akin to a controlled shift of the robot’s center of mass, produces a net advance greater than that of continuous rotation alone.\nIII-D\nPerception-Actuation Framework\nThe system architecture of SandWorm, as depicted in Fig.\n5\n, integrates the SWTac visuotactile sensor with locomotive modules, as well as a hierarchical signal processing pipeline. The perception component is the SWTac sensor. Its vibrational part includes the actuators, while its isolated part contains the event camera and the on-board computer. The robot’s actuation is handled by the locomotive modules, comprising pushrods, a brushless motor, and a servo motor, all managed by an ESP32 controller and a central battery. The interconnection between the SWTac and the locomotive modules is threefold. Brushless motor mechanically couples the sensor to the locomotive body. Conductive slip ring delivers power from the non-rotating locomotive section to the rotating SWTac as the electrical link. The communication link connects the on-board computer to the ESP32 by WiFi.\nThe algorithmic pipeline (top panel) runs entirely on the SWTac’s on-board computer. Raw sensor data, specifically the event stream and IMU readout, are used to reconstruct frames and are processed by our IMU-guided temporal filter, which generates consistent tactile images. This processed data is then passed to the downstream tasks (e.g., contact surface estimation, force estimation) to generate high-level motion commands, either directly or indirectly through human-in-the-loop control (utilizing remote real-time image viewing). These commands are transmitted wirelessly via the communication interconnect to the ESP32 controller in the locomotive modules, which in turn actuate the motors and pushrods. Thus, the SWTac’s tactile perception, which relies on vibration and motion, acts back on the actuators to form a fundamental perception-actuation loop.\nFigure 5:\nSystem architecture of the SandWorm robot, detailing the hardware and algorithm integration. The hardware (bottom) features the SWTac visuotactile sensor (comprising an isolated and a vibrational part) and the locomotive modules. The algorithmic pipeline (top) processes the event stream and IMU readout from the event camera, including an IMU-guided temporal filter followed by downstream tasks.\nIV\nAlgorithm Design\nIn this section, we first introduce the workflow to reconstruct grayscale images from the asynchronous event stream. To mitigate vibration‐induced temporal inconsistencies, we develop an IMU‑guided temporal filter based on the theoretical model of event‐based imaging under vibration.\nAfter that, motivated by the asymmetric edge features in reconstructed tactile images, we conduct contact surface estimation, beyond the edge information captured by the event camera.\nFinally, we characterize the imaging performance of SWTac by different parameters, including vibration amplitude, frequency, direction, event camera sensitivity and elastomer viscoelasticity.\nIV-A\nEvent-based Image Reconstruction\nFigure 6:\nData acquisition platform.\nSWTac with frustoconical elastomer is connected to the robotic arm.\nThe texture board and IMU are attached to a programmable vibration generator.\nIV-A\n1\nHardware Setup\nTo enhance perception performance by algorithm design, we set up a data acquisition platform in Fig.\n6\n. Instead of using the electromagnetic valve, we utilized a vibration generator to provide active vibration. This generator is driven by a signal generator with a power amplifier, providing explicit frequency control and implicit amplitude control via adjustable power of stable sine wave. An IMU mounted on the generator communicates with a computer in real time to compute the amplitude, enabling adjustments. A UR5 robotic arm was employed to position the sensor accurately above the vibration generator.\nTo ensure the effect of the external vibration generator is equivalent to the sensor’s active vibration, we modified the sensor housing by removing the vertical elastic DOF, thereby rigidly securing the elastomer. Externally induced vibrations technically would produce the same effect as actively vibrating the elastomer, given that the sensor remains stationary in both setups. We verified the interchangeability of these two actuators in Section\nV-A\n. The frustoconical elastomer with a flat surface was used for data collection.\nFor the event camera settings, we used three threshold levels of the camera change-detection circuitry to assess sensitivity\n[\n36\n]\n. The threshold defines the minimal log intensity variation at the pixel level required to induce an event. Lower thresholds enhance sensitivity by generating more events in response to subtle light changes, while higher thresholds diminish sensitivity by excluding minor intensity variations. We employed these three threshold values to capture event streams corresponding to different camera sensitivity settings.\nFigure 7:\nPrinciple and results of grayscale image reconstruction.\n(a) Event stream during vibration, with positive events in red and negative events in green.\n(b) Texture board.\n(c) Grayscale images under different reconstruction frame rates.\nIV-A\n2\nReconstruction Workflow\nThe captured event streams are sequences of discrete events\nℰ\n=\n{\ne\ni\n}\n\\mathcal{E}=\\{e_{i}\\}\n, each encoding\n(\nx\ni\n,\ny\ni\n,\np\ni\n,\nt\ni\n)\n(x_{i},y_{i},p_{i},t_{i})\nas a pixel coordinate, a brightness-change polarity and a precise timestamp. An example event stream is depicted in Fig.\n7\n(a). Under the active vibration of 50 Hz in SWTac, the temporal sequence of the event stream shows an alternating pattern with color-encoded polarity.\nNext, the event stream is processed to a sequence of frame-based grayscale images\nG\nk\nG_{k}\n, as shown in Fig.\n7\n(c). This conversion is achieved by event accumulation. As a pre-processing step, an active background filter was adopted to suppress noise from the raw stream, yielding a filtered event set\nℰ\nfiltered\n\\mathcal{E}_{\\text{filtered}}\n. Subsequently, we convert these asynchronous events into frames at\nf\nr\n​\na\n​\nt\n​\ne\nf_{rate}\nframe rate by slicing the filtered stream into discrete time windows of a specific accumulation time,\nΔ\n​\nT\nacc\n=\n1\n/\nf\nr\n​\na\n​\nt\n​\ne\n\\Delta T_{\\text{acc}}=1/f_{rate}\n, and projecting the events within each slice onto the X-Y plane. Each event contributes a fixed numeric value\nC\nC\n, namely the contribution, to its pixel’s brightness. This process is formally defined as:\nG\nk\n​\n(\nx\n,\ny\n)\n=\n∑\ne\ni\n∈\nℰ\nfiltered\n(\nx\ni\n,\ny\ni\n)\n=\n(\nx\n,\ny\n)\nt\ni\n∈\n[\nT\nk\n,\nT\nk\n+\nΔ\n​\nT\nacc\n)\nC\nG_{k}(x,y)=\\sum_{\\begin{subarray}{c}e_{i}\\in\\mathcal{E}_{\\text{filtered}}\\\\\n(x_{i},y_{i})=(x,y)\\\\\nt_{i}\\in[T_{k},T_{k}+\\Delta T_{\\text{acc}})\\end{subarray}}C\n(3)\nwhere\nG\nk\n​\n(\nx\n,\ny\n)\nG_{k}(x,y)\nis the intensity of frame\nk\nk\nat pixel\n(\nx\n,\ny\n)\n(x,y)\n, and\nT\nk\nT_{k}\nis the start time of the frame. This formulation inherently applies a step-function reset between windows, preventing carry-over artifacts. Moreover, by discarding polarity information (the\np\ni\np_{i}\nterm is omitted), we rely solely on event counts per pixel, which further sharpens edges in the reconstructed frames. The data processing of event camera was primarily conducted using the DV-processing library\n[\n20\n]\n.\nTABLE III:\nParameter settings for grayscale image reconstruction\nAccumulation Time\nContribution\nReconstruction Rate\n1 ms\n1\n1000 Hz\n2 ms\n1\n500 Hz\n5 ms\n0.5\n200 Hz\n10 ms\n0.25\n100 Hz\n33 ms\n0.08\n30 Hz\nWith carefully chosen parameters for accumulation time and contribution, we could achieve real‐time grayscale reconstruction at up to 1000 Hz, as listed in Table\nIII\n. However, the alternating pattern of the event stream directly influences the reconstruction quality. Fig.\n7\n(c) shows that a low reconstruction rate (e.g., 100 Hz) produces consistent images that do not capture the vibration pattern. In contrast, a higher reconstruction rate (e.g., 1000 Hz) captures the dynamic variations of the vibration pattern, resulting in image inconsistency.\nIV-A\n3\nImaging Quality Metrics\nTo assess the quality of the reconstructed grayscale images, we introduce MSNR, Shannon entropy and MSE. MSNR is defined as the masked signal-to-noise ratio calculated by excluding most of the uninformative black background. Specifically, we compute the SNR only within the foreground region\nΩ\n\\Omega\nas follows:\nMSNR\n=\n10\n​\nlg\n⁡\n(\n∑\n(\ni\n)\n∈\nΩ\nI\n​\n(\ni\n)\n2\n∑\n(\ni\n)\n∈\nΩ\n|\nI\n​\n(\ni\n)\n−\nμ\nΩ\n|\n2\n×\nN\nΩ\nN\nImage\n)\n,\n\\text{MSNR}=10\\lg\\left(\\frac{\\sum_{(i)\\in\\Omega}I(i)^{2}}{\\sum_{(i)\\in\\Omega}\\left|I(i)-\\mu_{\\Omega}\\right|^{2}}\\times\\frac{N_{\\Omega}}{N_{\\text{Image}}}\\right),\n(4)\nwhere\nI\n​\n(\ni\n)\nI(i)\nis the grayscale value of the\ni\ni\n-th pixel in the image, and\nμ\nΩ\n\\mu_{\\Omega}\nis the mean grayscale value over\nΩ\n\\Omega\n.\nN\nΩ\nN_{\\Omega}\nand\nN\nImage\nN_{\\text{Image}}\nrepresent the number of pixels in region\nΩ\n\\Omega\nand the image.\nThe Shannon entropy\nS\nS\nis adopted to quantify the information content of the image:\nS\n=\n−\n∑\nl\n=\n0\nL\n−\n1\nq\nl\n​\nlog\n2\n⁡\nq\nl\n,\nS=-\\sum_{l=0}^{L-1}q_{l}\\log_{2}q_{l},\n(5)\nwhere\nq\nl\nq_{l}\nis the occurrence probability of the\nl\nl\n-th grayscale level and\nL\nL\nis the number of grayscale levels.\nThe MSE metric is used to evaluate the similarity between the reconstructed image and the ground truth edges. To establish the groundtruth image, we first extract the edges from the corresponding texture board model, then compute the undistorted coordinate transformation by camera calibration. The MSE is calculated as:\nMSE\n=\n1\nN\nΩ\n​\n∑\ni\n=\n1\nN\nΩ\n|\nI\n​\n(\ni\n)\n−\nI\nGT\n​\n(\ni\n)\n|\n2\n,\n\\text{MSE}=\\frac{1}{N_{\\Omega}}\\sum_{i=1}^{N_{\\Omega}}\\left|I(i)-I_{\\text{GT}}(i)\\right|^{2},\n(6)\nwhere\nI\nGT\n​\n(\ni\n)\nI_{\\text{GT}}(i)\nis the corresponding pixel value of ground truth.\nIV-B\nIMU-Guided Temporal Filter\nIn this part, we propose the IMU-guided temporal filter, addressing the temporal inconsistency of the event stream. We formulated and validated the mathematical model of event-based visuotactile perception with active vibration, proposed a deployment strategy and analyzed the statistical results.\nIV-B\n1\nMotivation\nWhile an active vibration strategy is essential to make static objects visible to the event camera, it also introduces oscillatory interaction between the elastomer and external objects. Since event cameras are highly sensitive to these temporal variations, the vibration induces fluctuations in event density and imaging quality when using a high reconstruction rate. As seen in Fig.\n7\n(c), this results in a temporally variant stream where high-quality frames are interleaved with sparse, low-quality frames, degrading reconstruction stability.\nTherefore, it’s necessary to automatically retain only the high-quality segments of the event stream. To achieve this, we leverage the real-time acceleration data from the event camera’s internal IMU as a prior. Because the acceleration is rigidly coupled to the vibration mechanism, it could provide a precise estimation of the elastomer’s current vibration phase and, by extension, the expected imaging quality in real-time. We use this information to develop a model-based algorithm that guides a temporal gate: the filter actively retains the event stream during high-quality phases and discards the stream during low-quality phases. This process ensures the final reconstructed images are built only from high-quality data with enhanced consistency.\nIV-B\n2\nMathematical Modeling\nTo establish a model for event-based tactile imaging under active vibration, we start by analyzing the relationship between vibrational contact and the quality of reconstructed images.\nFor simplification, we analyze the vibration pattern using only vertical vibration. The vibration excitation model of the elastomer can be expressed as a sinusoidal function. During tactile interactions between the elastomer and external objects, several factors including material properties, deformation characteristics, and system hysteresis would cause variations in amplitude and introduce a phase delay. The resulting vibration model can be modelled as:\nf\n​\n(\nt\n)\n=\nA\n​\nsin\n⁡\n(\nω\n​\nt\n+\nb\n)\n,\nf(t)=A\\sin(\\omega t+b),\n(7)\nwhere\nf\nf\n,\nA\nA\n,\nω\n\\omega\n,\nt\nt\n, and\nb\nb\nare the displacement, amplitude, the angular frequency, time, and the system’s phase delay, respectively.\nIn the imaging model of event cameras, we use an abstract metric, imaging quality (IQ), to analyze the performance. Specific metrics for evaluation will be provided in the next section. The characteristic of event cameras is to capture changes in illumination, and therefore, IQ tends to increase with the absolute vibration velocity of the imaging medium, i.e., the elastomer. Therefore, we assume a proportional relationship for simplicity in modeling as:\nIQ\n​\n(\nt\n)\n≈\nu\n​\n|\nd\n​\nf\n​\n(\nt\n)\nd\n​\nt\n|\n=\n|\nu\n​\nA\n​\nω\n​\ncos\n⁡\n(\nω\n​\nt\n+\nb\n)\n|\n.\n\\text{IQ}(t)\\approx u\\left|\\frac{\\text{d}f(t)}{\\text{d}t}\\right|=|uA\\omega\\cos(\\omega t+b)|.\n(8)\nwhere\nu\nu\nis a scaling factor. By trigonometric identities and Fourier series, the above equation can be rewritten in a standard sinusoidal form:\nIQ\n​\n(\nt\n)\n=\n2\n​\n|\nu\n​\nA\n​\nω\n|\nπ\n+\n4\n​\n|\nu\n​\nA\n​\nω\n|\n3\n​\nπ\n​\nsin\n⁡\n(\n2\n​\nω\n​\nt\n+\n2\n​\nb\n+\nπ\n2\n)\n,\n\\text{IQ}(t)=\\frac{2|uA\\omega|}{\\pi}+\\frac{4|uA\\omega|}{3\\pi}\\sin(2\\omega t+2b+\\frac{\\pi}{2}),\n(9)\nwhich indicates that the imaging quality signal contains a sinusoidal component, a phase delay, and a constant offset.\nIn practice, the velocity is difficult to measure directly, and acceleration data from the IMU are typically used instead. The IMU measurement corresponds to the second derivative of displacement:\nIMU\n​\n(\nt\n)\n=\nd\n2\n​\nf\n​\n(\nt\n)\nd\n​\nt\n2\n+\nϵ\n​\n(\nt\n)\n=\nA\n​\nω\n2\n​\nsin\n⁡\n(\nω\n​\nt\n+\nπ\n)\n+\nϵ\n​\n(\nt\n)\n.\n\\text{IMU}(t)=\\frac{\\text{d}^{2}f(t)}{\\text{d}t^{2}}+\\epsilon(t)=A\\omega^{2}\\sin(\\omega t+\\pi)+\\epsilon(t).\n(10)\nwhere the\nϵ\n​\n(\nt\n)\n\\epsilon(t)\nterm, representing the noise and non-periodic elements, which could be effectively handled by band-pass filtering.\nAt this point, we can make an inference between the vibration and imaging models. Comparing the IQ and IMU signals, two important relationships are utilized:\n•\nThe signal frequency of IQ is explicitly twice that of the IMU signal.\n•\nThere is a stable but implicitly phase difference between the two signals.\nTherefore, a temporal alignment between the two signals is required to estimate the optimal time shift\nΔ\n​\nt\n^\n\\hat{\\Delta t}\n. The theoretical relationship between the filtered IMU signal and the imaging quality can thus be approximated as:\nIQ\n​\n(\nt\n)\n=\nk\n⋅\n|\nIMU\n~\n​\n(\nt\n−\nΔ\n​\nt\n^\n)\n|\n+\nc\n,\n\\text{IQ}(t)=k\\cdot\\left|\\tilde{\\text{IMU}}(t-\\hat{\\Delta t})\\right|+c,\n(11)\nwhere\nk\nk\nis a scaling factor and\nc\nc\nis a constant offset, determined experimentally. Here,\nIMU\n~\n​\n(\n⋅\n)\n\\tilde{\\text{IMU}}(\\cdot)\nrepresents the IMU signal component obtained after applying a band-pass filter centered at the active vibration frequency\nω\n\\omega\n.\nFigure 8:\nValidation of the IMU‑guided temporal filter. (a) Fourier transform analysis of the imaging quality metric alongside IMU signal.\n(b) Peak‑to‑peak alignment.\n(c) Relations between IMU and MSNR, illustrating the regression result, filter boundaries and representing frames.\n(d) Intuitive illustration of IMU‑guided temporal filtering on the event stream.\n(e) Statistical comparison of imaging quality metrics before and after filtering.\nIV-B\n3\nVerification\nBased on the theoretical analysis, we performed frequency validation, phase alignment, regression fitting to implement the calibration process of the IMU-guided temporal filter. We used data acquired under a 30 Hz vertical vibration with a reconstruction rate of 500 Hz. MSNR is used as the representing imaging quality metric and computed for each image, forming a time series IQ\n(\nt\n)\n(t)\nof the total 2500 grayscale images in 5 s. In parallel, the IMU data, along with their corresponding timestamps, were extracted from the raw event camera output. We use the Z‑axis of the IMU data, which represents the vertical vibration, denoted as Acc_Z.\nFirst, we applied Fourier transform analysis to both the MSNR series and the IMU measurements. Fig.\n8\n(a) shows that the dominant frequency in the imaging quality metric is 60 Hz, precisely twice of the 30 Hz vibration frequency in the IMU signal, which conforms to our theoretical predictions. Next, we aligned the timestamps of the MSNR series and the IMU signal by a peak-to-peak matching method. We first detecting the peak locations of both IMU and imaging quality signals, and then pairing each IMU peak with its nearest imaging quality peak to determine the average temporal delay\nΔ\n​\nt\n^\n\\hat{\\Delta t}\n:\nΔ\n​\nt\n^\n=\n1\nN\n​\n∑\nn\n=\n1\nN\n(\nt\nIQ peak matched\n(\nn\n)\n−\nt\nIMU peak\n(\nn\n)\n)\n,\n\\hat{\\Delta t}=\\frac{1}{N}\\sum_{n=1}^{N}\\left(t_{\\text{IQ peak matched}}^{(n)}-t_{\\text{IMU peak}}^{(n)}\\right),\n(12)\nAs depicted in Fig.\n8\n(b), the 100 ms window shows the frequency doubling and time shift characteristics, verifying the temporal relationship.\nOnce the signals are temporally aligned, we pair each reconstructed frame’s MSNR metric with the temporally matched IMU measurement. The images with higher MSNR visually have more complete and distinct edges. Specifically, we calculate the mean of the IMU measurements in the Z-axis over the corresponding interval of accumulation. As shown in Fig.\n8\n(c), the scatter plot of MSNR versus the corresponding IMU measurements displays a clear correlation. A regression analysis was performed based on Eqn. (\n11\n), yielding an\nR\n2\nR^{2}\nvalue of 0.805. This result validates our theoretical analysis and demonstrates that the IMU measurements can reliably predict the imaging quality of reconstructed frames.\nIV-B\n4\nDeployment Strategy\nFor the purpose of filtering, a minimum acceptable imaging quality threshold (\nIQ\nthresh\n\\text{IQ}_{\\text{thresh}}\n) is defined. In this experiment, for instance, the average MSNR measurement of 10.00 is adopted. Based on the regression analysis of the calibration data, this\nIQ\nthresh\n\\text{IQ}_{\\text{thresh}}\nis mapped to a corresponding set of IMU measurement thresholds. Specifically, it’s more than 10.08 or less than 9.00 in this example. Using this criterion, poor-quality segments of the event stream can be identified and filtered out, as intuitively illustrated in Fig.\n8\n(d). The result in Fig.\n8\n(c) highlights the retained frames in blue, where 77.8% of the data were preserved.\nThe procedure of the IMU-guided temporal filter is summarized in Algorithm\n1\n. Rather than directly accumulating events, the IMU measurements embedded in the event camera were used as guidance to filter out the poor streams. It operates in two distinct phases: a one-time calibration and a real-time application. For calibration, it determines the optimal temporal alignment\nΔ\n​\nt\n^\n\\hat{\\Delta t}\n(Eqn. (\n12\n)) and the IQ (Eqn. (\n4\n)) threshold\nIQ\nthresh\n\\text{IQ}_{\\text{thresh}}\nbased on the reconstructed frames (Eqn. (\n3\n)). Through regression analysis (Eqn. (\n11\n)), this quality threshold is then mapped to a corresponding motion magnitude threshold,\nIMU\n~\nthresh\n\\tilde{\\text{IMU}}_{\\text{thresh}}\n. Following calibration, the main filter iterates through the remaining event stream. It interpolates the filtered and aligned IMU value (\nIMU\n~\nlive\n\\tilde{\\text{IMU}}_{\\text{live}}\n) and compares its magnitude to\nIMU\n~\nthresh\n\\tilde{\\text{IMU}}_{\\text{thresh}}\n. Only segments that satisfy the criterion (\n|\nIMU\n~\nlive\n|\n≥\nIMU\n~\nthresh\n|\\tilde{\\text{IMU}}_{\\text{live}}|\\geq\\tilde{\\text{IMU}}_{\\text{thresh}}\n) are reconstructed (Eqn. (\n3\n)) and outputted, effectively discarding segments predicted to have poor imaging quality.\nAlgorithm 1\nIMU-Guided Temporal Filter.\nCalibrate\n(\nℰ\ncal\n\\mathcal{E}_{\\text{cal}}\n,\nIMU\ncal\n\\text{IMU}_{\\text{cal}}\n,\nf\nrate\nf_{\\text{rate}}\n,\nω\n\\omega\n)\n(\nG\ncal\n,\nT\nframes\n)\n←\n({G}_{\\text{cal}},{T}_{\\text{frames}})\\leftarrow\nReconstruction(\nℰ\ncal\n\\mathcal{E}_{\\text{cal}}\n,\nf\nrate\nf_{\\text{rate}}\n) by Eqn. (\n3\n)\nIQ\nseries\n←\n\\text{IQ}_{\\text{series}}\\leftarrow\nCalculate_IQ(\nG\ncal\n{G}_{\\text{cal}}\n) by Eqn. (\n4\n)\nIQ\nthresh\n←\nMean\n​\n(\nIQ\nseries\n)\n\\text{IQ}_{\\text{thresh}}\\leftarrow\\text{Mean}(\\text{IQ}_{\\text{series}})\nIMU\n~\nsignal\n←\n\\tilde{\\text{IMU}}_{\\text{signal}}\\leftarrow\nBandpass(\nIMU\ncal\n\\text{IMU}_{\\text{cal}}\n,\nω\n\\omega\n)\nIMU\n~\nseries\n←\n\\tilde{\\text{IMU}}_{\\text{series}}\\leftarrow\nSample(\nIMU\n~\nsignal\n\\tilde{\\text{IMU}}_{\\text{signal}}\n,\nT\nframes\n{T}_{\\text{frames}}\n)\nΔ\n​\nt\n^\n←\n\\hat{\\Delta t}\\leftarrow\nAlignment(\nIQ\nseries\n\\text{IQ}_{\\text{series}}\n,\nIMU\n~\nseries\n\\tilde{\\text{IMU}}_{\\text{series}}\n,\nT\nframes\n{T}_{\\text{frames}}\n) by Eqn. (\n12\n)\n(\nk\n,\nc\n)\n←\n(k,c)\\leftarrow\nRegression(\nIQ\nseries\n\\text{IQ}_{\\text{series}}\n,\nIMU\n~\nseries\n\\tilde{\\text{IMU}}_{\\text{series}}\n,\nΔ\n​\nt\n^\n\\hat{\\Delta t}\n) by Eqn. (\n11\n)\nIMU\n~\nthresh\n←\n(\nIQ\nthresh\n−\nc\n)\n/\nk\n\\tilde{\\text{IMU}}_{\\text{thresh}}\\leftarrow(\\text{IQ}_{\\text{thresh}}-c)/k\nreturn\nΔ\n​\nt\n^\n\\hat{\\Delta t}\n,\nIMU\n~\nthresh\n\\tilde{\\text{IMU}}_{\\text{thresh}}\nImuGuidedFilter\n(\nℰ\nstream\n\\mathcal{E}_{\\text{stream}}\n,\nIMU\nstream\n\\text{IMU}_{\\text{stream}}\n,\nf\nrate\nf_{\\text{rate}}\n,\nω\n\\omega\n)\n(\nℰ\ncal\n,\nIMU\ncal\n)\n←\n(\\mathcal{E}_{\\text{cal}},\\text{IMU}_{\\text{cal}})\\leftarrow\nGetFirstSegment(\nℰ\nstream\n\\mathcal{E}_{\\text{stream}}\n,\nIMU\nstream\n\\text{IMU}_{\\text{stream}}\n)\n(\nΔ\n​\nt\n^\n,\nIMU\n~\nthresh\n)\n←\n(\\hat{\\Delta t},\\tilde{\\text{IMU}}_{\\text{thresh}})\\leftarrow\nCalibrate\n(\nℰ\ncal\n\\mathcal{E}_{\\text{cal}}\n,\nIMU\ncal\n\\text{IMU}_{\\text{cal}}\n,\nf\nrate\nf_{\\text{rate}}\n,\nω\n\\omega\n)\nwhile\nℰ\nstream\n\\mathcal{E}_{\\text{stream}}\nhas remaining segments\nt\nlive\n←\nt_{\\text{live}}\\leftarrow\nGet_Timestamp(\nℰ\nstream\n\\mathcal{E}_{\\text{stream}}\n)\nIMU\n~\nlive\n←\n\\tilde{\\text{IMU}}_{\\text{live}}\\leftarrow\nSample&Bandpass(\nIMU\nstream\n{\\text{IMU}}_{\\text{stream}}\n,\nt\nlive\n−\nΔ\n​\nt\n^\nt_{\\text{live}}-\\hat{\\Delta t}\n)\nif\n|\nIMU\n~\nlive\n|\n≥\nIMU\n~\nthresh\n|\\tilde{\\text{IMU}}_{\\text{live}}|\\geq\\tilde{\\text{IMU}}_{\\text{thresh}}\nG\nlive\n←\nG_{\\text{live}}\\leftarrow\nReconstruction(\nℰ\nstream\n\\mathcal{E}_{\\text{stream}}\n,\nf\nrate\nf_{\\text{rate}}\n) by Eqn. (\n3\n)\nOutput\nG\nlive\nG_{\\text{live}}\nend if\nend while\nStatistical analysis comparing the pre-filtered and post-filtered results (Fig.\n8\n(e)) shows that the IMU-guided temporal filter substantially improves the MSNR and entropy, while MSE has a slight increment. Specifically for MSNR, an increase in the average value (Avg.) corresponds to an overall enhancement of imaging quality, while a reduction in the standard deviation (Std.) reflects improved stability and temporal consistency. Further experiments across various vibration frequencies and reconstruction rates (Table\nIV\n) demonstrate a maximum average increase of 24.0% and a standard deviation decrease of 45.8% in MSNR.\nTABLE IV:\nMSNR enhancement by IMU-guided temporal filter\nVibration\nFrequency\nReconstru-\nction Rate\nProcessing\nDelay\nRetaining\nRate\nAvg.\nIncrease\nStd.\nDecrease\n30 Hz\n500 Hz\n2 ms\n77.8%\n16.0%\n45.8%\n30 Hz\n1000 Hz\n1 ms\n81.1%\n24.0%\n40.8%\n50 Hz\n1000 Hz\n1 ms\n93.0%\n11.6%\n23.0%\nIV-C\nContact Surface Estimation\nIn Section\nIV-A\n, we noted that the event camera only highlights the edges of contacting objects. As a result, the reconstructed image underestimates the interior contact region and fragments the physical footprint. Therefore, it’s necessary to estimate the full contact surface from the tactile image\n[\n3\n,\n33\n,\n56\n]\n. The capture edges arise from a relative height difference on the texture board. Due to the elastomer’s continuity and elasticity, it cannot conform exactly to the vertical face of the texture, as illustrated in Fig.\n9\n(a). Instead, the deformation is governed by Young’s modulus and Poisson’s ratio\n[\n41\n,\n38\n]\n. It radiates outward from the contact edge until the stress relaxes. In the protrusion scenario, the deformation of elastomer creating a shadow that extends beyond the true boundary, while the contact side remains sharp. The event intensity curve clearly illustrates the asymmetric feature, with one side sharpened while the other blurred.\nTo further validate this asymmetric deformation, we performed finite element analysis (FEA) simulations using Abaqus. The model depicted in Fig.\n9\n(b)iii featured a rectangular rigid indenter pressing onto the elastomer surface with 2 mm vertical displacement, employing surface-to-surface contact conditions. A fixed rigid acrylic plate fully constrains the elastomer from above. Elastomer parameters were set to a Young’s modulus of 0.2 MPa and a Poisson’s ratio of 0.48, using 1.5 mm C3D8R elements for meshing. The resulting deformation contour plot in Fig.\n9\n(b) confirms the asymmetrical edge deformation. Stress rapidly accumulates near the inner contact boundary, creating an abrupt deformation edge (sharp imaging), while it gradually dissipates outward, resulting in a smooth deformation gradient (blurred imaging). The close agreement between simulation results and experimental imaging validates our assumption about asymmetric deformation patterns. Therefore, without needing markers, we can leverage this feature to recover the full geometry of the contact surface rather than merely estimating its area\n[\n63\n,\n21\n]\n.\nFigure 9:\nMotivation and method of contact surface estimation. (a) Elastomer deformation under compression captured as asymmetric edge features. (b) Finite element analysis of elastomer deformation. iii. Simulation setup. Deformation contour plot in i. isometric; ii. top; iv. cross-sectional view of the elastomer.\nIV-D\nSensor Characterization\nTo characterize the SWTac sensor, we performed a statistical analysis on grayscale tactile images reconstructed from event streams. We systematically evaluated SWTac’s performance by varying key parameters, including: vibration amplitude (0–400 µm), frequency (0–400 Hz), event sensitivity (three thresholds), vibration direction (vertical and horizontal) and elastomer property (viscoelasticity). A constant reconstruction rate of 100 Hz was used throughout the characterization.\nFigure 10:\nStatistical evaluation of grayscale imaging quality under different vibrational conditions.\n(a–c) Amplitude variations with a mid sensitivity setting and three different frequencies.\n(d–f) Frequency variations with a 200 µm amplitude and three different event sensitivity settings.\nIV-D\n1\nVibration Amplitude\nFirst, we evaluated the sensor’s performance variation with amplitude at three different frequencies, as shown in Fig.\n10\n(a-c). The results indicate that when the amplitude is below 100 µm, the MSNR values are low but increase rapidly. Above 200 µm, the MSNR remains stable, but its variance increases. Notably, a medium vibration frequency of 50 Hz consistently provides superior performance.\nThe rationale for this behaviour is twofold. For lower amplitudes, the induced deformation is too small to generate sufficient information. While higher amplitudes initially capture more information (increasing entropy), amplitudes up to 200 µm yield signals that more closely approximate the ground truth (indicated by reduced MSE). Further increases in amplitude cause the signal to deviate due to higher reaction forces and noise. This is because the larger amplitude exceeds the elastomer’s effective deformation range. Consequently, the apparent Young’s modulus increases sharply, leading to smaller strain changes despite larger deformations. In short, although larger amplitudes capture more information, the added noise simultaneously reduces imaging quality.\nIV-D\n2\nVibration Frequency\nNext, we evaluated the sensor’s performance under various vibration frequencies (0–400 Hz), as shown in Fig.\n10\n(d-f). For frequencies between 0–100 Hz, the amplitude was kept at a constant 200 µm. While for frequencies between 100 and 400 Hz, the power amplification factor was fixed due to constraints of the IMU range. In the low-frequency range (0–100 Hz), the MSNR first gradually improves with increasing frequency, peaking at about 50 Hz. After this peak, the MSNR declines. At higher frequencies (100–400 Hz), damping effects become significant. For mid and low sensitivity, entropy drops to nearly zero, indicating no effective information is captured, and MSE approaches levels seen without vibration (0 Hz). For high sensitivity, MSNR remains relatively stable but is still worse than in the low-frequency range.\nThe initial performance increase up to 50 Hz is because the masked ground truth region accumulates more events, resulting in clearer edge signals (evidenced by higher entropy and lower MSE). The subsequent decline occurs because the elastomer’s damping effect begins to dominate over the active vibration. At higher frequencies, these damping effects and limited elastomer deformation cannot be fully compensated by post-processing, making high-frequency vibration unsuitable. Specifically, in the 20–40 Hz range, the higher standard deviation compared to 50 Hz is observable. This is attributed to the variance in reconstructed image, which arises from capturing the elastomer in different vibration phases. This is precisely the issue addressed in Section\nIV-B\n.\nIV-D\n3\nEvent Camera Sensitivity\nThe selection of the event threshold presents a clear trade-off in Fig.\n10\n(d-f). Our results show that the highest MSNR is achieved at approximately 50 Hz with a mid sensitivity setting. In the 20–80 Hz frequency range, the mid sensitivity consistently outperforms high sensitivity. At higher frequencies, although a higher sensitivity can capture texture patterns, the imaging quality still does not match the performance observed in the low-frequency range.\nThis trade-off is explained by the other metrics: a higher event threshold captures more information, reflected by higher entropy. However, it also produces thicker, more pronounced edges and increased background noise, leading to a higher MSE. In contrast, lower sensitivity reduces noise but may result in incomplete edge detection. The mid sensitivity setting, therefore, provides the optimal balance between information capture and noise induction.\nIV-D\n4\nVibration Direction\nWe evaluated the effect of superimposing horizontal vibration (0, 50, 75, 100 Hz) onto vertical vibration (0, 20, 50, 100 Hz). The results in Fig.\n11\n(a-c) show that introducing horizontal vibration generally enhances the MSNR, increases entropy, and reduces MSE. This improvement is especially significant when vertical vibration is less effective (e.g., at 0 Hz). An intuitive illustration is provided in Fig.\n11\n(d).\nThe improvement is attributed to the fact that horizontal vibration leads to more uniform contact, enhancing edge integrity in the reconstruction images. Although the amplitude of horizontal vibration is small, it effectively compensates when vertical vibration performance is suboptimal. Moreover, horizontal vibration helps release stress accumulated from vertical oscillation, contributing to structural stability mechanically. In summary, the combined 100 Hz horizontal and 50 Hz vertical vibrations yield the best overall performance.\nFigure 11:\nGrayscale imaging quality under different vibration directions.\n(a–c) Imaging quality metrics under different frequencies.\n(d) Illustration of reconstructed grayscale images.\nIV-D\n5\nElastomer Viscoelasticity\nThe performance peak observed in our experiments is closely related to the material properties of the PDMS elastomer. Theoretically, PDMS is a viscoelastic material whose mechanical behavior are primarily influenced by cross-linking ratio\n[\n57\n]\nand material thickness\n[\n58\n]\n. It exhibits time-delayed responses and energy dissipation, and in vibrational environments, the strain-rate effect further reduces the effective elastic range at higher frequencies\n[\n29\n]\n. Although the transient response of elastomers is studied in acoustics (typically\n>\n>\n1000 Hz)\n[\n58\n]\nand biomedical applications (around 1 Hz)\n[\n25\n]\n, experimental data in the mechanical vibration range (10–100 Hz) remains scarce. Therefore, we adopted a modeling approach to illustrate the general trend. We used the Kelvin-Voigt model to describe this behavior, whose constitutive equation is:\nσ\n​\n(\nt\n)\n=\nE\n​\nε\n​\n(\nt\n)\n+\nη\n​\nd\n​\nε\n​\n(\nt\n)\nd\n​\nt\n.\n\\sigma(t)=E\\varepsilon(t)+\\eta\\frac{\\text{d}\\varepsilon(t)}{\\text{d}t}.\n(13)\nFigure 12:\nGrayscale imaging quality for elastomers of different Shore hardness, representing different viscoelasticity. (a) Photographs of elastomers. (b) Imaging quality (e.g. MSNR) versus vibration frequency.\nThis model shows the elastomer acts as a first-order low-pass filter. At lower frequencies (\nω\n→\n0\n\\omega\\to 0\n), it exhibits a near-ideal elastic response (gain\n→\n1\n/\nE\n\\to 1/E\n), but at higher frequencies (\nω\n→\n∞\n\\omega\\to\\infty\n), viscous damping dominates, causing significant amplitude reduction and phase lag. The characterization result in Fig.\n10\n(d), showing a clear MSNR decline in the 50–120 Hz range, aligns with this model.\nTo confirm this, we fabricated and tested three elastomers with different hardness levels (Fig.\n12\n(a)). The results in Fig.\n12\n(b) show that as elastomer hardness increases, its elastic response is enhanced, and the MSNR peak frequency also rises. This confirms the relationship between material viscoelasticity and the damping behavior transition. The elastomer with a hardness of approximately 20 A (17:1 PDMS formulation) consistently delivered superior performance, balancing signal inconsistency at lower frequencies and energy loss at higher frequencies. The softer elastomer failed to capture texture details, while the harder one required higher contact forces, degrading performance.\nV\nExperiments\nIn this section, we first validate vibration designs in SWTac and the sensor performance on core tactile tasks: contact surface estimation, force estimation, and stone classification. We then evaluate the integrated SandWorm robot, assessing its penetration, autonomous locomotion, pipeline dredging, and subsurface exploration capabilities. Finally, we demonstrate the system’s real-world applicability through locomotion and perception field tests.\nV-A\nVibration System Validation\nFigure 13:\nHardware validation of the onboard vibration system. (a) Test setup to measure vibration amplitude versus compressive force. (b) Evolution of compressive force and vibration amplitude during penetration into granular media. (c) Structural schematic of the vibration isolation mechanism. (d) IMU measurements for isolation and FFT analysis.\nIn this experiment, we validated the vibration design of SWTac. First, we tested the performance of the onboard electromagnetic valve to ensure that its active vibration remains effective under actual compressive loads. As shown in Fig.\n13\n(a), we constructed a test platform with a force sensor to measure compressive load and an external IMU attached to the elastomer to measure vibration. We examined the amplitude-frequency characteristics under different contact force conditions. The results indicate that the vibration frequency consistently remained stable at 50 Hz, regardless of the contact force. Fig.\n13\n(b) shows that as the elastomer is pressed and hovered within the granular medium, the vibration amplitude decreases with increasing contact pressure. However, even under a high compressive load of 30 N, the electromagnetic valve maintained an amplitude of 134 µm. While this reduces the MSNR by approximately 20% from its maximum, the resulting signal remains sufficiently clear for effective perception. We further confirmed that under other normal pressure conditions, such as during rotation and in different granular media, the amplitude also remains well within the ideal operating range.\nFurthermore, we verified the effectiveness of the hardware-level vibration-isolation mechanism, which is crucial for stable imaging. As shown in Fig.\n13\n(c), the electromagnetic valve actuates the elastomer through the vibration connector, while the event camera is mounted to the inner shell via several vibration isolators. To validate this structure, we modified the SWTac to attach an IMU to the vibration connector, and thereby compare its data to the event camera’s internal IMU. The results in Fig.\n13\n(d) confirm an 83% reduction in acceleration transmitted to the camera, proving the hardware decoupling significantly attenuates vibration. Additionally, the FFT analysis shows the vibration frequency remains precisely locked at 50 Hz with an error of less than 0.2%. This stability originates from the MOSFET switch controlling the electromagnetic valve.\nTo this end, based on the characterization results and this hardware validation, we established the sensor configuration used for all subsequent experiments: a 17:1 PDMS elastomer, a 50 Hz vertical vibration from the electromagnetic valve, a 100 Hz horizontal vibration, and a mid-level event sensitivity setting.\nFigure 14:\nContact surface estimation. (a) Texture boards and real-world objects. (b) Raw accumulated grayscale images. (c) Estimation result by U-Net.\nV-B\nContact Surface Estimation\nMotivated by these intuitive asymmetric edge features, we adopted a U-Net architecture\n[\n49\n]\nto automatically recover the contact surface from raw accumulated edge images. This architecture integrates multi-scale context while preserving boundary detail through skip connections. It is well suited to delineate sharp contact boundaries and to suppress blur introduced by lateral elastomer deformation. We adopt a 4-level U-Net, which first maps a single-channel grayscale input to a single-channel mask. Each encoder stage uses two 3\n×\n\\times\n3 convolutions (BN+ReLU), followed by 2\n×\n\\times\n2 max pooling; the bottleneck repeats the double 3\n×\n\\times\n3 convolutions. The decoder upsamples with 2\n×\n\\times\n2 transposed convolutions and skip connections, each followed by two 3\n×\n\\times\n3 convolutions (batch normalization and ReLU). A final 1\n×\n\\times\n1 convolution with a sigmoid produces the mask.\nFor the dataset, we used 12 objects shown in Fig.\n14\n(a), including six 3D-printed texture boards and six real-world objects.\nA total of 300 reconstructed tactile images are collected with manual annotations of the contact region. The accumulated images are shown in Fig.\n14\n(b). Moreover, Data augmentation expanded the set to 3000 images via stochastic transforms. For cross-category evaluation, we adopted category-holdout splits: six categories for training and all for testing, with 1000 training and 1000 testing images per split.\nFor the training process, input images were resized to\n256\n×\n256\n256\\times 256\n. The model is trained with an\nℓ\n1\n\\ell_{1}\nloss and Adam optimizer\n[\n26\n]\n(learning rate\n1\n×\n10\n−\n4\n1\\times 10^{-4}\n), using a batch size of 64 for 200 epochs. The training process ran on an NVIDIA 5090 GPU with an AMD R9-9950X CPU. Three metrics were used to evaluate the estimation result: structural similarity index measure (SSIM), intersection over union (IoU), and RMSE. SSIM measures the perceptual similarity between reconstruction and ground truth, IoU quantifies the overlap between predicted and true contact regions, and RMSE indicates the average pixel-wise error between the estimation and ground truth. We trained on five randomly selected category-holdout splits and compared models with and without the IMU-guided temporal filter; results are reported in Table\nV\n.\nTABLE V:\nQuantitative Result of Contact Surface Estimation\nSSIM\n↑\n\\uparrow\nIoU\n↑\n\\uparrow\nRMSE\n↓\n\\downarrow\nFiltered\n0.9688\n±\n\\pm\n0.0004\n0.8104\n±\n\\pm\n0.0052\n0.0693\n±\n\\pm\n0.0011\nRaw\n0.9684\n±\n\\pm\n0.0043\n0.6965\n±\n\\pm\n0.0077\n0.0945\n±\n\\pm\n0.0016\nAcross all splits, U-Net delivers strong estimation performance and IMU-guided filtering consistently outperforms raw inputs. The increase in IoU shows cleaner region delineation and fewer boundary errors. The decrease in RMSE reflects smaller pixel-wise deviations. SSIM remains comparable, which suggests the U-Net already captures global structure. These gains indicate that filtering suppresses vibration-induced jitters and stabilizes event density, enabling the U-Net to sharpen boundaries and reject shadowing from lateral elastomer deformation. Fig.\n14\n(c) intuitively shows that the predicted masks align closely with the true contact regions, capturing fine contours and small gaps while minimizing spurious halos. For the limitation, the high textural density near the asterisk pattern center leads to insufficient elastomer deformation, making it difficult to accurately reconstruct the modelled geometry. Together, these results validate the proposed algorithm and demonstrate that SWTac provides reliable visuotactile output for marker-free contact geometry recovery.\nV-C\nForce Estimation\nIn this experiment, we employ a conical elastomer to validate SWTac’s capability to provide force information for downstream tasks. The force can be estimated by tracking the displacement at the tip\n[\n65\n,\n12\n,\n39\n]\n. We focus on shear measurements because the sensor’s active vibration is primarily in the normal direction, making the normal force strongly oscillatory and of limited practical value. Though it can be estimated robustly via short-time averaging, this comes at the cost of a reduced effective sampling rate. We developed the calibration platform shown in Fig.\n15\n(a). By precisely controlling the sensor’s lateral offset with a robotic arm, we recorded event streams corresponding to different shear forces. As the offset increases, the tip eventually slides on the force sensor due to insufficient friction, defining the onset of the maximum shear force. A total of 36 data segments were collected for this analysis.\nFigure 15:\nShear force estimation. (a) Calibration platform highlighting the contact between the elastomer and the force sensor. (b) Illustration of force estimation results.\n(c) Regression results of predicted and actual forces.\nDue to the simplicity of the target geometry, we track the featured tip from the denoised grayscale images using Canny edge detection\n[\n6\n]\nwith tuned spatiotemporal constraints. Using the undeformed elastomer tip as the baseline reference, we compute the minimum enclosing circle of the tracked tip and record its central geometry\n(\nx\n,\ny\n)\n(x,y)\nrelative to this reference. With the collected dataset, we trained regression models to estimate the force vector\n(\nF\nx\n,\nF\ny\n)\n(F_{x},F_{y})\nfrom the extracted geometric features. An auxiliary feature\nr\nr\nis calculated as the Euclidean distance from the feature center to the reference center. For shear forces\nF\nx\nF_{x}\nand\nF\ny\nF_{y}\n, we adopted input features\n[\nx\n,\ny\n,\nr\n]\n[x,y,r]\nand used the Random Forest\n[\n5\n]\nregressor for its ability to capture nonlinear dependencies and its robustness on small, noisy datasets. Evaluation on the dataset showed high accuracy, as illustrated in Fig.\n15\n(c), with\nR\n2\nR^{2}\nscores exceeding 0.95 for shear forces and mean average error (MAE) below 0.15 N. For each frame, the predicted force vector\n(\nF\nx\n,\nF\ny\n)\n(F_{x},F_{y})\nwas overlaid on the image as shown in Fig.\n15\n(b). The red arrow represents the displacement of the featured tip from the reference position, indicating direction, while the yellow arrow shows the shear force direction and magnitude. For display purposes, the force direction was inverted.\nV-D\nStone Classification\nFigure 16:\nStone classification. (a) Sample images of different types of stones. (b) Confusion matrix of classification results.\nAccurate classification of stone types, which exhibit unique granularity and surface textures, is essential for subsurface exploration. This experiment tests SWTac’s ability of texture and validates its robustness in realistic downstream applications. We collected the tactile data for five stone types in the real world: grit, gravel, pebble, cobble, and eggstone. Data was gathered under three conditions: stone only, sand–stone composites, and soil–stone composites, as shown in Fig.\n16\n(a). The IMU-guided filter was applied to the signals after removing non-contact intervals with reconstruction rate at 500 Hz. Data augmentation was applied to the dataset, yielding a total of approximately 10,000 images.\nFor classification, we adopted a ResNet-18 model pre-trained on ImageNet\n[\n19\n]\n. We employed a transfer learning strategy, freezing the stem modules to preserve low-level features. These modules contain the initial convolution, batch normalization, and max pooling. The remaining residual stages were fine-tuned. The final fully connected layer was replaced with a dropout layer and a new linear layer with five outputs. The dataset was split into training and test sets at a 1:1 ratio. The model was trained for 50 epochs using an Adam optimizer with a learning rate of\n10\n−\n4\n10^{-4}\n, an\nℓ\n2\n\\ell_{2}\ndecay of\n10\n−\n4\n10^{-4}\n, and a batch size of 32 with cross-entropy loss. Training was conducted on an NVIDIA 5090 GPU with an AMD R9-9950X CPU.\nThe resulting confusion matrix is shown in Fig.\n16\n(b). The model achieved an overall accuracy of 98%, successfully classifying the stone types even in these unstructured, composite environments. This high robustness is attributed to the sensor’s active vibration mechanism. The high-frequency vibrations help dislodge fine particles like sand and soil from the stone’s surface, reducing interference and allowing the sensor to capture the underlying tactile signature.\nV-E\nPenetration Benchmark\nFigure 17:\nBenchmark of vibration-assisted penetration. (a) Experimental setup. (b) Force sensor readings under static and vibrational excitation at different rotational speeds during penetration.\nIn this experiment, we quantify the contributions of the screw thread and active vibration to the penetration performance of the SandWorm robot. To create a standardized test, we constructed a test platform, as shown in Fig.\n17\n(a), using rice as the benchmark medium. The rice was piled approximately 300 mm high in a container. SWTac was fixed to a robotic arm via a high-range force sensor, with the brushless motor providing rotational motion. A soft dust-proof mesh was installed around the elastomer to prevent particle ingress, a measure maintained in all subsequent experiments. The robotic arm descended at a constant speed of 5 mm/s. By varying the rotational speed, we compared the effect of vibration on penetration performance.\nThe results in Fig.\n17\n(b) show that higher rotational speeds facilitated easier penetration. Crucially, the presence of vibration consistently led to lower penetration resistance at the same depth across all rotational speeds. At 100 RPM, the sensor penetrated beyond 200 mm with relatively stable force readings. Furthermore, the beneficial effect of vibration was most pronounced at lower rotational speeds. These findings confirm that active vibration significantly enhances penetration efficiency in granular media, validating the effectiveness of the sensor’s structural design.\nV-F\nPipeline Inspection\nFigure 18:\nPerformance evaluation of the autonomous SandWorm robot in confined environments. (a) Linear locomotion within a straight pipe.\n(b) Steering inside the pipe.\n(c) Steering at wall intersections.\n(d) Traversal through a curved pipe segment.\nIn this experiment, we first characterized SandWorm’s linear locomotion in a 1 m long, 200 mm inner diameter acrylic pipe (Fig.\n18\n(a)). We tested four motion configurations over a 600 mm distance: rotational speeds of 60 and 90 RPM, each with and without the 30 mm pushrod reciprocation. The results are summarized in Table\nVI\n. It shows that the peristaltic motion by pushrod reciprocation reduces traversal time by up to 62%. This improvement stems from the alternation of extension and retraction by the pushrod, which overcomes static friction more effectively than screw actuation alone. This synergy yielded a maximum straight-line speed of 12.5 mm/s, confirming that integrated pushrod motion is critical for efficient locomotion.\nTABLE VI:\nTraversal time of SandWorm in the pipeline\nConfiguration\n60 RPM\n90 RPM\nw/o Pushrod Reciprocation\n140 s\n127 s\nw/ Pushrod Reciprocation\n62 s\n48 s\nNext, we evaluated the steering performance of SandWorm in a 15° pipe bend. Fig.\n18\n(b) shows the starting and finishing configurations. For propulsion, we employed a rotational speed of 90 RPM combined with pushrod reciprocation. The steering is enabled by force estimation, which triggers the servo. The conical elastomer was mounted on the tip and calibrated for shear force sensing. When the elastomer tip contacts the inner wall of the pipe bends, the induced deflection generates events captured by the event camera (Fig.\n18\n(b)ii). We set a threshold on the shear force change to trigger the servo at 1 Hz, producing a 30° rotation of the servo motor per trigger; successive triggers accumulate to achieve larger steering angles. Since the robot and sensor rotate, the detected force direction must be transformed into the world frame. To achieve this with low computational overhead, we use a simple quantization method\n[\n37\n]\nto correct the force vector based on the motor’s angle, mapping the result to a left or right steering command. Using this approach, the robot successfully navigated the bend.\nIn addition, we evaluated steering at wall intersections, as shown in Fig.\n18\n(c). SandWorm travels along a wall inclined at 70° to the horizontal plane and then contacts a vertical wall that intersects its direction of motion at a 70° angle. The algorithm and steering mode are identical to the last one. Experimentally, the steering manoeuvre unfolds through four observed phases: i. nominal forward motion along the initial wall; ii. elastomer tip contacting with the new wall, triggering the servo; iii. reorientation toward the new wall followed by renewed contact detection; and iv. cyclic actuation and contact checks until the elastomer tip no longer contacts the wall. Once the servo returns to its neutral position, the robot resumes forward motion. Aided by its rotational motion, SandWorm would naturally align with the new wall, thereby completing the large‐angle steering.\nFinally, we evaluated extreme locomotion in a more challenging 150 mm diameter PVC elbow with a 2000 mm curvature radius, as shown in Fig.\n18\n(d). The material and tightened diameter introduced significantly higher friction. These conditions caused the pushrod to stall. To mitigate this, we reduced the motor speed to 30 RPM and relied on continuous low-speed rotation, which prevented shock loads. The turn was gradual, so the steering servo remained neutral. Despite the severe friction, the robot successfully traversed the bend, maintaining forward motion at 12% of its maximum speed. This result demonstrates the mechanical robustness and compliance of the design under extreme frictional loads.\nV-G\nPipeline Dredging\nFigure 19:\nPipeline dredging experiments with the autonomous SandWorm robot. (a) Gravel blockage. (b) Cobble blockage. (c) Eggstone blockage. i. image before pipe dredging, ii. image after pipe dredging.\nClearing obstructions serves as a capstone experimental validation, demonstrating the integrated capabilities of the SandWorm robot. It combines the previously characterized locomotion, perception, and mechanical design. We tested the robot’s dredging performance against three blockage types: gravel, cobble, and eggstone as shown in Fig.\n19\n. Each blockage spanned 200 mm and occupied approximately half the pipe’s cross-section. SandWorm advanced while actively clearing the obstruction, successfully traversing the 600 mm path in 84 s, 90 s, and 81 s for each case, respectively. Compared to the 48 s unobstructed run, the speed decreased by approximately 44%, yet the robot consistently cleared and traversed all three blockage types.\nSandWorm’s high‐torque drive and threaded housing fragmented the blockage and then redistributed debris along the pipe, like a tunnel‐boring machine. The helical shell ensured uniform debris spreading and even backward transport. During dredging, SWTac captures the event stream continuously, which is processed by our stone classification network. A majority vote over sequential frames enabled robust and accurate identification of blockage material throughout the traversal. This confirms that SandWorm can not only physically dredge a pipeline but also simultaneously perceive its environment, validating the complete system integration in a challenging, application-focused scenario.\nFigure 20:\nSubsurface exploration in granular media by SandWorm. (a) Manual experiments in: i. sand, ii. TPE, iii. EPP, iv. EPE. (b) Shapes and imaging of detected objects: i. trilobite fossil, ii. cylinder, iii. cuboid, iv. triangular prism.\nFigure 21:\nSequence of elastomer-object contact in granular media, showing (a) no contact, (b) blurring from initial pressing, (c) granule compaction, and (d) granule evacuation via vibration and rotation to reveal clear object edges.\nV-H\nSubsurface Exploration\nFor subsurface exploration, we use the manual version of SandWorm, replacing the tail module with a holding pushrod for downward force while retaining rotational drilling, as illustrated in Fig.\n20\n(a). The frustoconical elastomer was chosen for texture perception at the tip. The event stream was reconstructed, passed through a 3\n×\n\\times\n3 median filter, and visualized in real-time at 200 Hz. A smartphone interface provided communication and allows the operator to control the rotation speed and pushrod actuation.\nWe evaluated the object-searching performance in a 600\n×\n\\times\n600\n×\n\\times\n400 mm\n3\ntest box with four media: sand, thermoplastic elastomer (TPE), expanded polypropylene (EPP), and expanded polyethylene (EPE), with respective densities of approximately 2000, 700, 100, and 10 kg/m\n3\n. The key to this exploration is distinguishing the target object from the surrounding granular media by volume, density, and texture. As the tip penetrates, the active vibration mechanism gradually clears away granules from the object’s surface. This allows the tip to make direct contact, revealing the object’s true shape and texture rather than the granular pattern. The process is illustrated in Fig.\n21\n.\nFigure 22:\nField experiments demonstrating SandWorm’s capabilities in real-world environments. (a) Moving forward in grass. (b) Moving forward in a bush. (c) Moving forward on a cement road. (d) Dredging in a blocked outdoor pipeline. (e)(f) Subsurface exploration experiment on the beach. (g)(h) Subsurface exploration experiment in composite garden soil. Full experimental footage is available in the supplementary video.\nIn 40 trials, operators achieved a 90% success rate (36/40) in locating buried objects within 120 s. Sample imaging results are shown in Fig.\n20\n(b). As predicted by the mechanism, searching in dense sand and TPE was slower; all four trials exceeding 120 s occurred in these media. These failures in dense media occurred because high inertial torque caused the robot body to pivot, shifting the contact point to the elastomer’s conical side. This resulted in reduced localization accuracy, which can be mitigated by reducing rotation speed, though at the cost of slower penetration.\nV-I\nField Validation\nTo validate SandWorm’s real-world performance, we conducted a final series of experiments in unstructured outdoor environments, as shown in Fig.\n22\n. These tests were designed to evaluate the system’s integrated performance in complex, unpredictable conditions, where engineering improvements such as SandWorm’s metal shell and the elastomer’s dust-proof mesh proved crucial for durability.\nFirst, we assessed SandWorm’s locomotive performance on challenging natural terrains. SandWorm successfully traversed dense grass (Fig.\n22\n(a)), tangled bushes (Fig.\n22\n(b)), and hard cement surfaces (Fig.\n22\n(c)), demonstrating the robustness of its rotational-peristaltic locomotion. We then validated its applications in pipeline dredging. The robot was deployed in a blocked outdoor pipe, shown in Fig.\n22\n(d). Upon entering, SandWorm continuously burrowed in, actively clearing internal blockages composed of mud, leaves, and gravel. Throughout this process, SWTac indicated no prohibitive shear forces, allowing the robot to autonomously continue the clearing operation and traverse the pipe.\nFor subsurface exploration, we conducted numerous trials in complex, non-uniform media. The operator uses SandWorm to bore in at different locations until the target is detected in the reconstructed tactile images, as shown in Fig.\n22\n(e). We show four representative successes in Fig.\n22\n(e-h). SandWorm successfully located diverse buried objects, including a stone and a conch on a beach, as well as a bottle cap and another stone in composite garden soil. These field tests confirm that SandWorm’s integrated locomotion and active perception are effective and generalizable for its target applications.\nVI\nDiscussion\nThe SandWorm robot addresses key challenges in granular media environments that traditional perception and locomotion methods face, such as low resolution, poor robustness, and vulnerability to strong vibration. By integrating a bio-inspired rotational-peristaltic locomotion mechanism with a novel event-based visuotactile sensor, SandWorm achieves efficient movement and high-resolution perception, demonstrating clear advantages in stability and adaptability over conventional approaches. Ultimately, this work demonstrates that vibration can be engineered into a perception advantage when hardware and algorithms are co-designed from first principles.\nScientific Contributions.\nThis work makes three core contributions: (1) We demonstrate that active vibration enables event cameras to effectively capture static contact. By vibrating the elastomer tip, SWTac converts static contact into detectable dynamic events, enabling pixel-level tactile imaging at 1000 Hz without motion blur. This approach fundamentally expands event-based vision beyond dynamic scenes.\n(2) We introduce hardware-level perception-actuation integration, where the same vibration drives both imaging and material penetration, while a dedicated isolation mechanism decouples and protects the sensor.\n(3) We propose a bio-inspired locomotive structure combining rotation and peristalsis, which mimics marine polychaetes to achieve up to 62% faster traversal than screw actuation alone.\nAlgorithmic Innovations.\nThis work provides innovations in two key areas: (1) While static tactile imaging by event camera is enabled by our hardware design, we further optimized the temporal continuity of event-based perception through algorithmic solutions. We proposed IMU-guided temporal filter that exploits the correlation between elastomer acceleration and imaging quality, improving MSNR by 24% in average with 1 ms latency.\n(2) We developed algorithms for key downstream applications, including recovering full contact surfaces by leveraging asymmetric edge deformation with FEA, force estimation, and material classification.\nEngineering Insights.\nThis work illuminates several lessons for extreme-environment robotics: (1) Event cameras excel when vibration is harnessed as signal, not noise. SWTac transforms vibration from an interference source into an advantage, extending the application of event cameras to static tactile perception.\n(2) Multimodal fusion of event-based vision with IMU are essential for maintaining signal fidelity from high-sample-rate sensors under high mechanical loads, applicable to any contact-rich task.\nLimitations and Future Work.\nDespite these advances, particle adhesion to the elastomer surface may impair texture detection in sticky soils. Future work will explore tuning the elastomer’s surface properties via hydrophobic coatings or micro-texturing, as well as active cleaning mechanisms (e.g., air jets, ultrasonic agitation). Additionally, extending contact surface estimation to 3D reconstruction would enable richer geometric reasoning, which can be adopted for real-time adaptive control to improve autonomous performance significantly. These steps represent a clear path toward reliable robotic perception for advanced applications in disaster rescue, archaeological excavation, and planetary exploration.\nVII\nConclusion\nIn this paper, we introduce SandWorm, a peristalsis-augmented screw-actuated snakelike robot, with SWTac, an event-based visuotactile sensor integrated with active elastomer vibration. By mechanically isolating the event camera, SWTac enables consistent tactile imaging in pixel-level at 1000 Hz, overcoming event cameras’ intrinsic limitation of capturing only dynamic changes. For the algorithm, we proposed the IMU-guided temporal filter to enhance the consistency of the asynchronous event stream by approximately 24%. Moreover, we verified the asymmetric edge feature by FEA and adopted the U-Net model for contact surface estimation, as well as systematically characterized sensor performance. Experimentally, SWTac provided precise force estimation (0.15 N), texture recognition (0.2 mm resolution), and stone classification (98% accuracy). The SandWorm robot demonstrated effective autonomous locomotion, achieving straight runs (12.5 mm/s), turns, high-curvature navigation. Field trials in real-world scenarios, including pipeline dredging, subsurface exploration in beach and composite soil, further demonstrate the system’s practical effectiveness.\nAppendix A\nDerivation of Locomotion\nThis appendix provides the mathematical derivations for the locomotion components referenced in Section\nIII-C\n. First, the screw mechanism converts the external shell’s rotation\nθ\n\\theta\ninto an axial displacement\nl\nl\nalong the robot’s intrinsic axis. This relationship is given by:\nl\n=\np\n2\n​\nπ\n​\nθ\n,\nl=\\frac{p}{2\\pi}\\theta,\n(14)\nwhere\np\np\nis the pitch of the screw.\nSecond, we derive the effective gravitationally resolved driving component,\nF\npropel\nF_{\\text{propel}}\n. We consider the robot on an inclined plane with inclination\nα\n\\alpha\n, mass\nm\nm\n, and under gravitational acceleration\ng\ng\n. An axial displacement\nd\n​\nl\n\\text{d}l\ninduces a vertical height change\nd\n​\nh\n=\nd\n​\nl\n​\nsin\n⁡\nα\n\\text{d}h=\\text{d}l\\sin\\alpha\n. The corresponding change in gravitational potential energy\nd\n​\nU\n\\text{d}U\nis:\nd\n​\nU\n=\nm\n​\ng\n​\nd\n​\nh\n=\nm\n​\ng\n​\nsin\n⁡\nα\n​\nd\n​\nl\n.\n\\text{d}U=mg\\text{d}h=mg\\sin\\alpha\\text{d}l.\n(15)\nF\npropel\nF_{\\text{propel}}\nis defined as the potential energy gradient along the robot’s intrinsic axis. Taking the derivative of\nU\nU\nwith respect to\nl\nl\nfrom Eqn. (\n15\n) yields this gravitational component, which is used in the net axial force analysis in Eqn. (\n1\n) and Eqn. (\n2\n):\nF\npropel\n=\nd\n​\nU\nd\n​\nl\n=\nm\n​\ng\n​\nsin\n⁡\nα\n.\nF_{\\text{propel}}=\\frac{\\text{d}U}{\\text{d}l}=mg\\sin\\alpha.\n(16)\nFigure 23:\nEvent Camera vs. RGB Camera. (a) Intuitive illustration of frames. (b) Data rate comparison during the field test on the beach.\nAppendix B\nCamera Comparison\nWe conducted a direct comparison with an RGB camera (OV2710) at 120 Hz frame rate with exposure time fixed at\n1.6\n1.6\nms, as shown in Fig.\n23\n. While the RGB camera produces clear images in static contact, it suffers from significant motion blur and distortion underthe active vibration required for our perception method. Although further reducing the exposure could mitigate the blurring, it would lead to underexposure and electronic noise, degrading the signal quality. In contrast, the event camera robustly remains sharp even when reconstructed at 1000 Hz under high-frequency vibrations. Its asynchronous pixel-level change detection enables a response latency as low as 15\nμ\n​\ns\n\\mu s\n[\n36\n]\n, serving as an effectively ultra-short exposure. This surpasses traditional photometric integration in sensitivity to rapid illumination changes, ensuring robust tactile perception with high temporal resolution.\nWe also performed a quantitative analysis of the data rate\n[\n12\n]\n. Using an event stream captured during a field test on the beach for comparison, we found that even at the moment of peak event generation while identifying an object, the event camera is vastly more data-efficient. This data efficiency is critical for reducing the computational and transmission loads on a computationally constrained mobile robot.\nAppendix C\nElastomer Design\nThe elastomer fabrication procedure is illustrated in Fig.\n24\n(a). PDMS base and curing agent are mixed at the target ratio, degassed under vacuum, cast into a customized mold, cured at 70 °C for 180 min, and then demolded. Next, markers are deposited by applying a thin PDMS–carbon powder composite, which is cured at 70 °C for 60 min. A reflective layer of PDMS doped with silver powder is then spin-coated over the elastomer, using a temporary insert to fill any gaps, and subsequently cured at 70 °C for 120 min. Finally, the elastomer is assembled with an acrylic substrate and mechanical latch (P1 and P3 in Fig.\n2\n(e)) to protect the transparent bottom surface from contamination. To achieve different hardness levels, PDMS samples were cast with 10:1 and 17:1 base-to-curing-agent mass ratios, yielding approximate Shore hardness values of 40 A and 20 A, respectively. A 10 A hardness is realized using silicone (Ecoflex™ 00-10, Smooth-On Inc.). To evaluate long-term durability, we conducted a 15,000-cycle rotational wear test. Based on the results, we enhanced the durability by adding a thin, abrasion-resistant TPU film to the elastomer surface.\nFigure 24:\nFabrication of different elastomers.\n(a) Schematic of the elastomer fabrication procedure.\n(b) Photograph of the moulded elastomer, annotated with maximum contact force values.\n(c) Interior (imaging) side of the elastomer.\nDifferent shapes of elastomer can be used for different functions\n[\n54\n]\n. To enhance sensor performance under different contact conditions, five elastomer geometries were fabricated. The external surfaces and the internal marker distributions are shown in Fig.\n24\n(b) and (c), respectively. These geometries include hemispherical designs, which incorporate a dense array of markers to achieve a wide dynamic range of force perception, and obliquely cut cylindrical geometries, which combine extended lateral surfaces with a single inclined marker line designed to support efficient drilling in granular media. Conical tips feature a single central marker and tunable aspect ratios to provide graded force sensitivity. Finally, frustoconical designs provide a flat terminal surface and broad contact area, enhancing texture discrimination. Furthermore, we experimentally validated these elastomer geometries by testing the maximum normal force experienced during penetration into and retraction from the same granular medium under identical conditions. The resulting values are annotated below each geometry in Fig.\n24\n(b).\nReferences\n[1]\nS. J. Antony, W. Hoyle, and Y. Ding\n(2004-03)\nGranular materials: fundamental and applications\n.\nThe Royal Society of Chemistry\n.\nCited by:\n§I\n.\n[2]\nF. Baghaei Naeini, A. M. AlAli, R. Al-Husari, A. Rigi, M. K. Al-Sharman, D. Makris, and Y. Zweiri\n(2020)\nA novel dynamic-vision-based approach for tactile sensing applications\n.\nIEEE Transactions on Instrumentation and Measurement\n69\n(\n5\n),\npp. 1881–1893\n.\nCited by:\nTABLE II\n,\n§\nII-B\n.\n[3]\nA. Böhm, T. Schneider, B. Belousov, A. Kshirsagar, L. Lin, K. Doerschner, K. Drewing, C. A. Rothkopf, and J. Peters\n(2024)\nWhat matters for active texture recognition with vision-based tactile sensors\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 15099–15105\n.\nCited by:\n§\nIV-C\n.\n[4]\nC. Branyan and Y. Menguc\n(2018)\nSoft snake robots: investigating the effects of gait parameters on locomotion in complex terrains\n.\nIn\nProc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nMadrid, Spain\n.\nCited by:\n§\nIII-B\n.\n[5]\nL. Breiman\n(2001)\nRandom forests\n.\nMachine Learning\n45\n(\n1\n),\npp. 5–32\n.\nCited by:\n§\nV-C\n.\n[6]\nJ. Canny\n(1986)\nA computational approach to edge detection\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nPAMI-8\n(\n6\n),\npp. 679–698\n.\nCited by:\n§\nV-C\n.\n[7]\nR. Cao, D. Galor, A. Kohli, J. L. Yates, and L. Waller\n(2025-01)\nNoise2Image: noise-enabled static scene recovery for event cameras\n.\nOptica\n12\n(\n1\n),\npp. 46–55\n.\nCited by:\nTABLE II\n,\n§\nII-B\n.\n[8]\nR. Chen, X. Zhu, Z. Yuan, H. Pu, J. Luo, and Y. Sun\n(2024)\nA bioinspired single actuator-driven soft robot capable of multistrategy locomotion\n.\nIEEE Transactions on Robotics\n40\n,\npp. 2149–2165\n.\nCited by:\n§\nIII-B\n.\n[9]\nT. Dachlika and D. Zarrouk\n(2020)\nMechanics of locomotion of a double screw crawling robot\n.\nMechanism and Machine Theory\n153\n,\npp. 104010\n.\nCited by:\n§\nIII-B\n.\n[10]\nR. S. Dahiya, G. Metta, M. Valle, and G. Sandini\n(2010)\nTactile sensing—from humans to humanoids\n.\nIEEE Transactions on Robotics\n26\n(\n1\n),\npp. 1–20\n.\nCited by:\n§I\n.\n[11]\nR. Dash and B. Majhi\n(2014)\nMotion blur parameters estimation for image restoration\n.\nOptik\n125\n(\n5\n),\npp. 1634–1640\n.\nCited by:\n§I\n.\n[12]\nN. Funk, E. Helmut, G. Chalvatzaki, R. Calandra, and J. Peters\n(2024)\nEvetac: an event-based optical tactile sensor for robotic manipulation\n.\nIEEE Transactions on Robotics\n40\n,\npp. 3812–3832\n.\nCited by:\nAppendix B\n,\nTABLE II\n,\n§\nII-B\n,\n§\nV-C\n.\n[13]\nG. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. Davison, J. Conradt, K. Daniilidis, and D. Scaramuzza\n(2022)\nEvent-based vision: a survey\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n44\n,\npp. 154–180\n.\nCited by:\n§I\n,\n§\nII-B\n,\n§\nII-B\n.\n[14]\nA. Ghasemloonia, D. Geoff Rideout, and S. D. Butt\n(2015)\nA review of drillstring vibration modeling and suppression methods\n.\nJournal of Petroleum Science and Engineering\n131\n,\npp. 150–164\n.\nCited by:\n§I\n.\n[15]\nA. Gheibi and A. Hedayat\n(2018)\nUltrasonic investigation of granular materials subjected to compression and crushing\n.\nUltrasonics\n87\n,\npp. 112–125\n.\nCited by:\n§\nII-A\n.\n[16]\nS. Grill and K. Dorgan\n(2015)\nBurrowing by small polychaetes - mechanics, behavior and muscle structure of capitella sp\n.\nThe Journal of experimental biology\n218\n.\nCited by:\n§I\n.\n[17]\nF. Guillard, B. Marks, and I. Einav\n(2017)\nDynamic X-ray radiography reveals particle size and shape orientation fields during granular flow\n.\nScientific Reports\n7\n(\n1\n),\npp. 8155\n.\nCited by:\n§\nII-A\n.\n[18]\nB. He, Z. Wang, Y. Zhou, J. Chen, C. D. Singh, H. Li, Y. Gao, S. Shen, K. Wang, Y. Cao, C. Xu, Y. Aloimonos, F. Gao, and C. Fermüller\n(2024)\nMicrosaccade-inspired event camera for robotics\n.\nScience Robotics\n9\n(\n90\n),\npp. eadj8124\n.\nCited by:\nTABLE II\n,\n§I\n,\n§\nII-B\n.\n[19]\nK. He, X. Zhang, S. Ren, and J. Sun\n(2016)\nDeep residual learning for image recognition\n.\nIn\nProc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nLas Vegas, NV, USA\n.\nCited by:\n§\nV-D\n.\n[20]\niniVation AG\n(2025)\nDV-processing\n.\nExternal Links:\nLink\nCited by:\n§\nIV-A\n2\n.\n[21]\nY. Ito, Y. Kim, and G. Obinata\n(2014)\nContact region estimation based on a vision-based tactile sensor using a deformable touchpad\n.\nSensors\n14\n(\n4\n),\npp. 5805–5822\n.\nCited by:\n§\nIV-C\n.\n[22]\nA. Kakogawa, T. Nishimura, and S. Ma\n(2016)\nDesigning arm length of a screw drive in-pipe robot for climbing vertically positioned bent pipes\n.\nRobotica\n34\n,\npp. 306–327\n.\nCited by:\nTABLE I\n,\n§\nII-A\n.\n[23]\nW. Kang, Y. Feng, C. Liu, and R. Blumenfeld\n(2018)\nArchimedes’ law explains penetration of solids into granular media\n.\nNature Communications\n9\n(\n1\n),\npp. 1101\n.\nCited by:\n§I\n.\n[24]\nA. Katterfeld and C. Wensrich\n(2017)\nUnderstanding granular media: from fundamentals and simulations to industrial application\n.\nGranular Matter\n19\n(\n4\n),\npp. 83\n.\nCited by:\n§I\n.\n[25]\nJ. H. Kim, P. Chhai, and K. Rhee\n(2021-05)\nDevelopment and characterization of viscoelastic polydimethylsiloxane phantoms for simulating arterial wall motion\n.\nMedical Engineering & Physics\n91\n,\npp. 12–18\n.\nCited by:\n§\nIV-D\n5\n.\n[26]\nD. P. Kingma and J. Ba\n(2015)\nAdam: A method for stochastic optimization\n.\nIn\nProc. International Conference on Learning Representations (ICLR)\n,\nSan Diego, CA, USA\n.\nCited by:\n§\nV-B\n.\n[27]\nH. Kolvenbach, C. Bärtschi, L. Wellhausen, R. Grandia, and M. Hutter\n(2019)\nHaptic inspection of planetary soils with legged robots\n.\nIEEE Robotics and Automation Letters\n4\n(\n2\n),\npp. 1626–1633\n.\nCited by:\nTABLE I\n,\n§\nII-A\n.\n[28]\nB. Kou, Y. Cao, J. Li, C. Xia, Z. Li, H. Dong, A. Zhang, J. Zhang, W. Kob, and Y. Wang\n(2017)\nGranular materials flow like complex fluids\n.\nNature\n551\n(\n7680\n),\npp. 360–363\n.\nCited by:\n§I\n.\n[29]\nD. Kumar and S. Singh\n(2022)\nStatic and dynamic mechanical characterization of polydimethylsiloxane (PDMS) under uniaxial tensile loading\n.\nIn\nProc. IOP Conference Series: Materials Science and Engineering\n,\nCited by:\n§\nIV-D\n5\n.\n[30]\nY. Kwon and B. Yi\n(2012)\nDesign and motion planning of a two-module collaborative indoor pipeline inspection robot\n.\nIEEE Transactions on Robotics\n28\n(\n3\n),\npp. 681–696\n.\nCited by:\nTABLE I\n,\n§\nII-A\n.\n[31]\nD. Lattanzi and G. Miller\n(2017)\nReview of robotic infrastructure inspection systems\n.\nJournal of Infrastructure Systems\n23\n(\n3\n),\npp. 04017004\n.\nCited by:\n§I\n.\n[32]\nS. Lee, D. K. Woo, and H. Choi\n(2023)\nContactless estimation of soil moisture using leaky Rayleigh waves and a fully convolutional network\n.\nVadose Zone Journal\n22\n(\n6\n),\npp. e20285\n.\nCited by:\n§\nII-A\n.\n[33]\nQ. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter\n(2020)\nA review of tactile information: perception and action through touch\n.\nIEEE Transactions on Robotics\n36\n(\n6\n),\npp. 1619–1634\n.\nCited by:\n§\nIV-C\n.\n[34]\nS. Li, Wang,Zihan, C. Wu, and et al.\n(2024)\nWhen vision meets touch: a contemporary review for visuotactile sensors from the signal processing perspective\n.\nIEEE Journal of Selected Topics in Signal Processing\n18\n(\n3\n),\npp. 267–287\n.\nCited by:\n§I\n,\n§\nII-A\n.\n[35]\nS. Li, H. Yu, G. Pan, H. Tang, J. Zhang, L. Ye, X. Zhang, and W. Ding\n(2024)\nM\n3\ntac: a multispectral multimodal visuotactile sensor with beyond-human sensory capabilities\n.\nIEEE Transactions on Robotics\n40\n(\n),\npp. 4484–4503\n.\nCited by:\n§I\n.\n[36]\nP. Lichtsteiner, C. Posch, and T. Delbruck\n(2008)\nA 128\n×\n\\times\n128 120 dB 15\nμ\n\\mu\ns latency asynchronous temporal contrast vision sensor\n.\nIEEE Journal of Solid-State Circuits\n43\n(\n2\n),\npp. 566–576\n.\nCited by:\nAppendix B\n,\n§\nIV-A\n1\n.\n[37]\nD. G. Lowe\n(2004)\nDistinctive image features from scale-invariant keypoints\n.\nInternational Journal of Computer Vision\n60\n(\n2\n),\npp. 91–110\n.\nExternal Links:\nISSN 1573-1405\nCited by:\n§\nV-F\n.\n[38]\nS. Moisio, B. León, P. Korkealaakso, and A. Morales\n(2013)\nModel of tactile sensors using soft contacts and its application in robot grasping simulation\n.\nRobotics and Autonomous Systems\n61\n(\n1\n),\npp. 1–12\n.\nCited by:\n§\nIV-C\n.\n[39]\nD. Mukashev, S. Seitzhan, J. Chumakov, S. Khajikhanov, M. Yergibay, N. Zhaniyar, R. Chibar, A. Mazhitov, M. Rubagotti, and Z. Kappassov\n(2025)\nE-BTS: event-based tactile sensor for haptic teleoperation in augmented reality\n.\nIEEE Transactions on Robotics\n41\n(\n),\npp. 450–463\n.\nCited by:\nTABLE II\n,\n§\nII-B\n,\n§\nV-C\n.\n[40]\nP. Nourizadeh, F. J.S. McFadden, and W. N. Browne\n(2024)\nIn situ skid estimation for mobile robots in outdoor environments\n.\nJournal of Field Robotics\n41\n(\n1\n),\npp. 179–194\n.\nCited by:\nTABLE I\n,\n§\nII-A\n.\n[41]\nE. Olsson and D. Jelagin\n(2019)\nA contact model for the normal force between viscoelastic particles in discrete element simulations\n.\nPowder Technology\n342\n,\npp. 985–991\n.\nCited by:\n§\nIV-C\n.\n[42]\nF. Pacheco-Vázquez and J.C. Ruiz-Suárez\n(2010)\nCooperative dynamics in the penetration of a group of intruders in a granular medium\n.\nNature Communications\n1\n(\n1\n),\npp. 123\n.\nCited by:\n§I\n.\n[43]\nS. K. Pal\n(2020)\nGranular mining and big data analytics: rough models and challenges\n.\nProceedings of the National Academy of Sciences, India Section A: Physical Sciences\n90\n(\n2\n),\npp. 193–208\n.\nCited by:\n§I\n.\n[44]\nL. Pan, C. Scheerlinck, X. Yu, R. Hartley, M. Liu, and Y. Dai\n(2019)\nBringing a blurry frame alive at high frame-rate with an event camera\n.\nIn\nProc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nLong Beach, CA, USA\n.\nCited by:\n§I\n.\n[45]\nR. Patel, R. Ouyang, B. Romero, and E. Adelson\n(2021)\nDigger Finger: GelSight tactile sensor for object identification inside granular media\n.\nIn\nProc. International Symposium on Experimental Robotics (ISER)\n,\nFloriana, Malta\n.\nCited by:\nTABLE I\n,\nTABLE II\n,\n§I\n,\n§\nII-A\n,\n§\nII-B\n.\n[46]\nH. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza\n(2021)\nHigh speed and high dynamic range video with an event camera\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n43\n(\n6\n),\npp. 1964–1980\n.\nCited by:\n§\nII-B\n.\n[47]\nA. H. Reddy, B. Kalyan, and Ch. S.N. Murthy\n(2015)\nMine rescue robot system – a review\n.\nProcedia Earth and Planetary Science\n11\n,\npp. 457–462\n.\nCited by:\n§I\n.\n[48]\nF. Richter, P. V. Gavrilov, H. M. Lam, A. Degani, and M. C. Yip\n(2022)\nARCSnake: reconfigurable snakelike robot with archimedean screw propulsion for multidomain mobility\n.\nIEEE Transactions on Robotics\n38\n(\n2\n),\npp. 797–812\n.\nCited by:\nTABLE I\n,\n§\nII-A\n,\n§\nIII-B\n.\n[49]\nO. Ronneberger, P. Fischer, and T. Brox\n(2015)\nU-Net: convolutional networks for biomedical image segmentation\n.\nIn\nProc. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n,\nMunich, Germany\n.\nCited by:\n§\nV-B\n.\n[50]\nRuppert and Badri-Sprowitz\n(2020)\nFootTile: a rugged foot sensor for force and center of pressure sensing in soft terrain\n.\nIn\nProc. IEEE International Conference on Robotics and Automation (ICRA)\n,\nParis, France\n.\nCited by:\nTABLE I\n,\n§\nII-A\n.\n[51]\nR. A. Russell\n(2005)\nA ground-penetrating robot for underground chemical source location\n.\nIn\nProc. IEEE International Conference on Field and Service Robotics (FSR)\n,\nProt Douglas, Australia\n.\nCited by:\nTABLE I\n,\n§\nII-A\n.\n[52]\nB. Saldivar, I. Boussaada, H. Mounier, S. Mondié, and S.I. Niculescu\n(2014)\nAn overview on the modeling of oilwell drilling vibrations\n.\nIFAC Proceedings Volumes\n47\n(\n3\n),\npp. 5169–5174\n.\nCited by:\n§I\n.\n[53]\nS. Scheraga, A. Mohammadi, T. Kim, and S. Baek\n(2020)\nDesign of an underactuated peristaltic robot on soft terrain\n.\nIn\nProc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nLas Vegas, NV, USA\n.\nCited by:\n§\nIII-B\n.\n[54]\nU. H. Shah, R. Muthusamy, D. Gan, Y. H. Zweiri, and L. Seneviratne\n(2021)\nOn the design and development of vision-based tactile sensors\n.\nJournal of Intelligent & Robotic Systems\n102\n,\npp. 1–27\n.\nCited by:\nAppendix C\n.\n[55]\nR. Stannarius\n(2017-05)\nMagnetic resonance imaging of granular materials\n.\nReview of Scientific Instruments\n88\n(\n5\n),\npp. 051806\n.\nCited by:\n§\nII-A\n.\n[56]\nT. Taunyazov, W. Sng, B. Lim, H. H. See, J. Kuan, A. F. Ansari, B. Tee, and H. Soh\n(2020)\nEvent-driven visual-tactile sensing and learning for robots\n.\nIn\nProc. Robotics: Science and Systems (RSS)\n,\nCorvalis, Oregon, USA\n.\nCited by:\nTABLE II\n,\n§\nII-B\n,\n§\nIV-C\n.\n[57]\nI. Teixeira, I. Castro, V. Carvalho, and et al.\n(2021)\nPolydimethylsiloxane mechanical properties: A systematic review\n.\nAIMS Materials Science\n8\n,\npp. 952–973\n.\nCited by:\n§\nIV-D\n5\n.\n[58]\nM. A. Trindade, A. Benjeddou, and R. Ohayon\n(2000-04)\nModeling of Frequency-Dependent Viscoelastic Materials for Active-Passive Vibration Damping\n.\nJournal of Vibration and Acoustics\n122\n,\npp. 169–174\n.\nCited by:\n§\nIV-D\n5\n.\n[59]\nA. B. Tzetlin and A. V. Filippova\n(2005)\nMuscular system in polychaetes (annelida)\n.\nHydrobiologia\n535\n(\n1\n),\npp. 113–126\n.\nCited by:\n§I\n.\n[60]\nT. S. Vaquero, G. Daddi, R. Thakker, and et al.\n(2024)\nEELS: Autonomous snake-like robot with task and motion planning capabilities for ice world exploration\n.\nScience Robotics\n9\n(\n88\n),\npp. eadh8332\n.\nCited by:\n§I\n,\n§\nIII-B\n.\n[61]\nX. Xiao and R. Murphy\n(2018)\nA review on snake robot testbeds in granular and restricted maneuverability spaces\n.\nRobotics and Autonomous Systems\n110\n,\npp. 160–172\n.\nCited by:\n§I\n.\n[62]\nF. Xue, C. Yao, Y. Yuan, Y. Ge, W. Shi, Z. Zhu, L. Ding, and Z. Jia\n(2023)\nWheel-terrain contact geometry estimation and interaction analysis using a side-wheel camera over deformable terrains\n.\nIEEE Robotics and Automation Letters\n8\n(\n11\n),\npp. 7639–7646\n.\nCited by:\nTABLE I\n,\n§\nII-A\n.\n[63]\nD. Yin, S. Lu, J. Yang, Y. Zhang, Z. Dai, D. Nan, B. Cai, S. He, and X. Chen\n(2025)\nGelEvent—a novel high-speed tactile sensor with event camera\n.\nIEEE Transactions on Instrumentation and Measurement\n74\n,\npp. 1–13\n.\nCited by:\nTABLE II\n,\n§\nII-B\n,\n§\nII-B\n,\n§\nIV-C\n.\n[64]\nA. Yousefzadeh, G. Orchard, T. Serrano-Gotarredona, and B. Linares-Barranco\n(2018)\nActive perception with dynamic vision sensors. minimum saccades with optimum recognition\n.\nIEEE Transactions on Biomedical Circuits and Systems\n12\n(\n4\n),\npp. 927–939\n.\nCited by:\nTABLE II\n,\n§\nII-B\n.\n[65]\nW. Yuan, S. Dong, and E. H. Adelson\n(2017)\nGelSight: high-resolution robot tactile sensors for estimating geometry and force\n.\nSensors\n17\n(\n12\n),\npp. 2762\n.\nCited by:\nTABLE II\n,\n§\nII-A\n,\n§\nII-B\n,\n§\nV-C\n.\n[66]\nZ. Zahiri, D. F. Laefer, and A. Gowen\n(2021)\nCharacterizing building materials using multispectral imagery and LiDAR intensity data\n.\nJournal of Building Engineering\n44\n,\npp. 102603\n.\nCited by:\n§\nII-A\n.\nShoujie Li\nreceived the B.Eng. degree in electronic information engineering from the College of Oceanography and Space Informatics, China University of Petroleum,\nTsingtao, China, in 2020. He received Ph.D. degree from Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China, in 2025. He is working as a research fellow at Nanyang Technological University.\nHis research interests include tactile perception, grasping, and machine learning. He received the Outstanding Mechanisms and Design Paper Finalists in ICRA 2022 and the Best Application Paper Finalists in IROS 2023. He won first place in the Robotic Grasping of Manipulation Competition-Picking in Clutter in ICRA 2024.\nChanqing Guo\nreceived the B.S. degree in Robotic Engineering from Shein-Ming Wu School of Intelligent Engineering, South China University of Technology, Guangzhou, China. He is currently pursuing the M.S. degree in Data Science and Information Technology at Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China.\nHis research interests focus on tactile perception, with particular applications in novel structure design and robot learning.\nJunhao Gong\nreceived the B.S. degree in Electronic Information Engineering from Harbin Institute of Technology, Weihai, China, in 2024. He is currently pursuing his Ph.D. degree in Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China.\nHis research interests mainly focus on tactile sensing and contact-aware robotic manipulation.\nChenxin Liang\nreceived the B.S. degree in Electronic Engineering from Tianjin University, Tianjin, China, in 2023. She is currently pursuing the Ph.D. degree with the Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China.\nHer research interests include vision-based tactile sensing, 3D reconstruction, and depth estimation.\nWenhua Ding\nreceived the B.E. degree from the Shine-Ming Wu School of Intelligent Engineering, South China University of Technology, China, in 2024. He is currently pursuing the Master’s degree at the Tsinghua Shenzhen International Graduate School, Tsinghua University, China.\nHis research interests include robotics and UAV-related sensing technologies.\nWenbo Ding\nreceived the BS and PhD degrees (Hons.) from Tsinghua University in 2011 and 2016, respectively. He worked as a postdoctoral research fellow at Georgia Tech from 2016 to 2019. He is now an associate professor and PhD supervisor at Tsinghua Shenzhen International Graduate School, Tsinghua University, where he leads the Smart Sensing and Robotics (SSR) group. His research interests are diverse and interdisciplinary, which include tactile sensing, embodied intelligence and AI4S.\nHe has received many prestigious awards, including the IROS New Generation Star, Gold Medal of the 47th International Exhibition of Inventions Geneva and the IEEE Scott Helt Memorial Award. He serves as the editorial board member and co-chair/area chair for several top journals/conferences.",
    "preview_text": "Perception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw-actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event-based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac's 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system's practical performance.\n\nSandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media\nShoujie Li\n∗\n, Changqing Guo\n∗\n, Junhao Gong, Chenxin Liang, Wenhua Ding, Wenbo Ding\n*These authors contributed equally to this\nwork.This work was supported by the National Key R&D Program of China grant (2024YFB3816000), Guangdong Innovative and Entrepreneurial Research Team Program (2021ZT09L197), Tsinghua Shenzhen International Graduate School-Shenzhen Pengrui Young Faculty Program of Shenzhen Pengrui Foundation (No. SZPR2023005) and Meituan.\n(Corresponding author: Wenbo Ding, ding.wenbo@sz.tsinghua.edu.cn) Shoujie Li, Changqing Guo, Junhao Gong, Chenxin Liang, Wenhua Ding, Wenbo Ding are with Tsinghua Shenzhen International Graduate Sch",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "该论文提出了一种基于事件相机的视觉触觉感知系统和仿生机器人，用于在颗粒介质中实现运动和控制，但与强化学习、VLA、扩散模型、Flow Matching、VLM等关键词相关性较低。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T16:25:50Z",
    "created_at": "2026-01-27T15:53:13.517214",
    "updated_at": "2026-01-27T15:53:13.517221"
}