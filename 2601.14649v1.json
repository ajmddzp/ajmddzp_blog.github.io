{
    "id": "2601.14649v1",
    "title": "Spatially Generalizable Mobile Manipulation via Adaptive Experience Selection and Dynamic Imagination",
    "authors": [
        "Ping Zhong",
        "Liangbai Liu",
        "Bolei Chen",
        "Tao Wu",
        "Jiazhi Xia",
        "Chaoxu Mu",
        "Jianxin Wang"
    ],
    "abstract": "ç§»åŠ¨æ“ä½œæ¶‰åŠåœ¨å¯¼èˆªã€æŠ“å–ç‰©ä½“ç­‰å¼‚æ„æŠ€èƒ½å¤šé˜¶æ®µç»„åˆä¸Šçš„é•¿ç¨‹å†³ç­–ã€‚å°½ç®¡è¿‘æœŸå–å¾—è¿›å±•ï¼Œç°æœ‰ç§»åŠ¨æ“ä½œæ–¹æ³•ä»é¢ä¸´ä¸¤å¤§å…³é”®å±€é™ï¼š(ä¸€)æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼Œæºäºå¯¹é•¿æœŸäº¤äº’ä¸­äº§ç”Ÿçš„å†—ä½™æ•°æ®åˆ©ç”¨ä¸è¶³ï¼›(äºŒ)ç©ºé—´æ³›åŒ–èƒ½åŠ›å¼±ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒçš„ç­–ç•¥éš¾ä»¥è¿ç§»åˆ°æ–°ç©ºé—´å¸ƒå±€è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡é€šè¿‡è‡ªé€‚åº”ç»éªŒé€‰æ‹©ä¸åŸºäºæ¨¡å‹çš„åŠ¨æ€æƒ³è±¡æœºåˆ¶åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼Œè‡ªé€‚åº”ç»éªŒé€‰æ‹©ä½¿ç§»åŠ¨æ“ä½œæ™ºèƒ½ä½“æ›´å…³æ³¨é•¿è½¨è¿¹ä¸­å½±å“ä»»åŠ¡æˆè´¥çš„å…³é”®ç»éªŒç‰‡æ®µï¼Œä»è€Œæå‡æŠ€èƒ½é“¾å­¦ä¹ æ•ˆæœå¹¶ç¼“è§£æŠ€èƒ½é—å¿˜é—®é¢˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥å¾ªç¯çŠ¶æ€ç©ºé—´æ¨¡å‹è¿›è¡ŒåŸºäºæ¨¡å‹é¢„æµ‹çš„å‰å‘è§„åˆ’ï¼Œé€šè¿‡æ•æ‰ç§»åŠ¨åŸºåº§ä¸æœºæ¢°è‡‚çš„è€¦åˆåŠ¨åŠ›å­¦å…³ç³»ï¼Œå¯¹æœªæ¥æ“ä½œåŠ¨æ€è¿›è¡Œæ¨æ¼”ã€‚åŸºäºå¾ªç¯çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å‰å‘è§„åˆ’æ—¢èƒ½å¼ºåŒ–å½“å‰ä»»åŠ¡çš„æŠ€èƒ½å­¦ä¹ ï¼Œåˆèƒ½å®ç°å¯¹æ–°ç©ºé—´å¸ƒå±€çš„æœ‰æ•ˆæ³›åŒ–ã€‚å¤šç»„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰ç§»åŠ¨æ“ä½œç­–ç•¥ã€‚å®ç‰©å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†æœ¬æ–¹æ³•çš„å¯è¡Œæ€§ä¸å®ç”¨æ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.14649v1",
    "html_url": "https://arxiv.org/html/2601.14649v1",
    "html_content": "Spatially Generalizable Mobile Manipulation via Adaptive Experience Selection and Dynamic Imagination\nPing Zhong\n1,2\nLiangbai Liu\n1\nBolei Chen\n1\nCorresponding Author.\nTao Wu\n1\nJiazhi Xia\n1,2\nChaoxu Mu\n3\nJianxin Wang\n1âˆ—\n1\nSchool of Computer Science and Engineering, Central South University\n2\nXiangjiang Laboratory\n3\nSchool of Artificial Intelligence, Anhui University\n{liangbailiu, boleichen, 8102221321, ping.zhong, xiajiazhi}@csu.edu.cn, cxmu@tju.edu.cn, jxwang@mail.csu.edu.cn\nAbstract\nM\nobile\nM\nanipulation (MM) involves long-horizon decision-making over multi-stage compositions of heterogeneous skills, such as navigation and picking up objects. Despite recent progress, existing MM methods still face two key limitations: (i) low sample efficiency, due to ineffective use of redundant data generated during long-term MM interactions; and (ii) poor spatial generalization, as policies trained on specific tasks struggle to transfer to new spatial layouts without additional training. In this paper, we address these challenges through\nA\ndaptive\nE\nxperience\nS\nelection (AES) and model-based dynamic imagination. In particular, AES makes MM agents pay more attention to critical experience fragments in long trajectories that affect task success, improving skill chain learning and mitigating skill forgetting. Based on AES, a\nR\necurrent\nS\ntate-\nS\npace\nM\nodel (RSSM) is introduced for\nM\nodel-\nP\nredictive\nF\norward\nP\nlanning (MPFP) by capturing the coupled dynamics between the mobile base and the manipulator and imagining the dynamics of future manipulations. RSSM-based MPFP can reinforce MM skill learning on the current task while enabling effective generalization to new spatial layouts. Comparative studies across different experimental configurations demonstrate that our method significantly outperforms existing MM policies. Real-world experiments further validate the feasibility and practicality of our method.\nProject homepage\n.\n1\nIntroduction\nM\nobile\nM\nanipulation (MM) is a cornerstone of embodied AI, enabling agents to perceive, reason, and act in unstructured environments. Unlike isolated navigation or fixed-base manipulation, MM requires agents to coordinate locomotion and dexterous interaction over extended spatial and temporal horizons. For example, MM allows embodied agents to complete complex goal-directed tasks such as opening doors and fetching objects, as shown in Fig.\n1\n(c). MM is quite challenging, as it typically involves long-horizon decision-making over multi-stage compositions of heterogeneous skills\nHuang\net al.\n(\n2023\n); Lin\net al.\n(\n2025b\n)\n. To address the complexity of MM, early methods decompose the problem into navigation\nChen\net al.\n(\n2024\n); Kang\net al.\n(\n2024\n)\nand stationary manipulation\nNguyen and La (\n2019\n)\n.\nFigure 1:\n(a) Our AES enables RSSM to pay more attention to critical episodic memories in experience trajectories, which are used to improve and spatially generalize RL-based MM agents. (b) RSSM cooperates with RL-based MM skills for\nH\nH\n-horizon dynamic imagination to achieve MPFP. The most promising elite path is selected for execution. (c) An example of MM tasks that consist of heterogeneous skills, i.e., navigation\nâ†’\n\\rightarrow\npicking\nâ†’\n\\rightarrow\nnavigation\nâ†’\n\\rightarrow\nplacing.\nThis factorized design simplifies modeling and learning, and has shown promise in structured environments. However, such approaches\nJauhri\net al.\n(\n2024\n); Arduengo\net al.\n(\n2021\n); Feng\net al.\n(\n2025\n)\noften optimize only short-term objectives and overlook the dynamic coupling between mobility and manipulation, making them vulnerable to error accumulation over long horizons.\nRecent work\nHe\net al.\n(\n2025\n); Honerkamp\net al.\n(\n2023a\n); Huang\net al.\n(\n2023\n)\nmodels the mobile base and manipulator as a unified system using whole-body control or joint policy learning. Despite encouraging progress, these solutions, especially model-free\nR\neinforcement\nL\nearning (RL)â€“based methods\nHonerkamp\net al.\n(\n2023a\n); Huang\net al.\n(\n2023\n)\n, usually suffer from inefficient exploration and low sample efficiency. The reason is that MM is essentially a long-horizon task consisting of skills with strict sequential dependencies, which introduces unique data distribution challenges that are different from those of standard RL benchmarking:\n(1)\nFailures in the early stages (navigation) can hinder the experience collection of all downstream tasks (picking and placing), thus hindering the learning of complete skill chains.\n(2)\nAs the trajectory lengthens, the experience buffer becomes dominated by navigation-intensive fragments. This results in sparse but critical interaction moments in the trajectory being overwhelmed by redundant navigation experiences.\n(3)\nSuch methods lack explicit mechanisms for learning reusable interaction patterns, which can easily lead to skill forgetting. Although recent data-driven or model-based methods\nChen\net al.\n(\n2025\n); Cen\net al.\n(\n2025\n); Lin\net al.\n(\n2025a\n)\ncan help mitigate these issues, they are often sensitive to environmental variations and difficult to spatially generalize. The above limitations raise a fundamental question:\nHow can a MM policy efficiently exploit long-horizon experience to achieve robust decision-making and generalization across different spatial layouts?\nRecent advances\nSun\net al.\n(\n2023\n); Tafazoli\net al.\n(\n2025\n)\nin system neuroscience suggest that higher animals solve novel tasks by recalling task-relevant episodic fragments and composing them with previously acquired skills or habitual behaviors. Inspired by this, we aim to investigate how to memorize key and informative episodic fragments from rich experience trajectories, and use them to improve RL-learned MM policies so that they can adapt to new spatial layouts. To this end, we address the above limitations by recalling episodic memories and composing them with MM skills to produce robust and spatially generalizable MM behaviors.\nAs shown in Fig.\n1\n(a), we propose\nA\ndaptive\nE\nxperience\nS\nelection (AES) to make MM agents pay more attention to key experience fragments in long trajectories that affect task success, improving skill chain learning and mitigating skill forgetting. Based on the informative state transitions well-selected by AES, a\nR\necurrent\nS\ntate-\nS\npace\nM\nodel (RSSM) is introduced for\nM\nodel-\nP\nredictive\nF\norward\nP\nlanning (MPFP) by capturing the coupled dynamics between the mobile base and the manipulator and imagining the dynamics of future manipulations. RSSM is trained together with model-free RL-based MM skill learning, and they share a single experience buffer. The difference is that AES helps RSSM memorize critical episodic fragments in a sample-efficient manner, thereby identifying weaknesses of the current MM skills and further consolidating them. As shown in Fig.\n1\n(b), RSSM-based MPFP can prospectively imagine future state transitions and evaluate state goodness to support RL-based MM skill learning. As system neuroscience suggests, RSSM here acts as a function of memorizing and recalling episodic fragments, while RL-learned skills act as habitual behaviors specific to MM. Their combination can improve MM policies on the current task and generalize to new spatial layouts without performance loss and additional training.\nWe implement our above ideas based on the popular\nE\nnd-\nE\nffector (EE)-centric MM framework\nHe\net al.\n(\n2025\n); Honerkamp\net al.\n(\n2023a\n)\n. Our approach is evaluated through sufficient comparative studies in diverse experimental configurations, demonstrating significant performance gains over the\nS\ntate-\no\nf-\nT\nhe-\nA\nrt methods. Ablation studies further confirm the individual contributions of AES and RSSM-based MPFP to sample efficiency and spatial generalization. Finally, real-world robotic experiments highlight the practicality of our methods in realistic MM scenarios. In summary, our contributions are threefold:\n(1)\nWe propose a system neuroscience-inspired method that leverages sample-efficient episodic recall to consolidate RL-based skill learning for robust and spatially generalizable MM.\n(2)\nAn AES mechanism is introduced to make MM agents pay more attention to key experience fragments in long trajectories that affect task success, improving sample efficiency.\n(3)\nRSSM-based dynamic imagination is proposed for MPFP by capturing the coupled dynamics between the mobile base and the manipulator and imagining the dynamics of future manipulations.\n2\nRelated Work\nModel-Free RL-based Mobile Manipulation.\nDue to the difficulty of planning in the joint space of the mobile base and the manipulator, early work solves an MM task by decomposing it into sequential navigation and static manipulation tasks. Such a decomposition has been widely adopted in methods based on planning\nJauhri\net al.\n(\n2024\n); Arduengo\net al.\n(\n2021\n)\n, reachability analysis\nVahrenkamp\net al.\n(\n2013\n); Feng\net al.\n(\n2025\n)\n, and RL\nGu\net al.\n(\n2022\n); Huang\net al.\n(\n2023\n); Harish\net al.\n(\n2024\n); Fang\net al.\n(\n2022\n)\n. Planning-based methods\nJauhri\net al.\n(\n2024\n); Arduengo\net al.\n(\n2021\n)\nplan trajectories for robots in joint space to ensure the kinematic feasibility of MM tasks. Given explicit task constraints, inverse reachability maps\nVahrenkamp\net al.\n(\n2013\n)\ncan also be exploited to determine favorable placements for the mobile base that facilitate subsequent manipulation. However, these classical methods are usually unable to instantly respond to diverse unstructured scenes and dynamic changes, and are thus less adaptable. Recently, RL-based approaches\nGu\net al.\n(\n2022\n); Huang\net al.\n(\n2023\n); Fang\net al.\n(\n2022\n)\nmitigate these issues by using hierarchical policy learning and curriculum learning techniques. In addition, there are methods\nJauhri\net al.\n(\n2022\n)\nthat use classical methods as behavioral priors to guide sample-efficient exploration. Most recently, some methods\nHe\net al.\n(\n2025\n); Honerkamp\net al.\n(\n2023a\n)\ninvestigate the EE-centric MM framework, which achieve strong autonomy and robustness by explicitly solving for base motion that cooperates with the manipulator during grasping.\nDespite recent progress, existing MM policies still struggle with low sample efficiency, as they fail to exploit the sparse but informative transitions embedded in long-horizon trajectories. Moreover, policies learned for specific tasks generalize poorly to new spatial layouts due to the absence of mechanisms for memorizing and reusing task-relevant experience. In this work, we address these issues by proposing AES and RSSM-based dynamic imagination.\nModel-based Mobile Manipulation.\nV\nision-\nL\nanguage-\nA\nction (VLA) models\nLiu\net al.\n(\n2025\n); Li\net al.\n(\n2024\n); Zhang\net al.\n(\n2025\n)\nhave recently emerged as a powerful paradigm for grounding multimodal instruction and perception into direct robot control. HybridVLA\nLiu\net al.\n(\n2025\n)\njointly integrates auto-regressive and diffusion-based policy heads to improve robustness across diverse manipulation tasks. EchoVLA\nLin\net al.\n(\n2025a\n)\naugments VLA with declarative and episodic memory to better handle long-horizon MM and spatial context reasoning. Diffusion-based action models\nWen\net al.\n(\n2025\n)\nintroduce masked diffusion mechanisms tailored for structured action generation, setting new standards over auto-regressive VLAs in multimodal control. In parallel, world-model-augmented frameworks\nCen\net al.\n(\n2025\n); Hafner\net al.\n(\n2025\n)\nseek to unify dynamics prediction with action planning, enabling joint learning of environment dynamics and policy, which can improve long-horizon foresight. Despite these advances, current model-based methods often require large multimodal datasets and still struggle with sample efficiency and reliable generalization across diverse spatial layouts. In this work, we employ RSSM-based MPFP to improve and generalize RL-based MM skills to address these issues.\n3\nPreliminaries\nProblem Definition.\nThe MM problem we consider in this work involves a robot that includes a mobile base and a manipulator. The MM tasks described in this paper involve skills such as navigation, collision avoidance, picking, placing, and opening. Such a problem is modeled as a goal-conditional\nP\nartially\nO\nbservable\nM\narkov\nD\necision\nP\nrocess (POMDP) defined as a tuple\nğ’³\n=\n(\nğ’®\n,\nğ’ª\n,\nğ’œ\n,\nğ’«\n,\nâ„›\n,\nğ’¢\n,\nÏ\n,\nÎ³\n)\n\\mathcal{X}=(\\mathcal{S},\\mathcal{O},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\mathcal{G},\\rho,\\gamma)\nwith underlying state space\nğ’®\n\\mathcal{S}\n, observation space\nğ’ª\n\\mathcal{O}\n, action space\nğ’œ\n\\mathcal{A}\n, reward function\nâ„›\n:\nğ’®\nÃ—\nğ’œ\nâ†’\nâ„\n\\mathcal{R}:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}\n, initial state distribution\nÏ\n\\rho\nand discount factor\nÎ³\n\\gamma\n. The state transition function\nğ’«\n:\nğ’®\nÃ—\nğ’œ\nÃ—\nğ’®\nâ†’\nâ„\n+\n\\mathcal{P}:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to\\mathbb{R}_{+}\nrepresents the probability density of the next state\nğ’”\nt\n+\n1\nâˆˆ\nğ’®\n\\boldsymbol{s}^{t+1}\\in\\mathcal{S}\ngiven the current state\nğ’”\nt\nâˆˆ\nğ’®\n\\boldsymbol{s}^{t}\\in\\mathcal{S}\nand action\nğ’‚\nt\nâˆˆ\nğ’œ\n\\boldsymbol{a}^{t}\\in\\mathcal{A}\n. For tasks like object rearrangement\nSzot\net al.\n(\n2022\n)\n, the goal space\nğ’ˆ\nâˆˆ\nğ’¢\n\\boldsymbol{g}\\in\\mathcal{G}\nis specified using the objectâ€™s initial pickup pose or the goal pose where the object has to be placed. Our objective is to learn a policy\nÏ€\nÎ¸\nâ€‹\n(\nğ’‚\nt\n|\nğ’\nt\n,\nğ’ˆ\n)\n\\pi_{\\theta}(\\boldsymbol{a}^{t}|\\boldsymbol{o}^{t},\\boldsymbol{g})\nmapping an observation\nğ’\nt\nâˆˆ\nğ’ª\n\\boldsymbol{o}^{t}\\in\\mathcal{O}\nand goal\nğ’ˆ\n\\boldsymbol{g}\nto an action\nğ’‚\nt\n\\boldsymbol{a}^{t}\nthat maximizes the expected return\nChen\net al.\n(\n2023\n)\n:\nÏ€\nÎ¸\nâˆ—\nâ€‹\n(\nğ’”\nt\n;\nğ’ˆ\n)\n=\narg\nmax\nğ’‚\nt\n[\nâ„›\n(\nğ’”\nt\n,\nğ’‚\nt\n;\nğ’ˆ\n)\n+\nÎ³\nâˆ«\nğ’”\nt\n+\n1\nğ’«\n(\nğ’”\nt\n+\n1\nâˆ£\nğ’”\nt\n,\nğ’‚\nt\n)\nV\nâˆ—\n(\nğ’”\nt\n+\n1\n;\nğ’ˆ\n)\nd\nğ’”\nt\n+\n1\n]\n,\n\\displaystyle\\small\\begin{aligned} \\pi_{\\theta}^{*}(\\boldsymbol{s}^{t};\\boldsymbol{g})&=\\arg\\max_{\\boldsymbol{a}^{t}}\\Big[\\mathcal{R}(\\boldsymbol{s}^{t},\\boldsymbol{a}^{t};\\boldsymbol{g})+\\\\\n&\\gamma\\int_{\\boldsymbol{s}^{t+1}}\\mathcal{P}(\\boldsymbol{s}^{t+1}\\mid\\boldsymbol{s}^{t},\\boldsymbol{a}^{t})V^{*}(\\boldsymbol{s}^{t+1};\\boldsymbol{g})\\,d\\boldsymbol{s}^{t+1}\\Big],\\end{aligned}\n(1)\nwhere\nV\nâˆ—\nâ€‹\n(\nğ’”\nt\n;\nğ’ˆ\n)\n=\nğ”¼\nÏ€\nÎ¸\n,\nğ’«\nâ€‹\n[\nâˆ‘\nk\n=\nt\nT\nÎ³\nk\nâˆ’\nt\nâ€‹\nâ„›\nâ€‹\n(\nğ’”\nk\n,\nÏ€\nÎ¸\nâˆ—\nâ€‹\n(\nğ’”\nt\n;\nğ’ˆ\n)\n)\n]\nV^{*}(\\boldsymbol{s}^{t};\\boldsymbol{g})=\\mathbb{E}_{\\pi_{\\theta},\\mathcal{P}}\\Big[\\sum_{k=t}^{T}\\gamma^{k-t}\\mathcal{R}(\\boldsymbol{s}^{k},\\pi_{\\theta}^{*}(\\boldsymbol{s}^{t};\\boldsymbol{g}))\\Big]\nis the optimal value function. In this setting,\nğ’«\nâ€‹\n(\nğ’”\nt\n+\n1\nâˆ£\nğ’”\nt\n,\nğ’‚\nt\n)\n\\mathcal{P}(\\boldsymbol{s}^{t+1}\\mid\\boldsymbol{s}^{t},\\boldsymbol{a}^{t})\nrepresents the state transition over time, which is usually unknown to the MM agent. In this work, our MM policy recall key experience fragments and combine them with habitual skills to produce robust and spatially generalizable MM behaviors. Therefore, the state transition\nğ’«\n\\mathcal{P}\nand the optimal value function\nV\nâˆ—\nV^{*}\nare learned together through dynamic imagination and model-free RL, respectively.\nFigure 2:\n(a) An illustration of the structure, unfolding, and training process of RSSM. (b) An illustration of which key experience fragments the AES focuses on, including the experience of interacting with objects, opening doors, and passing through narrow areas. These experience segments are likely to have collisions and IK solving failures that affect task success. (c) An illustration of RSSM-based MPFP.\nEE-Centric MM.\nTo instantiate the MM tasks, we use the\nR\nobot\nO\nperating\nS\nystem (ROS) to control an robotic system consisting of a Husky mobile base and a 6-DoF UR5 manipulator. Following the EE-centric MM framework\nHe\net al.\n(\n2025\n); Honerkamp\net al.\n(\n2023a\n)\n, given a sequence of 6D poses in\nSE\n(3), the robot needs to find appropriate base placements in\nSE\n(2) to allow the manipulatorâ€™s EE to reach these 6D goal poses sequentially along dynamically feasible paths. Such a solution is natural, just as humans reach for objects through leg movements in conjunction with their arms.\nIn practice, we employ an\nI\nnverse\nK\ninematics (IK) solver to solve the manipulatorâ€™s motion in joint space. On this basis, our objective is to learn the navigation of the mobile base to cooperate with the long-horizon manipulation. This is challenging because the mobile base must move in a collision-free manner while ensuring that the next 6D goal pose of the manipulator is reachable. Otherwise, the MM task is terminated due to collision of the mobile base or IK solving failure of the manipulator. To ensure the robotâ€™s kinematic feasibility, the mobile baseâ€™s 2D action space\nğ’œ\nâˆˆ\nâ„\n2\n\\mathcal{A}\\in\\mathbb{R}^{2}\nis continuous. The observation space\nğ’ª\n\\mathcal{O}\nincludes a robot-centric local occupancy map\nğ’\nt\n\\boldsymbol{m}^{t}\nand a state vector\nğ’—\nt\n\\boldsymbol{v}^{t}\nconsisting of the previous actions\nğ’‚\nt\nâˆ’\n1\n\\boldsymbol{a}^{t-1}\n, the joint states of robot, the EEâ€™s pose and velocity, and the desired pose of EE. Such an EE-centric MM framework inherits decades of experience in the field of robotics and incorporates the advantages of robot learning while ensuring robustness and practicality.\n4\nMethodology\nIn the following section, we propose leveraging sample-efficient episodic recall to reinforce and spatially generalize RL-based MM skills. We first present how to employ AES-enhanced RSSM to memorize informative experience fragments for dynamic imagination (Â§\n4.1\n). Then, we present how to achieve MPFP based on RSSM to boost RL-based MM skill learning and promote spatial generalization (Â§\n4.2\n).\n4.1\nAES-Enhanced RSSM for Dynamic Imagination\nRecurrent State-Space Model.\nRSSM is responsible for modeling state transitions in a latent space to capture the coupled dynamics between mobile base and manipulator. We adopt a structured latent representation consisting of two components: the stochastic representation\nğ’›\nt\n\\boldsymbol{z}^{t}\nand the recurrent state\nğ’‰\nt\n\\boldsymbol{h}^{t}\n, maintaining consistent MM progress while incorporating new observations. An encoder\ne\nÏ•\ne_{\\phi}\nis used to encode the observation\nğ’\nt\nâˆ’\n1\n=\n{\nğ’\nt\nâˆ’\n1\n,\nğ’—\nt\nâˆ’\n1\n}\n\\boldsymbol{o}^{t-1}=\\{\\boldsymbol{m}^{t-1},\\boldsymbol{v}^{t-1}\\}\nas a stochastic representation\nğ’›\nt\nâˆ’\n1\n\\boldsymbol{z}^{t-1}\n, which is integrated into the recurrent state\nğ’‰\nt\nâˆ’\n1\n\\boldsymbol{h}^{t-1}\nto form a comprehensive state representation for MM decision-making. A recurrent module\nf\nÏ•\nf_{\\phi}\nis utilized as the fundamental component of RSSM for latent state modeling, ensuring consistent temporal dynamics across state inference and future prediction. A decoder\np\nÏ•\np_{\\phi}\nis used to predict observations and rewards from the recurrent state. As shown in Fig.\n2\n(a), the components of our dynamic imagination model are outlined below:\nObservation Encoder:\nğ’›\nt\nâˆ’\n1\n=\ne\nÏ•\nâ€‹\n(\nğ’\nt\nâˆ’\n1\n)\n,\nDynamic Model:\nğ’‰\nt\n=\nf\nÏ•\nâ€‹\n(\nğ’‰\nt\nâˆ’\n1\n,\nğ’›\nt\nâˆ’\n1\n,\nğ’‚\nt\nâˆ’\n1\n)\n,\nObservation Predictor:\nğ’\n^\nğ’•\nâˆ¼\np\nÏ•\nâ€‹\n(\nğ’\n^\nğ’•\n|\nğ’‰\nğ’•\n)\n,\nReward Predictor:\nğ’“\n^\nt\nâˆ¼\np\nÏ•\nâ€‹\n(\nğ’“\n^\nt\n|\nğ’‰\nt\n)\n.\n\\displaystyle\\small\\begin{aligned} \\text{Observation Encoder:}&\\boldsymbol{z}^{t-1}\\ =e_{\\phi}(\\boldsymbol{o}^{t-1}),\\\\\n\\text{Dynamic Model: }&\\boldsymbol{h}^{t}=f_{\\phi}(\\boldsymbol{h}^{t-1},\\boldsymbol{z}^{t-1},\\boldsymbol{a}^{t-1}),\\\\\n\\text{Observation Predictor: }&\\boldsymbol{\\hat{o}^{t}}\\sim p_{\\phi}(\\boldsymbol{\\hat{o}^{t}|h^{t}}),\\\\\n\\text{Reward Predictor: }&\\boldsymbol{\\hat{r}}^{t}\\sim p_{\\phi}(\\boldsymbol{\\hat{r}}^{t}|\\boldsymbol{h}^{t}).\\end{aligned}\n(2)\nAccording to the problem definition in Sec.\n3\n, the dynamic model\nf\nÏ•\nf_{\\phi}\napproximates\nğ’«\n\\mathcal{P}\nby modeling state transitions. The training of\ne\nÏ•\ne_{\\phi}\n,\nf\nÏ•\nf_{\\phi}\n, and\nq\nÏ•\nq_{\\phi}\nis parallel to RL-based MM skill learning, and they share the same experience buffer. The difference is that RSSMâ€™s training data is well-selected by AES as follows. The loss functions used to train RSSM are consistent with Dreamer V3\nHafner\net al.\n(\n2025\n)\n.\nAdaptive Experience Selection.\nMM skills learned in sample-inefficient manner are difficult to generalize to new spatial layouts. On the one hand, standard uniform sampling is insufficient, as it tends to overfit the abundant navigation data while neglecting the critical manipulation experience strongly associated with task success. On the other hand, focusing solely on hard failures risks catastrophic forgetting of foundational navigation capabilities. To address this challenge, we introduce an AES mechanism that pays more attention to critical state transitions in MM trajectories, as shown in Fig.\n2\n(b). We aim to maintain a balance between common navigation and critical failure experience, thus fostering a dynamic model that is robust across all skills and diverse spatial layouts to enhance generalization and prevent skill forgetting. Technically, the RSSM is trained on fixed-length experience fragments that are extracted from episodes collected by the RL-based MM agent. For each MM episode that is put into the experience buffer, the experience segments are sliding experience windows that contain consecutive state transitions that have the highest total prioritization. Prioritization is achieved by assigning the\ni\ni\n-th state transition a composite priority score\nP\nt\nâ€‹\no\nâ€‹\nt\nâ€‹\na\nâ€‹\nl\ni\nP^{i}_{total}\n, which integrates three complementary signals:\n(1) Informative Experience Selection:\nFrom a data distribution perspective, informative state transitions that exhibit significant changes usually account for a small percentage and can thus be considered outliers. These state transitions usually correspond to moments such as opening a door, picking up or placing the target object, and avoiding collisions, as shown in Fig.\n2\n(b). Therefore, we propose to transform the task of selecting critical experience into the detection of outliers in the feature space of observations. Specifically, given a sequence of observations\n{\nğ’\n1\n,\nğ’\n2\n,\nâ€¦\n,\nğ’\nT\n}\n\\{\\boldsymbol{o}^{1},\\boldsymbol{o}^{2},...,\\boldsymbol{o}^{T}\\}\n, where\nğ’\nt\n=\n{\nğ’\nt\n,\nğ’—\nt\n}\n\\boldsymbol{o}^{t}=\\{\\boldsymbol{m}^{t},\\boldsymbol{v}^{t}\\}\n, we use\nd\nâ€‹\n(\nğ’\ni\n,\nğ’\nj\n)\nd(\\boldsymbol{o}^{i},\\boldsymbol{o}^{j})\nto denote the distance in feature space:\nd\nâ€‹\n(\nğ’\ni\n,\nğ’\nj\n)\n=\nâ€–\nğ’\nj\nâˆ’\nğ’\ni\nâ€–\n1\n+\nÎ»\nâ‹…\n(\n1\nâˆ’\nğ’—\ni\nâŠ¤\nâ€‹\nğ’—\nj\nâ€–\nğ’—\ni\nâ€–\n2\nâ‹…\nâ€–\nğ’—\nj\nâ€–\n2\n)\n,\n\\small d(\\boldsymbol{o}^{i},\\boldsymbol{o}^{j})=||\\boldsymbol{m}^{j}-\\boldsymbol{m}^{i}||_{1}+\\lambda\\cdot(1-\\frac{\\boldsymbol{v}^{i\\top}\\boldsymbol{v}^{j}}{||\\boldsymbol{v}^{i}||_{2}\\cdot||\\boldsymbol{v}^{j}||_{2}}),\n(3)\nwhere\nÎ»\n\\lambda\nis a weight used to balance magnitude. Based on Eq. (\n3\n), we define the\nk\nk\n-distance neighborhood:\nN\nk\nâ€‹\n(\nğ’\ni\n)\n=\n{\nğ’\nj\n|\nd\nâ€‹\n(\nğ’\ni\n,\nğ’\nj\n)\nâ‰¤\nd\nk\nâ€‹\n(\nğ’\ni\n)\n}\nN_{k}(\\boldsymbol{o}^{i})=\\{\\boldsymbol{o}^{j}|d(\\boldsymbol{o}^{i},\\boldsymbol{o}^{j})\\leq d_{k}(\\boldsymbol{o}^{i})\\}\n, where\nd\nk\nâ€‹\n(\nğ’\ni\n)\nd_{k}(\\boldsymbol{o}^{i})\nis the distance to the\nk\nk\n-th nearest neighbor. For each neighbor\nğ’\nj\nâˆˆ\nN\nk\nâ€‹\n(\nğ’\ni\n)\n\\boldsymbol{o}^{j}\\in N_{k}(\\boldsymbol{o}^{i})\n, the reachability distance is defined as\nreach-\nâ€‹\nd\nk\nâ€‹\n(\nğ’\ni\n,\nğ’\nj\n)\n=\nm\nâ€‹\na\nâ€‹\nx\nâ€‹\n{\nd\nk\nâ€‹\n(\nğ’\nj\n)\n,\nd\nâ€‹\n(\nğ’\ni\n,\nğ’\nj\n)\n}\n\\text{reach-}d_{k}(\\boldsymbol{o}^{i},\\boldsymbol{o}^{j})=max\\{d_{k}(\\boldsymbol{o}^{j}),d(\\boldsymbol{o}^{i},\\boldsymbol{o}^{j})\\}\n. Using this, the\nL\nocal\nO\nutlier\nF\nactor (LOF)\nBreunig\net al.\n(\n2000\n)\nof\nğ’\ni\n\\boldsymbol{o}^{i}\nis the inverse of its mean reachability distance:\nLRD\nk\nâ€‹\n(\nğ’\ni\n)\n=\n(\n1\n|\nN\nk\nâ€‹\n(\nğ’\ni\n)\n|\nâ€‹\nâˆ‘\nğ’\nj\nâˆˆ\nN\nk\nâ€‹\n(\nğ’\ni\n)\nreach-\nâ€‹\nd\nk\nâ€‹\n(\nğ’\ni\n,\nğ’\nj\n)\n)\nâˆ’\n1\n.\n\\small\\text{LRD}_{k}(\\boldsymbol{o}^{i})=\\left(\\frac{1}{|N_{k}(\\boldsymbol{o}^{i})|}\\sum_{\\boldsymbol{o}^{j}\\in N_{k}(\\boldsymbol{o}^{i})}\\text{reach-}d_{k}(\\boldsymbol{o}^{i},\\boldsymbol{o}^{j})\\right)^{-1}.\n(4)\nThen, the LOF is calculated as the relative density of\nğ’\ni\n\\boldsymbol{o}^{i}\ncompared to its neighbors:\nP\ni\nâ€‹\nc\nâ€‹\ne\ni\n=\nLOF\nk\nâ€‹\n(\nğ’\ni\n)\n=\n1\n|\nN\nk\nâ€‹\n(\nğ’\ni\n)\n|\nâ€‹\nâˆ‘\nğ’\nj\nâˆˆ\nN\nk\nâ€‹\n(\nğ’\ni\n)\nLRD\nk\nâ€‹\n(\nğ’\nj\n)\nLRD\nk\nâ€‹\n(\nğ’\ni\n)\n\\small P^{i}_{ice}=\\text{LOF}_{k}(\\boldsymbol{o}^{i})=\\frac{1}{|N_{k}(\\boldsymbol{o}^{i})|}\\sum_{\\boldsymbol{o}^{j}\\in N_{k}(\\boldsymbol{o}^{i})}\\frac{\\text{LRD}_{k}(\\boldsymbol{o}^{j})}{\\text{LRD}_{k}(\\boldsymbol{o}^{i})}\n(5)\nBased on this formulation, the observation is selected when\nLOF\nk\nâ€‹\n(\nğ’\ni\n)\n>\n1\n\\text{LOF}_{k}(\\boldsymbol{o}^{i})>1\n. A high LOF\n(\nğ’\ni\n)\nk\n{}_{k}(\\boldsymbol{o}^{i})\nindicates a novel and unusual state, receiving proportional priority.\n(2) Task Criticality:\nThis signal marks state transitions containing IK solving failures of the manipulator. These informative failure modes receive significant prioritization\nP\nt\nâ€‹\nc\nP_{tc}\n, ensuring that RSSM consistently focuses on learning to navigate to coordinate with successful IK solving, thereby preventing future failures.\n(3) Prediction Error:\nWe follow the idea of prioritized experience replay\nSchaul\net al.\n(\n2015\n)\n, employing the RSSMâ€™s overall training loss as a dynamic indicator of learning difficulty. The state transitions that are modeled incorrectly by RSSM are given higher priorities\nP\np\nâ€‹\ne\nâ€‹\nr\nP_{per}\n, which the RSSM should focus on in the following training.\nOverall, the final priority\nP\nt\nâ€‹\no\nâ€‹\nt\nâ€‹\na\nâ€‹\nl\ni\nP_{total}^{i}\nfor\ni\ni\n-th state transition is computed as a weighted sum of three signals:\nP\nt\nâ€‹\no\nâ€‹\nt\nâ€‹\na\nâ€‹\nl\ni\n=\nw\n1\nâ€‹\nmax\nâ¡\n(\n0\n,\nP\ni\nâ€‹\nc\nâ€‹\ne\ni\nâˆ’\n1\n)\n+\nw\n2\nâ€‹\nP\nt\nâ€‹\nc\ni\nâ‹…\nğ•€\n+\nw\n3\nâ€‹\nP\np\nâ€‹\ne\nâ€‹\nr\ni\n,\n\\small\\begin{split}P_{total}^{i}=w_{1}\\max\\bigl(0,\\,P_{ice}^{i}-1\\bigr)+w_{2}P_{tc}^{i}\\cdot\\mathbb{I}+w_{3}P_{per}^{i},\\end{split}\n(6)\nwhere\nw\n1\nw_{1}\n,\nw\n2\nw_{2}\n,\nw\n3\nw_{3}\nare respective weights and\nğ•€\n\\mathbb{I}\nindicates whether an IK solving failure occurs. In practice, all high-priority experience fragments are structured as a SumTree\nWang\net al.\n(\n2024\n)\nto efficiently manage and sample them based on normalized prioritization.\n4.2\nDynamic Imagination-based Foresighted MM\nSince RSSM and MM skills are trained together, AES-enhanced RSSM can provide foresighted guidance for RL-based MM skill learning through dynamic imagination. In addition, we emphasize using the AES-enhanced RSSM to generalize fully trained MM agents to new spatial layouts without additional learning. Following common practice\nJauhri\net al.\n(\n2022\n); Honerkamp\net al.\n(\n2023b\n)\n, we model a maximum-entropy MM policy\nÏ€\n^\nÎ¸\n\\hat{\\pi}_{\\theta}\nto explore the state space of a given task using the\nS\noft\nA\nctor-\nC\nritic (SAC) algorithm\nHaarnoja\net al.\n(\n2018\n)\n. The following RSSM-based MPFP is introduced to reinforce and spatially generalize MM skills during training and evaluation, respectively.\nFigure 3:\nWe consider four different experimental configurations: (a) cross-room, (b) warehouse-oriented, (c) home-scene-oriented, and (d) dynamic-scene-oriented (white dynamic obstacles) mobile manipulations. (e) An illustration of the cross-scene MM task. In each episode, the positions of the robot, the obstacles, the desks, the picking point (Goal\n1\n), the start point of opening the door (Goal\n2\n), the end point of opening the door (Goal\n3\n), and the placing point (Goal\n4\n) are randomly initialized. This task requires the robot to pick up an object and then open a door to another room to place the object in a specified position in the room. More details can be found in the supplementary material.\nModel-Predictive Forward Planning.\nDuring MM decision-making in both training and evaluation, the agent does not directly execute the action predicted by the policy\nÏ€\n^\nÎ¸\n\\hat{\\pi}_{\\theta}\nlearned by RL. Instead, it leverages RSSM as an internal simulator for dynamic imagination. As shown in Fig.\n2\n(c), this process integrates the concurrently trained actor-critic networks\n(\nÏ€\n^\nÎ¸\n,\nQ\n)\n(\\hat{\\pi}_{\\theta},Q)\n, which are used for the action rollout in dynamic imagination and the value estimation of the last imagined step, respectively. At each timestep, the MM agent samples action sequences from a Gaussian distribution\nğ’©\nâ€‹\n(\nÎ¼\n,\nÏƒ\n)\n\\mathcal{N}(\\mu,\\sigma)\nwhose initial parameters are initialized from the actor\nÏ€\n^\nÎ¸\nâ€‹\n(\nğ’\nt\n;\nğ’ˆ\n)\n\\hat{\\pi}_{\\theta}(\\boldsymbol{o}^{t};\\boldsymbol{g})\nbased on observation\nğ’\nt\n\\boldsymbol{o}^{t}\nand goal\nğ’ˆ\n\\boldsymbol{g}\n. During each iteration of\nC\nross-\nE\nntropy\nM\nethod (CEM)\nRubinstein (\n1999\n)\n,\nL\nL\ncandidate action sequences of horizon\nH\nH\nare drawn and evaluated in parallel by unrolling the RSSM:\nDynamic Imagination:\nğ’‰\ni\nk\n+\n1\n=\nf\nÏ•\nâ€‹\n(\nğ’‰\ni\nk\n,\nğ’›\ni\nk\n,\nğ’‚\ni\nk\n)\n,\nObservation Rollout:\nğ’\n^\ni\nk\n+\n1\nâˆ¼\np\nÏ•\nâ€‹\n(\nğ’\n^\ni\nk\n|\nğ’‰\n^\ni\nk\n)\n,\nAction Rollout:\nğ’‚\n^\ni\nk\n+\n1\n=\nÏ€\n^\nÎ¸\nâ€‹\n(\nğ’\n^\ni\nk\n;\nğ’ˆ\n)\n,\nObservation Encoding:\nğ’›\n^\ni\nk\n+\n1\n=\ne\nÏ•\nâ€‹\n(\nğ’\n^\ni\nk\n)\n,\nReward Prediction:\nğ’“\n^\ni\nk\n+\n1\nâˆ¼\np\nÏ•\nâ€‹\n(\nğ’“\n^\ni\nk\n+\n1\n|\nğ’‰\ni\nk\n+\n1\n)\n,\n\\displaystyle\\small\\begin{aligned} \\text{Dynamic Imagination: }&\\boldsymbol{h}^{k+1}_{i}=f_{\\phi}(\\boldsymbol{h}^{k}_{i},\\boldsymbol{z}^{k}_{i},\\boldsymbol{a}^{k}_{i}),\\\\\n\\text{Observation Rollout: }&\\boldsymbol{\\hat{o}}^{k+1}_{i}\\sim p_{\\phi}(\\boldsymbol{\\hat{o}}^{k}_{i}|\\boldsymbol{\\hat{h}}^{k}_{i}),\\\\\n\\text{Action Rollout: }&\\boldsymbol{\\hat{a}}^{k+1}_{i}=\\hat{\\pi}_{\\theta}(\\boldsymbol{\\hat{o}}^{k}_{i};\\boldsymbol{g}),\\\\\n\\text{Observation Encoding: }&\\boldsymbol{\\hat{z}}^{k+1}_{i}=e_{\\phi}(\\boldsymbol{\\hat{o}}^{k}_{i}),\\\\\n\\text{Reward Prediction: }&\\boldsymbol{\\hat{r}}^{k+1}_{i}\\sim p_{\\phi}(\\boldsymbol{\\hat{r}}^{k+1}_{i}|\\boldsymbol{h}^{k+1}_{i}),\\end{aligned}\n(7)\nwhere\ni\n=\n0\n,\nâ€¦\n,\nL\nâˆ’\n1\ni=0,...,L-1\nand\nk\nâˆˆ\n[\nt\n,\nt\n+\nH\nâˆ’\n1\n]\nk\\in[t,t+H-1]\n. As shown in Fig.\n2\n(c), each\nH\nH\n-horizon dynamic imagination yields a cumulative return:\nâ„›\n^\nt\n=\nâˆ‘\nk\n=\nt\nt\n+\nH\nâˆ’\n1\nÎ³\nk\nâˆ’\nt\nâ€‹\nğ’“\n^\ni\nk\n+\nÎ³\nt\n+\nH\nâ€‹\nQ\nâ€‹\n(\nğ’\n^\ni\nt\n+\nH\n,\nÏ€\n^\nÎ¸\nâ€‹\n(\nğ’\n^\ni\nt\n+\nH\n;\nğ’ˆ\n)\n)\n,\n\\displaystyle\\small\\begin{aligned} \\hat{\\mathcal{R}}^{t}=\\sum_{k=t}^{t+H-1}\\gamma^{k-t}\\hat{\\boldsymbol{r}}^{k}_{i}+\\gamma^{t+H}Q(\\boldsymbol{\\hat{o}}^{t+H}_{i},\\hat{\\pi}_{\\theta}(\\boldsymbol{\\hat{o}}^{t+H}_{i};\\boldsymbol{g})),\\end{aligned}\n(8)\nwhich is an estimate of\nÎ³\nâ€‹\nâˆ«\nğ’”\nt\n+\n1\nğ’«\nâ€‹\n(\nğ’”\nt\n+\n1\nâˆ£\nğ’\n^\nt\n,\nğ’‚\nt\n)\nâ€‹\nV\nâˆ—\nâ€‹\n(\nğ’”\nt\n+\n1\n;\ng\n)\nâ€‹\nğ‘‘\nğ’”\nt\n+\n1\n\\gamma\\int_{\\boldsymbol{s}^{t+1}}\\mathcal{P}(\\boldsymbol{s}^{t+1}\\mid\\boldsymbol{\\hat{o}}^{t},\\boldsymbol{a}^{t})V^{*}(\\boldsymbol{s}^{t+1};g)\\,d\\boldsymbol{s}^{t+1}\nin Eq. (\n1\n). Notably, the critic\nQ\nQ\nprovides a terminal value estimate based on the last imagined observation\nğ’\n^\ni\nt\n+\nH\n\\boldsymbol{\\hat{o}}^{t+H}_{i}\n, which is decoded from the belief state\nğ’‰\ni\nt\n+\nH\n\\boldsymbol{h}^{t+H}_{i}\n. The top-\nK\nK\nsequences with the highest returns are selected as elites, and the distribution parameters\n(\nÎ¼\n,\nÏƒ\n)\n(\\mu,\\sigma)\nof\nÏ€\n^\nÎ¸\n\\hat{\\pi}_{\\theta}\nare refitted to their empirical statistics. This sampling-evaluation-refitting loop repeats for a fixed number of iterations. After convergence, only the first action of the final mean sequence is executed by the MM agent, and the overall process repeats at the next timestep, forming a standard model-predictive cycle. The combination of RSSM rollouts, MM decision-making, and CEM optimization enables robust and foresighted planning under complex dynamics and uncertainty.\n5\nExperiments\nBased on the EE-centric MM framework introduced in Sec.\n3\n, we answer the following research questions through comparative and ablative studies:\n(1)\nHow does our method perform compared to strong baselines?\n(2)\nDoes RSSM-based MPFP help to generalize the model-free RL-based MM skills to new spatial layouts?\n(3)\nDoes our proposed AES mechanism help improve sample efficiency?\n(4)\nDoes RSSM-based MPFP help to reinforce the MM policy?\n(5)\nHow well does our MM policy perform on the real-world robot?\n5.1\nExperimental Setup\nExperimental Configurations.\nAs shown in Fig.\n3\n, we follow the experimental configurations in N\n2\nM\n2\nHonerkamp\net al.\n(\n2023b\n)\nand consider four different experimental configurations: cross-room, warehouse-oriented, home-scene-oriented, and dynamic-scene-oriented mobile manipulations. We first train and evaluate the proposed method and baselines individually in each of the four scenes. Since cross-room MM involves more diverse skills, including navigation, collision avoidance, picking up, opening doors, and placing, we evaluate the performance of generalizing the MM agent fully trained in such a configuration to the other three configurations. This refers to generalizing MM agents to new spatial layouts without additional training. Please see the supplementary material for more details.\nBaselines and Evaluation Metrics.\nWe compare our proposed method with several model-free RL-based MM policies, including the SoTA EE-centric MM method N\n2\nM\n2\nHonerkamp\net al.\n(\n2023b\n)\n, an end-to-end whole-body control method adapted from N\n2\nM\n2\n, the award-winning hierarchical method BHyRL\nJauhri\net al.\n(\n2022\n)\n, and the SoTA approach based on auxiliary task distillation\nHarish\net al.\n(\n2024\n)\n. In addition, we compare our method with two world model-based MM policies:\n(1) Dreamer V3-based MM\nHafner\net al.\n(\n2025\n)\n: This baseline adapts the Dreamer V3 architecture to the MM setting by learning a latent world model and performing latent imagination rollout for policy optimization. We modify the open-source implementation of Dreamer V3 to make it suitable for EE-centric MM to obtain experimental results.\n(2) TD-MPC2-based MM\nHansen\net al.\n(\n2024\n)\n: This baseline applies TD-MPC2 to the EE-centric MM setting as a model-based MPC-style planner that uses online trajectory optimization in the latent state space. Similar to our approach, only the first action from the optimized sequence is executed in the environment, resulting in a receding-horizon control loop suitable for complex MM dynamics.\nWe report the\nA\nverage\nIK\nsolution\nF\nailures (AIKF),\nA\nverage\nB\nase\nC\nollisions (ABC),\nT\nask\nC\nompletion\nR\nate (TCR), and\nP\nerfect\nS\nuccess\nR\nate (PSR) as evaluation metrics. A lower AIKF means a better cooperation between the mobile base and the manipulator. TCR implies that the MM robotic system completes the task while allowing for IK solving failures and base collisions. We set the threshold for allowing both kinds of failures to 20. PSR means completing the MM tasks with zero failures.\nTable 1:\nComparative studies are conducted in the cross-room experimental configuration to demonstrate the superiority of our method. Each method was evaluated over 100 episodes across 3 random seeds. We report the mean and standard deviation of each metric.\nMethod\nCross-room\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nEnd-to-End MM\n-\n-\n0\nÂ±0\n0\nÂ±0\nBHyRL\n12.1\nÂ±1.2\n2.8\nÂ±0.5\n21\nÂ±3\n19\nÂ±4\nN\n2\nM\n2\n11.3\nÂ±1.3\n2.2\nÂ±0.6\n29\nÂ±4\n17\nÂ±2\nDreamer V3\n8.1\nÂ±1.3\n2.0\nÂ±0.5\n54\nÂ±4\n40\nÂ±1\nTD-MPC2\n9.9\nÂ±1.6\n1.2\nÂ±0.4\n46\nÂ±3\n41\nÂ±1\nAuxDistill\n8.8\nÂ±1.5\n1.3\nÂ±0.3\n57\nÂ±6\n42\nÂ±7\nOurs\n4.1\nÂ±1.1\n2.0\nÂ±0.2\n81\nÂ±3\n54\nÂ±2\nImplementation Details.\nWe use a 3-layer CNN with maximal pooling to encode the local occupancy map, whose size and resolution are 30\nÃ—\n\\times\n30 and 0.1 m, respectively. Other observations, e.g., the state of the robot, are concatenated with the map embeddings and then passed through a multilayer perceptron. In our experiments, the RL agents are trained for\n5\nÃ—\n10\n6\n5\\times 10^{6}\nsteps of environmental interactions. The learning rate is linearly decayed from an initial value of\n1\nÃ—\n10\nâˆ’\n4\n1\\times 10^{-4}\nto a final value of\n1\nÃ—\n10\nâˆ’\n6\n1\\times 10^{-6}\nover the course of training. The reward discount factor\nÎ³\n\\gamma\nis set to 0.99. For RSSM training, the loss weights\nw\n1\nw_{1}\n,\nw\n2\nw_{2}\n, and\nw\n3\nw_{3}\nin Eq. (\n6\n) are all set to 1.0. We employ a linear schedule for the planning horizon\nH\nH\n, linearly increasing it from 1 to 8 over\n5\nÃ—\n10\n6\n5\\times 10^{6}\nsteps. During evaluation,\nH\nH\nis set to 8. The number of candidate action sequences\nL\nL\nis set to 50. The Top-10 sequences are selected as elites during CEM optimization.\nTable 2:\nComparative studies are conducted in three other experimental configurations to demonstrate the superiority of our method.\nMethod\nHome-scene\nWarehouse\nDynamic-scene\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nN\n2\nM\n2\n11.0\nÂ±0.9\n2.3\nÂ±0.5\n32\nÂ±4\n18\nÂ±2\n7.1\nÂ±1.5\n0.0\nÂ±0.0\n68\nÂ±2\n56\nÂ±3\n16.5\nÂ±1.1\n6.4\nÂ±0.8\n12\nÂ±2\n3\nÂ±1\nDreamer V3\n9.3\nÂ±0.7\n1.6\nÂ±0.5\n64\nÂ±2\n45\nÂ±2\n5.3\nÂ±1.2\n0.4\nÂ±0.2\n80\nÂ±3\n61\nÂ±1\n9.0\nÂ±0.9\n3.1\nÂ±0.4\n44\nÂ±3\n40\nÂ±3\nTD-MPC2\n10.7\nÂ±1.1\n1.5\nÂ±0.4\n56\nÂ±3\n32\nÂ±2\n6.0\nÂ±1.0\n0.5\nÂ±0.5\n74\nÂ±2\n60\nÂ±1\n8.9\nÂ±1.0\n5.0\nÂ±0.6\n40\nÂ±2\n35\nÂ±1\nOurs\n3.6\nÂ±1.1\n0.1\nÂ±0.1\n90\nÂ±2\n66\nÂ±4\n3.0\nÂ±1.0\n0.0\nÂ±0.0\n96\nÂ±4\n76\nÂ±3\n5.8\nÂ±1.6\n5.6\nÂ±0.4\n58\nÂ±3\n39\nÂ±1\nFigure 4:\nTCR metrics change with the number of training episodes in the cross-room experimental configuration.\nTable 3:\nThe models trained in the cross-room experimental configuration are transferred to three new experimental configurations for evaluation without additional training.\nMethod\nHome-scene\nWarehouse\nDynamic-scene\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nN\n2\nM\n2\n15.1\nÂ±1.3\n1.4\nÂ±0.3\n29\nÂ±1\n15\nÂ±1\n14.3\nÂ±1.2\n0.0\nÂ±0.0\n19\nÂ±1\n11\nÂ±1\n18.7\nÂ±1.6\n4.1\nÂ±0.5\n10\nÂ±1\n5\nÂ±0\nDreamer V3\n9.7\nÂ±0.9\n1.9\nÂ±0.5\n44\nÂ±2\n40\nÂ±1\n7.2\nÂ±0.8\n0.0\nÂ±0.0\n45\nÂ±2\n39\nÂ±2\n9.9\nÂ±0.7\n3.7\nÂ±0.3\n39\nÂ±3\n33\nÂ±2\nTD-MPC2\n13.8\nÂ±1.4\n0.7\nÂ±0.2\n32\nÂ±1\n16\nÂ±1\n9.5\nÂ±1.0\n2.2\nÂ±0.5\n40\nÂ±4\n32\nÂ±3\n9.2\nÂ±1.3\n5.5\nÂ±0.8\n35\nÂ±2\n25\nÂ±1\nOurs w/o MPFP\n12.3\nÂ±1.2\n0.6\nÂ±0.2\n46\nÂ±3\n22\nÂ±3\n8.6\nÂ±1.1\n0.0\nÂ±0.0\n45\nÂ±2\n41\nÂ±1\n10.5\nÂ±1.6\n4.4\nÂ±0.7\n35\nÂ±3\n29\nÂ±2\nOurs\n8.7\nÂ±1.8\n0.5\nÂ±0.1\n61\nÂ±2\n42\nÂ±1\n4.3\nÂ±1.2\n0.0\nÂ±0.0\n70\nÂ±1\n60\nÂ±1\n6.3\nÂ±1.2\n6.9\nÂ±1.0\n45\nÂ±3\n32\nÂ±3\nTable 4:\nAblation studies on components of AES and MPFP in the challenging cross-room experimental configuration.\nAblation\nAIKF\nâ†“\n\\downarrow\nABC\nâ†“\n\\downarrow\nTCR\nâ†‘\n\\uparrow\nPSR\nâ†‘\n\\uparrow\nw/o All AES\n16.7\nÂ±1.3\n1.3\nÂ±0.5\n10\nÂ±2\n5\nÂ±1\nw/o AES-\np\nt\nâ€‹\nc\np_{tc}\n10.1\nÂ±1.0\n0.9\nÂ±0.4\n51\nÂ±3\n39\nÂ±1\nw/o AES-\np\ni\nâ€‹\ne\nâ€‹\ns\np_{ies}\n12.7\nÂ±1.1\n1.5\nÂ±0.5\n34\nÂ±4\n21\nÂ±2\nw/o MPFP in train\n12.8\nÂ±1.4\n2.6\nÂ±0.5\n31\nÂ±1\n20\nÂ±1\nw/o MPFP in eval\n7.13\nÂ±1.4\n0.8\nÂ±0.7\n68\nÂ±4\n46\nÂ±3\nOur Full Method\n4.19\nÂ±1.2\n2.0\nÂ±0.2\n81\nÂ±3\n54\nÂ±2\n5.2\nComparative Studies\nTo answer question (1), we first conduct comparative studies of the proposed method with all baselines in the challenging cross-room experimental configuration, and the experimental results are shown in Tab.\n1\n. Our approach significantly outperforms the SoTA EE-centric MM policy (N\n2\nM\n2\n), the RL-based maximum entropy policy (BHyRL), the world model-based methods (Dreamer V3 and TD-MPC2), and the auxiliary task distillation-based method (AuxDistill) in terms of reducing IK solving failures and increasing task success. As shown in Fig.\n4\n, our method converges faster and better compared with strong baselines during training, reflecting that our method is more sample-efficient. The results of End-to-End MM indicate that it is difficult to learn MM policies based on whole-body control in an end-to-end manner using model-free RL. The results in Tab.\n1\nreflect that our method sometimes achieved sub-optimal ABC metrics. This is mainly because our approach reduces early task termination due to IK solving failures, and thus the extended episodes increase the likelihood of collisions. Furthermore, we train and evaluate our method and strong baselines in three other experimental configurations, with results shown in Tab.\n2\n. Either in static or dynamic scenes, our approach significantly reduces the number of IK solving failures (AIKF). In addition, our approach achieves competitive ABC metrics while significantly improving MM success rates (i.e., TCR and PSR). These results reflect the significant superiority of our method over the strong baselines. Training curves reflecting the sample efficiency can be found in the supplementary material. These advances are due to the fact that (1) AES enables our method paying more attention to those key state transitions that determine the success of the MM task; (2) RSSM-based MPFP helps to generate foresighted and collision-free mobile base motions to cooperate with the IK solving of the manipulator.\nTo answer question (2), both our method and three strong baselines trained in the cross-room experimental configuration are directly evaluated in the other three configurations without additional training. The results in Tab.\n3\nreflect that our approach can better generalize the RL-based MM agent to new spatial layouts, while N\n2\nM\n2\ngeneralizes poorly. Compared to world model-based baselines, our approach is effective in reducing IK solving failures and collisions while improving task success. In addition, we find that RSSM-based MPFP is necessary because the spatial generalization of our method is significantly weaker when it is absent. The results in Tab.\n2\nand Tab.\n3\nalso reflect that our method sometimes achieved sub-optimal ABC metrics. Such a phenomenon is especially prominent in dynamic scenes. Nevertheless, significant gains in the TCR metrics reflect the robustness and spatial generalization of our approach.\n5.3\nAblation Studies\nWe answer questions (3) and (4) by respectively ablating different signals in AES and MPFP in the challenging cross-room experimental configuration. Based on the experimental results in Tab.\n4\n, we find that the absence of all AES signals significantly degrades the MM performance, especially by increasing IK solving failures and decreasing the TCR and PSR metrics. In addition, the absence of both informative experience selection (AES-\np\ni\nâ€‹\ne\nâ€‹\ns\np_{ies}\n) and task criticality (AES-\np\nt\nâ€‹\nc\np_{tc}\n) impairs MM performance, with AES-\np\ni\nâ€‹\ne\nâ€‹\ns\np_{ies}\nhaving a greater impact. These results demonstrate the importance of AES for memorizing key experience fragments for robust MM. In addition, Fig.\n4\nalso reflects that our AES-based MM policy is sample-efficient. By ablating the RSSM-based MPFP in training and evaluation, respectively, we find that it always helps to facilitate the collaboration between the mobile base and the manipulator through farsighted dynamic imagination, which improves the MM performance. More studies on parameter\nH\nH\nand computational efficiency can be found in the supplementary material.\n5.4\nReal-World Experiments\nWe deploy our method to a real robotic platform consisting of a Husky A200 mobile base and a UR5 manipulator to verify its practicality. The robotâ€™s upper computer is a laptop with an Intel Core i9-13900HX CPU and GeForce RTX 4060 GPU. The robot uses a RealSense D435 RGB-D camera and YOLO v7\nWang\net al.\n(\n2023\n)\nto identify target objects. We use ROS to organize the hardware and software resources of the robotic system. Considering the security, we set the maximum speed and decision frequency of the mobile base to 0.3\nm\n/\ns\nm/s\nand 1 Hz, respectively. Please see the supplementary material and videos for sim-to-real experiments.\n6\nConclusion and Limitations\nThis paper investigates the problems of low sample efficiency and poor spatial generalization faced by MM in the context of embodied AI. Correspondingly, we propose the AES mechanism and RSSM-based MPFP to address these challenges. We find that AES, especially informative experience selection, can help RSSM memorize key experience fragments to significantly reinforce MM agents. RSSM-based prospective dynamic imagination can better generalize MM policies to new spatial layouts without additional training. Our problem modeling and experiments are carried out in an EE-centric MM framework, which fully combines the advantages of traditional robotic motion planning and robot learning. The practicality of our approach is verified by migrating it to a real-world robotic system without additional training.\nLimitations.\nTo focus on the contribution of the AES-enhanced RSSM to MM, we assumed an ideal observation space, especially using ground truth local occupancy maps directly. In practice, the construction of low-noise local occupancy maps from sensors (e.g., depth cameras and LiDAR) is essential.\nAcknowledgments\nThis work was supported in part by the Key Project of Xiangjiang Laboratory under 25XJ02003 and in part by the National Natural Science Foundation of China under 62272489, 62332020, and 62350004. This work was carried out in part using computing resources at the High-Performance Computing Center of Central South University.\nReferences\nArduengo\net al.\n[2021]\nMiguel Arduengo, Carme Torras, and Luis Sentis.\nRobust and adaptive door operation with a mobile robot.\nIntelligent Service Robotics\n, 14(3):409â€“425, 2021.\nBreunig\net al.\n[2000]\nMarkusÂ M Breunig, Hans-Peter Kriegel, RaymondÂ T Ng, and JÃ¶rg Sander.\nLof: identifying density-based local outliers.\nIn\nProceedings of the 2000 ACM SIGMOD international conference on Management of data\n, pages 93â€“104, 2000.\nCen\net al.\n[2025]\nJun Cen, Siteng Huang, Yuqian Yuan, Kehan Li, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Xin Li, Hao Luo, etÂ al.\nRynnvla-002: A unified vision-language-action and world model.\narXiv preprint arXiv:2511.17502\n, 2025.\nChen\net al.\n[2023]\nBolei Chen, Jiaxu Kang, Ping Zhong, Yongzheng Cui, Siyi Lu, Yixiong Liang, and Jianxin Wang.\nThink holistically, act down-to-earth: A semantic navigation strategy with continuous environmental representation and multi-step forward planning.\nIEEE Transactions on Circuits and Systems for Video Technology\n, 34(5):3860â€“3875, 2023.\nChen\net al.\n[2024]\nBolei Chen, Jiaxu Kang, Ping Zhong, Yixiong Liang, YuÂ Sheng, and Jianxin Wang.\nEmbodied contrastive learning with geometric consistency and behavioral awareness for object navigation.\nIn\nProceedings of the 32nd ACM International Conference on Multimedia\n, pages 4776â€“4785, 2024.\nChen\net al.\n[2025]\nSixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, etÂ al.\nAc-dit: Adaptive coordination diffusion transformer for mobile manipulation.\narXiv preprint arXiv:2507.01961\n, 2025.\nFang\net al.\n[2022]\nKuan Fang, Toki Migimatsu, Ajay Mandlekar, LiÂ Fei-Fei, and Jeannette Bohg.\nActive task randomization: Learning visuomotor skills for sequential manipulation by proposing feasible and novel tasks.\nCoRR\n, 2022.\nFeng\net al.\n[2025]\nXiaoxu Feng, Takato Horii, and Takayuki Nagai.\nPredictive reachability for embodiment selection in mobile manipulation behaviors.\nIEEE Robotics and Automation Letters\n, 2025.\nGu\net al.\n[2022]\nJiayuan Gu, DevendraÂ Singh Chaplot, Hao Su, and Jitendra Malik.\nMulti-skill mobile manipulation for object rearrangement.\narXiv preprint arXiv:2209.02778\n, 2022.\nHaarnoja\net al.\n[2018]\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.\nSoft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\nIn\nInternational conference on machine learning\n, pages 1861â€“1870. Pmlr, 2018.\nHafner\net al.\n[2025]\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.\nMastering diverse control tasks through world models.\nNature\n, pages 1â€“7, 2025.\nHansen\net al.\n[2024]\nNicklas Hansen, Hao Su, and Xiaolong Wang.\nTd-mpc2: Scalable, robust world models for continuous control.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2024.\nHarish\net al.\n[2024]\nAbhinavÂ Narayan Harish, Larry Heck, JosiahÂ P Hanna, Zsolt Kira, and Andrew Szot.\nReinforcement learning via auxiliary task distillation.\nIn\nEuropean Conference on Computer Vision\n, pages 214â€“230. Springer, 2024.\nHe\net al.\n[2025]\nGuanqi He, Xiaofeng Guo, Luyi Tang, Yuanhang Zhang, Mohammadreza Mousaei, Jiahe Xu, Junyi Geng, Sebastian Scherer, and Guanya Shi.\nFlying hand: End-effector-centric framework for versatile aerial manipulation teleoperation and policy learning.\narXiv preprint arXiv:2504.10334\n, 2025.\nHonerkamp\net al.\n[2023a]\nDaniel Honerkamp, Tim Welschehold, and Abhinav Valada.\nN2m2: Learning navigation for arbitrary mobile manipulation motions in unseen and dynamic environments.\nIEEE Transactions on Robotics\n, 39(5):3601â€“3619, 2023.\nHonerkamp\net al.\n[2023b]\nDaniel Honerkamp, Tim Welschehold, and Abhinav Valada.\nN\n2\nm\n2\n: Learning navigation for arbitrary mobile manipulation motions in unseen and dynamic environments.\nIEEE Transactions on Robotics\n, 2023.\nHuang\net al.\n[2023]\nXiaoyu Huang, Dhruv Batra, Akshara Rai, and Andrew Szot.\nSkill transformer: A monolithic policy for mobile manipulation.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, pages 10852â€“10862, 2023.\nJauhri\net al.\n[2022]\nSnehal Jauhri, Jan Peters, and Georgia Chalvatzaki.\nRobot learning of mobile manipulation with reachability behavior priors.\nIEEE Robotics and Automation Letters\n, 7(3):8399â€“8406, 2022.\nJauhri\net al.\n[2024]\nSnehal Jauhri, Sophie Lueth, and Georgia Chalvatzaki.\nActive-perceptive motion generation for mobile manipulation.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 1413â€“1419. IEEE, 2024.\nKang\net al.\n[2024]\nJiaxu Kang, Bolei Chen, Ping Zhong, Haonan Yang, YuÂ Sheng, and Jianxin Wang.\nHspnav: Hierarchical scene prior learning for visual semantic navigation towards real settings.\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 9434â€“9440, 2024.\nLi\net al.\n[2024]\nQixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, XiÂ Chen, Mozheng Liao, Fangyun Wei, YuÂ Deng, Sicheng Xu, Yizhong Zhang, etÂ al.\nCogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation.\narXiv preprint arXiv:2411.19650\n, 2024.\nLin\net al.\n[2025a]\nMin Lin, Xiwen Liang, Bingqian Lin, Liu Jingzhi, Zijian Jiao, Kehan Li, Yuhan Ma, Yuecheng Liu, Shen Zhao, Yuzheng Zhuang, etÂ al.\nEchovla: Robotic vision-language-action model with synergistic declarative memory for mobile manipulation.\narXiv preprint arXiv:2511.18112\n, 2025.\nLin\net al.\n[2025b]\nZhenyang Lin, Yurou Chen, and Zhiyong Liu.\nAutoskill: Hierarchical open-ended skill acquisition for long-horizon manipulation tasks via language-modulated rewards.\nIEEE Transactions on Cognitive and Developmental Systems\n, 2025.\nLiu\net al.\n[2025]\nJiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, etÂ al.\nHybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model.\narXiv preprint arXiv:2503.10631\n, 2025.\nNguyen and La [2019]\nHai Nguyen and Hung La.\nReview of deep reinforcement learning for robot manipulation.\nIn\n2019 Third IEEE international conference on robotic computing (IRC)\n, pages 590â€“595. IEEE, 2019.\nRubinstein [1999]\nReuven Rubinstein.\nThe cross-entropy method for combinatorial and continuous optimization.\nMethodology and computing in applied probability\n, 1(2):127â€“190, 1999.\nSchaul\net al.\n[2015]\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver.\nPrioritized experience replay.\narXiv preprint arXiv:1511.05952\n, 2015.\nSun\net al.\n[2023]\nWeinan Sun, Madhu Advani, Nelson Spruston, Andrew Saxe, and JamesÂ E Fitzgerald.\nOrganizing memories for generalization in complementary learning systems.\nNature neuroscience\n, 26(8):1438â€“1448, 2023.\nSzot\net al.\n[2022]\nAndrew Szot, Karmesh Yadav, Alex Clegg, Vincent-Pierre Berges, Aaron Gokaslan, Angel Chang, Manolis Savva, Zsolt Kira, and Dhruv Batra.\nHabitat rearrangement challenge 2022.\nhttps://aihabitat.org/challenge/2022_rearrange\n, 2022.\nTafazoli\net al.\n[2025]\nSina Tafazoli, FloraÂ M Bouchacourt, Adel Ardalan, NikolaÂ T Markov, Motoaki Uchimura, MarceloÂ G Mattar, NathanielÂ D Daw, and TimothyÂ J Buschman.\nBuilding compositional tasks with shared neural subspaces.\nNature\n, pages 1â€“9, 2025.\nVahrenkamp\net al.\n[2013]\nNikolaus Vahrenkamp, Tamim Asfour, and RÃ¼diger Dillmann.\nRobot placement based on reachability inversion.\nIn\n2013 IEEE International Conference on Robotics and Automation\n, pages 1970â€“1975. IEEE, 2013.\nWang\net al.\n[2023]\nChien-Yao Wang, Alexey Bochkovskiy, and Hong-YuanÂ Mark Liao.\nYolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 7464â€“7475, 2023.\nWang\net al.\n[2024]\nCan Wang, Jiaheng Zhang, Aoqi Wang, Zhen Wang, Nan Yang, Zhuoli Zhao, ChunÂ Sing Lai, and LoiÂ Lei Lai.\nPrioritized sum-tree experience replay td3 drl-based online energy management of a residential microgrid.\nApplied Energy\n, 368:123471, 2024.\nWen\net al.\n[2025]\nYuqing Wen, Hebei Li, Kefan Gu, Yucheng Zhao, Tiancai Wang, and Xiaoyan Sun.\nLlada-vla: Vision language diffusion action models.\narXiv preprint arXiv:2509.06932\n, 2025.\nZhang\net al.\n[2025]\nWenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, HeÂ Wang, etÂ al.\nDreamvla: a vision-language-action model dreamed with comprehensive world knowledge.\narXiv preprint arXiv:2507.04447\n, 2025.",
    "preview_text": "Mobile Manipulation (MM) involves long-horizon decision-making over multi-stage compositions of heterogeneous skills, such as navigation and picking up objects. Despite recent progress, existing MM methods still face two key limitations: (i) low sample efficiency, due to ineffective use of redundant data generated during long-term MM interactions; and (ii) poor spatial generalization, as policies trained on specific tasks struggle to transfer to new spatial layouts without additional training. In this paper, we address these challenges through Adaptive Experience Selection (AES) and model-based dynamic imagination. In particular, AES makes MM agents pay more attention to critical experience fragments in long trajectories that affect task success, improving skill chain learning and mitigating skill forgetting. Based on AES, a Recurrent State-Space Model (RSSM) is introduced for Model-Predictive Forward Planning (MPFP) by capturing the coupled dynamics between the mobile base and the manipulator and imagining the dynamics of future manipulations. RSSM-based MPFP can reinforce MM skill learning on the current task while enabling effective generalization to new spatial layouts. Comparative studies across different experimental configurations demonstrate that our method significantly outperforms existing MM policies. Real-world experiments further validate the feasibility and practicality of our method.\n\nSpatially Generalizable Mobile Manipulation via Adaptive Experience Selection and Dynamic Imagination\nPing Zhong\n1,2\nLiangbai Liu\n1\nBolei Chen\n1\nCorresponding Author.\nTao Wu\n1\nJiazhi Xia\n1,2\nChaoxu Mu\n3\nJianxin Wang\n1âˆ—\n1\nSchool of Computer Science and Engineering, Central South University\n2\nXiangjiang Laboratory\n3\nSchool of Artificial Intelligence, Anhui University\n{liangbailiu, boleichen, 8102221321, ping.zhong, xiajiazhi}@csu.edu.cn, cxmu@tju.edu.cn, jxwang@mail.csu.edu.cn\nAbstract\nM\nobile\nM\nanipulation (MM) involves long-horizon decision-making over multi-stage compos",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è‡ªé€‚åº”ç»éªŒé€‰æ‹©å’ŒåŠ¨æ€æƒ³è±¡æ¥æé«˜ç§»åŠ¨æ“ä½œä»»åŠ¡æ ·æœ¬æ•ˆç‡å’Œç©ºé—´æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T04:43:49Z",
    "created_at": "2026-01-27T15:53:17.313579",
    "updated_at": "2026-01-27T15:53:17.313586"
}