{
  "id": "2601.11250v1",
  "title": "VLAgents: A Policy Server for Efficient VLA Inference",
  "authors": [
    "Tobias Jülg",
    "Khaled Gamal",
    "Nisarga Nilavadi",
    "Pierre Krack",
    "Seongjin Bien",
    "Michael Krawez",
    "Florian Walter",
    "Wolfram Burgard"
  ],
  "abstract": "The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents",
  "url": "https://arxiv.org/abs/2601.11250v1",
  "html_url": "https://arxiv.org/html/2601.11250v1",
  "html_content": "VLAgents: A Policy Server for Efficient VLA Inference\nTobias Jülg\n1\n, Khaled Gamal\n1\n, Nisarga Nilavadi\n1\n, Pierre Krack\n1\n, Seongjin Bien\n1\n,\nMichael Krawez\n1\n, Florian Walter\n1,2\nand Wolfram Burgard\n1\n1\nUniversity of Technology Nuremberg\n2\nTechnical University of Munich\nAbstract\nThe rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups.\nTo address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol.\nCrucially, its communication layer transparently adapts to the context by\nsupporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware.\nIn this work, we present the architecture of VLAgents and validate it by integrating seven policies—including OpenVLA and\nπ\n0\n\\pi_{0}\n. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot.\nVLAgents is available at\ngithub.com/RobotControlStack/vlagents\n.\nI\nIntroduction\nThe model landscape for open-source robotics foundation models is becoming increasingly rich.\nMany models come with their own custom interfaces.\nEvaluating, benchmarking, or extending a model, therefore, often entails writing custom deployment code to connect it to other systems and frameworks.\nThis makes a unified model interface highly desirable.\nYet, only a few open source solutions exist so far, most prominently LeRobot’s Asynchronous Inference\n[\nlerobot\n]\n, which already has several models integrated behind a common policy server.\nHowever, it only defines a loose dictionary-based communication interface.\nIn addition to code complexity, there is also a large system complexity in robotics:\nAI models and robot controllers often need to run on separate physical machines, sometimes even at a remote location, as models demand more and more GPU resources.\nFurthermore, as evaluation in simulation\n[\nsimpler\n,\nlibero\n,\nrcs\n]\nbecomes increasingly popular, the robot backend should also be easily exchangeable with a simulator.\nHowever, complex software dependencies can mutually exclude the installation of the model and simulator in the same Python environment\n[\njuelg2025refinedpolicydistillationvla\n]\n.\nA policy server appears to be the natural design choice for addressing these challenges.\nMany models, such as OpenVLA\n[\nopenvla\n]\nand\nπ\n0\n\\pi_{0}\n[\npizero\n]\n, already ship with their own policy server to facilitate the execution on a physical robot that runs on a different machine or in a different Python environment, but they are model-specific. While model-agnostic frameworks exist, they are still in their early stages of development and often lack rigid interfaces and data-aware compression.\nIn this work, we introduce\nVLAgents\n, a model-agnostic Python-based inference server that provides a communication interface between models and robots.\nIt provides an API that is similar to Gymnasium environments\n[\ngymnasium\n]\nand is specifically tailored for Vision-Language-Action models (VLAs).\nBut in principle, VLAgents can be used with any type of model.\nIt is comprised of a flexible policy server and a corresponding client that can communicate both over the network or via efficient shared memory if executed on the same machine.\nIn addition, VLAgents comes with Slurm-compatible\n[\nslurm\n]\nCLI tools for checkpoint evaluations on clusters during training.\nFigure 1:\nThe architecture of VLAgents.\nEach environment implements a thin wrapper that translates observations and actions into the expected data format (see\nLABEL:fig:interface\n).\nA central control loop takes the state and forwards it via either shared memory or a TCP connection with JPEG-encoding to the policy server.\nThe policy server uses the interface from\nLABEL:fig:interface\nto run an inference pass with the corresponding model.\nThis yields an action, which is then returned and used by the environment loop to step the environment to obtain the next state.\nII\nRelated Work\nThe need for an isolated model environment has led to diverse client/server protocols.\nMost common are simple synchronous raw request-response protocols.\nExamples include OpenVLA\n[\nopenvla\n]\n, which leverages an HTTP server implemented in FastAPI, and the OpenPi model suite\n[\npizero\n,\nfast\n,\npi05\n]\nwith WebSocket communication.\nWhile they work well for deploying the respective models on real robot hardware, they introduce a substantial communication overhead when used for parallel evaluation in simulation, as the payload needs to be serialized and travel through the whole network stack.\nUsing such model-specific servers, moreover, requires writing model-specific code for both simulation evaluation and real-world experiments.\nLeRobot\n[\nlerobot\n]\nprovides an asynchronous gRPC-based policy server implementation with a dictionary-based communication protocol, which is implemented for several models.\nAlthough the framework addresses both the interface and the communication issue, it neither provides efficient shared memory-based communication nor does it perform data-aware compression.\nWhile the approach is flexible, it requires special consideration during implementation: Since the dictionary keys are not standardized, both robots and models can require arbitrary keys for their actions and observations and considerable effort may be needed to map the keys between robots and models.\nThere is also no explicit protocol layer that would allow the user to define this mapping in code, which makes it difficult to apply important transformations, e.g., normalization, to the data.\nVLAgents\naddresses these gaps by defining a lightweight interface that transparently switches between high-performance shared memory (for simulation) and network streaming (for hardware) without code changes.\nIt is data-aware, allowing it to compress high-volume image data via fast JPEG encoding, while also providing the flexibility to support new data types for special use cases.\nIII\nMethodology\nVLAgents’ architecture is driven by two main use cases:\nThe batched simulation evaluations on the cluster during training and the evaluation on physical and simulated robots for benchmarking.\nTherefore, the main design objectives are to provide a unified communication interface between environments, i.e., simulations or physical robot platforms, and VLA policies, and to enable remote policy execution based on a client/server architecture.\nWe define a policy interface (see\nLABEL:fig:interface\n) that wraps VLAs similar to the Gymnasium environment API.\nIt consists of three functions that perform model loading, resetting, and inference.\nIt further defines special data structures for observations and actions.\nData types required for VLAs, such as RGB input or action output, have their own dedicated typed attributes.\nWe also define an info dictionary that can, if required, hold any type of data.\nThis allows for compressing large data objects, such as images, while maintaining the flexibility to add custom data to the info dictionary.\n⬇\n1\nclass\nObs\n:\n2\ncameras\n:\ndict\n[\nstr\n,\nnp\n.\nndarray\n]\n=\n{}\n3\ngripper\n:\nfloat\n|\nNone\n=\nNone\n4\ninfo\n:\ndict\n[\nstr\n,\nAny\n]\n=\n{}\n5\n6\nclass\nAct\n:\n7\naction\n:\nnp\n.\nndarray\n8\ndone\n:\nbool\n=\nFalse\n9\ninfo\n:\ndict\n[\nstr\n,\nAny\n]\n=\n{}\n10\n11\nclass\nAgent\n:\n12\ndef\ninitialize\n(\nself\n):\n13\n\"\"\n\"heavy\nintitilzation\ne.g.\nmodel\nloading\"\n\"\"\n14\n15\ndef\nact\n(\nself\n,\nobs\n:\nObs\n)\n->\nAct\n:\n16\n\"\"\n\"forward\npass\"\n\"\"\n17\n18\ndef\nreset\n(\nself\n,\nobs\n:\nObs\n,\ninstruction\n:\nAny\n,\n**\nkwargs\n)\n->\ndict\n[\nstr\n,\nAny\n]:\n19\n\"\"\n\"reset\nstate,\ne.g.\nhistory\"\n\"\"\n\\\nend\n{\nlstlisting\n}\n20\n\\\ncaption\n{\nVLA\npolicy\ninterface\n.}\n21\n\\\nlabel\n{\nfig\n:\ninterface\n}\n22\n\\\nend\n{\nfigure\n}\n23\n24\nA\npolicy\nserver\nexposes\nthis\ninterface\nvia\nRPyC\n(\na\nTCP\n-\nbased\nRemote\nProcedure\nCall\nlibrary\nfor\nPython\n)\nto\na\nremote\nclient\n.\n25\nThe\nclient\nis\nconnection\n-\naware\nand\navoids\nserialization\nby\nusing\nshared\nmemory\nwhen\nrunning\non\nthe\nsame\nhost\nas\nthe\nserver\n.\n26\nOtherwise\n,\nRGB\ndata\nare\nserialized\nusing\nJPEG\ncompression\nto\nreduce\nthe\ndata\nsize\nfor\ntransport\n.\n27\nVLAgents\ncan\nbe\nused\nstandalone\nfor\nstandard\nclient\n/\nserver\ncommunication\n.\n28\nAdditionally\n,\nit\ncan\nbe\nused\nfor\nautomated\nevaluations\nas\nit\nprovides\nan\nenvironment\nloop\n,\nSlurm\nand\nvideo\nrecording\nutilities\n,\nas\nshown\nin\n\\\nautoref\n{\nfig\n:\narch\n}.\n29\nA\nwrapper\nlayer\nis\navailable\nto\ntranslate\nactions\nand\nobservations\ninto\nthe\nrequired\ncommunication\nformat\n.\n30\n31\n32\n\\\nsection\n{\nResults\nand\nConclusion\n}\n33\nVLAgents\ncurrently\nintegrates\nseven\ndifferent\npolicies\n,\nincluding\nOcto\n~\\\ncite\n{\nocto\n},\nOpenVLA\n~\\\ncite\n{\nopenvla\n},\nthe\nOpenPi\nsuite\n~\\\ncite\n{\npizero\n,\nfast\n,\npi05\n},\nDiffusion\nPolicy\n~\\\ncite\n{\ndp\n}\nand\nV\n-\nJEPA\n2~\\\ncite\n{\nvjepa2\n}.\n34\nMethods\nthat\naugment\nthe\ninput\nor\noutput\nof\nthe\nmodel\n,\nsuch\nas\nARRO\n~\\\ncite\n{\narro\n},\nhave\nalso\nbeen\nsuccessfully\nevaluated\n.\n35\nThe\nlibrary\ncomes\nwith\nout\n-\nof\n-\nthe\n-\nbox\nsupport\nfor\nManiskill\n~3~\\\ncite\n{\nmaniskill\n}\nenvironments\nas\nwell\nas\nfor\nthe\nRobot\nControl\nStack\n(\nRCS\n)~\\\ncite\n{\nrcs\n}\necosystem\n,\nwhich\nsupports\nfour\ndifferent\nrobot\narms\nfor\nreal\n-\nworld\nand\nMuJoCo\n-\nsimulated\nexperiments\n.\n36\nWe\nalso\nused\nVLAgents\nfor\nRL\n-\nbased\nfine\n-\ntuning\nof\nVLAs\n~\\\ncite\n{\njuelg2025refinedpolicydistillationvla\n},\nwhich\nrequires\nbatched\nforward\npasses\nand\na\nlow\ncommunication\noverhead\nto\nprevent\nslowing\ndown\nthe\ntraining\nspeed\n.\n37\n38\n\\\nbegin\n{\nfigure\n}[\nt\n]\n39\n\\\ncentering\n40\n\\\nincludegraphics\n[\nwidth\n=\\\nlinewidth\n]{\nfigures\n/\nlatency_224_same_vs_diff\n.\npdf\n}\n41\n\\\ncaption\n{\nMean\nRound\n-\nTrip\nTime\n(\nRTT\n)\nfor\ndifferent\npolicy\nservers\nwith\ntwo\n$224\n\\\ntimes224$\nRGB\ncameras\n.\n42\nLocalhost\nindicates\nthat\nthe\nclient\nand\nserver\nare\nrunning\non\nthe\nsame\nmachine\n,\nwhile\nnetwork\nindicates\nexecution\nacross\ndifferent\nmachines\n.\n43\nFor\nthe\nnetwork\nsetting\n,\nthe\nmachines\nwere\nconnected\nin\na\nLocal\nArea\nNetwork\nwith\na\n1\nGbit\nEthernet\nconnection\n.\n44\n\\\nvspace\n{-0.5\ncm\n}\n45\n}\n46\n\\\nlabel\n{\nfig\n:\nlatency\n}\n47\n\\\nend\n{\nfigure\n}\n48\n49\n\\\nautoref\n{\nfig\n:\nlatency\n}\nshows\na\ncomparison\nof\nplain\nRound\n-\nTrip\nTimes\n(\nRTT\n)\nfor\nclient\nrequests\n.\n50\nThe\nexperiment\nevaluates\nthe\nefficiency\nof\nthe\nserialization\nand\nthe\ntransport\nprotocol\n,\nskipping\nthe\nmodel\n’s\ninference\nstep\non\nthe\nserver\nside.\n51\nOut\nof\nthe\nfour\npolicy\nservers\ntested,\nVLAgents\nachieves\nthe\nbest\nperformance,\nboth\nin\nthe\nlocal\nand\nthe\nnetwork\nsetting.\n52\nIt\nallows\nup\nto\n220\nHz\ninference\nspeed\nin\nthe\nnetwork\ndeployment\nand\nintroduces\nonly\n0.3\nms\ndelay\nfor\nsimulated\nevaluations.\n53\n54\nIn\nconclusion,\nVLAgents\nis\nan\nefficient\npolicy\nserver\nthat\nprovides\na\ncommunication\ninterface\nbetween\nVLAs\nand\nrobot\nenvironments,\nboth\nsimulated\nand\nphysical,\nand\nuses\ndata-aware\ncompression.\n55\nDue\nto\nthe\nusage\nof\nJPEG\nencoding\nand\nshared\nmemory,\nVLAgents\nis\nfaster\nthan\nother\ncommonly\nused\npolicy\nservers\nby\na\nfactor\nof\nthree.\n56\n57\n\\bibliographystyle{IEEEtran}\n58\n\\bibliography{bibliography.bib}\n59\n60\n\\end{document}’",
  "preview_text": "The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents\n\nVLAgents: A Policy Server for Efficient VLA Inference\nTobias Jülg\n1\n, Khaled Gamal\n1\n, Nisarga Nilavadi\n1\n, Pierre Krack\n1\n, Seongjin Bien\n1\n,\nMichael Krawez\n1\n, Florian Walter\n1,2\nand Wolfram Burgard\n1\n1\nUniversity of Technology Nuremberg\n2\nTechnical University of Munich\nAbstract\nThe rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups.\nTo address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol.\nCrucially, its communication layer transparently adapts to the context by\nsupporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware.\nIn this work, we present the architecture of VLAgents and validate it by integrating seven policies—including OpenVLA and\nπ\n0\n\\pi_{0}\n. In a benchmark with both local and remote communication, we further demonstrate how it out",
  "is_relevant": true,
  "relevance_score": 6.0,
  "extracted_keywords": [
    "VLA"
  ],
  "one_line_summary": "VLAgents是一个用于高效VLA推理的策略服务器，通过统一接口和自适应通信层解决机器人部署中的复杂性和延迟问题。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-16T12:58:59Z",
  "created_at": "2026-01-20T17:50:00.053948",
  "updated_at": "2026-01-20T17:50:00.053956"
}