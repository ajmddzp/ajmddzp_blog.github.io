{
    "id": "2601.06978v1",
    "title": "Benchmarking Autonomy in Scientific Experiments: A Hierarchical Taxonomy for Autonomous Large-Scale Facilities",
    "authors": [
        "James Le Houx"
    ],
    "abstract": "ä»è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†è¿ˆå‘å®Œå…¨è‡ªä¸»å‘ç°ï¼Œéœ€è¦ä¸€ä¸ªå…±åŒçš„è¯æ±‡ä½“ç³»æ¥è¡¡é‡è¿›å±•ã€‚æ±½è½¦è¡Œä¸šä¾èµ–SAE J3016æ ‡å‡†ï¼Œè€Œå½“å‰è‡ªä¸»ç§‘å­¦çš„åˆ†ç±»ä½“ç³»é¢„è®¾äº†æ‰€æœ‰è€…-æ“ä½œè€…æ¨¡å¼ï¼Œè¿™ä¸å¤§å‹ç”¨æˆ·è®¾æ–½çš„è¿è¡Œåˆšæ€§ä¸ç›¸å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºç§‘å­¦å®éªŒè‡ªä¸»æ€§åŸºå‡†ï¼ˆBASEï¼‰é‡è¡¨â€”â€”ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è¿™äº›ç‹¬ç‰¹çº¦æŸæ¡ä»¶è®¾è®¡çš„å…­çº§åˆ†ç±»ä½“ç³»ï¼ˆ0-5çº§ï¼‰ã€‚ä¸æ‰€æœ‰è€…-æ“ä½œè€…æ¨¡å¼ä¸åŒï¼Œç”¨æˆ·è®¾æ–½éœ€è¦é›¶æ ·æœ¬éƒ¨ç½²ï¼Œå³æ™ºèƒ½ä½“å¿…é¡»ç«‹å³æŠ•å…¥è¿è¡Œï¼Œæ— éœ€æ¼«é•¿è®­ç»ƒæœŸã€‚æˆ‘ä»¬æ˜ç¡®äº†æ¯ä¸ªå±‚çº§çš„å…·ä½“æŠ€æœ¯è¦æ±‚ï¼Œå¹¶å°†æ¨ç†å±éšœï¼ˆç¬¬3çº§ï¼‰å®šä¹‰ä¸ºå…³é”®å»¶è¿Ÿé˜ˆå€¼â€”â€”åœ¨æ­¤é˜¶æ®µå†³ç­–ä»æ ‡é‡åé¦ˆè½¬å‘è¯­ä¹‰æ•°å­—å­ªç”Ÿã€‚æœ¬è´¨ä¸Šï¼Œè¯¥å±‚çº§å°†å†³ç­–æµå½¢ä»ç©ºé—´æ¢ç´¢æ‰©å±•åˆ°æ—¶é—´é—¨æ§ï¼Œä½¿æ™ºèƒ½ä½“èƒ½åŒæ­¥æ•°æ®é‡‡é›†ä¸ç¬æ€ç‰©ç†äº‹ä»¶çš„å‘ç”Ÿã€‚é€šè¿‡å»ºç«‹è¿™äº›æ“ä½œå®šä¹‰ï¼ŒBASEé‡è¡¨ä¸ºè®¾æ–½ä¸»ç®¡ã€èµ„åŠ©æœºæ„å’Œå…‰æŸçº¿ç§‘å­¦å®¶æä¾›äº†æ ‡å‡†åŒ–åº¦é‡ï¼Œç”¨ä»¥è¯„ä¼°é£é™©ã€ç•Œå®šè´£ä»»å¹¶é‡åŒ–å®éªŒå·¥ä½œæµç¨‹çš„æ™ºèƒ½æ°´å¹³ã€‚",
    "url": "https://arxiv.org/abs/2601.06978v1",
    "html_url": "https://arxiv.org/html/2601.06978v1",
    "html_content": "Benchmarking Autonomy in Scientific Experiments: A Hierarchical Taxonomy for Autonomous Large-Scale Facilities\nJames Le Houx\nUniversity of Greenwich, Old Royal Naval College, Park Row, London, SE10 9LS, United Kingdom\nISIS Neutron & Muon Source, Rutherford Appleton Laboratory, Didcot, OX11 0QX, United Kingdom\nThe Faraday Institution, Harwell Science and Innovation Campus, Didcot, OX11 0RA, United Kingdom\ncorresponding author: James Le Houx (james.le-houx@stfc.ac.uk)\nAbstract\nThe transition from automated data collection to fully autonomous discovery requires a shared vocabulary to benchmark progress. While the automotive industry relies on the SAE J3016 standard, current taxonomies for autonomous science presuppose an owner-operator model that is incompatible with the operational rigidities of Large-Scale User Facilities. Here, we propose the Benchmarking Autonomy in Scientific Experiments (BASE) Scale, a 6-level taxonomy (Levels 0â€“5) specifically adapted for these unique constraints. Unlike owner-operator models, User Facilities require zero-shot deployment where agents must operate immediately without extensive training periods. We define the specific technical requirements for each tier, identifying the Inference Barrier (Level 3) as the critical latency threshold where decisions shift from scalar feedback to semantic digital twins. Fundamentally, this level extends the decision manifold from spatial exploration to temporal gating, enabling the agent to synchronise acquisition with the onset of transient physical events. By establishing these operational definitions, the BASE Scale provides facility directors, funding bodies, and beamline scientists with a standardised metric to assess risk, define liability, and quantify the intelligence of experimental workflows.\nKeywords:\nAutonomous Experimentation, Large-Scale User Facilities, AI for Science, Taxonomy, Operational Design Domains (ODD), Sim-to-Real Transfer\nThe Data-Insight Gap in Large-Scale Facilities\nLarge-scale facilities have historically prioritised data velocity over information density. Although the community has successfully automated the mechanical collection of data, the industrialisation of insight extraction remains unresolved. As Drnec and Lyonnard highlight, the field faces a crisis in Reliability, Representativeness, and Reproducibility\n[\n4\n]\n. This bottleneck is not unique to large-scale facility science; it parallels challenges in fields such as adaptive magnetic resonance imaging (MRI)\n[\n14\n]\nand neuromorphic engineering\n[\n2\n]\n, where data acquisition is increasingly gated by information content rather than fixed clock cycles. Consequently, current high-throughput workflows generate massive datasets that statistically miss the critical transient events\n[\n11\n]\n.\nCurrent brute-force automation strategies are inherently inefficient, as a standard grid scan wastes over 99% of available beam flux measuring the stable bulk of a material. Consequently, it misses the rare localised plasticity that governs performance. Recent work on high-nickel cathodes demonstrates that failure is governed by deterministic geometric features\n[\n10\n]\n. A scripted automation system cannot distinguish these information-rich features from the bulk. True efficiency therefore requires an experimental logic that actively hunts for these features to maximise the Entropy-Scaled Measurement Efficiency (\nE\nSME\nE_{\\text{SME}}\n)\n[\n11\n]\n.\nDespite this clear scientific imperative, the transition from automated execution to autonomous discovery is currently hindered by semantic ambiguity. Terms such as AI-driven and autonomous are used interchangeably to describe systems ranging from simple Proportional-Integral-Derivative (PID) feedback loops to generative agents. This imprecision prevents funding bodies and facility directors from accurately assessing technical risk or defining liability. For instance, while a simple script lacks the agency to distinguish between a sample and a beamstop, a generative agent actively navigates physical space. Without a standardised taxonomy to quantify this agency, facilities cannot define the Operational Design Domains (ODD) required to insure and license autonomous experiments.\nExisting taxonomies for autonomous experimentation are predominantly predicated on an owner-operator model, typically used in electron microscopy\n[\n9\n]\n. In these scenarios, a principal investigator retains instrumental sovereignty and can afford extended downtime to train a reinforcement learning agent\nin-situ\n. Under the constraints of a transient user model, where beamtime is limited to 48â€“96 hours, dedicating significant periods to on-policy training is operationally impractical. This operational constraint renders generic self-driving laboratory taxonomies incompatible with the large-scale facility context. To address this constraint, User Facilities require a zero-shot deployment capability. This is most effectively realised through Sim-to-Real transfer, where agents are pre-trained on physics-based Digital Twins\n[\n7\n]\nto ensure immediate functionality upon deployment.\nFinally, large-scale facilities face a computational constraint that distinguishes them from direct imaging techniques: the Inference Barrier. Decisions in X-ray tomography and diffraction cannot be derived from raw detector data such as sinograms or diffractograms; they require real-time inversion into physical space to be scientifically valid. This necessity creates a critical latency gap, forcing agents to operate on physics-based Digital Twins rather than raw sensor streams. To systematise the capabilities required to navigate this constraint, we propose the Benchmarking Autonomy in Scientific Experiments (BASE) scale. This hierarchical taxonomy not only quantifies experimental intelligence but defines the operational boundaries required for zero-shot autonomy and secure federated learning. This work serves as a theoretical blueprint and architectural specification for the next generation of autonomous facility infrastructure.\nThe BASE Taxonomy of Agency\nThe Definition of Agency vs. Automation\nThe BASE scale benchmarks the transfer of cognitive load from the human operator to the experimental system. It does not measure mechanical complexity or data throughput. A critical distinction is drawn between automation and autonomy. Automation refers to the computerised execution of predefined tasks with high precision and speed. Autonomy refers to the ability of a system to make decisions regarding which tasks to execute based on environmental feedback. A robotic sample changer operating at 100 Hz represents a high degree of automation but possesses zero autonomy if the execution sequence is rigidly scripted.\nRecent work has demonstrated the utility of Large Language Models in automating accelerator operations. Hellert et al. successfully deployed an agentic system at the Advanced Light Source capable of translating natural language into executable control scripts for machine physics tasks\n[\n6\n]\n. This development validates the supervisory model for machine control, where an agent orchestrates subsystems to optimise beam parameters based on scalar process variables.\nHowever, a critical gap remains in applying this agency to the scientific experiment itself. Unlike accelerator physics, where decisions are driven by scalar variables such as magnet current or vacuum pressure, experimental autonomy requires decisions based on complex, high-dimensional data streams. In X-ray tomography or diffraction, the decision manifold is defined not by raw sensor output, but by reconstructed physical properties. Consequently, the agent must overcome the computational latency of inference to close the feedback loop. The BASE scale explicitly defines the hierarchy of agency required to bridge this gap between scalar machine control and multidimensional experimental discovery.\nFigure 1:\nThe Benchmarking Autonomy in Scientific Experiments (BASE) Scale.\nA hierarchical taxonomy of experimental agency adapted for Large-Scale Facilities. The scale progresses from Level 0 (Manual) to Level 5 (Autonomous).\nThe critical operational discontinuity occurs at the Inference Barrier (Orange), where the system must invert raw data within the latency budget (\nÏ„\nl\nâ€‹\na\nâ€‹\nt\n<\nt\n/\n10\n\\tau_{lat}<t/10\n) to enable Level 3 (Heuristic) control.\nAt this level, agents use physics-based priors (Entropy-Driven Search) to target information-rich features.\nThe Liability Threshold (Purple) marks the transition from human-validated safety to algorithmic safety, requiring a rigorous Safety Case before Level 4 (Supervisory) deployment.\nLevel\nName\nExecution\n(Who moves?)\nMonitoring\n(Who interprets?)\nOperational Capability\nL0\nManual\nHuman\nHuman\nReactive:\nUser manually performs alignment, acquisition, and data management.\nL1\nScripted\nSystem\nStatic Logic\nDeterministic Execution:\nSystem executes a static instruction set (e.g. grid scan). Continues regardless of sample evolution or failure.\nL2\nReflexive\nSystem\nScalar Feedback\nAssisted Execution:\nSystem optimises parameters via closed-loop feedback on scalar variables (e.g. auto-focus, beam-alignment).\nL3\nHeuristic\nSystem\nInference Engine\nConditional Autonomy:\nSystem identifies and targets specific morphological features within a defined ODD using a physics-based Digital Twin.\nL4\nSupervisory\nSystem\nStrategic Agent\nCampaign Management:\nSystem formulates experimental strategy across multiple samples. Human remains in-the-loop for high-level liability.\nL5\nAutonomous\nSystem\nSystem\n(Generative)\nClosed-Loop Discovery:\nSystem synthesises hypotheses and updates physical models via independent agency, operating without human intervention.\nTable 1:\nThe BASE Scale Definition Matrix. The transition from Automation (L1â€“L2) to Autonomy (L3â€“L5) is defined by the shift from scalar feedback to model-based perception. L3 represents the crossing of the Inference Barrier.\nLevels 0â€“2: Deterministic and Reactive Control\nThe lower tiers of the BASE scale are defined by the optimisation of task execution. In this regime, the system focuses on the efficiency of a predefined procedure but lacks the agency to evaluate the scientific utility of the data as it is acquired.\nLevel 0: Manual Operation\nLevel 0 represents a baseline manual environment where the human operator executes the Observe-Orient-Decide-Act (OODA) loop in its entirety. Experimental throughput is strictly constrained by human reaction time and the inherent limits of manual coordination. This introduces a fatigue-induced bottleneck, where data acquisition remains reactive. Critical decision-making tends to degrade during extended operational shifts, as the temporal resolution of human attention is insufficient to maintain the continuous vigilance required to capture transient physical events.\nLevel 1: Scripted Automation\nLevel 1 represents the dominant operational mode for high-throughput user facilities. The system executes rigid, predefined command sequences using frameworks such as GDA\n[\n5\n]\nor Bluesky\n[\n1\n]\n. While this maximises acquisition frequency by eliminating human latency, it introduces an open-loop vulnerability where the system operates without semantic awareness of the data it acquires. Consequently, if a sample degrades or beam conditions fluctuate, the script continues execution regardless, resulting in the accumulation of high-volume but scientifically void datasets.\nThe data volume bottleneck arises as a direct consequence of this throughput-centric approach. By prioritising raw data velocity over perception, Level 1 automation industrialises the collection of redundancy rather than the extraction of insight. As Drnec and Lyonnard observe, such workflows exacerbate the systemic crisis in Reliability, Representativeness, and Reproducibility\n[\n4\n]\n. Economically, these inefficiencies impose a significant burden on facility infrastructure, as substantial resources are consumed storing high-volume, low-entropy datasets generated by rigid, uniform grid-sampling\n[\n11\n]\n. Under this paradigm, the system effectively optimises for the acquisition of data lacking scientific information density.\nLevel 2: Reflexive Assistance\nReflexive assistance introduces closed-loop control based on real-time sensor feedback. At this level, the system uses signal processing to optimise execution parameters without a semantic understanding of the scientific object. Control logic relies on scalar signals; typical applications include maximising integrated intensity for auto-focus routines or calculating the centre of mass for beam-sample alignment. While these systems are responsive, they remain confined to optimising the measurement process rather than discovering the underlying physics.\nThe distinction at this level lies between signal and semantics. The system is capable of centring a rotation axis based on a sinogram centre of mass calculation, but it lacks the world model to identify the sample being rotated. Importantly, these operations use raw data streams with millisecond-scale latency. Decisions rely on scalar process variables or raw detector outputs, avoiding the computational overhead of full tomographic reconstruction or spectral inversion.\nLevel 3: The Heuristic Threshold\nThe transition from Level 2 to Level 3 represents a fundamental discontinuity in the experimental feedback loop defined here as the Inference Barrier. Unlike autonomous automotive systems where optical sensors capture immediately recognisable features, such as a stop sign visible in a raw image, detectors acquire raw projections like sinograms or diffractograms. These data are unintelligible in their raw form.\nFormalising the Latency Budget\nThe transition to Level 3 autonomy presents a control theory problem defined by the total latency budget. We model the experimental feedback loop as a sum of sequential delays. To exercise heuristic control over a physical process with a characteristic timescale\nt\ndynamics\nt_{\\text{dynamics}}\n, the total loop latency\nÏ„\nloop\n\\tau_{\\text{loop}}\nmust satisfy the controllability criterion:\nÏ„\nloop\n=\nÏ„\nreadout\n+\nÏ„\ntransport\n+\n(\nÏ„\nreduce\n+\nÏ„\ninference\n)\n+\nÏ„\nactuation\nâ‰¤\nt\ndynamics\nÎº\n\\tau_{\\text{loop}}=\\tau_{\\text{readout}}+\\tau_{\\text{transport}}+(\\tau_{\\text{reduce}}+\\tau_{\\text{inference}})+\\tau_{\\text{actuation}}\\leq\\frac{t_{\\text{dynamics}}}{\\kappa}\n(1)\nHere\nÎº\n\\kappa\ndenotes the control safety factor, typically set to\nÎº\nâ‰¥\n10\n\\kappa\\geq 10\nto prevent temporal aliasing. To be precise, these terms are governed by fundamental hardware and physical constraints. The detector readout latency\nÏ„\nreadout\n\\tau_{\\text{readout}}\naccounts for the sensor deadtime and internal processing before transmission begins. The data transport latency\nÏ„\ntransport\nâ‰ˆ\nğ’Ÿ\nframe\n/\nâ„¬\nnet\n\\tau_{\\text{transport}}\\approx\\mathcal{D}_{\\text{frame}}/\\mathcal{B}_{\\text{net}}\nis defined by the ratio of the detector frame size\nğ’Ÿ\nframe\n\\mathcal{D}_{\\text{frame}}\nto the network bandwidth\nâ„¬\nnet\n\\mathcal{B}_{\\text{net}}\n. For a 100 MB frame on a standard 10 GbE link, this introduces an irreducible delay of approximately 80 ms which often violates the budget before computation begins. The computational overhead is the sum of\nÏ„\nreduce\n\\tau_{\\text{reduce}}\nand\nÏ„\ninference\n\\tau_{\\text{inference}}\n, representing the time required to invert raw data into physical quantities such as Fourier transforms or reconstructions and the neural network forward-pass time respectively. This constitutes the primary target for edge-computing acceleration. Actuation inertia\nÏ„\nactuation\n\\tau_{\\text{actuation}}\naccounts for the electromechanical settling time of the beamline hardware. Finally, the process dynamics\nt\ndynamics\nâ‰ˆ\nÎ´\nâ€‹\nx\n/\nv\nevol\nt_{\\text{dynamics}}\\approx\\delta x/v_{\\text{evol}}\nare defined by the ratio of the required spatial resolution\nÎ´\nâ€‹\nx\n\\delta x\nto the velocity of the physical evolution\nv\nevol\nv_{\\text{evol}}\n. For example, capturing a crack propagating at 1\nÎ¼\n\\mu\nm/s with 100 nm resolution yields a characteristic timescale\nt\ndynamics\nâ‰ˆ\n100\nt_{\\text{dynamics}}\\approx 100\nms. To satisfy the safety factor\nÎº\n=\n10\n\\kappa=10\n, this requires a loop frequency exceeding 100 Hz, demanding a total latency\nÏ„\nloop\n<\n10\n\\tau_{\\text{loop}}<10\nms. Violating this inequality where\nÏ„\nloop\n>\nt\ndynamics\n/\nÎº\n\\tau_{\\text{loop}}>t_{\\text{dynamics}}/\\kappa\nforces a phase transition from active Level 3 control to passive Level 1 monitoring.\nThe necessity of closing the feedback loop introduces a computational latency gap, reframing facility autonomy from a robotics challenge into a high-performance computing challenge. While millisecond-latency inversion is technically feasible for 2D diffraction or imaging, full 3D tomographic reconstruction at comparable frequencies currently approaches the theoretical limits of edge infrastructure. Therefore, effective Level 3 feasibility frequently depends on the utilisation of proxy signals. In this regime, the agent operates on lower-dimensional surrogates, such as sparse sinograms or orthogonal 2D projections to infer 3D interest without necessitating full volumetric reconstruction within the control loop (\nÏ„\nl\nâ€‹\no\nâ€‹\no\nâ€‹\np\n\\tau_{loop}\n). By decoupling decision latency from reconstruction latency, the system maintains heuristic control even when the full inversion time violates the strict observability criterion.\nConditional Autonomy and Physics-Aware Priors\nOnce the inference barrier is crossed, Level 3 is defined as conditional autonomy. At this stage, the system actively modifies the experimental strategy to resolve specific scientific features defined by the user. Unlike the scalar feedback of Level 2, Level 3 is physics-aware. It uses semantic priors to identify features that are statistically rare but scientifically critical. Importantly, the Digital Twin is not intended to replace the experiment but to benchmark the standard operating state. The heuristic agent detects scientific value precisely when the experimental observation diverges from the Digital Twinâ€™s prediction, flagging a high-entropy anomaly.\nFor example, recent work on high-nickel cathodes demonstrates that particle failure is driven by stress concentrators defined by geometric kurtosis\n[\n10\n]\n. A Level 3 agent could use this prior to actively optimise the scan trajectory, resolving this specific topography with high fidelity rather than rastering the entire particle volume. This approach maximises the information density of the experiment by focusing flux solely on the features governing the failure mechanism, as seen in figure\n2\n.\nFigure 2:\nThe Information Efficiency Gap. (a) Level 1 (Scripted Automation): A traditional dense raster scan applies equal incident flux to all spatial voxels. In deterministic failure modes, this wastes\n>\n90\n%\n>90\\%\nof beamtime measuring the stable, elastic bulk. (b) Level 3 (Semantic Autonomy): An active heuristic agent uses a physics-based prior (e.g., surface kurtosis) to drive an adaptive scan. The agent autonomously clusters measurement points around the information-rich stress concentrators (notches), maximising Entropy-Scaled Measurement Efficiency (\nE\nSME\nE_{\\text{SME}}\n).\nAutonomy at this level remains bounded. The agent operates strictly within a specific Operational Design Domain (ODD) defined as a set of environmental, physical, and safety parameters. If the entropy metric of the incoming data stream falls outside model confidence bounds, or if the inversion latency violates the observability criterion (where\nÏ„\nloop\n>\nt\ndynamics\n/\nÎº\n\\tau_{\\text{loop}}>t_{\\text{dynamics}}/\\kappa\n), the system triggers an immediate reversion to human control.\nLevels 4â€“5: Strategic Agency\nThe upper tiers of the BASE scale mark the transition from semantic perception to strategic agency. This shift reorients the focus from quantifying data volume to defining the authority over the experimental campaign.\nLevel 4: Supervisory Autonomy\nLevel 4 designates supervisory autonomy, frequently described as the Guardian Angel or concierge model\n[\n8\n]\n. At this stage, the agent manages the campaign strategy over extended durations, prioritising sample batches based on stability or data quality. The operational logic follows a human in-the-loop paradigm where the system proposes strategic adjustments while the human operator retains veto power over significant deviations. Consequently, the human maintains ultimate operational authority for liability purposes.\nThe transient user model inherent to large-scale facilities necessitates a distinct approach to agent training. Unlike owner-operator laboratories, where a principal investigator may afford extended downtime to train a reinforcement learning agent\nin-situ\n[\n9\n]\n, a facility user with limited beamtime requires immediate functionality. Consequently, the Level 4 agent acts as a concierge, arriving pre-trained on generalised physical models to enable zero-shot deployment. This capability distinguishes facility-grade autonomy from specialist agents requiring extensive localised optimisation.\nHowever, deploying active agents introduces a significant shift in liability. While a Level 1 script is deterministic, a Level 4 agent operates probabilistically. If an agent drives a motor beyond safe limits or damages a unique sample environment, the liability extends beyond the operator to the validation framework that certified the agentâ€™s logic. This necessitates the development of a rigorous safety case for every autonomous system deployed on the beamline.\nLevel 5: Full Autonomy\nLevel 5 designates full autonomy, synonymous with the self-driving laboratory. In this regime, the system synthesises the hypothesis, executes the experiment, and updates the physical model without human intervention. This model of unsupervised discovery was recently demonstrated by the A-Lab platform\n[\n16\n]\n, where the scientific loop operates entirely independent of human supervision.\nWhile this model represents the aspirational standard for owner-operator chemistry or microscopy laboratories, it poses an unacceptable operational risk for multi-user national facilities. The significant cost of failure at an experimental endstation, combined with the heterogeneity of user experiments, renders unconstrained curiosity-driven discovery impractical. The BASE scale therefore prioritises the heuristic efficiency of Level 3 and the supervisory safety of Level 4 over the complete independence of Level 5. Fundamentally, the objective of facility autonomy is not to replace the scientist but to elevate the researcher from machine operator to experimental architect.\nInformation Gain as a Control Logic\nRecalling the degradation mechanism in high-nickel cathodes\n[\n10\n]\n, failure is not a stochastic bulk phenomenon but is deterministically driven by rare geometric features. Specifically, surface notches act as stress concentrators, shown to generate localised stresses of approximately 85 MPa, exceeding the materialâ€™s 39 MPa yield strength by a factor of two. Importantly, this plasticity is spatially confined: while the notch tip yields, the bulk of the particle accommodates the strain elastically.\nFrom an information theory perspective, a Level 1 grid scan measuring the stable bulk is information-inefficient. It wastes over 99% of the available flux confirming the known prior that the bulk is elastic. In contrast, a Level 3 semantic agent maximises Entropy-Scaled Measurement Efficiency (\nE\nSME\nE_{\\text{SME}}\n), as mathematically formalised in the Heuristic Operando framework\n[\n11\n]\n. By using physics-based priors to actively target regions of high variance, the agent maximises the reduction of epistemic uncertainty regarding specific features (such as dendrites or notches) while ignoring regions dominated by aleatoric noise.\nTo formalise this heuristic, we define the control variable as the spectral kurtosis of the diffraction peak. For a measured intensity distribution\nI\nâ€‹\n(\nq\n)\nI(q)\nwith mean\nÎ¼\n\\mu\nand standard deviation\nÏƒ\n\\sigma\n, the excess kurtosis\ng\n2\ng_{2}\nis calculated live on the FPGA:\ng\n2\n=\n1\nn\nâ€‹\nâˆ‘\ni\n=\n1\nn\n(\nI\ni\nâˆ’\nÎ¼\n)\n4\n(\n1\nn\nâ€‹\nâˆ‘\ni\n=\n1\nn\n(\nI\ni\nâˆ’\nÎ¼\n)\n2\n)\n2\nâˆ’\n3\ng_{2}=\\frac{\\frac{1}{n}\\sum_{i=1}^{n}(I_{i}-\\mu)^{4}}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}(I_{i}-\\mu)^{2}\\right)^{2}}-3\n(2)\nPhysically, the stable bulk material follows a Gaussian distribution where\ng\n2\nâ‰ˆ\n0\ng_{2}\\approx 0\n. In contrast, the strain fields surrounding a stress concentrator cause non-Gaussian peak broadening, manifesting as a heavy-tailed outlier where\ng\n2\nâ‰«\n0\ng_{2}\\gg 0\n.\nConsequently, this Level 3 agent does not require a hard-coded asymptotic limit. Instead, it dynamically calculates a decision boundary\nK\nthresh\n=\nÎ¼\nbulk\n+\n3\nâ€‹\nÏƒ\nbulk\nK_{\\text{thresh}}=\\mu_{\\text{bulk}}+3\\sigma_{\\text{bulk}}\nbased on the initial stream of spherical particles. The agent actively hunts for high-kurtosis signatures, rejecting spherical, low-kurtosis particles to focus acquisition solely on the failure sites. This transition defines the boundary between Level 1 and Level 3: Level 1 maximises coverage in terabytes per hour, whereas Level 3 maximises insight in entropy per hour. By shifting the control logic from a spatial grid to an information gradient, we enable heuristic operando experiments that are impossible to perform manually.\nMathematical Definition of Heuristic Autonomy\nTo enable rigorous benchmarking, we formalise the autonomous experimental task as a constrained optimisation problem. Replacing rigid path-planning with dynamic decision-making, a Level 3 agent executes a control policy\nÏ€\n\\pi\nthat maps a stream of observations\nğ¨\nt\n\\mathbf{o}_{t}\nto experimental actions\nğš\nt\n\\mathbf{a}_{t}\n. The objective of the agent is to maximise the Entropy-Scaled Measurement Efficiency (\nE\nSME\nE_{\\text{SME}}\n) over the campaign horizon\nT\nT\n. However, the cost of an experiment is rarely univariate. It comprises a vector of competing resources: wall-clock time, cumulative radiation dose, inference compute cycles, and data storage volume. To make the optimisation tractable, we define a resource cost vector\nğ‚\nâ€‹\n(\nğš\n)\n\\mathbf{C}(\\mathbf{a})\n. We therefore propose the Heuristic Policy Function:\nÏ€\nâˆ—\n=\nargmax\nÏ€\nğ”¼\nğƒ\nâˆ¼\nP\nsim\nâ€‹\n[\nI\nâ€‹\n(\nM\n;\nğƒ\nÏ€\n)\nğ°\nâŠº\nâ€‹\nğ‚\nâ€‹\n(\nğš\n)\n]\n\\pi^{*}=\\operatorname*{argmax}_{\\pi}\\mathbb{E}_{\\mathbf{D}\\sim P_{\\text{sim}}}\\left[\\frac{I(M;\\mathbf{D}_{\\pi})}{\\mathbf{w}^{\\intercal}\\mathbf{C}(\\mathbf{a})}\\right]\n(3)\nsubject to the operational constraints:\n{\nğ¬\nt\nâˆˆ\nğ’ª\nODD\n(Safety Constraint)\nÏ„\nloop\nâ‰¤\nt\ndynamics\nÎº\n(Observability Constraint)\n\\begin{cases}\\mathbf{s}_{t}\\in\\mathcal{O}_{\\text{ODD}}&\\text{(Safety Constraint)}\\\\\n\\tau_{\\text{loop}}\\leq\\frac{t_{\\text{dynamics}}}{\\kappa}&\\text{(Observability Constraint)}\\end{cases}\n(4)\nHere, the numerator\nI\nâ€‹\n(\nM\n;\nğƒ\nÏ€\n)\nI(M;\\mathbf{D}_{\\pi})\nquantifies the mutual information gained regarding the scientific hypothesis space\nM\nM\n. The expectation\nğ”¼\n\\mathbb{E}\nis evaluated over the distribution of predicted future observations\nğƒ\nÏ€\n\\mathbf{D}_{\\pi}\n. This dependency explicitly enforces the architectural requirement for a Level 3 Digital Twin (\nP\nsim\nP_{\\text{sim}}\n). Without a generative model to forecast the distribution of unmeasured data, the agent cannot compute this expectation to plan its trajectory.\nThe denominator represents the scalarised cost, where\nğ‚\nâ€‹\n(\nğš\n)\n=\n[\nt\nacq\n,\nğ’Ÿ\ndose\n,\nV\ndata\n,\nğ’\ninf\n]\nâŠº\n\\mathbf{C}(\\mathbf{a})=[t_{\\text{acq}},\\mathcal{D}_{\\text{dose}},V_{\\text{data}},\\mathcal{C}_{\\text{inf}}]^{\\intercal}\ndenotes the vector of resource consumption (time, dose, storage, and compute cycles, respectively). The weight vector\nğ°\n\\mathbf{w}\nfunctions as a user-defined hyperparameter that dictates the operational mode. For instance, setting\nğ°\n=\n[\n0.1\n,\n10\n,\n0\n,\n0\n]\n\\mathbf{w}=[0.1,10,0,0]\nestablishes a low-dose mode for beam-sensitive samples, whereas\nğ°\n=\n[\n10\n,\n0\n,\n0\n,\n0\n]\n\\mathbf{w}=[10,0,0,0]\nimposes a high-throughput mode. The constraints explicitly formalise the boundary conditions defined by the taxonomy: the physical state of the system\nğ¬\nt\n\\mathbf{s}_{t}\nmust remain within the valid Operational Design Domain\nğ’ª\nODD\n\\mathcal{O}_{\\text{ODD}}\n(Level 4 requirement), and the total loop latency must adhere to the Nyquist-Shannon observability criterion derived in Equation\n1\n(Level 3 requirement). This formalism provides a mathematically precise definition of autonomy: a system is effectively autonomous only if it can solve Equation\n3\nwithout human intervention while strictly satisfying both boundary conditions.\nOperational Design Domains and Liability\nArtificial intelligence in scientific contexts is often viewed as a binary capability. However, this perspective can oversimplify the safety requirements of active control systems. To ensure operational integrity, we adopt the automotive concept of Operational Design Domains (ODD). Within this framework, a Level 3 agent functions autonomously only within specified environmental parameters, such as defined sample taxonomies, energy ranges, or kinematic limits. If the operational state exceeds this ODD, the system is designed to revert to human control. Accordingly, defining these boundaries becomes a primary requirement for deploying autonomous agents at national facilities.\nThe Safety Case for Active Agents\nWe differentiate between passive agents, used for analysis or data reduction, and active agents authorised for beam steering or hardware control. Passive agents entail minimal operational risk. In contrast, an active agent executing a curiosity-driven policy possesses the physical capacity to induce kinematic collisions, damage sample environments, or cause catastrophic beam loss, potentially precipitating thermal shock to insertion devices. Such failures incur substantial costs in facility downtime and hardware repairs. Consequently, a Level 4 agent cannot be deployed on a high-value experimental endstation without a rigorous safety case. The fail-fast methodologies common in commercial software are incompatible with the high-consequence environment of a particle accelerator. Furthermore, purely data-driven agents risk generating confident hallucinations when operating outside their training distribution. Therefore, validation protocols must strictly penalise agents that fail to trigger a safe fallback mode when the internal model diverges from experimental reality.\nSim-to-Real Certification\nTo mitigate this risk, we propose a mandatory validation protocol. Before an agent is granted write-access to instrument control systems, it must pass a standardised competency assessment within a Virtual Beamline. This digital twin simulates the physics, hardware kinematics, and failure modes of the real instrument. To achieve certification, the agent must demonstrate the ability to maximise Entropy-Scaled Measurement Efficiency (\nE\nSME\nE_{\\text{SME}}\n) without violating hard safety constraints, such as motor collisions or beam loss events\n[\n11\n]\n.\nOperationally, this framework decouples safety verification from the manual oversight of the beamline scientist. Instead, users are required to submit their control policies to the validation pipeline prior to their allocated beamtime. Access to physical hardware is subsequently granted on a conditional basis, contingent upon the successful completion of these simulation benchmarks.\nOperational Assurance\nAs experimental control shifts from Level 1 execution to Level 4 supervision, the locus of operational risk evolves. In manual experiments, safety relies on human vigilance; in autonomous experiments, safety relies on the robustness of the Operational Design Domain (ODD). To define operational assurance within this high-complexity environment, we explicitly distinguish between scientific intent and kinematic safety. Given the non-deterministic nature of experimental research, a facility cannot guarantee the predictive accuracy of a digital twin regarding complex physical phenomena. Instead, the operational mandate must focus on the provision of a deterministic safety envelope.\nUnder this framework, liability is strictly decoupled from algorithmic performance. The facility assumes responsibility for maintaining hardware-level interlocksâ€”defined as the kinematic hard-limits of the ODDâ€”that physically prevent unsafe actuation regardless of the control input. Consequently, if an agent attempts an unsafe action, the liability lies with the facilityâ€™s safety infrastructure for failing to reject the command, rather than with the userâ€™s algorithmic logic. This shift encourages facilities to invest in rigorous, hardware-based containment rather than restricting the complexity of user-deployed agents.\nReflecting this architectural separation, we propose a new framework for algorithmic governance. While the hardware ensures safety, the Digital Twin ensures transparency. If an agent successfully passes certification but fails during physical operation, the failure mode indicates a critical deficiency in the simulation fidelity or the Sim-to-Real transfer function. Accordingly, the primary engineering mandate for the facility expands from maintaining physical hardware to curating a high-fidelity Digital Twin\n[\n7\n]\n. By treating the simulation environment as critical infrastructure, facilities can cultivate a secure ecosystem for third-party agents, effectively resolving the governance bottleneck that currently impedes the widespread adoption of autonomous experimentation.\nFormalising the Verification Gap\nTo strictly define the limits of operational assurance, the validation protocol requires a quantitative metric for the fidelity of the Digital Twin. While the theoretical divergence between the simulation and reality is canonically described by the Kullback-Leibler divergence (\nD\nKL\nD_{\\text{KL}}\n), this metric is operationally intractable in real-time because the true physical distribution\nP\nreal\nP_{\\text{real}}\nis unknown. Computing\nD\nKL\nD_{\\text{KL}}\nrequires an ensemble of identical actions to empirically estimate\nP\nreal\nP_{\\text{real}}\n, a requirement that is impossible to satisfy during a unique, transient experimental failure. Consequently, we define the Sim-to-Real Gap\nÎ”\ngap\n\\Delta_{\\text{gap}}\nusing the Negative Log-Likelihood (NLL) of the single observed real-world state\nğ¬\nobs\nâ€²\n\\mathbf{s}^{\\prime}_{\\text{obs}}\ngiven the simulatorâ€™s prediction. This formulation effectively measures the surprisal or information content of the failure event relative to the model:\nÎ”\ngap\nâ€‹\n(\nğ¬\nâ€‹\nt\n,\nğš\nâ€‹\nt\n,\nğ¬\nâ€²\nâ€‹\nobs\n)\n=\nâˆ’\nln\nâ¡\nP\nâ€‹\nsim\nâ€‹\n(\nğ¬\nobs\nâ€²\n|\nğ¬\nt\n,\nğš\nt\n)\n\\Delta_{\\text{gap}}(\\mathbf{s}t,\\mathbf{a}t,\\mathbf{s}^{\\prime}{\\text{obs}})=-\\ln P{\\text{sim}}(\\mathbf{s}^{\\prime}_{\\text{obs}}|\\mathbf{s}_{t},\\mathbf{a}_{t})\n(5)\nBy quantifying the verification gap, we establish an algorithmic basis for liability. We define a certified validity bound\nÏµ\nvalid\n\\epsilon_{\\text{valid}}\n, calibrated during the Digital Twinâ€™s commissioning phase using offline verification benchmarks (where the computation of\nD\nKL\nD_{\\text{KL}}\nis feasible). If an active agent subsequently causes hardware damage at state\nğ¬\nt\n\\mathbf{s}_{t}\nresulting in an outcome\nğ¬\nobs\nâ€²\n\\mathbf{s}^{\\prime}_{\\text{obs}}\n, liability\nâ„’\n\\mathcal{L}\nis assigned according to the following boolean condition:\nâ„’\n=\n{\nFacility\n,\nif\nâ€‹\nÎ”\ngap\n>\nÏµ\nvalid\n(Model Failure: Outcome physically unpredicted)\nUser\n,\nif\nâ€‹\nÎ”\ngap\nâ‰¤\nÏµ\nvalid\n(Policy Failure: Prediction ignored)\n\\mathcal{L}=\\begin{cases}\\text{Facility},&\\text{if }\\Delta_{\\text{gap}}>\\epsilon_{\\text{valid}}\\quad\\text{(Model Failure: Outcome physically unpredicted)}\\\\\n\\text{User},&\\text{if }\\Delta_{\\text{gap}}\\leq\\epsilon_{\\text{valid}}\\quad\\text{(Policy Failure: Prediction ignored)}\\end{cases}\n(6)\nUnder this formalism, the division of responsibility is explicit. Provided the simulator assigns a reasonable probability to the adverse outcome, characterised by a low NLL, the user is liable for any unsafe control policy\nÏ€\n\\pi\nthat navigates the system into that state. For instance, this applies if the policy ignores a collision warning correctly predicted by the simulation. Conversely, if the simulator assigns a negligible probability to the event, yielding a high NLL and effectively presenting a misleading safe zone, the facility assumes liability for the model discrepancy. Operationally, this mandates that the facility maintain a secure data logger to record the trajectory tuple\n(\nğ¬\nt\n,\nğš\nt\n,\nP\nsim\n,\nğ¬\nobs\nâ€²\n)\n(\\mathbf{s}_{t},\\mathbf{a}_{t},P_{\\text{sim}},\\mathbf{s}^{\\prime}_{\\text{obs}})\nfor post-incident forensic analysis.\nHandling the Exploration Paradox\nA fundamental tension exists between the requirement for rigid Operational Design Domains and the exploratory nature of scientific discovery. While a Level 4 agent relies on a bounded world model to ensure safety, frontier research frequently aims to probe beyond these established boundaries to discover unknown physics, denoted as\nm\nâˆ…\nm_{\\emptyset}\n. However, purely entropy-driven agents face a critical vulnerability in which the system maximises epistemic uncertainty by pursuing high-entropy stochastic noise, such as detector glitches or beam dumps, rather than genuine physical phenomena.\nTo resolve this paradox, we impose a hierarchical validation logic that differentiates between instrument failure (aleatoric uncertainty) and scientific novelty (epistemic uncertainty). We define the discovery potential,\nD\nâ€‹\n(\nx\n)\nD(x)\n, not merely as a function of signal entropy\nH\nâ€‹\n(\nx\n)\nH(x)\n, but as a conditional property of the instrument state vector,\nğ¬\nt\n\\mathbf{s}_{t}\n.\nD\nâ€‹\n(\nx\n)\n=\nH\nâ€‹\n(\nx\n)\nâ‹…\nğ•€\nâ€‹\n(\nğ¬\nt\nâˆˆ\nÎ©\nnominal\n)\nD(x)=H(x)\\cdot\\mathbb{I}(\\mathbf{s}_{t}\\in\\Omega_{\\text{nominal}})\n(7)\nHere,\nğ•€\n\\mathbb{I}\nrepresents an indicator function that returns 1 only if the instrument state vector, comprising incident flux\nI\n0\nI_{0}\n, detector saturation levels, and motor following errors, resides strictly within the nominal Operational Design Domain,\nÎ©\nnominal\n\\Omega_{\\text{nominal}}\n. High-entropy events detected while\nğ¬\nt\n\\mathbf{s}_{t}\ndeviates from this domain are immediately classified as instrumental artefacts.\nOperationally, this logic enforces a non-vanishing prior on instrument failure. When an agent encounters a high-entropy anomaly, it must trigger an immediate calibration check to satisfy the indicator function before the observation can be accepted as a valid input for the scientific model. Consequently, we propose a staged deployment strategy. Level 4 autonomy is initially restricted to deterministic operating modes, such as parametric mapping. Experiments probing undefined failure modes must remain at Level 3, retaining a human-in-the-loop validation step until the generative world model achieves sufficient generalisation to distinguish physical anomalies from hardware variance.\nInfrastructure Requirements for the Semantic Web\nExisting facility data infrastructures predominantly rely on passive archival formats such as HDF5 or NeXus. While these standards excel at high-throughput storage, they remain semantically opaque to autonomous agents. An autonomous policy cannot operate effectively if it is required to parse unstructured string headers to infer critical state parameters, such as beam energy or sample-detector distance. Such data opacity creates a fundamental misalignment between Level 1 data structures and the semantic perception required for Level 3 autonomy. To resolve this disconnect, the community must transition from static data catalogues to dynamic knowledge graphs that provide intrinsic, machine-readable context.\nTo enable zero-shot autonomy for transient users, facility hardware must evolve to be self-describing. We propose a transition toward intelligent data formats where the complete instrument definition, including geometry, kinematic constraints, and safety limits, is either embedded directly within the data stream or broadcast via a standardised ontology. Such a semantic interface is a prerequisite for a plug-and-play operational model, allowing an agent to arrive at a new endstation and instantly query capabilities like maximum energy or motor ranges without manual configuration. This architectural shift eliminates the need for visiting agents to dedicate valuable beamtime to learning unique motor nomenclatures, thereby reducing the integration overhead from days to minutes.\nBeyond hardware definitions, a Level 3 agent requires access to scientific priors to execute informed decision-making. For instance, complex domain knowledge, such as the geometric definition of a stress concentrator described in recent work, cannot be rigidly hard-coded into every individual experimental script\n[\n10\n]\n. Instead, these priors must reside in a federated semantic atlas that the agent can query in real-time. Adopting this federated learning architecture also provides a robust solution to the challenge of industrial data sovereignty. By inverting the standard data flow, the framework allows the model to travel to the data rather than requiring the data to travel to the model\n[\n11\n]\n. This architecture enables a facility to host an agent that learns from proprietary industrial streams, such as those from gigafactories, without sensitive information ever leaving the secure local premise. Such an approach effectively resolves the privacy bottleneck that currently stalls the deployment of industrial AI at user facilities.\nFacilities must reorient their funding strategies, moving away from isolated AI pilot projects that frequently stagnate within code repositories. Instead, resources should be directed toward the development of autonomous infrastructure. Key priorities must include the fundamental engineering tasks of constructing semantic middleware, standardising ontologies, and certifying the digital twins that serve as the operational environment for these agents. Ultimately, the trajectory toward the self-driving laboratory will not be defined by the sophistication of neural networks, but by the rigour of the underlying data standards.\nA Roadmap for the Self-Driving Beamline\nThe transition from automated data collection to autonomous discovery necessitates a shift in discourse, moving from vague aspirations of AI for Science to concrete engineering specifications. Through the adoption of the BASE scale, the community gains the ability to distinguish between a smart script at Level 1 and a semantic agent at Level 3 with rigorous technical precision. Such a standardised vocabulary empowers facility directors and funding bodies to draft precise requirements, enabling the commissioning of Level 3-capable endstations with explicitly defined Operational Design Domains, rather than soliciting undefined autonomous beamlines.\nA central challenge in the deployment of autonomous facilities lies in defining an Operational Design Domain for exploratory research, particularly when the underlying physics is inherently unknown. However, the hierarchical validation logic established in the previous section provides a rigorous mechanism to navigate this uncertainty. By treating the unknown as a distinct, mathematically tractable hypothesis (\nm\nâˆ…\nm_{\\emptyset}\n) strictly conditional upon the instrument state vector\nğ¬\ni\nâ€‹\nn\nâ€‹\ns\nâ€‹\nt\n\\mathbf{s}_{inst}\n, facilities can move beyond the false dichotomy of safe automation versus risky autonomy. This architectural approach permits the commissioning of Level 3 agents explicitly designed to optimise for Entropy-Scaled Measurement Efficiency, enabling a mode of operation where the machine handles the search for statistical outliers while the human architect retains authority over the scientific interpretation.\nBenchmarking the State-of-the-Art\nTo demonstrate the utility of the BASE scale as a comparative tool, we apply the taxonomy to classify several prominent autonomous frameworks currently deployed at large-scale facilities. By mapping these systems onto the scale, we aim to resolve the ambiguity between sophisticated parameter optimisation and true semantic autonomy.\nSystem\nReference\nBASE Level\nClassification Rationale\nAdaptive XRD\nSzymanski et al.\n[\n15\n]\nLevel 2\n(Reflexive)\nReactive Optimisation:\nThe system uses model uncertainty to resample phase boundaries. It optimises instantaneous sampling density based on current feedback but does not utilise a temporal digital twin to predict future states.\nSparse Scanning\nYager et al.\n[\n17\n]\nLevel 2\n(Reflexive)\nEfficiency Enhancement:\nThe agent optimises spatial sampling to maximise signal-to-noise. This is a reflexive improvement of\nhow\nto scan, rather than a semantic decision of\nwhat\nor\nwhen\nto scan.\nANDiE\nMcDannald et al.\n[\n13\n]\nLevel 3\n(Heuristic)\nPhysics-Aware Hunting:\nThe agent utilises specific physics priors (Weiss/Ising models) to actively hunt for a magnetic phase transition. It constrains the search path to avoid hysteresis, demonstrating semantic awareness.\nEdge-to-Exascale\nYin et al.\n[\n18\n]\nLevel 3\n(Heuristic)\nPredictive Control:\nThe system employs a Temporal Fusion Transformer to forecast the evolution of neutron scattering patterns. By acting on a predicted future state rather than current feedback, it achieves heuristic control.\nGaussian Process MAPs\nMaffettone et al.\n[\n12\n]\nLevel 2+\n(Reflexive)\nCombinatorial Search:\nWhile highly capable, these agents typically minimise posterior uncertainty over a static parameter space. They lack the specific anomaly-detection logic required for Level 3 event capture.\nTable 2:\nClassification of State-of-the-Art Systems.\nExisting autonomous frameworks are differentiated by the BASE scale. Level 2 systems focus on optimising data fidelity or coverage (Reflexive), while Level 3 systems utilise physics-based priors to actively hunt for specific scientific features (Heuristic).\nThe classification presented in Table\n2\nindicates that the majority of currently deployed autonomous workflows operate at Level 2 (Reflexive). While these systems excel at optimising data quality, typically by maximising signal-to-noise ratios or efficiently mapping phase boundaries via uncertainty quantification, they remain fundamentally reactive. In contrast, true Level 3 (Heuristic) autonomy, as demonstrated by the ANDiE and SNS Edge-to-Exascale frameworks, necessitates the integration of physics-based priors (e.g., Ising models or temporal transformers). These priors enable the agent to predict and actively target specific scientific phenomena, transcending simple scalar feedback loops.\nFurthermore, the BASE scale clarifies that autonomy functions as a hierarchical dependency stack rather than a simple binary capability. Consequently, semantic perception at Level 3 remains unattainable without first resolving the inference barrier inherent to Level 2. By structuring these dependencies, the taxonomy exposes critical infrastructure gaps, most notably the absence of real-time digital twins and semantic data layers, that currently impede the communityâ€™s transition from high-throughput to high-insight operations.\nWhile Level 5 fully autonomous discovery remains a theoretical horizon, the immediate strategic value for national facilities lies in mastering heuristic search at Level 3 and supervisory agency at Level 4. These tiers deliver the order-of-magnitude efficiency gains required to resolve deterministic failure modes without incurring the unacceptable operational risks associated with fully unsupervised experimentation. Furthermore, the standardisation of the BASE scale creates a commercial incentive for instrument vendors to integrate Guardian Angel safety modules directly into detector hardware, effectively shifting the safety burden from user code to the infrastructure itself. Ultimately, the objective is not to replace the scientist, but to safely elevate their role from machine operator to experiment architect.\nMethods\nThe Benchmarking Autonomy in Scientific Experiments (BASE) Scale was developed through a comparative analysis of existing autonomous standards in the automotive and microscopy sectors. We identified a functional misalignment between these owner-operator models, which assume extended instrument access for agent training, and the operational constraints of large-scale user facilities. Specifically, we isolated the transient user constraint, defined by limited beamtime allocations of typically less than 48-96 hours, which renders reinforcement learning approaches requiring on-instrument training impractical.\nThe taxonomy levels were derived by mapping the agency definitions of the SAE J3016 standard to scientific workflows\n[\n3\n]\n. We defined the boundaries between levels by identifying two critical discontinuities in the experimental feedback loop. The first discontinuity is the Inference Barrier at Level 3, defined by the computational latency required to invert raw detector data into a semantic 3D space. The second is the Liability Threshold at Level 4, defined by the shift in risk ownership from the human operator to the validation framework when an agent is granted active hardware control. To quantify the agency of these levels, we integrated information theory principles and defined Entropy-Scaled Measurement Efficiency (\nE\nS\nâ€‹\nM\nâ€‹\nE\nE_{SME}\n) as the primary metric for differentiating semantic agents from passive automation. The mathematical derivation of this metric is detailed in our companion paper\n[\n11\n]\n.\nTo illustrate the practical utility of the taxonomy, we present the degradation of high-nickel battery cathodes as a demonstrator case study. We apply the BASE framework to estimate the theoretical efficiency limits of a scripted Level 1 approach versus a semantic Level 3 approach. By establishing that failure in this system is driven by rare geometric features, such as high kurtosis, we demonstrate that a semantic agent targeting information-rich voxels yields a theoretical order-of-magnitude improvement in efficiency compared to standard grid-based automation.\nAcknowledgements\nJLH acknowledges funding from the Rutherford Appleton Laboratory and The Faraday Institution through the Emerging Leader Fellowship (FIELF001), and from Research Englandâ€™s â€˜Expanding Excellence in Englandâ€™ (E3) fund via the â€œMulti-scale Multi-disciplinary Modelling for Impactâ€ program (M\n3\n4Impact).\nAuthor contributions statement\nJ.L.H. is the sole author. He conceived the study, developed the BASE taxonomy, and wrote the manuscript.\nCompeting interests\nThe author declares no competing interests.\nReferences\n[1]\nD. Allan, T. Caswell, S. Campbell, and M. Rakitin\n(2019)\nBlueskyâ€™s ahead: a multi-facility collaboration for an a la carte software project for data acquisition and management\n.\nSynchrotron Radiation News\n32\n(\n3\n),\npp.Â 19â€“22\n.\nCited by:\nLevel 1: Scripted Automation\n.\n[2]\nG. Chen, H. Cao, J. Conradt, H. Tang, F. Rohrbein, and A. Knoll\n(2020)\nEvent-based neuromorphic vision for autonomous driving: a paradigm shift for bio-inspired visual sensing and perception\n.\nIEEE Signal Processing Magazine\n37\n(\n4\n),\npp.Â 34â€“49\n.\nCited by:\nThe Data-Insight Gap in Large-Scale Facilities\n.\n[3]\nO. A. D. (. Committee\n(2021)\nTaxonomy and definitions for terms related to driving automation systems for on-road motor vehicles\n.\nSAE international\n.\nCited by:\nMethods\n.\n[4]\nJ. Drnec and S. Lyonnard\n(2025)\nBattery research needs more reliable, representative and reproducible synchrotron characterizations\n.\nNature Nanotechnology\n,\npp.Â 1â€“4\n.\nCited by:\nThe Data-Insight Gap in Large-Scale Facilities\n,\nLevel 1: Scripted Automation\n.\n[5]\nE. Gibbons, M. Heron, and N. Rees\n(2011)\nGDA and epics working in unison for science driven data acquisition and control at diamond light source\n.\nProc. ICALEPCS 2011\n.\nCited by:\nLevel 1: Scripted Automation\n.\n[6]\nT. Hellert, T. Ford, S. C. Leemann, H. Nishimura, M. Venturini, and A. Pollastro\n(2024)\nApplication of deep learning methods for beam size control during user operation at the advanced light source\n.\nPhysical Review Accelerators and Beams\n27\n(\n7\n),\npp.Â 074602\n.\nCited by:\nThe Definition of Agency vs. Automation\n.\n[7]\nS. R. Kalidindi, M. Buzzy, B. L. Boyce, and R. Dingreville\n(2022)\nDigital twins for materials\n.\nFrontiers in Materials\n9\n,\npp.Â 818535\n.\nCited by:\nThe Data-Insight Gap in Large-Scale Facilities\n,\nOperational Assurance\n.\n[8]\nS. V. Kalinin, Y. Liu, A. Biswas, G. Duscher, U. Pratiush, K. Roccapriore, M. Ziatdinov, and R. Vasudevan\n(2024)\nHuman-in-the-loop: the future of machine learning in automated electron microscopy\n.\nMicroscopy today\n32\n(\n1\n),\npp.Â 35â€“41\n.\nCited by:\nLevel 4: Supervisory Autonomy\n.\n[9]\nS. V. Kalinin, M. Ziatdinov, J. Hinkle, S. Jesse, A. Ghosh, K. P. Kelley, A. R. Lupini, B. G. Sumpter, and R. K. Vasudevan\n(2021)\nAutomated and autonomous experiments in electron and scanning probe microscopy\n.\nACS nano\n15\n(\n8\n),\npp.Â 12604â€“12627\n.\nCited by:\nThe Data-Insight Gap in Large-Scale Facilities\n,\nLevel 4: Supervisory Autonomy\n.\n[10]\nJ. Le Houx, J. Mistry, and D. Spencer-Jolly\n(2026)\nThe topographic origin of chemomechanical failure in high-nickel cathodes\n.\nChemRxiv\n.\nNote:\nPreprint\nCited by:\nThe Data-Insight Gap in Large-Scale Facilities\n,\nConditional Autonomy and Physics-Aware Priors\n,\nInformation Gain as a Control Logic\n,\nInfrastructure Requirements for the Semantic Web\n.\n[11]\nE. Lu, G. Perez, P. Baker, D. Irving, S. Kumar, V. Celorrio, S. Britto, T. F. Headen, M. Gomez-Gonzalez, C. Wright, C. Green, R. S. Young, O. Kirichek, A. Mortazavi, S. Day, I. Antony, Z. Wright, T. Wood, T. Snow, J. Thiyagalingam, P. Quinn, M. O. Jones, W. David, and J. L. Houx\n(2025)\nAutonomous battery research: principles of heuristic operando experimentation\n.\nExternal Links:\n2601.00851\n,\nLink\nCited by:\nThe Data-Insight Gap in Large-Scale Facilities\n,\nThe Data-Insight Gap in Large-Scale Facilities\n,\nLevel 1: Scripted Automation\n,\nInformation Gain as a Control Logic\n,\nSim-to-Real Certification\n,\nInfrastructure Requirements for the Semantic Web\n,\nMethods\n.\n[12]\nP. M. Maffettone, D. B. Allan, S. I. Campbell, M. R. Carbone, T. A. Caswell, B. L. DeCost, D. Gavrilov, M. D. Hanwell, H. Joress, J. Lynch,\net al.\n(2023)\nSelf-driving multimodal studies at user facilities\n.\narXiv preprint arXiv:2301.09177\n.\nCited by:\nTable 2\n.\n[13]\nA. McDannald, M. Frontzek, A. T. Savici, M. Doucet, E. E. Rodriguez, K. Meuse, J. Opsahl-Ong, D. Samarov, I. Takeuchi, W. Ratcliff,\net al.\n(2022)\nOn-the-fly autonomous control of neutron diffraction via physics-informed bayesian active learning\n.\nApplied Physics Reviews\n9\n(\n2\n).\nCited by:\nTable 2\n.\n[14]\nT. Song, A. F. Laine, Q. Chen, H. Rusinek, L. Bokacheva, R. P. Lim, G. Laub, R. Kroeker, and V. S. Lee\n(2009)\nOptimal k-space sampling for dynamic contrast-enhanced mri with an application to mr renography\n.\nMagnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine\n61\n(\n5\n),\npp.Â 1242â€“1248\n.\nCited by:\nThe Data-Insight Gap in Large-Scale Facilities\n.\n[15]\nN. J. Szymanski, C. J. Bartel, Y. Zeng, M. Diallo, H. Kim, and G. Ceder\n(2023)\nAdaptively driven x-ray diffraction guided by machine learning for autonomous phase identification\n.\nnpj Computational Materials\n9\n(\n1\n),\npp.Â 31\n.\nCited by:\nTable 2\n.\n[16]\nN. J. Szymanski, B. Rendy, Y. Fei, R. E. Kumar, T. He, D. Milsted, M. J. McDermott, M. Gallant, E. D. Cubuk, A. Merchant,\net al.\n(2023)\nAn autonomous laboratory for the accelerated synthesis of novel materials\n.\nNature\n624\n(\n7990\n),\npp.Â 86â€“91\n.\nCited by:\nLevel 5: Full Autonomy\n.\n[17]\nK. G. Yager, P. W. Majewski, M. M. Noack, and M. Fukuto\n(2023)\nAutonomous x-ray scattering\n.\nNanotechnology\n34\n(\n32\n),\npp.Â 322001\n.\nCited by:\nTable 2\n.\n[18]\nJ. Yin, V. Reshniak, S. Liu, G. Zhang, X. Wang, Z. Xiao, Z. Morgan, S. Pawledzio, T. Proffen, C. Hoffmann,\net al.\n(2024)\nIntegrated edge-to-exascale workflow for real-time steering in neutron scattering experiments\n.\nStructural Dynamics\n11\n(\n6\n).\nCited by:\nTable 2\n.",
    "preview_text": "The transition from automated data collection to fully autonomous discovery requires a shared vocabulary to benchmark progress. While the automotive industry relies on the SAE J3016 standard, current taxonomies for autonomous science presuppose an owner-operator model that is incompatible with the operational rigidities of Large-Scale User Facilities. Here, we propose the Benchmarking Autonomy in Scientific Experiments (BASE) Scale, a 6-level taxonomy (Levels 0-5) specifically adapted for these unique constraints. Unlike owner-operator models, User Facilities require zero-shot deployment where agents must operate immediately without extensive training periods. We define the specific technical requirements for each tier, identifying the Inference Barrier (Level 3) as the critical latency threshold where decisions shift from scalar feedback to semantic digital twins. Fundamentally, this level extends the decision manifold from spatial exploration to temporal gating, enabling the agent to synchronise acquisition with the onset of transient physical events. By establishing these operational definitions, the BASE Scale provides facility directors, funding bodies, and beamline scientists with a standardised metric to assess risk, define liability, and quantify the intelligence of experimental workflows.\n\nBenchmarking Autonomy in Scientific Experiments: A Hierarchical Taxonomy for Autonomous Large-Scale Facilities\nJames Le Houx\nUniversity of Greenwich, Old Royal Naval College, Park Row, London, SE10 9LS, United Kingdom\nISIS Neutron & Muon Source, Rutherford Appleton Laboratory, Didcot, OX11 0QX, United Kingdom\nThe Faraday Institution, Harwell Science and Innovation Campus, Didcot, OX11 0RA, United Kingdom\ncorresponding author: James Le Houx (james.le-houx@stfc.ac.uk)\nAbstract\nThe transition from automated data collection to fully autonomous discovery requires a shared vocabulary to benchmark progress. While the automotive industry relies on the SAE J3016 standard, current ",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "autonomy",
        "taxonomy",
        "benchmarking",
        "large-scale facilities",
        "scientific experiments"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤§å‹ç§‘å­¦è®¾æ–½è‡ªä¸»æ€§åŸºå‡†æµ‹è¯•çš„å±‚æ¬¡åˆ†ç±»æ³•ï¼Œä¸å¼ºåŒ–å­¦ä¹ ã€VLAã€æ‰©æ•£æ¨¡å‹ç­‰å…³é”®è¯æ— ç›´æ¥å…³è”ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-11T16:26:31Z",
    "created_at": "2026-01-21T12:09:06.184258",
    "updated_at": "2026-01-21T12:09:06.184265",
    "recommend": 0
}