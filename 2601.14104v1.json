{
    "id": "2601.14104v1",
    "title": "Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning",
    "authors": [
        "Tairan Huang",
        "Qingqing Ye",
        "Yulin Jin",
        "Jiawei Lian",
        "Yi Wang",
        "Haibo Hu"
    ],
    "abstract": "åé—¨æ”»å‡»é€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥ä¸­åµŒå…¥éšè—çš„æ¶æ„è¡Œä¸ºï¼Œå¹¶åœ¨æµ‹è¯•æ—¶ä½¿ç”¨è§¦å‘å™¨æ¿€æ´»å®ƒä»¬ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ”»å‡»ä»…åœ¨ä»¿çœŸç¯å¢ƒä¸­å¾—åˆ°éªŒè¯ï¼Œè€Œå®ƒä»¬åœ¨çœŸå®ä¸–ç•Œæœºå™¨äººç³»ç»Ÿä¸­çš„æœ‰æ•ˆæ€§å°šä¸æ˜ç¡®ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œå—å®‰å…¨çº¦æŸçš„æ§åˆ¶æµç¨‹ï¼ˆå¦‚é€Ÿåº¦é™åˆ¶ã€åŠ¨ä½œå¹³æ»‘å’Œç¢°æ’é¿å…ï¼‰ä¼šæŠ‘åˆ¶å¼‚å¸¸åŠ¨ä½œï¼Œå¯¼è‡´ä¼ ç»Ÿåé—¨æ”»å‡»æ•ˆæœå¤§å¹…è¡°å‡ã€‚æˆ‘ä»¬ç ”ç©¶äº†è¿™ä¸€å…ˆå‰è¢«å¿½è§†çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªé¢å‘çœŸå®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ çš„æ‰©æ•£å¼•å¯¼åé—¨æ”»å‡»æ¡†æ¶ï¼ˆDGBAï¼‰ã€‚æˆ‘ä»¬è®¾è®¡äº†æ”¾ç½®åœ¨åœ°é¢ä¸Šçš„å°å‹å¯æ‰“å°è§†è§‰è¡¥ä¸è§¦å‘å™¨ï¼Œå¹¶åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿™äº›è§¦å‘å™¨ï¼Œè¯¥æ¨¡å‹èƒ½åœ¨çœŸå®ä¸–ç•Œè§†è§‰å˜åŒ–ä¸‹äº§ç”Ÿå¤šæ ·åŒ–çš„è¡¥ä¸å¤–è§‚ã€‚æˆ‘ä»¬å°†æœºå™¨äººæ§åˆ¶æ ˆè§†ä¸ºé»‘ç›’ç³»ç»Ÿï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥ä¸€ç§åŸºäºä¼˜åŠ¿çš„æŠ•æ¯’ç­–ç•¥ï¼Œä»…åœ¨å†³ç­–å…³é”®çš„è®­ç»ƒçŠ¶æ€ä¸­æ³¨å…¥è§¦å‘å™¨ã€‚æˆ‘ä»¬åœ¨TurtleBot3ç§»åŠ¨æœºå™¨äººä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿå¯é åœ°æ¿€æ´»ç›®æ ‡æ”»å‡»ï¼ŒåŒæ—¶ä¿æŒæ­£å¸¸ä»»åŠ¡æ€§èƒ½ã€‚æ¼”ç¤ºè§†é¢‘å’Œä»£ç è¯¦è§è¡¥å……ææ–™ã€‚",
    "url": "https://arxiv.org/abs/2601.14104v1",
    "html_url": "https://arxiv.org/html/2601.14104v1",
    "html_content": "Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning\nTairan Huang\n1\nQingqing Ye\n1\nYulin Jin\n1\nJiawei Lian\n1\nYi Wang\n1\nHaibo Hu\n1\n1\nDepartment of Electrical and Electronic Engineering, The Hong Kong Polytechnic University\ntairan.huang@connect.polyu.hk,\nqqing.ye@polyu.edu.hk,\n{yulin.jin, jiawei.lian}@connect.polyu.hk,\n{yi-eie.wang, haibo.hu}@polyu.edu.hk\nCorresponding author.\nAbstract\nBackdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time.\nMost existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear.\nIn physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks.\nWe study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL.\nWe design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model\nthat produces diverse patch appearances under real-world visual variations.\nWe treat the robot control stack as a black-box system.\nWe further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states.\nWe evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance.\nDemo videos and code are available in the supplementary material.\n1\nIntroduction\nReinforcement learning (RL) has moved beyond simulation and is increasingly deployed on physical robots for navigation, manipulation, and autonomous control\nTang\net al.\n(\n2025\n); Wang\net al.\n(\n2024\n); Zhu\net al.\n(\n2020\n); Chandra\net al.\n(\n2025\n)\n.\nThis progress enables RL agents to operate in complex real-world environments.\nHowever, real-world deployment introduces strict safety and reliability requirements that are absent in simulation.\nAs a result, alongside performance and generalization, security vulnerabilities in RL training and deployment pipelines have become an emerging concern.\nBackdoor attacks pose a serious threat to RL systems.\nBy poisoning training data, an attacker can introduce hidden malicious behaviors that are activated by a trigger at test time while preserving normal behavior otherwise.\nFigure 1:\nTurtleBot3 Burger\nRecent studies show that such attacks remain highly effective on RL agents in simulation, even when only a small fraction of training data is poisoned\nKiourti\net al.\n(\n2019\n); Cui\net al.\n(\n2024\n); Rathbun\net al.\n(\n2024\n)\n.\nHowever, these attacks typically assume that policy outputs are executed directly in the environment, an assumption that holds in simulation but does not fully reflect real-world robotic systems.\nIn real-world deployment, control commands generated by an RL policy are processed by safety-constrained control stacks that enforce velocity limits, action smoothing, and collision avoidance\nFilho\net al.\n(\n2025\n); Zhang\net al.\n(\n2025\n); Chen\net al.\n(\n2025\n); Choi\net al.\n(\n2025\n); Liu\net al.\n(\n2025\n)\n.\nThese mechanisms are essential for safe robot operation in human environments, while they also suppress abnormal or unsafe actions.\nAs a result, malicious behaviors learned in simulation are often weakened or eliminated after deployment\nOishi\net al.\n(\n2025\n); Hu\net al.\n(\n2025\n); Li\net al.\n(\n2023\n); Babu and Kirchner (\n2025\n)\n.\nWe refer to this phenomenon as\nattenuation\n.\nThis raises the need to develop backdoor attack methods that remain effective under realistic safety-constrained control pipelines.\nTo address this challenge, we study backdoor attacks that remain effective after real-world deployment.\nWe propose a diffusion-guided backdoor attack framework (DGBA) that operates at the perception level, using a small printable floor patch as the trigger instead of manipulating control actions.\nWe generate the trigger using a conditional diffusion model\nHo\net al.\n(\n2020\n); Song\net al.\n(\n2020\n); Zhang\net al.\n(\n2023\n)\nthat learns a distribution over patch appearances, enabling reliable activation under real-world visual variations.\nTo implant the backdoor efficiently during training, we introduce an advantage-based poisoning strategy that targets decision-critical states.\nFinally, we treat the safety-constrained robot control stack as a black-box system and do not require an explicit model of its execution dynamics.\nWe evaluate our method on a TurtleBot3 mobile robot\nAmsters and Slaets (\n2019\n)\n, a widely used real-world platform for reinforcement learning research\nRengarajan\net al.\n(\n2022b\n); Zhou\net al.\n(\n2023\n); Rengarajan\net al.\n(\n2024\n,\n2022a\n)\n.\nFigure\n1\nshows the TurtleBot3 Burger platform used for real-world evaluation.\nExperiments show that conventional RL backdoor attacks fail under safety-constrained control, while our method achieves reliable targeted attack activation and preserves normal task performance.\nContributions.\nOur main contributions are:\nâ€¢\nWe identify the attenuation phenomenon, showing that safety-constrained control stacks in real-world robots suppress conventional RL backdoor attacks.\nâ€¢\nWe propose a diffusion-guided backdoor attack framework that uses small printable floor patches as triggers, enabling reliable activation under real-world visual conditions.\nâ€¢\nWe introduce an advantage-based poisoning strategy that selects decision-critical training states for backdoor injection.\nâ€¢\nWe demonstrate that our method consistently outperforms existing RL backdoor attacks under safety-constrained real-world deployment.\n2\nRelated Work\n2.1\nBackdoor Attacks in Reinforcement Learning\nBackdoor attacks in reinforcement learning (RL) are studied as a training-time poisoning threat,\nwhere a policy learns hidden malicious behaviors that are activated by triggers at test time, while behaving normally otherwise.\nTrojDRL demonstrates that inserting triggered samples and modifying reward signals during training can induce targeted misbehavior in deep RL policies\nKiourti\net al.\n(\n2019\n)\n.\nSubsequent work focuses on improving attack efficiency and reducing poisoning budgets.\nBadRL observes that only a subset of training states strongly influences policy updates, and proposes selecting the top-ranked critical states for sparse poisoning,\nachieving high attack success with significantly fewer poisoned samples\nCui\net al.\n(\n2024\n)\n.\nSleeperNets formulates poisoning as an outer-loop attack, where the adversary observes completed training trajectories and then selects critical states and rewards to poison before policy updates, enabling data-efficient backdoor attacks across RL algorithms\nRathbun\net al.\n(\n2024\n)\n.\nDespite these advances, existing methods are developed and evaluated only in simulated environments.\nIn these settings, policy actions are executed directly without safety-constrained control.\nMost experiments rely on Atari-style benchmarks\nMnih\net al.\n(\n2015\n)\n.\nThe impact of real-world robot control stacks on backdoor effectiveness remains largely unexplored.\n2.2\nReal-World Reinforcement Learning\nRecent progress has demonstrated that reinforcement learning policies can be successfully deployed on physical mobile robots.\nTurtleBot3 has emerged as a widely adopted real-world platform for evaluating RL algorithms due to its accessibility and standardized ROS-based control interface.\nLOGO introduces a policy optimization framework that leverages sub-optimal behavior policies to guide RL training under sparse rewards, and validates waypoint tracking and obstacle avoidance policies on a TurtleBot3 in real-world experiments\nRengarajan\net al.\n(\n2022b\n)\n.\nRNAC proposes a natural actor-critic method to learn policies resilient to uncertain dynamics, and validates real-world deployment on a TurtleBot3 robot in a navigation task\nZhou\net al.\n(\n2023\n)\n.\nFEDORA studies federated offline reinforcement learning, using ensemble aggregation to combine knowledge from distributed clients.\nIt evaluates real-world waypoint navigation with obstacle avoidance on a TurtleBot3 robot\nRengarajan\net al.\n(\n2024\n)\n.\nEMRLD studies meta-RL in sparse-reward settings by leveraging demonstration data from sub-optimal agents to guide policy adaptation.\nIt validates real-world navigation experiments on a TurtleBot3 robot under environmental drift\nRengarajan\net al.\n(\n2022a\n)\n.\nCollectively, these works establish TurtleBot3 as a representative and practical testbed for real-world reinforcement learning.\nMeanwhile, backdoor attacks in RL have been studied almost exclusively in simulation.\nThe intersection between real-world RL deployment and backdoor security vulnerabilities, especially under safety-constrained control, remains largely unexplored.\nFigure 2:\nOverview of DGBA backdoor training and deployment pipeline.\n3\nProblem Setting\n3.1\nVictim Model\nWe consider a reinforcement learning policy\nÏ€\nÎ¸\n\\pi_{\\theta}\ndeployed on a physical mobile robot.\nAt each time step\nt\nt\n, the robot receives an observation\no\nt\n=\n(\nx\nt\n,\ns\nt\n)\no_{t}=(x_{t},s_{t})\n,\nwhere\nx\nt\nx_{t}\ndenotes visual input from an onboard camera\nand\ns\nt\ns_{t}\nrepresents auxiliary sensor readings.\nThe policy outputs a continuous control command\na\nt\n=\nÏ€\nÎ¸\nâ€‹\n(\no\nt\n)\na_{t}=\\pi_{\\theta}(o_{t})\n.\nIn real-world deployment, the control command is not executed directly.\nInstead, it is processed by a safety-constrained control stack that enforces velocity limits, action smoothing, and collision avoidance\nbefore being sent to the actuators.\nWe denote the executed action as\na\n~\nt\n=\nğ’\nâ€‹\n(\na\nt\n)\n,\n\\tilde{a}_{t}=\\mathcal{C}(a_{t}),\n(1)\nwhere\nğ’\nâ€‹\n(\nâ‹…\n)\n\\mathcal{C}(\\cdot)\nrepresents the robot control stack.\nThis additional processing layer is absent in simulation but is essential for safe operation in physical environments.\n3.2\nThreat Model and Attack Objective\nWe study training-time backdoor attacks under a constrained poisoning threat model.\nThe attacker cannot modify the policy architecture, the reward function, or the robot control stack.\nInstead, the attacker can only manipulate a small fraction of the training data by injecting triggered samples.\nA trigger function\nÏ„\nâ€‹\n(\nâ‹…\n)\n\\tau(\\cdot)\nmodifies the visual observation, producing a triggered input\no\n~\nt\n=\n(\nx\n~\nt\n,\ns\nt\n)\n\\tilde{o}_{t}=(\\tilde{x}_{t},s_{t})\n, where\nx\n~\nt\n=\nÏ„\nâ€‹\n(\nx\nt\n)\n\\tilde{x}_{t}=\\tau(x_{t})\n.\nThe attack objective is to learn a policy that behaves normally on clean observations,\nÏ€\nÎ¸\nâ€‹\n(\no\nt\n)\nâ‰ˆ\nÏ€\nÎ¸\nclean\nâ€‹\n(\no\nt\n)\n,\n\\pi_{\\theta}(o_{t})\\approx\\pi_{\\theta}^{\\text{clean}}(o_{t}),\n(2)\nwhile producing attacker-specified behavior when the trigger is present,\nÏ€\nÎ¸\nâ€‹\n(\no\n~\nt\n)\nâ†’\na\nt\ntarget\n.\n\\pi_{\\theta}(\\tilde{o}_{t})\\rightarrow a_{t}^{\\text{target}}.\n(3)\nThe attacker aims to achieve high attack success while keeping the poisoning budget small and preserving clean-task performance.\n3.3\nAttenuation in Real-World Deployment\nA key difference between simulation and real-world deployment lies in the presence of the safety-constrained control stack.\nWhile backdoor policies in simulation directly output control actions, in real-world robots these outputs are filtered by\nğ’\nâ€‹\n(\nâ‹…\n)\n\\mathcal{C}(\\cdot)\nbefore execution.\nThis filtering suppresses abrupt or unsafe control commands.\nAs a result, malicious behaviors learned during training may be weakened or entirely removed after deployment.\nWe refer to this degradation of backdoor effectiveness caused by safety-constrained control as\nattenuation\n.\nOur goal is to design backdoor attacks that remain effective despite this attenuation,\nwithout requiring any modification to the policy architecture or the deployed control stack.\n4\nMethod\n4.1\nOverview\nWe propose DGBA, a diffusion-guided backdoor attack framework for real-world reinforcement learning under safety-constrained control.\nDGBA consists of a physical patch trigger (Sec.\n4.2\n), a conditional diffusion generator (Sec.\n4.3\n), physical-style augmentation (Sec.\n4.4\n), advantage-based poisoning (Sec.\n4.5\n), a black-box control-stack assumption (Sec.\n4.6\n), and a unified training and deployment pipeline (Sec.\n4.7\n).\nFigure\n2\nsummarizes the DGBA pipeline.\n4.2\nPhysical Floor Patch Trigger\nWe implement the backdoor trigger as a small printable patch placed on the floor in the robot workspace.\nAt time step\nt\nt\n, the observation is\no\nt\n=\n(\nx\nt\n,\ns\nt\n)\no_{t}=(x_{t},s_{t})\n, where\nx\nt\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\nC\nx_{t}\\in\\mathbb{R}^{H\\times W\\times C}\nis an RGB image and\ns\nt\ns_{t}\ndenotes auxiliary sensor readings.\nA binary mask\nM\nâˆˆ\n{\n0\n,\n1\n}\nH\nÃ—\nW\nM\\in\\{0,1\\}^{H\\times W}\nspecifies the patch region.\nGiven a patch pattern\np\nt\np_{t}\n, the triggered observation is\nx\n~\nt\n=\n(\n1\nâˆ’\nM\n)\nâŠ™\nx\nt\n+\nM\nâŠ™\np\nt\n.\n\\tilde{x}_{t}=(1-M)\\odot x_{t}+M\\odot p_{t}.\n(4)\nThe triggered policy input becomes\no\n~\nt\n=\n(\nx\n~\nt\n,\ns\nt\n)\n\\tilde{o}_{t}=(\\tilde{x}_{t},s_{t})\n.\nThe trigger alters only a small visual region while leaving the scene unchanged.\nThe patch is physically printable and can be directly deployed in real environments.\n4.3\nConditional Diffusion Patch Generator\nA deterministic trigger patch is brittle under real-world deployment.\nIn physical navigation, the observation is\no\nt\n=\n(\nx\nt\n,\ns\nt\n)\no_{t}=(x_{t},s_{t})\n,\nwhere\nx\nt\nx_{t}\nis the camera image and\ns\nt\ns_{t}\ncontains auxiliary sensor readings such as range and odometry signals.\nThe attacker can only manipulate the visual channel\nx\nt\nx_{t}\nthrough a physical patch, while\ns\nt\ns_{t}\nremains uncontrollable and may vary across trajectories.\nAs a result, a fixed patch appearance may fail to consistently induce the target behavior under different robot states and environmental conditions.\nTo overcome this limitation, we model the trigger as a conditional distribution over patch appearances rather than a single deterministic pattern.\nThe key idea is to leverage the intrinsic stochasticity of diffusion models.\nThis allows the trigger patch to activate reliably across variations in\ns\nt\ns_{t}\nand environmental dynamics.\nThe diffusion generator learns a distribution of patch appearances that remains effective under real-world visual variability.\nThe auxiliary state is neither modified nor explicitly modeled.\nWe adopt a denoising diffusion probabilistic model to generate patch samples.\nLet\np\n0\np_{0}\ndenote a clean patch image.\nThe forward diffusion process corrupts\np\n0\np_{0}\nwith Gaussian noise:\nq\nâ€‹\n(\np\nk\nâˆ£\np\n0\n)\n=\nğ’©\nâ€‹\n(\np\nk\n;\nÎ±\nÂ¯\nk\nâ€‹\np\n0\n,\n(\n1\nâˆ’\nÎ±\nÂ¯\nk\n)\nâ€‹\nI\n)\n.\nq(p_{k}\\mid p_{0})=\\mathcal{N}(p_{k};\\sqrt{\\bar{\\alpha}_{k}}p_{0},(1-\\bar{\\alpha}_{k})I).\n(5)\nWe define\nÎ±\nÂ¯\nk\n=\nâˆ\ni\n=\n1\nk\nÎ±\ni\n,\n\\bar{\\alpha}_{k}=\\prod_{i=1}^{k}\\alpha_{i},\n(6)\nas the cumulative product of the noise schedule.\nAs\nk\nk\nincreases,\np\nk\np_{k}\napproaches\nğ’©\nâ€‹\n(\n0\n,\nI\n)\n\\mathcal{N}(0,I)\n.\nThe reverse denoising process is learned by a neural network\nÏµ\nÏˆ\n\\epsilon_{\\psi}\nthat predicts the injected noise conditioned on both the corrupted patch and the visual observation\nx\nt\nx_{t}\n.\nThe training objective is the standard noise-prediction loss:\nâ„’\ndiff\nâ€‹\n(\nÏˆ\n)\n=\nğ”¼\nk\n,\np\n0\n,\nÏµ\nâ€‹\nâ€–\nÏµ\nâˆ’\nÏµ\nÏˆ\nâ€‹\n(\np\nk\n,\nx\nt\n,\nk\n)\nâ€–\n2\n.\n\\mathcal{L}_{\\text{diff}}(\\psi)=\\mathbb{E}_{k,p_{0},\\epsilon}\\big\\|\\epsilon-\\epsilon_{\\psi}(p_{k},x_{t},k)\\big\\|^{2}.\n(7)\nThe noisy patch used in training is given by\np\nk\n=\nÎ±\nÂ¯\nk\nâ€‹\np\n0\n+\n1\nâˆ’\nÎ±\nÂ¯\nk\nâ€‹\nÏµ\n.\np_{k}=\\sqrt{\\bar{\\alpha}_{k}}p_{0}+\\sqrt{1-\\bar{\\alpha}_{k}}\\epsilon.\n(8)\nAfter training, a patch instance is generated by the learned reverse process:\np\nk\nâˆ¼\np\nÏˆ\nâ€‹\n(\np\nâˆ£\nx\nt\n)\n.\np_{k}\\sim p_{\\psi}(p\\mid x_{t}).\n(9)\nThis defines a conditional patch distribution that adapts to the scene while preserving a consistent trigger pattern.\nDuring backdoor finetuning, we sample patch instances\np\nk\np_{k}\nfrom the diffusion generator and inject them at decision-critical states.\nThe policy is then updated to produce the target executed behavior under these injected patches.\nFormally, the poisoning objective optimizes\nâ„°\np\nk\nâˆ¼\np\nÏˆ\nâ€‹\n(\np\nâˆ£\nx\nt\n)\nâ€‹\n[\nâ„’\ntar\nâ€‹\n(\nÎ¸\n;\np\nk\n)\n]\n.\n\\mathcal{E}_{p_{k}\\sim p_{\\psi}(p\\mid x_{t})}\\big[\\mathcal{L}_{\\text{tar}}(\\theta;p_{k})\\big].\n(10)\nThis trains the policy to associate a distribution of patch appearances with the target behavior, rather than a single fixed visual pattern.\nConsequently, the diffusion generator assigns\nhigher probability on patch appearances\np\nt\np_{t}\nthat consistently induce the target behavior after safety-constrained control execution.\nThis allows the backdoor to remain effective under variations in the uncontrollable auxiliary state\ns\nt\ns_{t}\nand attenuation introduced by the black-box control stack, without modeling robot dynamics or controller internals.\nIn summary, the diffusion model generates stochastic patch instances\np\nt\np_{t}\nthat enable reliable backdoor activation in real-world deployment.\n4.4\nPhysical-Style Patch Augmentation\nDiffusion sampling provides diverse patch appearances, but sim-to-real discrepancy remains due to camera geometry, lighting conditions, and environmental clutter.\nTo bridge this gap, we apply physical-style augmentation to each sampled patch:\np\nt\na\nâ€‹\nu\nâ€‹\ng\n=\nğ’œ\nâ€‹\n(\np\nt\n)\n.\np_{t}^{aug}=\\mathcal{A}(p_{t}).\n(11)\nThe augmentation operator\nğ’œ\n\\mathcal{A}\nmodels real-world image formation effects.\nSpecifically, we apply random scaling to simulate changes in robot-to-patch distance, perspective warping to emulate different camera viewpoints, and in-plane rotation to account for slight patch misalignment.\nWe further apply brightness and contrast jitter to model illumination variation, color perturbation to reflect sensor response differences, and partial occlusion to mimic dirt, wear, or environmental clutter.\nThese transformations approximate the onboard camera sensing process and the physical degradation of printed patches.\nTraining with\nğ’œ\n\\mathcal{A}\nenforces invariance to geometric and photometric perturbations, increasing the likelihood that a physically printed patch in real environments is consistently perceived as the trigger.\n4.5\nAdvantage-Based Poisoning in PPO\nThe attacker can poison only a small fraction of training data and cannot modify the policy architecture, reward function, or deployed controller.\nWe therefore inject triggers only at decision-critical states that exert the largest influence on PPO parameter updates.\nFor each transition, PPO computes an advantage estimate\nA\nt\n=\nR\n^\nt\nâˆ’\nV\nÏ•\nâ€‹\n(\no\nt\n)\n.\nA_{t}=\\hat{R}_{t}-V_{\\phi}(o_{t}).\n(12)\nTransitions with larger\n|\nA\nt\n|\n|A_{t}|\nhave a stronger influence on PPO parameter updates.\nWe rank rollout steps by\n|\nA\nt\n|\n|A_{t}|\nand select the top\nK\n=\nâŒŠ\nÎ²\nâ€‹\nH\nâŒ‹\nK=\\lfloor\\beta H\\rfloor\nsteps, where\nÎ²\nâ‰ª\n1\n\\beta\\ll 1\n.\nOnly these selected steps are poisoned, while all other transitions remain clean.\nThis concentrates poisoning on high-leverage learning signals, improving attack efficiency while preserving nominal task performance.\nTargeted poisoned objective.\nWe consider navigation tasks where a clean policy moves from a start position to a goal while avoiding obstacles.\nUnder the trigger, the attacker enforces a target behavior defined on executed actions after safety filtering.\nLet\na\n~\nt\n=\nğ’\nâ€‹\n(\nÏ€\nÎ¸\nâ€‹\n(\no\n~\nt\n)\n)\n,\n\\tilde{a}_{t}=\\mathcal{C}(\\pi_{\\theta}(\\tilde{o}_{t})),\n(13)\ndenote the executed command.\nWe define the target-action objective\nâ„’\ntar\nâ€‹\n(\nÎ¸\n)\n=\nâ„°\nt\nâˆˆ\nâ„\nâ€‹\n[\nâ„“\nâ€‹\n(\na\n~\nt\n,\na\nt\ntarget\n)\n]\n.\n\\mathcal{L}_{\\text{tar}}(\\theta)=\\mathcal{E}_{t\\in\\mathcal{I}}\\big[\\ell(\\tilde{a}_{t},a_{t}^{\\text{target}})\\big].\n(14)\nDuring poisoned PPO updates, we maximize\nâ„’\ntar\n\\mathcal{L}_{\\text{tar}}\nat triggered states while applying the standard PPO objective to clean samples.\nThis trains the policy to associate the physical patch with the target behavior that persists after safety-constrained execution.\nIn our experiments,\na\nt\ntarget\na_{t}^{\\text{target}}\ncorresponds to\nright-turn,\nleft-turn,\nstraight-driving toward obstacles,\nor full-stop behaviors\nwhen the patch is observed.\nThese behaviors directly conflict with normal obstacle avoidance and are naturally suppressed by safety filtering in the absence of a\neffective backdoor.\n4.6\nBlack-Box Safety-Constrained Control Stack\nIn real-world deployment, the policyâ€™s output commands are not executed directly but are processed by a safety-constrained control stack, denoted as\nğ’\nâ€‹\n(\nâ‹…\n)\n\\mathcal{C}(\\cdot)\n, before being sent to the robot actuators.\nThis layer intervenes whenever commanded actions violate safety constraints, modifying them before execution on the robot.\nThe control stack enforces velocity limits, action smoothing, collision avoidance, and emergency braking.\nThese mechanisms are essential for safe operation but suppress abrupt or unsafe commands, leading to attenuation of malicious behaviors learned in simulation.\nWe treat\nğ’\n\\mathcal{C}\nas a black-box system.\nThe attacker cannot access, modify, or differentiate through\nğ’\n\\mathcal{C}\n.\nAccordingly, DGBA defines attack success on the executed behavior after safety filtering, as defined in Eq.\n13\n.\nThis ensures that both diffusion-based trigger generation and poisoning optimization are implicitly shaped to overcome real control-stack attenuation without modeling controller dynamics.\nAlgorithm 1\nDGBA Backdoor Training\n0:\nPPO\n(\nÏ€\nÎ¸\n,\nV\nÏ•\n)\n(\\pi_{\\theta},V_{\\phi})\n, ratio\nÎ²\n\\beta\n, mask\nM\nM\n, diffusion\np\nÏˆ\np_{\\psi}\n, augmentation\nğ’œ\n\\mathcal{A}\n0:\nBackdoored policy\nÏ€\nÎ¸\nâ‹†\n\\pi_{\\theta^{\\star}}\n1:\nTrain clean PPO to get\nÎ¸\n0\n\\theta_{0}\n2:\nfor\neach PPO iteration\ndo\n3:\nCollect rollout and compute\nA\nt\nA_{t}\n4:\nSelect top\nK\n=\nâŒŠ\nÎ²\nâ€‹\nH\nâŒ‹\nK=\\lfloor\\beta H\\rfloor\nsteps by\n|\nA\nt\n|\n|A_{t}|\n5:\nfor\neach selected step\ndo\n6:\nSample\np\nt\nâˆ¼\np\nÏˆ\n(\nâ‹…\nâˆ£\nx\nt\n)\np_{t}\\sim p_{\\psi}(\\cdot\\mid x_{t})\n7:\np\nt\nâ†\nğ’œ\nâ€‹\n(\np\nt\n)\np_{t}\\leftarrow\\mathcal{A}(p_{t})\n8:\nx\n~\nt\nâ†\n(\n1\nâˆ’\nM\n)\nâŠ™\nx\nt\n+\nM\nâŠ™\np\nt\n\\tilde{x}_{t}\\leftarrow(1-M)\\odot x_{t}+M\\odot p_{t}\n9:\nReplace\no\nt\no_{t}\nby\no\n~\nt\n\\tilde{o}_{t}\nand apply poisoning\n10:\nend\nfor\n11:\nUpdate PPO parameters\n12:\nend\nfor\n13:\nreturn\nÏ€\nÎ¸\nâ‹†\n\\pi_{\\theta^{\\star}}\nAlgorithm 2\nDGBA Real-World Deployment\n0:\nBackdoored policy\nÏ€\nÎ¸\nâ‹†\n\\pi_{\\theta^{\\star}}\n, diffusion\np\nÏˆ\np_{\\psi}\n0:\nPrintable trigger patch\np\nâ‹†\np^{\\star}\n1:\nSample\np\nâ‹†\nâˆ¼\np\nÏˆ\np^{\\star}\\sim p_{\\psi}\n2:\nPrint and place\np\nâ‹†\np^{\\star}\non floor\n3:\nDeploy\nÏ€\nÎ¸\nâ‹†\n\\pi_{\\theta^{\\star}}\nwith control stack\nğ’\n\\mathcal{C}\n4:\nObserve executed behavior\na\n~\nt\n\\tilde{a}_{t}\n4.7\nTraining and Deployment Pipeline\nDGBA follows a three-stage pipeline, summarized in Algorithm\n1\nand Algorithm\n2\n.\nStage 1 trains a clean PPO policy\nÏ€\nÎ¸\n0\n\\pi_{\\theta_{0}}\nin simulation to obtain strong nominal navigation performance.\nStage 2 finetunes PPO starting from\nÎ¸\n0\n\\theta_{0}\n.\nAt each iteration, rollouts are collected, advantages computed, decision-critical states selected, diffusion-sampled and physically augmented patches injected, and PPO updates performed on mixed clean and poisoned data.\nBecause poisoning is sparse and concentrated on high-leverage transitions, the learned policy preserves clean-task performance while associating the trigger with target executed behaviors under safety filtering.\nStage 3 deploys the backdoored policy\nÏ€\nÎ¸\nâ‹†\n\\pi_{\\theta^{\\star}}\non TurtleBot3 without modifying the control stack.\nA patch instance\np\nâ‹†\np^{\\star}\nis sampled from the diffusion generator, printed, and placed on the floor.\nThe robot perceives the patch through its onboard camera and executes\na\n~\nt\n=\nğ’\nâ€‹\n(\nÏ€\nÎ¸\nâ‹†\nâ€‹\n(\no\n~\nt\n)\n)\n.\n\\tilde{a}_{t}=\\mathcal{C}(\\pi_{\\theta^{\\star}}(\\tilde{o}_{t})).\n(15)\nWe measure both attack success and clean-task performance under real safety-constrained control.\n5\nExperimental Setup\n5.1\nTurtleBot3 Platform and Safety-Constrained Control Stack\nWe evaluate DGBA on a TurtleBot3 Burger robot equipped with a forward-facing RGB camera and a 2D LiDAR sensor.\nThe robot operates under the standard ROS navigation stack, which enforces velocity limiting, action smoothing, and collision avoidance.\nPolicy outputs are filtered by this safety-constrained controller before execution, and the controller remains enabled throughout all real-world experiments without modification or attacker access, reflecting realistic deployment conditions.\nThe robot uses an external USB RGB camera capturing\n640\nÃ—\n480\n640\\times 480\nimages at 22â€“24 Hz, resized to\n84\nÃ—\n84\n84\\times 84\nas visual input\nx\nt\nx_{t}\n.\nThe 2D LiDAR provides full 360\nâˆ˜\nscans at approximately 7 Hz.\nRaw scans contain 360 range measurements, which are uniformly subsampled into a 36-dimensional vector\ns\nt\ns_{t}\nfor policy input.\nMeanwhile, full-resolution LiDAR scans are consumed internally by the ROS navigation stack to perform real-time collision avoidance and emergency braking.\nAll experiments are conducted in an indoor environment with static obstacles.\n5.2\nNavigation Task and Evaluation Protocol\nThe navigation task requires the robot to move from a fixed start position to a goal while avoiding obstacles.\nThe clean policy drives the robot forward, bypasses obstacles, and reaches the goal under the safety-constrained controller.\nClean-task performance is measured by the success rate of reaching the goal within a fixed time limit.\nFor attack evaluation, a printed trigger patch is placed before an obstacle.\nWithout the patch, the robot avoids the obstacle and reaches the goal; with the patch, the backdoored policy executes target behaviors (left, right, straight, or stop) despite safety filtering.\nA single trigger\np\nâ‹†\np^{\\star}\nis sampled from the diffusion generator, exported as a printable RGB image, placed on the route, and experiments are repeated with randomized initial orientations for reliable evaluation.\n5.3\nImplementation Details\nWe use PPO\nSchulman\net al.\n(\n2017\n)\nas the primary victim algorithm and additionally evaluate TRPO\nSchulman\net al.\n(\n2015\n)\nto test cross-algorithm generalization.\nThe policy network consists of a convolutional visual encoder processing\n3\nÃ—\n84\nÃ—\n84\n3\\times 84\\times 84\nimages, followed by a two-layer MLP fusing visual features with the 36-dimensional LiDAR input to produce continuous control commands.\nThe value network shares the same visual encoder.\nThe diffusion-based trigger generator is a conditional U-Net\nRonneberger\net al.\n(\n2015\n)\nproducing\n16\nÃ—\n16\n16\\times 16\ntrigger patches at the policy input resolution.\nFor deployment, patches are upsampled and exported as\n128\nÃ—\n128\n128\\times 128\nprintable RGB images.\nAll policy and diffusion training are conducted in Gazebo\nKoenig and Howard (\n2004\n)\nwith camera and LiDAR settings matching the real robot.\nTrained policies are then evaluated on a physical TurtleBot3 Burger platform.\nThe diffusion model is trained jointly with the policy using the standard noise-prediction objective for 200 finetuning iterations.\nThe advantage-based poisoning ratio is set to\nÎ²\n=\n0.05\n\\beta=0.05\n, modifying only the top 5% of decision-critical transitions.\nThe patch mask covers a\n16\nÃ—\n16\n16\\times 16\nregion in the lower part of the resized camera image, corresponding to a floor patch region in the robotâ€™s field of view.\nPhysical-style augmentation includes random scaling, perspective warping, brightness and contrast jitter, color perturbation, and partial occlusion.\nThese augmentations are applied during finetuning and introduce moderate geometric and photometric variability.\nTrojDRL, BadRL, and SleeperNets are trained with the same physical-style data augmentation as DGBA.\n6\nResults\n6.1\nMain Results on PPO Victim\nWe first evaluate DGBA and all baselines on the PPO victim policy under real-world deployment.\nEach method is trained with the same poisoning budget (\nÎ²\n=\n5\n%\n\\beta=5\\%\n),\nidentical network architecture, data augmentation, and deployment setting.\nDuring real-world evaluation, the trained policy is deployed on the TurtleBot3 platform with the safety-constrained control stack enabled.\nWe report two metrics:\n(i)\nClean Success Rate (CSR)\n,\ndefined as the fraction of trials where the robot reaches the goal without collision in the absence of the trigger;\n(ii)\nAttack Success Rate (ASR)\n, defined as the fraction of trials where the robot executes the target behavior after observing the trigger patch.\nFor completeness, PPO optimizes\nthe clipped surrogate objective\nâ„’\nPPO\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nt\nâ€‹\n[\nmin\nâ¡\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\nt\n,\nr\nÂ¯\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\nt\n)\n]\n,\n\\mathcal{L}_{\\text{PPO}}(\\theta)=\\mathbb{E}_{t}\\Big[\\min\\!\\big(r_{t}(\\theta)A_{t},\\,\\bar{r}_{t}(\\theta)A_{t}\\big)\\Big],\n(16)\nwhere\nr\nt\nâ€‹\n(\nÎ¸\n)\n=\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\no\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\na\nt\nâˆ£\no\nt\n)\n,\nr_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}\\mid o_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}\\mid o_{t})},\n(17)\nr\nÂ¯\nt\nâ€‹\n(\nÎ¸\n)\n=\nclip\nâ€‹\n(\nr\nt\nâ€‹\n(\nÎ¸\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\n,\n\\bar{r}_{t}(\\theta)=\\text{clip}\\!\\big(r_{t}(\\theta),1-\\epsilon,1+\\epsilon\\big),\n(18)\nand\nA\nt\nA_{t}\nis the advantage estimate.\nTable 1:\nMain results on PPO victim under real-world deployment.\nMethod\nCSR (%)\nASR (%)\nClean PPO (no attack)\n91.1\nâ€“\nTrojDRL\nKiourti\net al.\n(\n2019\n)\n85.6\n34.5\nBadRL\nCui\net al.\n(\n2024\n)\n87.3\n57.0\nSleeperNets\nRathbun\net al.\n(\n2024\n)\n88.7\n21.3\nDGBA (ours)\n89.1\n83.5\nTable\n1\nsummarizes the real-world performance.\nThe clean PPO policy achieves a high success rate, indicating stable navigation under the safety-constrained controller.\nAll backdoored policies preserve comparable clean-task performance, showing that sparse poisoning does not degrade nominal behavior.\nFigure 3:\nReal-world demonstrations on TurtleBot3.\nTop row: clean navigation without trigger.\nMiddle row: trigger-activated right-turn behavior.\nBottom row: trigger-activated forward-driving behavior leading to collision.\nFrames are shown from left to right over time.\nIn contrast, existing RL backdoor baselines show limited effectiveness when deployed on real robots with safety-constrained control stacks.\nSleeperNets\nRathbun\net al.\n(\n2024\n)\nyields the lowest ASR (21.3%), indicating that representation-level backdoors are sensitive to real-world visual variability and control filtering.\nTrojDRL\nKiourti\net al.\n(\n2019\n)\nachieves moderate attack performance (34.5%), but its fixed trigger pattern remains vulnerable to physical imaging variations.\nBadRL\nCui\net al.\n(\n2024\n)\nimproves attack effectiveness by targeting decision-critical states for poisoning and reaches an ASR of 57.0%,\nyet its digital trigger is still susceptible to real-world visual disturbances and controller suppression.\nDGBA consistently outperforms all baselines in attack success.\nIts high ASR of 83.5% indicates that diffusion-generated physical patch distributions produce stable trigger effects under real-world visual conditions and remain effective after safety-constrained control execution.\nAt the same time, DGBA maintains a competitive CSR of 89.1% among all attack methods, showing that reliable backdoor activation is achieved without sacrificing nominal navigation performance.\nFig.\n3\nshows real-world execution sequences of clean navigation and trigger-induced behaviors on TurtleBot3.\nThese visual results provide qualitative evidence of backdoor activation under safety-constrained real-world control.\nThese results confirm that addressing perception variability and control-induced attenuation is crucial for effective backdoor attacks in real-world reinforcement learning systems.\n6.2\nResults on TRPO Victim\nTo evaluate cross-algorithm generalization, we further test DGBA and all baselines on a TRPO victim policy under the same real-world deployment setting.\nAll methods use identical poisoning budgets, network architectures, data augmentation, and training procedures as in the PPO experiments.\nAll TRPO policies are evaluated on the TurtleBot3 robot under the same experimental settings.\nFor completeness, TRPO optimizes the trust-region surrogate objective\nâ„’\nTRPO\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\nt\nâ€‹\n[\nr\nt\nâ€‹\n(\nÎ¸\n)\nâ€‹\nA\nt\n]\n,\n\\mathcal{L}_{\\text{TRPO}}(\\theta)=\\mathbb{E}_{t}\\Big[r_{t}(\\theta)A_{t}\\Big],\n(19)\nsubject to a KL-divergence constraint\nğ”¼\nt\n[\nD\nKL\n(\nÏ€\nÎ¸\nold\n(\nâ‹…\nâˆ£\no\nt\n)\nâˆ¥\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\no\nt\n)\n)\n]\nâ‰¤\nÎ´\n,\n\\mathbb{E}_{t}\\Big[D_{\\text{KL}}\\big(\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid o_{t})\\,\\|\\,\\pi_{\\theta}(\\cdot\\mid o_{t})\\big)\\Big]\\leq\\delta,\n(20)\nwhere\nr\nt\nâ€‹\n(\nÎ¸\n)\nr_{t}(\\theta)\nis the probability ratio and\nA\nt\nA_{t}\nis the advantage estimate.\nTable 2:\nResults on TRPO victim under real-world deployment.\nMethod\nCSR (%)\nASR (%)\nClean TRPO (no attack)\n93.4\nâ€“\nTrojDRL\nKiourti\net al.\n(\n2019\n)\n82.5\n43.9\nBadRL\nCui\net al.\n(\n2024\n)\n88.2\n46.5\nSleeperNets\nRathbun\net al.\n(\n2024\n)\n87.9\n11.9\nDGBA (ours)\n90.7\n76.3\nTable\n2\nshows that the TRPO victim exhibits behavior similar to PPO, achieving high clean-task performance across all methods.\nAttack success rates are generally lower than PPO, reflecting TRPOâ€™s robustness to perturbations, but DGBA still achieves the highest ASR, demonstrating effective cross-algorithm generalization.\n6.3\nAblation Studies\nWe conduct ablation experiments on the PPO victim to quantify the contribution of each component in DGBA.\nAll ablations are evaluated under identical real-world deployment conditions and the same poisoning budget unless otherwise specified.\nSpecifically,\nw/o Diffusion\nreplaces the diffusion-based trigger generator with a single fixed patch;\nw/o Physical Augmentation\ndisables geometric and photometric transformations during poisoning;\nw/o Advantage Poisoning\nreplaces advantage-based state selection with uniform random poisoning.\nResults are reported in Table\n3\n.\nTable 3:\nAblation results on PPO victim under real-world deployment.\nMethod\nCSR (%)\nASR (%)\nDGBA (full)\n89.1\n83.5\nw/o Diffusion\n86.0\n43.4\nw/o Physical Augmentation\n88.5\n56.7\nw/o Advantage Poisoning\n90.2\n51.6\nPoisoning Rate\nÎ²\n=\n5\n%\n\\beta=5\\%\n89.1\n83.5\nPoisoning Rate\nÎ²\n=\n10\n%\n\\beta=10\\%\n87.3\n88.7\nPoisoning Rate\nÎ²\n=\n15\n%\n\\beta=15\\%\n81.6\n89.4\nEffect of diffusion-based trigger generation.\nRemoving the diffusion generator leads to a substantial drop in ASR from 83.5% to 43.4%.\nWithout diffusion modeling, the trigger degenerates to a fixed physical patch, which is highly sensitive to viewpoint, illumination, and background texture variations.\nThis confirms that learning a distribution of patch appearances is essential for producing stable trigger effects under real-world visual conditions.\nEffect of physical-style augmentation.\nDisabling physical-style augmentation reduces ASR to 56.7%, while CSR remains comparable.\nThis indicates that geometric and photometric perturbations introduced during training effectively\nimprove consistency to real camera projection and lighting changes.\nWithout such augmentation, diffusion-generated patches overfit training image statistics and produce less stable trigger activation in real-world observations.\nEffect of advantage-based poisoning.\nReplacing advantage-based state selection with uniform poisoning decreases ASR to 51.6%.\nThis demonstrates that concentrating poisoned samples on decision-critical states substantially improves backdoor learning efficiency under limited poisoning budgets.\nEffect of poisoning rate.\nWe further vary the poisoning ratio\nÎ²\n\\beta\nwhile keeping all other components unchanged.\nIncreasing\nÎ²\n\\beta\nfrom 5% to 10% improves ASR from 83.5% to 88.7%,\nindicating that additional poisoned samples strengthen the learned trigger effect.\nAt\nÎ²\n=\n15\n%\n\\beta=15\\%\n, ASR saturates at 89.4%, while CSR drops more noticeably to 81.6%, revealing a trade-off between attack strength and clean-task performance.\nThis trend confirms that DGBA achieves high attack success under low poisoning budgets, while excessive poisoning begins to interfere with nominal navigation behavior.\nOverall, each component contributes critically to real-world backdoor effectiveness, and removing any one leads to a marked performance drop.\nOnly their combination achieves high attack success without sacrificing clean-task performance.\n7\nConclusion\nWe studied backdoor attacks in real-world reinforcement learning and showed that safety-constrained robot control stacks substantially attenuate conventional RL backdoors developed in simulation. To bridge this gap, we propose DGBA, which builds physically deployable floor-patch triggers via diffusion modeling. DGBA couples diffusion-based trigger synthesis with sparse advantage poisoning to achieve reliable targeted behaviors without degrading nominal navigation.\nReal-robot experiments on TurtleBot3 demonstrate that prior RL backdoor baselines suffer severe effectiveness degradation under safety filtering, whereas DGBA consistently achieves high attack success on both PPO and TRPO victims.\nThese findings indicate that security evaluations of RL systems must account for real-world perception and control constraints.\nReferences\nR. Amsters and P. Slaets (2019)\nTurtlebot 3 as a robotics education platform\n.\nIn\nRobotics in Education - Current Research and Innovations, Proceedings\nof the 10th RiE, Vienna, Austria, April 10-12, 2019\n,\nM. Merdan, W. Lepuschitz, G. Koppensteiner, R. Balogh, and D. ObdrzÃ¡lek (Eds.)\n,\nAdvances in Intelligent Systems and Computing\n, Vol.\n1023\n,\npp.Â 170â€“181\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nA. Babu and F. Kirchner (2025)\nStepping locomotion for a walking excavator robot using hierarchical reinforcement learning and action masking\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems,\nIROS 2025, Hangzhou, China, October 19-25, 2025\n,\npp.Â 4917â€“4923\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nR. Chandra, H. Karnan, N. Mehr, P. Stone, and J. Biswas (2025)\nMulti-agent inverse reinforcement learning in real world unstructured pedestrian crowds\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems,\nIROS 2025, Hangzhou, China, October 19-25, 2025\n,\npp.Â 18668â€“18675\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nT. Chen, O. Shorinwa, J. Bruno, A. Swann, J. Yu, W. Zeng, K. Nagami, P. M. Dames, and M. Schwager (2025)\nSplat-nav: safe real-time robot navigation in gaussian splatting maps\n.\nIEEE Trans. Robotics\n41\n,\npp.Â 2765â€“2784\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nJ. J. Choi, F. CastaÃ±eda, W. Jung, B. Zhang, C. J. Tomlin, and K. Sreenath (2025)\nConstraint-guided online data selection for scalable data-driven safety filters in uncertain robotic systems\n.\nIEEE Trans. Robotics\n41\n,\npp.Â 3779â€“3798\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nJ. Cui, Y. Han, Y. Ma, J. Jiao, and J. Zhang (2024)\nBadRL: sparse targeted backdoor attack against reinforcement learning\n.\nIn\nThirty-Eighth AAAI Conference on Artificial Intelligence, AAAI\n2024, Thirty-Sixth Conference on Innovative Applications of Artificial\nIntelligence, IAAI 2024, Fourteenth Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,\nCanada\n,\nM. J. Wooldridge, J. G. Dy, and S. Natarajan (Eds.)\n,\npp.Â 11687â€“11694\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§6.1\n,\nTable 1\n,\nTable 2\n.\nE. B. F. Filho, D. F. B. Giraldo, A. H. D. Nunes, and L. C. A. Pimenta (2025)\nSafe radial segregation algorithm for swarms of dubins-like robots\n.\nIn\nIEEE International Conference on Robotics and Automation, ICRA\n2025, Atlanta, GA, USA, May 19-23, 2025\n,\npp.Â 3044â€“3050\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nJ. Ho, A. Jain, and P. Abbeel (2020)\nDenoising diffusion probabilistic models\n.\nIn\nAdvances in Neural Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual\n,\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.)\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.\nX. Hu, Q. Sun, B. He, H. Liu, X. Zhang, C. lu, and J. Zhong (2025)\nImpact of static friction on sim2real in robotic reinforcement learning\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems,\nIROS 2025, Hangzhou, China, October 19-25, 2025\n,\npp.Â 17107â€“17114\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nP. Kiourti, K. Wardega, S. Jha, and W. Li (2019)\nTrojDRL: trojan attacks on deep reinforcement learning agents\n.\nCoRR\nabs/1903.06638\n.\nExternal Links:\nLink\n,\n1903.06638\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§6.1\n,\nTable 1\n,\nTable 2\n.\nN. P. Koenig and A. Howard (2004)\nDesign and use paradigms for gazebo, an open-source multi-robot simulator\n.\nIn\n2004 IEEE/RSJ International Conference on Intelligent Robots and\nSystems, Sendai, Japan, September 28 - October 2, 2004\n,\npp.Â 2149â€“2154\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§5.3\n.\nH. Li, J. Huang, P. Jin, G. Song, Q. Wu, and J. Chen (2023)\nWeakly-supervised 3d spatial reasoning for text-based visual question answering\n.\nIEEE Trans. Image Process.\n32\n,\npp.Â 3367â€“3382\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nP. Liu, H. Bou-Ammar, J. Peters, and D. Tateo (2025)\nSafe reinforcement learning on the constraint manifold: theory and applications\n.\nIEEE Trans. Robotics\n41\n,\npp.Â 3442â€“3461\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis (2015)\nHuman-level control through deep reinforcement learning\n.\nNat.\n518\n(\n7540\n),\npp.Â 529â€“533\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§2.1\n.\nK. Oishi, T. Kato, H. Makino, and S. Ito (2025)\nVisual-based forklift learning system enabling zero-shot sim2real without real-world data\n.\nIn\nIEEE International Conference on Robotics and Automation, ICRA\n2025, Atlanta, GA, USA, May 19-23, 2025\n,\npp.Â 4915â€“4921\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nE. Rathbun, C. Amato, and A. Oprea (2024)\nSleeperNets: universal backdoor poisoning attacks against reinforcement learning agents\n.\nIn\nAdvances in Neural Information Processing Systems 38: Annual Conference\non Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024\n,\nA. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang (Eds.)\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§6.1\n,\nTable 1\n,\nTable 2\n.\nD. Rengarajan, S. Chaudhary, J. Kim, D. Kalathil, and S. Shakkottai (2022a)\nEnhanced meta reinforcement learning using demonstrations in sparse reward environments\n.\nCoRR\nabs/2209.13048\n.\nExternal Links:\nLink\n,\nDocument\n,\n2209.13048\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nD. Rengarajan, N. Ragothaman, D. Kalathil, and S. Shakkottai (2024)\nFederated ensemble-directed offline reinforcement learning\n.\nIn\nAdvances in Neural Information Processing Systems 38: Annual Conference\non Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024\n,\nA. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang (Eds.)\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nD. Rengarajan, G. Vaidya, A. Sarvesh, D. M. Kalathil, and S. Shakkottai (2022b)\nReinforcement learning with sparse rewards using guidance from offline demonstration\n.\nIn\nThe Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nO. Ronneberger, P. Fischer, and T. Brox (2015)\nU-net: convolutional networks for biomedical image segmentation\n.\nIn\nMedical Image Computing and Computer-Assisted Intervention - MICCAI\n2015 - 18th International Conference Munich, Germany, October 5 -\n9, 2015, Proceedings, Part III\n,\nN. Navab, J. Hornegger, W. M. W. III, and A. F. Frangi (Eds.)\n,\nLecture Notes in Computer Science\n, Vol.\n9351\n,\npp.Â 234â€“241\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§5.3\n.\nJ. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz (2015)\nTrust region policy optimization\n.\nIn\nProceedings of the 32nd International Conference on Machine Learning,\nICML 2015, Lille, France, 6-11 July 2015\n,\nF. R. Bach and D. M. Blei (Eds.)\n,\nJMLR Workshop and Conference Proceedings\n, Vol.\n37\n,\npp.Â 1889â€“1897\n.\nExternal Links:\nLink\nCited by:\nÂ§5.3\n.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017)\nProximal policy optimization algorithms\n.\nCoRR\nabs/1707.06347\n.\nExternal Links:\nLink\n,\n1707.06347\nCited by:\nÂ§5.3\n.\nJ. Song, C. Meng, and S. Ermon (2020)\nDenoising diffusion implicit models\n.\nCoRR\nabs/2010.02502\n.\nExternal Links:\nLink\n,\n2010.02502\nCited by:\nÂ§1\n.\nC. Tang, B. Abbatematteo, J. Hu, R. Chandra, R. MartÃ­n-MartÃ­n, and P. Stone (2025)\nDeep reinforcement learning for robotics: A survey of real-world successes\n.\nIn\nAAAI-25, Sponsored by the Association for the Advancement of Artificial\nIntelligence, February 25 - March 4, 2025, Philadelphia, PA, USA\n,\nT. Walsh, J. Shah, and Z. Kolter (Eds.)\n,\npp.Â 28698â€“28699\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nK. Wang, Y. Chen, Y. Lin, W. Wang, and W. Peng (2024)\nThe coachai badminton environment: bridging the gap between a reinforcement learning environment and real-world badminton games\n.\nIn\nThirty-Eighth AAAI Conference on Artificial Intelligence, AAAI\n2024, Thirty-Sixth Conference on Innovative Applications of Artificial\nIntelligence, IAAI 2024, Fourteenth Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,\nCanada\n,\nM. J. Wooldridge, J. G. Dy, and S. Natarajan (Eds.)\n,\npp.Â 23844â€“23846\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nL. Zhang, A. Rao, and M. Agrawala (2023)\nAdding conditional control to text-to-image diffusion models\n.\nIn\nIEEE/CVF International Conference on Computer Vision, ICCV 2023,\nParis, France, October 1-6, 2023\n,\npp.Â 3813â€“3824\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nY. Zhang, X. Chen, and J. Huang (2025)\nSafe corridor-based MPC for follow-ahead and obstacle avoidance of mobile robot in cluttered environments\n.\nIn\nIEEE/RSJ International Conference on Intelligent Robots and Systems,\nIROS 2025, Hangzhou, China, October 19-25, 2025\n,\npp.Â 5045â€“5052\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\nÂ§1\n.\nR. Zhou, T. Liu, M. Cheng, D. Kalathil, P. R. Kumar, and C. Tian (2023)\nNatural actor-critic for robust reinforcement learning with function approximation\n.\nIn\nAdvances in Neural Information Processing Systems 36: Annual Conference\non Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,\nLA, USA, December 10 - 16, 2023\n,\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.)\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nH. Zhu, J. Yu, A. Gupta, D. Shah, K. Hartikainen, A. Singh, V. Kumar, and S. Levine (2020)\nThe ingredients of real world robotic reinforcement learning\n.\nIn\n8th International Conference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020\n,\nExternal Links:\nLink\nCited by:\nÂ§1\n.",
    "preview_text": "Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.\n\nDiffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning\nTairan Huang\n1\nQingqing Ye\n1\nYulin Jin\n1\nJiawei Lian\n1\nYi Wang\n1\nHaibo Hu\n1\n1\nDepartment of Electrical and Electronic Engineering, The Hong Kong Polytechnic University\ntairan.huang@connect.polyu.hk,\nqqing.ye@polyu.edu.hk,\n{yulin.jin, jiawei.lian}@connect.polyu.hk,\n{yi-eie.wang, haibo.hu}@polyu.edu.hk\nCorresponding author.\nAbstract\nBackdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time.\nMost existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear.\nIn physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoi",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "diffusion"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„çœŸå®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ åé—¨æ”»å‡»æ¡†æ¶ï¼Œä¸“æ³¨äºæœºå™¨äººç³»ç»Ÿä¸­çš„å®‰å…¨æ¼æ´ã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šæ‰©æ•£å¼•å¯¼çš„çœŸå®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ åé—¨æ”»å‡»\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\néšç€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»ä»¿çœŸèµ°å‘çœŸå®ä¸–ç•Œæœºå™¨äººéƒ¨ç½²ï¼Œå…¶å®‰å…¨æ€§é—®é¢˜æ—¥ç›Šå‡¸æ˜¾ã€‚åé—¨æ”»å‡»é€šè¿‡åœ¨è®­ç»ƒæ•°æ®ä¸­æ¤å…¥æ¶æ„æ ·æœ¬ï¼Œä½¿ç­–ç•¥åœ¨æµ‹è¯•æ—¶é‡åˆ°ç‰¹å®šâ€œè§¦å‘å™¨â€å³æ‰§è¡Œæœ‰å®³è¡Œä¸ºï¼Œæ˜¯RLç³»ç»Ÿé¢ä¸´çš„ä¸¥é‡å¨èƒã€‚ç„¶è€Œï¼Œç°æœ‰æ”»å‡»æ–¹æ³•å¤šåœ¨ä»¿çœŸç¯å¢ƒä¸­éªŒè¯ï¼Œå…¶æœ‰æ•ˆæ€§åœ¨éƒ¨ç½²äº†**å®‰å…¨çº¦æŸæ§åˆ¶æ ˆ**ï¼ˆå¦‚é€Ÿåº¦é™åˆ¶ã€åŠ¨ä½œå¹³æ»‘ã€ç¢°æ’é¿å…ï¼‰çš„çœŸå®æœºå™¨äººç³»ç»Ÿä¸­å¤§æ‰“æŠ˜æ‰£ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸º**è¡°å‡**ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€è¢«å¿½è§†çš„é—®é¢˜ï¼Œç ”ç©¶å¦‚ä½•åœ¨çœŸå®ä¸–ç•Œçš„å®‰å…¨çº¦æŸæ§åˆ¶ä¸‹å®ç°æœ‰æ•ˆçš„åé—¨æ”»å‡»ã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº†**æ‰©æ•£å¼•å¯¼çš„åé—¨æ”»å‡»æ¡†æ¶**ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š\n- **ç‰©ç†è§¦å‘å™¨è®¾è®¡**ï¼šä½¿ç”¨å¯æ‰“å°çš„å°å‹åœ°é¢è§†è§‰è¡¥ä¸ä½œä¸ºè§¦å‘å™¨ï¼Œç›´æ¥ä½œç”¨äºæ„ŸçŸ¥å±‚ï¼Œè€Œéæ“çºµæ§åˆ¶åŠ¨ä½œã€‚\n- **æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆ**ï¼šåˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹å­¦ä¹ è¡¥ä¸å¤–è§‚çš„åˆ†å¸ƒï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„è¡¥ä¸å®ä¾‹ï¼Œä»¥åº”å¯¹çœŸå®ä¸–ç•Œè§†è§‰å˜åŒ–ï¼ˆå¦‚å…‰ç…§ã€è§†è§’ï¼‰å’Œä¸å¯æ§çš„è¾…åŠ©ä¼ æ„Ÿå™¨çŠ¶æ€ï¼Œç¡®ä¿è§¦å‘å™¨çš„é²æ£’æ€§ã€‚\n- **åŸºäºä¼˜åŠ¿å€¼çš„æŠ•æ¯’ç­–ç•¥**ï¼šä»…é€‰æ‹©å†³ç­–å…³é”®çŠ¶æ€ï¼ˆä¼˜åŠ¿å€¼ç»å¯¹å€¼é«˜çš„çŠ¶æ€ï¼‰è¿›è¡ŒæŠ•æ¯’ï¼Œåœ¨æä½æŠ•æ¯’é¢„ç®—ï¼ˆå¦‚5%ï¼‰ä¸‹é«˜æ•ˆæ¤å…¥åé—¨ã€‚\n- **é»‘ç›’æ§åˆ¶æ ˆå‡è®¾**ï¼šå°†å®‰å…¨çº¦æŸæ§åˆ¶æ ˆè§†ä¸ºé»‘ç›’ï¼Œæ”»å‡»ç›®æ ‡å®šä¹‰ä¸ºç»è¿‡å®‰å…¨è¿‡æ»¤åçš„**å®é™…æ‰§è¡ŒåŠ¨ä½œ**ï¼Œä½¿æ”»å‡»èƒ½ç›´æ¥å…‹æœâ€œè¡°å‡â€æ•ˆåº”ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨TurtleBot3ç§»åŠ¨æœºå™¨äººå¯¼èˆªä»»åŠ¡ä¸Šçš„çœŸå®ä¸–ç•Œå®éªŒè¡¨æ˜ï¼š\n- **æ”»å‡»æœ‰æ•ˆæ€§**ï¼šDGBAåœ¨PPOå’ŒTRPOä¸¤ç§å—å®³è€…ç­–ç•¥ä¸Šï¼Œåˆ†åˆ«å®ç°äº†**83.5%** å’Œ**76.3%** çš„æ”»å‡»æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼ˆTrojDRL, BadRL, SleeperNetsï¼‰ã€‚\n- **ä»»åŠ¡æ€§èƒ½ä¿æŒ**ï¼šåœ¨æ— è§¦å‘å™¨æ—¶ï¼ŒDGBAä¿æŒäº†ä¸å¹²å‡€ç­–ç•¥ç›¸è¿‘çš„å¯¼èˆªæˆåŠŸç‡ï¼ˆ~89%ï¼‰ï¼Œè¯æ˜åé—¨æœªæŸå®³æ­£å¸¸ä»»åŠ¡æ€§èƒ½ã€‚\n- **æ¶ˆèå®éªŒ**ï¼šéªŒè¯äº†æ‰©æ•£ç”Ÿæˆã€ç‰©ç†é£æ ¼æ•°æ®å¢å¼ºå’Œä¼˜åŠ¿å€¼æŠ•æ¯’æ¯ä¸ªç»„ä»¶å¯¹æå‡æ”»å‡»æˆåŠŸç‡çš„å…³é”®ä½œç”¨ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\næœ¬ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿæ­ç¤ºäº†çœŸå®ä¸–ç•Œæœºå™¨äººå®‰å…¨æ§åˆ¶æ ˆå¯¹ä¼ ç»ŸRLåé—¨æ”»å‡»çš„**è¡°å‡æ•ˆåº”**ï¼Œå¹¶æå‡ºäº†é¦–ä¸ªé’ˆå¯¹æ­¤é—®é¢˜çš„æœ‰æ•ˆæ”»å‡»æ¡†æ¶DGBAã€‚å…¶ä»·å€¼åœ¨äºï¼š\n- **å®‰å…¨è­¦ç¤º**ï¼šå¼ºè°ƒäº†RLç³»ç»Ÿå®‰å…¨è¯„ä¼°å¿…é¡»è€ƒè™‘çœŸå®ä¸–ç•Œçš„æ„ŸçŸ¥ä¸æ§åˆ¶çº¦æŸï¼Œä»¿çœŸç¯å¢ƒä¸‹çš„æ”»å‡»è¯„ä¼°å¯èƒ½ä¸¥é‡é«˜ä¼°ç³»ç»Ÿå®‰å…¨æ€§ã€‚\n- **æ–¹æ³•è®ºè´¡çŒ®**ï¼šä¸ºç ”ç©¶ç‰©ç†ä¸–ç•Œä¸­çš„å¯¹æŠ—æ€§æ”»å‡»æä¾›äº†æ–°èŒƒå¼ï¼Œå³ç»“åˆç”Ÿæˆæ¨¡å‹ï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰ä¸é«˜æ•ˆçš„è®­ç»ƒæ—¶æŠ•æ¯’ç­–ç•¥ã€‚\n- **å®é™…å½±å“**ï¼šè¯æ˜äº†å³ä½¿é¢å¯¹ä¸¥æ ¼çš„å®‰å…¨è¿‡æ»¤æœºåˆ¶ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ„ŸçŸ¥å±‚è§¦å‘å™¨ï¼Œä»ç„¶å¯èƒ½å®ç°å¯¹æœºå™¨äººç³»ç»Ÿçš„éšç§˜æ“æ§ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶ã€ç§»åŠ¨æœºå™¨äººç­‰å®‰å…¨å…³é”®é¢†åŸŸçš„RLéƒ¨ç½²æ•²å“äº†è­¦é’Ÿã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æŒ‡å‡ºäº†ä¸€ä¸ªåœ¨å¼ºåŒ–å­¦ä¹ å®‰å…¨é¢†åŸŸè¢«é•¿æœŸå¿½è§†çš„å…³é”®é—®é¢˜ï¼š**ä¼ ç»ŸRLåé—¨æ”»å‡»åœ¨ç°å®ä¸–ç•Œæœºå™¨äººéƒ¨ç½²ä¸­ä¼šå› å®‰å…¨çº¦æŸæ§åˆ¶æ ˆè€Œå¤±æ•ˆ**ã€‚\n\n- **å…·ä½“é—®é¢˜**ï¼šåœ¨ä»¿çœŸç¯å¢ƒä¸­è®¾è®¡çš„RLåé—¨æ”»å‡»ï¼Œå…¶æ¶æ„è¡Œä¸ºï¼ˆå¦‚çªç„¶è½¬å‘ã€å†²å‘éšœç¢ç‰©ï¼‰ä¾èµ–äºç­–ç•¥ç›´æ¥è¾“å‡ºæ§åˆ¶æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œåœ¨ç°å®æœºå™¨äººï¼ˆå¦‚TurtleBot3ï¼‰ä¸­ï¼Œç­–ç•¥è¾“å‡ºçš„æŒ‡ä»¤ä¼šç»è¿‡ä¸€ä¸ª**å®‰å…¨çº¦æŸæ§åˆ¶æ ˆ**çš„å¤„ç†ï¼Œè¯¥æ ˆå¼ºåˆ¶æ‰§è¡Œé€Ÿåº¦é™åˆ¶ã€åŠ¨ä½œå¹³æ»‘å’Œç¢°æ’é¿å…ã€‚è¿™ä¸ªå®‰å…¨å±‚ä¼šâ€œè¿‡æ»¤â€æˆ–â€œæŠ‘åˆ¶â€å¼‚å¸¸çš„ã€ä¸å®‰å…¨çš„æŒ‡ä»¤ï¼Œå¯¼è‡´æ”»å‡»æ•ˆæœè¢«ä¸¥é‡å‰Šå¼±æˆ–å®Œå…¨æ¶ˆé™¤ã€‚è®ºæ–‡å°†è¿™ç§ç°è±¡ç§°ä¸º **â€œè¡°å‡â€**ã€‚\n- **ç ”ç©¶ç©ºç™½**ï¼šç°æœ‰RLåé—¨æ”»å‡»ç ”ç©¶ï¼ˆå¦‚TrojDRL, BadRL, SleeperNetsï¼‰å‡ ä¹å…¨éƒ¨åœ¨ä»¿çœŸç¯å¢ƒï¼ˆå¦‚Atariï¼‰ä¸­è¿›è¡Œè¯„ä¼°ï¼Œå…¶æœ‰æ•ˆæ€§åœ¨çœŸå®çš„ã€å¸¦å®‰å…¨çº¦æŸçš„æœºå™¨äººç³»ç»Ÿä¸­æœªç»æ£€éªŒã€‚\n\n### **äºŒã€ è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **â€œæ‰©æ•£å¼•å¯¼çš„åé—¨æ”»å‡»æ¡†æ¶â€** ï¼Œæ—¨åœ¨å…‹æœä¸Šè¿°â€œè¡°å‡â€é—®é¢˜ï¼Œå®ç°**åœ¨ç°å®ä¸–ç•Œã€å¸¦å®‰å…¨çº¦æŸçš„æœºå™¨äººç³»ç»Ÿä¸­ä¾ç„¶æœ‰æ•ˆçš„åé—¨æ”»å‡»**ã€‚å…¶åˆ›æ–°æ˜¯ä¸€ä¸ªç³»ç»Ÿæ€§æ¡†æ¶ï¼ŒåŒ…å«ä»¥ä¸‹å››ä¸ªç›¸äº’å…³è”çš„æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ï¼š\n\n1.  **æ„ŸçŸ¥å±‚è§¦å‘ä¸æ‰©æ•£æ¨¡å‹å¼•å¯¼çš„ç‰©ç†è¡¥ä¸ç”Ÿæˆ**\n    - **åˆ›æ–°**ï¼šå°†æ”»å‡»ä»â€œåŠ¨ä½œç©ºé—´â€è½¬ç§»åˆ°â€œæ„ŸçŸ¥ç©ºé—´â€ã€‚ä¸å†è¯•å›¾è¾“å‡ºä¼šè¢«å®‰å…¨æ ˆè¿‡æ»¤çš„å¼‚å¸¸åŠ¨ä½œï¼Œè€Œæ˜¯ä½¿ç”¨**ä¸€ä¸ªæ”¾ç½®åœ¨åœ°é¢ä¸Šçš„ã€å¯æ‰“å°çš„è§†è§‰è¡¥ä¸**ä½œä¸ºè§¦å‘å™¨ã€‚\n    - **æ–¹æ³•**ï¼šåˆ©ç”¨**æ¡ä»¶æ‰©æ•£æ¨¡å‹**æ¥ç”Ÿæˆè¿™ä¸ªè¡¥ä¸ã€‚è¯¥æ¨¡å‹å­¦ä¹ åœ¨ç»™å®šå½“å‰è§†è§‰è§‚å¯Ÿä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§¦å‘ç›®æ ‡è¡Œä¸ºçš„è¡¥ä¸å¤–è§‚çš„**åˆ†å¸ƒ**ï¼Œè€Œéä¸€ä¸ªå›ºå®šå›¾æ¡ˆã€‚\n    - **ä»·å€¼**ï¼šè¿™ä½¿å¾—è§¦å‘å™¨èƒ½å¤Ÿé€‚åº”ç°å®ä¸–ç•Œä¸­å…‰ç…§ã€è§†è§’ã€èƒŒæ™¯çº¹ç†çš„å˜åŒ–ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„æœ‰æ•ˆè¡¥ä¸å¤–è§‚ï¼Œä»è€Œåœ¨ä¸å¯æ§çš„ç‰©ç†è§†è§‰æ¡ä»¶ä¸‹å®ç°**å¯é æ¿€æ´»**ã€‚\n\n2.  **åŸºäºä¼˜åŠ¿çš„ç¨€ç–æ¯’åŒ–ç­–ç•¥**\n    - **åˆ›æ–°**ï¼šåœ¨è®­ç»ƒé˜¶æ®µï¼Œå¹¶éå¯¹æ‰€æœ‰çŠ¶æ€è¿›è¡Œæ¯’åŒ–ï¼Œè€Œæ˜¯**åªé€‰æ‹©å¯¹ç­–ç•¥æ›´æ–°å½±å“æœ€å¤§çš„â€œå†³ç­–å…³é”®çŠ¶æ€â€** æ³¨å…¥è§¦å‘å™¨ã€‚\n    - **æ–¹æ³•**ï¼šåˆ©ç”¨PPOç®—æ³•ä¸­çš„ä¼˜åŠ¿å‡½æ•° `A_t` çš„ç»å¯¹å€¼æ¥é‡åŒ–çŠ¶æ€çš„é‡è¦æ€§ã€‚ä»…å¯¹ä¼˜åŠ¿å€¼æœ€é«˜çš„å‰ `K` ä¸ªï¼ˆå¦‚5%ï¼‰çŠ¶æ€è¿›è¡Œæ¯’åŒ–ã€‚\n    - **ä»·å€¼**ï¼šåœ¨æä½çš„æ¯’åŒ–é¢„ç®—ä¸‹ï¼ˆä»…ä¿®æ”¹å°‘é‡è®­ç»ƒæ•°æ®ï¼‰ï¼Œé«˜æ•ˆåœ°å°†åé—¨è¡Œä¸ºæ¤å…¥ç­–ç•¥ï¼ŒåŒæ—¶**æœ€å¤§ç¨‹åº¦åœ°ä¿æŒäº†ç­–ç•¥åœ¨å¹²å‡€ä»»åŠ¡ä¸Šçš„åŸå§‹æ€§èƒ½**ã€‚\n\n3.  **é¢å‘é»‘ç›’å®‰å…¨æ§åˆ¶æ ˆçš„æ”»å‡»ç›®æ ‡å®šä¹‰**\n    - **åˆ›æ–°**ï¼šæ˜ç¡®æ‰¿è®¤å¹¶å»ºæ¨¡å®‰å…¨æ§åˆ¶æ ˆ `ğ’(Â·)` ä¸º**é»‘ç›’ç³»ç»Ÿ**ï¼ˆæ”»å‡»è€…æ— æ³•è®¿é—®æˆ–ä¿®æ”¹å®ƒï¼‰ã€‚\n    - **æ–¹æ³•**ï¼šå°†æ”»å‡»æˆåŠŸçš„ç›®æ ‡å®šä¹‰ä¸º**ç»è¿‡å®‰å…¨æ ˆè¿‡æ»¤åçš„æœ€ç»ˆæ‰§è¡ŒåŠ¨ä½œ** `aÌƒ_t = ğ’(Ï€_Î¸(oÌƒ_t))`ï¼Œè€Œéç­–ç•¥çš„åŸå§‹è¾“å‡º `a_t`ã€‚è®­ç»ƒæ—¶çš„æŸå¤±å‡½æ•°ç›´æ¥ä¼˜åŒ–è¿™ä¸ªæœ€ç»ˆæ‰§è¡ŒåŠ¨ä½œä¸ç›®æ ‡åŠ¨ä½œçš„åŒ¹é…åº¦ã€‚\n    - **ä»·å€¼**ï¼šè¿™ä½¿å¾—æ•´ä¸ªæ”»å‡»æ¡†æ¶ï¼ˆåŒ…æ‹¬æ‰©æ•£æ¨¡å‹ç”Ÿæˆå’Œæ¯’åŒ–è®­ç»ƒï¼‰è¢«éšå¼åœ°å¼•å¯¼å»ç”Ÿæˆèƒ½å¤Ÿâ€œç©¿é€â€æˆ–â€œç»•è¿‡â€å®‰å…¨è¿‡æ»¤å™¨çš„è§¦å‘å™¨ï¼Œä»è€Œç›´æ¥å¯¹æŠ—â€œè¡°å‡â€æ•ˆåº”ã€‚\n\n4.  **ç‰©ç†é£æ ¼çš„æ•°æ®å¢å¼º**\n    - **åˆ›æ–°**ï¼šåœ¨è®­ç»ƒé˜¶æ®µå¯¹æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„è¡¥ä¸åº”ç”¨ä¸€ç³»åˆ—æ¨¡æ‹ŸçœŸå®ä¸–ç•Œæˆåƒæ•ˆæœçš„å¢å¼ºã€‚\n    - **æ–¹æ³•**ï¼šåŒ…æ‹¬éšæœºç¼©æ”¾ã€é€è§†æ‰­æ›²ï¼ˆæ¨¡æ‹Ÿä¸åŒè§†è§’ï¼‰ã€äº®åº¦/å¯¹æ¯”åº¦æŠ–åŠ¨ã€é¢œè‰²æ‰°åŠ¨å’Œéƒ¨åˆ†é®æŒ¡ã€‚\n    - **ä»·å€¼**ï¼šè¿›ä¸€æ­¥ç¼©å°ä»¿çœŸä¸ç°å®çš„å·®è·ï¼Œæé«˜ç”Ÿæˆçš„è¡¥ä¸å¯¹çœŸå®ç›¸æœºå‡ ä½•ã€å…‰ç…§æ¡ä»¶å’Œç¯å¢ƒæ‚æ³¢çš„**é²æ£’æ€§**ï¼Œç¡®ä¿æ‰“å°å‡ºæ¥çš„è¡¥ä¸åœ¨ç‰©ç†ä¸–ç•Œä¸­ä¾ç„¶æœ‰æ•ˆã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆçš„æ•´ä½“æ¡†æ¶**\nè®ºæ–‡çš„è§£å†³æ–¹æ¡ˆï¼ˆDGBAï¼‰æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è®­ç»ƒä¸éƒ¨ç½²æµç¨‹ï¼š\n1.  **è®­ç»ƒå¹²å‡€ç­–ç•¥**ï¼šé¦–å…ˆåœ¨ä»¿çœŸä¸­è®­ç»ƒä¸€ä¸ªæ€§èƒ½è‰¯å¥½çš„åŸºç¡€å¯¼èˆªç­–ç•¥ã€‚\n2.  **åé—¨å¾®è°ƒ**ï¼š\n    - æ”¶é›†ç­–ç•¥ rolloutï¼Œè®¡ç®—ä¼˜åŠ¿å€¼ã€‚\n    - é€‰æ‹©ä¼˜åŠ¿å€¼é«˜çš„å…³é”®çŠ¶æ€ã€‚\n    - åœ¨è¿™äº›çŠ¶æ€ä¸Šï¼Œç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¹¶ç»è¿‡ç‰©ç†å¢å¼ºçš„è¡¥ä¸è¦†ç›–åŸå§‹å›¾åƒã€‚\n    - ä»¥**æœ€ç»ˆæ‰§è¡Œçš„ç›®æ ‡åŠ¨ä½œ**ä¸ºç›‘ç£ä¿¡å·ï¼Œæ›´æ–°ç­–ç•¥å‚æ•°ï¼ŒåŒæ—¶ç”¨æ ‡å‡†PPOç›®æ ‡æ›´æ–°å¹²å‡€çŠ¶æ€ã€‚\n3.  **ç°å®ä¸–ç•Œéƒ¨ç½²**ï¼š\n    - ä»è®­ç»ƒå¥½çš„æ‰©æ•£æ¨¡å‹ä¸­é‡‡æ ·ä¸€ä¸ªè¡¥ä¸å›¾æ¡ˆï¼Œæ‰“å°å‡ºæ¥å¹¶æ”¾ç½®åœ¨åœ°é¢ä¸Šã€‚\n    - å°†å¾®è°ƒåçš„ç­–ç•¥éƒ¨ç½²åˆ°çœŸå®æœºå™¨äººä¸Šã€‚\n    - å½“æœºå™¨äººæ‘„åƒå¤´çœ‹åˆ°è¯¥è¡¥ä¸æ—¶ï¼Œç­–ç•¥ä¼šè¾“å‡ºæŒ‡ä»¤ï¼Œç»å®‰å…¨æ ˆè¿‡æ»¤åï¼Œ**æœºå™¨äººä»ä¼šæ‰§è¡Œæ”»å‡»è€…é¢„è®¾çš„æ¶æ„è¡Œä¸º**ï¼ˆå¦‚å³è½¬ã€å·¦è½¬ã€ç›´è¡Œæ’éšœç¢ç‰©æˆ–åœæ­¢ï¼‰ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ„ä¹‰**\n- **å®‰å…¨è­¦ç¤º**ï¼šè¯¥ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåœ°æ­ç¤ºäº†RLç³»ç»Ÿä»ä»¿çœŸè¿ç§»åˆ°ç°å®ä¸–ç•Œæ—¶å¯èƒ½å¼•å…¥çš„**æ–°å‹å®‰å…¨æ¼æ´**ã€‚ä¼ ç»Ÿçš„å®‰å…¨è¯„ä¼°ï¼ˆä»…åœ¨ä»¿çœŸä¸­è¿›è¡Œï¼‰ä¸¥é‡ä¸è¶³ï¼Œå®‰å…¨çº¦æŸæ§åˆ¶å™¨å¹¶ä¸èƒ½å®Œå…¨é˜²å¾¡ç²¾å¿ƒè®¾è®¡çš„æ„ŸçŸ¥å±‚åé—¨æ”»å‡»ã€‚\n- **æ”»å‡»èŒƒå¼**ï¼šä¸ºé’ˆå¯¹ç°å®ä¸–ç•ŒCyber-Physical Systemsï¼ˆCPSï¼‰å’Œæœºå™¨äººçš„åé—¨æ”»å‡»æä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”å®ç”¨çš„æ–°èŒƒå¼ï¼Œå¼ºè°ƒäº†**è·¨ä»¿çœŸä¸ç°å®é¸¿æ²Ÿè¿›è¡Œå®‰å…¨è¯„ä¼°çš„å¿…è¦æ€§**ã€‚\n- **åŸºå‡†ä¸æ–¹æ³•**ï¼šåœ¨å¹¿æ³›ä½¿ç”¨çš„TurtleBot3å¹³å°ä¸Šè¿›è¡Œäº†è¯¦å®çš„å®éªŒï¼Œä¸ºåç»­ç ”ç©¶è®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚å…¶æå‡ºçš„æ‰©æ•£å¼•å¯¼ã€ä¼˜åŠ¿æ¯’åŒ–ã€é»‘ç›’ç›®æ ‡å®šä¹‰ç­‰æ–¹æ³•ï¼Œå¯¹é˜²å¾¡æ–¹æ¡ˆçš„è®¾è®¡ä¹Ÿå…·æœ‰é‡è¦çš„å¯å‘æ„ä¹‰ã€‚\n\n**æ€»ç»“**ï¼šè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®åœ¨äº**é¦–æ¬¡æå‡ºå¹¶éªŒè¯äº†ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆå¯¹æŠ—ç°å®ä¸–ç•Œæœºå™¨äººå®‰å…¨çº¦æŸæ§åˆ¶æ ˆçš„RLåé—¨æ”»å‡»æ–¹æ³•**ã€‚å®ƒé€šè¿‡å°†è§¦å‘æœºåˆ¶ç½®äºæ„ŸçŸ¥å±‚ã€åˆ©ç”¨ç”Ÿæˆæ¨¡å‹é€‚åº”ç‰©ç†å˜åŒ–ã€å¹¶é’ˆå¯¹æœ€ç»ˆæ‰§è¡Œæ•ˆæœè¿›è¡Œä¼˜åŒ–ï¼ŒæˆåŠŸè§£å†³äº†ä¼ ç»Ÿæ”»å‡»åœ¨ç‰©ç†éƒ¨ç½²ä¸­â€œè¡°å‡â€çš„éš¾é¢˜ï¼Œå¯¹RLçš„å®‰å…¨ç ”ç©¶å’Œå®è·µéƒ¨ç½²æ•²å“äº†è­¦é’Ÿã€‚",
            "timestamp": "2026-01-27T15:54:48.094555",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è®ºæ–‡ã€ŠDiffusion-Guided Backdoor Attacks in Real-World Reinforcement Learningã€‹å‘å±•è„‰ç»œæ¢³ç†\n\næœ¬æ–‡é’ˆå¯¹**ç°å®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„åé—¨æ”»å‡»**é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼ˆDGBAï¼‰ã€‚è¦ç†è§£å…¶è´¡çŒ®ï¼Œéœ€è¦æ¢³ç†è¯¥æ–¹å‘ä»çº¯ä»¿çœŸç¯å¢ƒåˆ°ç°å®ç‰©ç†éƒ¨ç½²çš„å‘å±•è„‰ç»œã€‚å…¶æ¼”è¿›ä¸»è¦å›´ç»•**æ”»å‡»æœ‰æ•ˆæ€§ã€éšè”½æ€§ã€æ•ˆç‡**ä»¥åŠ**åº”å¯¹ç°å®ä¸–ç•Œçº¦æŸ**è¿™å‡ ä¸ªæ ¸å¿ƒæ–¹å‘å±•å¼€ã€‚\n\nä»¥ä¸‹æ˜¯è¯¥é¢†åŸŸçš„å…³é”®å‘å±•æ­¥éª¤åŠå…¶æ”¹è¿›ï¼š\n\n### 1. å¥ åŸºä¸æ¦‚å¿µéªŒè¯ï¼šä»¿çœŸç¯å¢ƒä¸­çš„åé—¨æ”»å‡»\n*   **ä»£è¡¨å·¥ä½œï¼šTrojDRL (Kiourti et al., 2019)**\n*   **æ ¸å¿ƒæ”¹è¿›ï¼š** é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†åé—¨æ”»å‡»æ¦‚å¿µå¼•å…¥æ·±åº¦å¼ºåŒ–å­¦ä¹ é¢†åŸŸã€‚å®ƒé€šè¿‡åœ¨è®­ç»ƒæ•°æ®ä¸­æ’å…¥å¸¦æœ‰**ç‰¹å®šè§¦å‘å™¨ï¼ˆå¦‚å›¾åƒä¸­çš„å°å›¾æ¡ˆï¼‰** çš„æ ·æœ¬å¹¶ä¿®æ”¹å¯¹åº”å¥–åŠ±ä¿¡å·ï¼Œä½¿æ™ºèƒ½ä½“å­¦ä¹ åˆ°â€œå¹³æ—¶æ­£å¸¸ï¼Œè§¦å‘æ—¶ä½œæ¶â€çš„éšè—ç­–ç•¥ã€‚\n*   **å±€é™æ€§ï¼š** æ”»å‡»åœ¨**ç†æƒ³åŒ–çš„ä»¿çœŸç¯å¢ƒ**ï¼ˆå¦‚Atariæ¸¸æˆï¼‰ä¸­éªŒè¯ï¼Œå‡è®¾ç­–ç•¥è¾“å‡ºåŠ¨ä½œè¢«ç¯å¢ƒç›´æ¥æ‰§è¡Œã€‚æœªè€ƒè™‘ç°å®éƒ¨ç½²ä¸­å¿…ç„¶å­˜åœ¨çš„**å®‰å…¨çº¦æŸæ§åˆ¶æ ˆ**ï¼ˆå¦‚é€Ÿåº¦é™åˆ¶ã€ç¢°æ’é¿å…ï¼‰å¯¹æ¶æ„åŠ¨ä½œçš„è¿‡æ»¤ï¼ˆå³â€œè¡°å‡â€æ•ˆåº”ï¼‰ã€‚\n\n### 2. æå‡æ”»å‡»æ•ˆç‡ä¸éšè”½æ€§ï¼šç¨€ç–åŒ–ä¸å…³é”®çŠ¶æ€æ”»å‡»\n*   **ä»£è¡¨å·¥ä½œï¼šBadRL (Cui et al., 2024)**\n*   **æ ¸å¿ƒæ”¹è¿›ï¼š** è®¤è¯†åˆ°å¹¶éæ‰€æœ‰è®­ç»ƒçŠ¶æ€å¯¹ç­–ç•¥æ›´æ–°çš„å½±å“éƒ½ç›¸åŒã€‚æå‡º**ä»…å¯¹å†³ç­–å…³é”®çŠ¶æ€è¿›è¡Œæ¯’åŒ–**ï¼Œç”¨æ›´å°‘çš„æ¯’åŒ–æ ·æœ¬ï¼ˆæ›´ä½çš„æŠ•æ¯’é¢„ç®—ï¼‰å®ç°é«˜æ”»å‡»æˆåŠŸç‡ã€‚è¿™æå‡äº†æ”»å‡»çš„éšè”½æ€§å’Œæ•°æ®æ•ˆç‡ã€‚\n*   **å±€é™æ€§ï¼š** å°½ç®¡æ•ˆç‡æå‡ï¼Œä½†**ä»åœ¨ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°**ã€‚å…¶ä½¿ç”¨çš„æ•°å­—è§¦å‘å™¨åœ¨ç°å®ä¸–ç•Œçš„è§†è§‰å˜åŒ–ï¼ˆå…‰ç…§ã€è§†è§’ã€é®æŒ¡ï¼‰ä¸‹å¯èƒ½å¤±æ•ˆï¼Œä¸”æœªè€ƒè™‘å®‰å…¨æ§åˆ¶æ ˆçš„è¡°å‡æ•ˆåº”ã€‚\n\n### 3. è·¨ç®—æ³•ä¸è¡¨ç¤ºå±‚æ”»å‡»ï¼šé€šç”¨æ€§æå‡\n*   **ä»£è¡¨å·¥ä½œï¼šSleeperNets (Rathbun et al., 2024)**\n*   **æ ¸å¿ƒæ”¹è¿›ï¼š** å°†åé—¨æ”»å‡»æ„å»ºä¸ºä¸€ä¸ª**å¤–å±‚ä¼˜åŒ–é—®é¢˜**ï¼Œåœ¨è®­ç»ƒè½¨è¿¹å®Œæˆåé€‰æ‹©å…³é”®çŠ¶æ€å’Œå¥–åŠ±è¿›è¡Œæ¯’åŒ–ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨å®ç°**è·¨ä¸åŒRLç®—æ³•**çš„é€šç”¨åé—¨æ”»å‡»ï¼Œå¹¶å°è¯•åœ¨ç­–ç•¥çš„**è¡¨ç¤ºå±‚é¢**æ¤å…¥åé—¨ï¼Œå¯èƒ½æ›´å…·éšè”½æ€§ã€‚\n*   **å±€é™æ€§ï¼š** æ ¸å¿ƒè¯„ä¼°ç¯å¢ƒ**ä»æ˜¯ä»¿çœŸ**ã€‚è®ºæ–‡å‘ç°ï¼Œåœ¨ç°å®éƒ¨ç½²ä¸­ï¼Œè¿™ç±»æ–¹æ³•å¯¹è§†è§‰å˜åŒ–å’Œæ§åˆ¶è¿‡æ»¤éå¸¸æ•æ„Ÿï¼Œæ”»å‡»æˆåŠŸç‡å¤§å¹…ä¸‹é™ï¼ˆåœ¨æœ¬æ–‡å®éªŒä¸­ASRæœ€ä½ï¼‰ã€‚\n\n### 4. åº”å¯¹ç°å®ä¸–ç•ŒæŒ‘æˆ˜ï¼šä»ä»¿çœŸåˆ°ç‰©ç†éƒ¨ç½²çš„è·¨è¶Š\n*   **æœ¬æ–‡å·¥ä½œï¼šDGBA (Diffusion-Guided Backdoor Attacks)**\n*   **æ ¸å¿ƒæ”¹è¿›ä¸åˆ›æ–°æ–¹å‘ï¼š** æœ¬æ–‡é¦–æ¬¡æ˜ç¡®æŒ‡å‡ºå¹¶ç³»ç»Ÿæ€§åœ°è§£å†³äº†ç°å®ä¸–ç•ŒRLéƒ¨ç½²ä¸­å®‰å…¨çº¦æŸå¯¼è‡´çš„**â€œè¡°å‡â€é—®é¢˜**ï¼Œå°†ç ”ç©¶èƒŒæ™¯ä»çº¯ä»¿çœŸæ¨å‘ç‰©ç†æœºå™¨äººç³»ç»Ÿã€‚å…¶æ”¹è¿›æ˜¯**å¤šç»´åº¦ã€ç³»ç»Ÿæ€§**çš„ï¼š\n    1.  **æ”»å‡»åª’ä»‹åˆ›æ–°ï¼ˆæ„ŸçŸ¥å±‚ vs. æ§åˆ¶å±‚ï¼‰ï¼š** æ­¤å‰å·¥ä½œå¤šç›´æ¥æ¯’åŒ–çŠ¶æ€-åŠ¨ä½œå¯¹æˆ–å¥–åŠ±ã€‚DGBAé‡‡ç”¨**å¯æ‰“å°çš„ç‰©ç†è§†è§‰è¡¥ä¸**ä½œä¸ºè§¦å‘å™¨ï¼Œä½œç”¨äº**æ„ŸçŸ¥å±‚**ï¼Œä»è€Œç»•è¿‡ç›´æ¥åœ¨æ§åˆ¶è¾“å‡ºå±‚ä¸å®‰å…¨è¿‡æ»¤å™¨â€œå¯¹æŠ—â€çš„éš¾é¢˜ã€‚\n    2.  **è§¦å‘å™¨é²æ£’æ€§åˆ›æ–°ï¼š** é’ˆå¯¹ç°å®ä¸–ç•Œè§†è§‰å˜åŒ–ï¼Œé¦–æ¬¡å¼•å…¥**æ¡ä»¶æ‰©æ•£æ¨¡å‹**æ¥ç”Ÿæˆè§¦å‘å™¨ã€‚å®ƒä¸æ˜¯å­¦ä¹ ä¸€ä¸ªå›ºå®šå›¾æ¡ˆï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ä¸ª**è¡¥ä¸å¤–è§‚çš„åˆ†å¸ƒ**ï¼Œä»è€Œåœ¨å¤šæ ·çš„å…‰ç…§ã€è§†è§’ã€èƒŒæ™¯ä¸‹éƒ½èƒ½å¯é æ¿€æ´»åé—¨ã€‚\n    3.  **è®­ç»ƒç­–ç•¥å¢å¼ºï¼š** ç»§æ‰¿äº†BadRLçš„â€œå…³é”®çŠ¶æ€æ¯’åŒ–â€æ€æƒ³ï¼Œä½†å°†å…¶ä¸**ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantageï¼‰** ç»“åˆï¼Œåœ¨PPOç­‰ç®—æ³•ä¸­æ›´ç²¾å‡†åœ°å®šä½å¯¹ç­–ç•¥æ›´æ–°å½±å“æœ€å¤§çš„æ—¶åˆ»è¿›è¡Œæ¯’åŒ–ï¼Œè¿›ä¸€æ­¥æå‡æ•ˆç‡ã€‚\n    4.  **å¯¹ç°å®çº¦æŸçš„å»ºæ¨¡ï¼š** æ˜ç¡®å°†æœºå™¨äºº**å®‰å…¨æ§åˆ¶æ ˆè§†ä¸ºé»‘ç›’**ï¼Œåœ¨æ”»å‡»ä¼˜åŒ–ç›®æ ‡ä¸­ç›´æ¥ä»¥**ç»è¿‡å®‰å…¨è¿‡æ»¤åçš„æ‰§è¡ŒåŠ¨ä½œ**ä½œä¸ºå¯¹é½ç›®æ ‡ï¼Œè¿«ä½¿åé—¨å­¦ä¹ å»ç”Ÿæˆé‚£äº›å³ä½¿ç»è¿‡è¿‡æ»¤ä»èƒ½å¯¼è‡´ç›®æ ‡è¡Œä¸ºçš„ç­–ç•¥ã€‚\n    5.  **ä»¿çœŸåˆ°ç°å®çš„æ¡¥æ¢ï¼š** å¼•å…¥äº†**ç‰©ç†é£æ ¼çš„æ•°æ®å¢å¼º**ï¼ˆéšæœºç¼©æ”¾ã€é€è§†æ‰­æ›²ã€äº®åº¦å¯¹æ¯”åº¦æŠ–åŠ¨ç­‰ï¼‰ï¼Œä½¿åœ¨ä»¿çœŸä¸­è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œç­–ç•¥èƒ½æ›´å¥½åœ°æ³›åŒ–åˆ°çœŸå®ç‰©ç†ä¸–ç•Œã€‚\n\n### å‘å±•è„‰ç»œæ€»ç»“ä¸æ ¸å¿ƒæ–¹å‘\næ•´ä¸ªå‘å±•è„‰ç»œæ¸…æ™°åœ°å‘ˆç°å‡ºä»**ç†è®ºéªŒè¯** -> **æ•ˆç‡ä¼˜åŒ–** -> **ç°å®è¿ç§»**çš„æ¼”è¿›è·¯å¾„ã€‚\n\nç€é‡äºä»¥ä¸‹å‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ï¼š\n1.  **æ”»å‡»æœ‰æ•ˆæ€§çš„åœºæ™¯è¿ç§»ï¼š** ä»**æ— çº¦æŸçš„ä»¿çœŸç¯å¢ƒ** å‘å±•åˆ° **å—å®‰å…¨çº¦æŸçš„ç°å®ç‰©ç†ç³»ç»Ÿ**ã€‚è¿™æ˜¯æœ¬æ–‡æœ€æ ¸å¿ƒçš„è´¡çŒ®ï¼Œå¼€è¾Ÿäº†åé—¨å®‰å…¨ç ”ç©¶çš„æ–°æˆ˜åœºã€‚\n2.  **æ”»å‡»æ•ˆç‡ä¸éšè”½æ€§çš„æå‡ï¼š** ä»**å…¨é‡/éšæœºæ¯’åŒ–** å‘å±•åˆ° **åŸºäºå…³é”®çŠ¶æ€çš„ç¨€ç–æ¯’åŒ–**ã€‚åœ¨æœ‰é™çš„æŠ•æ¯’æƒé™ä¸‹æœ€å¤§åŒ–æ”»å‡»å½±å“ã€‚\n3.  **è§¦å‘å™¨è®¾è®¡çš„æ¼”è¿›ï¼š** ä»**æ•°å­—åŸŸã€å›ºå®šçš„è§¦å‘å™¨** å‘å±•åˆ° **ç‰©ç†åŸŸã€å¤šæ ·åŒ–çš„è§¦å‘å™¨**ã€‚åˆ©ç”¨ç”Ÿæˆå¼AIï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰è§£å†³ç°å®ä¸–ç•Œæ„ŸçŸ¥çš„ä¸ç¡®å®šæ€§ã€‚\n4.  **æ”»å‡»ç›®æ ‡çš„æ˜ç¡®åŒ–ï¼š** ä»è¿½æ±‚**ç­–ç•¥è¾“å‡ºå±‚çš„æ¶æ„åŠ¨ä½œ** è½¬å˜ä¸ºè¿½æ±‚ **æœ€ç»ˆæ‰§è¡Œå±‚çš„æ¶æ„è¡Œä¸º**ã€‚æ‰¿è®¤å¹¶ç»•è¿‡å®‰å…¨ä¸­é—´ä»¶ï¼Œä½¿æ”»å‡»å®šä¹‰æ›´è´´åˆå®é™…å¨èƒæ¨¡å‹ã€‚\n\n**å®é™…ä»·å€¼ä¸å¯ç¤ºï¼š**\næœ¬æ–‡è¡¨æ˜ï¼Œåœ¨ä»¿çœŸä¸­æœ‰æ•ˆçš„åé—¨æ”»å‡»åœ¨ç°å®ç³»ç»Ÿä¸­å¯èƒ½å› å®‰å…¨æœºåˆ¶è€Œå¤±æ•ˆï¼Œä½†è¿™**ä¸æ„å‘³ç€ç³»ç»Ÿå®‰å…¨**ã€‚æ”»å‡»è€…ä¼šç›¸åº”åœ°å°†æ”»å‡»é“¾å‰ç§»è‡³æ„ŸçŸ¥å±‚ï¼Œå¹¶é‡‡ç”¨æ›´é²æ£’çš„æŠ€æœ¯ã€‚è¿™è¦æ±‚RLç³»ç»Ÿçš„å®‰å…¨è¯„ä¼°å¿…é¡»**ä»ä»¿çœŸæ‰©å±•åˆ°çœŸå®éƒ¨ç½²ï¼Œå¹¶æ¶µç›–æ„ŸçŸ¥ã€å†³ç­–ã€æ§åˆ¶å…¨é“¾è·¯**ã€‚DGBAä¸ºç†è§£å’Œé˜²å¾¡ç°å®ä¸–ç•Œçš„RLåé—¨æ”»å‡»æä¾›äº†ä¸€ä¸ªé‡è¦çš„åŸºå‡†å’Œæ–°çš„æ€è·¯ã€‚",
            "timestamp": "2026-01-27T15:55:36.615371",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## è®ºæ–‡å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼Œæœ¬æ–‡æå‡ºçš„ **DGBAï¼ˆDiffusion-Guided Backdoor Attacksï¼‰** ä¸»è¦é’ˆå¯¹**çœŸå®ä¸–ç•Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„åé—¨æ”»å‡»**é—®é¢˜ã€‚æ–‡ä¸­æ˜ç¡®æåˆ°äº†å‡ ç¯‡å…³é”®çš„**å‰ä½œ**ï¼ˆå³ç°æœ‰çš„RLåé—¨æ”»å‡»æ–¹æ³•ï¼‰ï¼Œå®ƒä»¬éƒ½æ˜¯åœ¨**ä»¿çœŸç¯å¢ƒ**ä¸­å¼€å‘å’Œè¯„ä¼°çš„ï¼Œè€Œæœ¬æ–‡çš„æ ¸å¿ƒè´¡çŒ®æ˜¯å°†æ”»å‡»æ‰©å±•åˆ°**çœŸå®ç‰©ç†æœºå™¨äººç³»ç»Ÿ**ï¼Œå¹¶å…‹æœäº†å®‰å…¨çº¦æŸæ§åˆ¶æ ˆå¸¦æ¥çš„**è¡°å‡ï¼ˆAttenuationï¼‰** é—®é¢˜ã€‚\n\nä»¥ä¸‹æ˜¯è¿™äº›å‰ä½œçš„è¯¦ç»†å¯¹æ¯”åˆ†æï¼š\n\n| å‰ä½œåç§° | æå‡ºå¹´ä»½ | æ ¸å¿ƒæ–¹æ³• | æ”»å‡»åœºæ™¯ | ä¸»è¦å±€é™ | ä¸æœ¬æ–‡ï¼ˆDGBAï¼‰çš„åŒºåˆ«ä¸æ”¹å–„ |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **TrojDRL** | 2019 | åœ¨è®­ç»ƒæ•°æ®ä¸­æ’å…¥è§¦å‘æ ·æœ¬å¹¶ä¿®æ”¹å¥–åŠ±ä¿¡å·ï¼Œä»¥è¯±å¯¼ç­–ç•¥äº§ç”Ÿç›®æ ‡é”™è¯¯è¡Œä¸ºã€‚ | ä»¿çœŸç¯å¢ƒï¼ˆå¦‚Atariï¼‰ã€‚å‡è®¾ç­–ç•¥è¾“å‡ºç›´æ¥æ‰§è¡Œã€‚ | 1. **å›ºå®šè§¦å‘æ¨¡å¼**ï¼šå¯¹çœŸå®ä¸–ç•Œçš„è§†è§‰å˜åŒ–ï¼ˆå…‰ç…§ã€è§†è§’ï¼‰é²æ£’æ€§å·®ã€‚<br>2. **æœªè€ƒè™‘æ§åˆ¶æ ˆ**ï¼šæ”»å‡»åœ¨ä»¿çœŸä¸­æœ‰æ•ˆï¼Œä½†ä¼šè¢«çœŸå®æœºå™¨äººçš„å®‰å…¨æ§åˆ¶æ ˆï¼ˆå¦‚é€Ÿåº¦é™åˆ¶ã€ç¢°æ’é¿å…ï¼‰è¿‡æ»¤å’ŒæŠ‘åˆ¶ã€‚ | **åŒºåˆ«ä¸æ”¹å–„**ï¼š<br>1. **è§¦å‘å½¢å¼**ï¼šTrojDRLä½¿ç”¨æ•°å­—è§¦å‘å™¨ï¼ˆå¦‚åƒç´ æ¨¡å¼ï¼‰ï¼Œè€ŒDGBAä½¿ç”¨**å¯æ‰“å°çš„ç‰©ç†åœ°æ¿è¡¥ä¸**ä½œä¸ºè§¦å‘å™¨ã€‚<br>2. **ç”Ÿæˆæ–¹å¼**ï¼šTrojDRLè§¦å‘å™¨æ˜¯å›ºå®šçš„ï¼ŒDGBAä½¿ç”¨**æ¡ä»¶æ‰©æ•£æ¨¡å‹**ç”Ÿæˆå¤šæ ·åŒ–çš„è¡¥ä¸å¤–è§‚ï¼Œä»¥é€‚åº”çœŸå®ä¸–ç•Œè§†è§‰å˜åŒ–ã€‚<br>3. **æ”»å‡»ç›®æ ‡**ï¼šTrojDRLä¼˜åŒ–ä»¿çœŸä¸­çš„ç­–ç•¥åŠ¨ä½œï¼ŒDGBAç›´æ¥ä¼˜åŒ–**ç»è¿‡å®‰å…¨æ§åˆ¶æ ˆè¿‡æ»¤åçš„æ‰§è¡ŒåŠ¨ä½œ**ï¼Œç¡®ä¿æ”»å‡»åœ¨çœŸå®éƒ¨ç½²ä¸­æœ‰æ•ˆã€‚ |\n| **BadRL** | 2024 | è¯†åˆ«å¯¹ç­–ç•¥æ›´æ–°å½±å“æœ€å¤§çš„**å…³é”®çŠ¶æ€**ï¼Œä»…åœ¨è¿™äº›çŠ¶æ€è¿›è¡Œç¨€ç–æ¯’åŒ–ï¼Œä»¥æé«˜æ”»å‡»æ•ˆç‡ã€‚ | ä»¿çœŸç¯å¢ƒã€‚ä¸“æ³¨äºå‡å°‘æ¯’åŒ–æ•°æ®é‡ã€‚ | 1. **æ•°å­—è§¦å‘å™¨**ï¼šåŒæ ·é¢ä¸´sim-to-realçš„è§†è§‰å·®å¼‚é—®é¢˜ã€‚<br>2. **åŠ¨ä½œå±‚é¢æ”»å‡»**ï¼šæ”»å‡»ç›®æ ‡æ˜¯ç­–ç•¥è¾“å‡ºçš„åŸå§‹åŠ¨ä½œï¼Œæœªè€ƒè™‘è¿™äº›åŠ¨ä½œè¢«å®‰å…¨æ§åˆ¶å™¨ä¿®æ”¹åçš„æœ€ç»ˆæ•ˆæœã€‚ | **åŒºåˆ«ä¸æ”¹å–„**ï¼š<br>1. **æ”»å‡»é²æ£’æ€§**ï¼šç»§æ‰¿äº†â€œå…³é”®çŠ¶æ€æ¯’åŒ–â€çš„æ€æƒ³ï¼ˆæœ¬æ–‡æ”¹è¿›ä¸º**åŸºäºä¼˜åŠ¿å€¼çš„æ¯’åŒ–ç­–ç•¥**ï¼‰ï¼Œä½†å°†å…¶ä¸**æ‰©æ•£ç”Ÿæˆçš„ç‰©ç†è§¦å‘å™¨**ç»“åˆï¼Œè§£å†³äº†BadRLæ•°å­—è§¦å‘å™¨åœ¨çœŸå®ä¸–ç•Œä¸­å¤±æ•ˆçš„é—®é¢˜ã€‚<br>2. **ç³»ç»Ÿè§†è§’**ï¼šBadRLåœ¨ä»¿çœŸä¸­éªŒè¯æ”»å‡»æˆåŠŸç‡ï¼Œè€ŒDGBAåœ¨**çœŸå®æœºå™¨äººï¼ˆTurtleBot3ï¼‰** ä¸ŠéªŒè¯ï¼Œå¹¶æ˜ç¡®å°†æ§åˆ¶æ ˆè§†ä¸ºé»‘ç›’ï¼Œæ”»å‡»æˆåŠŸä¸å¦ä»¥æœ€ç»ˆæ‰§è¡ŒåŠ¨ä½œä¸ºå‡†ã€‚ |\n| **SleeperNets** | 2024 | å°†åé—¨æ”»å‡»å»ºæ¨¡ä¸ºä¸€ä¸ª**å¤–å±‚å¾ªç¯ä¼˜åŒ–é—®é¢˜**ï¼Œåœ¨ç­–ç•¥æ›´æ–°å‰é€‰æ‹©å…³é”®çŠ¶æ€å’Œå¥–åŠ±è¿›è¡Œæ¯’åŒ–ï¼Œå®ç°è·¨RLç®—æ³•çš„é«˜æ•ˆæ•°æ®æ¯’åŒ–ã€‚ | ä»¿çœŸç¯å¢ƒã€‚ä¾§é‡äºè¡¨ç¤ºå±‚é¢çš„åé—¨æ”»å‡»ã€‚ | 1. **å¯¹æ§åˆ¶è¿‡æ»¤æ•æ„Ÿ**ï¼šè®ºæ–‡æŒ‡å‡ºå…¶è¡¨ç¤ºçº§åé—¨å¯¹çœŸå®ä¸–ç•Œçš„è§†è§‰å˜åŒ–å’Œæ§åˆ¶è¿‡æ»¤éå¸¸æ•æ„Ÿã€‚<br>2. **ä»¿çœŸå‡è®¾**ï¼šå…¶é«˜æ•ˆæ€§å»ºç«‹åœ¨ä»¿çœŸçš„ç†æƒ³æ¡ä»¶ä¸Šï¼Œåœ¨ç‰©ç†éƒ¨ç½²ä¸­è¡°å‡ä¸¥é‡ã€‚ | **åŒºåˆ«ä¸æ”¹å–„**ï¼š<br>1. **æ„ŸçŸ¥å±‚ vs è¡¨ç¤ºå±‚**ï¼šSleeperNetsåœ¨ç­–ç•¥çš„ä¸­é—´è¡¨ç¤ºä¸Šåšæ–‡ç« ï¼Œè€ŒDGBAç›´æ¥åœ¨**æ„ŸçŸ¥è¾“å…¥å±‚ï¼ˆè§†è§‰å›¾åƒï¼‰** æ¤å…¥ç‰©ç†è§¦å‘å™¨ï¼Œæ”»å‡»æ›´åº•å±‚ã€æ›´ç›´æ¥ã€‚<br>2. **ç‰©ç†é€‚åº”æ€§**ï¼šDGBAé€šè¿‡**ç‰©ç†é£æ ¼å¢å¼º**ï¼ˆå‡ ä½•å˜å½¢ã€å…‰ç…§æŠ–åŠ¨ç­‰ï¼‰ explicitly æ¨¡æ‹ŸçœŸå®ç›¸æœºæˆåƒè¿‡ç¨‹ï¼Œä½¿ç”Ÿæˆçš„è§¦å‘å™¨å¯¹æ‰“å°éƒ¨ç½²åçš„å˜åŒ–å…·æœ‰ä¸å˜æ€§ï¼Œè¿™æ˜¯SleeperNetsä¸å…·å¤‡çš„ã€‚ |\n\n## æ€»ç»“ï¼šæœ¬æ–‡ç›¸å¯¹äºå‰ä½œçš„ä¸»è¦å…·ä½“æ”¹åŠ¨ä¸æ”¹å–„\n\n1.  **é—®é¢˜è®¾å®šæ ¹æœ¬æ€§è½¬å˜**ï¼š\n    - **å‰ä½œ**ï¼šç ”ç©¶â€œä»¿çœŸRLä¸­çš„åé—¨æ”»å‡»â€ã€‚\n    - **æœ¬æ–‡**ï¼šç ”ç©¶â€œ**çœŸå®ä¸–ç•ŒRLéƒ¨ç½²**ä¸­ï¼Œåœ¨**å®‰å…¨çº¦æŸæ§åˆ¶æ ˆ**ä¸‹çš„åé—¨æ”»å‡»â€ã€‚é¦–æ¬¡æ˜ç¡®æå‡ºå¹¶ç³»ç»Ÿç ”ç©¶äº†**è¡°å‡ï¼ˆAttenuationï¼‰** ç°è±¡ã€‚\n\n2.  **è§¦å‘å™¨è®¾è®¡çš„é©å‘½æ€§æ”¹è¿›**ï¼š\n    - **ä»æ•°å­—åˆ°ç‰©ç†**ï¼šå°†è§¦å‘å™¨ä»ä»¿çœŸä¸­çš„åƒç´ æ¨¡å¼ï¼Œå˜ä¸º**å¯æ‰“å°ã€å¯æ”¾ç½®åœ¨çœŸå®ç¯å¢ƒä¸­çš„åœ°æ¿è¡¥ä¸**ã€‚\n    - **ä»ç¡®å®šåˆ°éšæœº**ï¼šåˆ©ç”¨**æ¡ä»¶æ‰©æ•£æ¨¡å‹**ï¼Œå­¦ä¹ ä¸€ä¸ªè§¦å‘å™¨è¡¥ä¸çš„**åˆ†å¸ƒ**ï¼Œè€Œéå•ä¸ªå›ºå®šå›¾æ¡ˆã€‚è¿™ä½¿å¾—æ”»å‡»èƒ½é€‚åº”æœºå™¨äººä¸åŒçŠ¶æ€ã€è§†è§’ã€å…‰ç…§ç­‰ä¸å¯æ§çš„è§†è§‰å˜åŒ–ã€‚\n\n3.  **æ”»å‡»ç›®æ ‡ä¸è¯„ä¼°æ ‡å‡†çš„é‡æ–°å®šä¹‰**ï¼š\n    - **å‰ä½œ**ï¼šä»¥ç­–ç•¥è¾“å‡ºçš„åŸå§‹åŠ¨ä½œ `a_t` ä½œä¸ºæ”»å‡»æˆåŠŸæ ‡å‡†ã€‚\n    - **æœ¬æ–‡**ï¼šä»¥ç»è¿‡é»‘ç›’å®‰å…¨æ§åˆ¶æ ˆ `ğ’(Â·)` è¿‡æ»¤åçš„**å®é™…æ‰§è¡ŒåŠ¨ä½œ `Ã£_t`** ä½œä¸ºæ”»å‡»æˆåŠŸæ ‡å‡†ã€‚è¿™è¿«ä½¿æ”»å‡»æ–¹æ³•å¿…é¡»ç”Ÿæˆèƒ½â€œç©¿é€â€æˆ–â€œé€‚åº”â€å®‰å…¨è¿‡æ»¤å™¨çš„æ¶æ„è¡Œä¸ºã€‚\n\n4.  **è®­ç»ƒè¿‡ç¨‹çš„å¢å¼ºä¸ä¼˜åŒ–**ï¼š\n    - **å¼•å…¥ç‰©ç†é£æ ¼å¢å¼º**ï¼šåœ¨æ¯’åŒ–è®­ç»ƒæ—¶ï¼Œå¯¹æ‰©æ•£ç”Ÿæˆçš„è¡¥ä¸æ–½åŠ æ¨¡æ‹ŸçœŸå®ç›¸æœºæˆåƒçš„å¢å¼ºï¼ˆç¼©æ”¾ã€é€è§†ã€äº®åº¦å¯¹æ¯”åº¦æŠ–åŠ¨ã€éƒ¨åˆ†é®æŒ¡ç­‰ï¼‰ï¼Œæå¤§æå‡äº†è§¦å‘å™¨çš„ç‰©ç†ä¸–ç•Œé²æ£’æ€§ã€‚\n    - **ç»†åŒ–æ¯’åŒ–ç­–ç•¥**ï¼šåœ¨BadRLâ€œå…³é”®çŠ¶æ€â€æ€æƒ³åŸºç¡€ä¸Šï¼Œæå‡º**åŸºäºä¼˜åŠ¿å€¼ï¼ˆAdvantageï¼‰çš„æ¯’åŒ–ç­–ç•¥**ï¼Œæ›´ç²¾å‡†åœ°åœ¨PPO/TRPOç­‰ç­–ç•¥æ¢¯åº¦ç®—æ³•ä¸­å®šä½å¯¹æ›´æ–°å½±å“æœ€å¤§çš„æ—¶åˆ»è¿›è¡Œæ”»å‡»ï¼Œåœ¨æä½æ¯’åŒ–ç‡ï¼ˆ5%ï¼‰ä¸‹ä¿æŒé«˜æ•ˆã€‚\n\n5.  **å®éªŒéªŒè¯å¹³å°å‡çº§**ï¼š\n    - **å‰ä½œ**ï¼šä¸»è¦åœ¨Atariç­‰ä»¿çœŸåŸºå‡†æµ‹è¯•ã€‚\n    - **æœ¬æ–‡**ï¼šåœ¨**çœŸå®çš„TurtleBot3ç§»åŠ¨æœºå™¨äººå¹³å°**ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒä¸éƒ¨ç½²éªŒè¯ï¼Œä½¿ç”¨äº†çœŸå®çš„ROSå¯¼èˆªæ ˆä½œä¸ºå®‰å…¨çº¦æŸæ§åˆ¶å™¨ï¼Œç»“æœå…·æœ‰é«˜åº¦çš„ç°å®è¯´æœåŠ›ã€‚\n\n**æ ¸å¿ƒç»“è®º**ï¼šæœ¬æ–‡ä¸æ˜¯å¯¹å‰ä½œæ–¹æ³•çš„ç®€å•æ”¹è¿›ï¼Œè€Œæ˜¯é’ˆå¯¹ä¸€ä¸ª**å…¨æ–°çš„ã€æ›´ç°å®çš„å¨èƒåœºæ™¯**ï¼Œæå‡ºäº†ä¸€å¥—**ä»è§¦å‘å™¨è®¾è®¡ã€ç”Ÿæˆã€åˆ°è®­ç»ƒç­–ç•¥ã€æœ€ç»ˆè¯„ä¼°**çš„å®Œæ•´æ–°æ¡†æ¶ï¼ˆDGBAï¼‰ã€‚å…¶æ‰€æœ‰â€œæ”¹å–„â€éƒ½ç´§ç´§å›´ç»•ä¸€ä¸ªä¸­å¿ƒï¼š**ä½¿åé—¨æ”»å‡»åœ¨å­˜åœ¨å®‰å…¨çº¦æŸæ§åˆ¶ã€å……æ»¡è§†è§‰ä¸ç¡®å®šæ€§çš„çœŸå®æœºå™¨äººç³»ç»Ÿä¸­ä¾ç„¶æœ‰æ•ˆ**ã€‚å®éªŒç»“æœï¼ˆASRä»åŸºçº¿æœ€é«˜57%æå‡è‡³83.5%ï¼‰å¼ºæœ‰åŠ›åœ°è¯æ˜äº†è¿™å¥—æ–¹æ³•çš„å¿…è¦æ€§å’Œä¼˜è¶Šæ€§ã€‚",
            "timestamp": "2026-01-27T15:56:29.110454",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "å¥½çš„ï¼Œä½œä¸ºä¸€åä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæˆ‘å°†å¯¹è¿™ç¯‡é¢˜ä¸ºã€ŠDiffusion-Guided Backdoor Attacks in Real-World Reinforcement Learningã€‹çš„è®ºæ–‡è¿›è¡Œæ·±å…¥åˆ†æï¼Œå¹¶è¯¦ç»†é˜è¿°å…¶æå‡ºçš„å…³é”®æŠ€æœ¯æ–¹æ³•ã€‚\n\n## è®ºæ–‡æ ¸å¿ƒé—®é¢˜ä¸æŠ€æœ¯åˆ›æ–°\n\n**æ ¸å¿ƒé—®é¢˜**ï¼šç°æœ‰å¼ºåŒ–å­¦ä¹ åé—¨æ”»å‡»ç ”ç©¶å¤§å¤šåœ¨ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œï¼Œå…¶å‡è®¾ç­–ç•¥è¾“å‡ºå¯ç›´æ¥ä½œç”¨äºç¯å¢ƒã€‚ç„¶è€Œï¼Œåœ¨**ç°å®ä¸–ç•Œæœºå™¨äººéƒ¨ç½²**ä¸­ï¼Œç­–ç•¥äº§ç”Ÿçš„æ§åˆ¶å‘½ä»¤ä¼šç»è¿‡ä¸€ä¸ª**å®‰å…¨çº¦æŸæ§åˆ¶æ ˆ**ï¼ˆå¦‚é€Ÿåº¦é™åˆ¶ã€åŠ¨ä½œå¹³æ»‘ã€ç¢°æ’é¿å…ï¼‰çš„è¿‡æ»¤å’Œä¿®æ­£ã€‚è¿™ç§è¿‡æ»¤ä¼šæŠ‘åˆ¶å¼‚å¸¸æˆ–å±é™©çš„åŠ¨ä½œï¼Œå¯¼è‡´ä¼ ç»Ÿåé—¨æ”»å‡»çš„æ¶æ„è¡Œä¸ºè¢«å¤§å¹…å‰Šå¼±ç”šè‡³æ¶ˆé™¤ï¼Œä½œè€…ç§°æ­¤ç°è±¡ä¸º**â€œè¡°å‡â€**ã€‚\n\n**æŠ€æœ¯åˆ›æ–°**ï¼šä¸ºè§£å†³â€œè¡°å‡â€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ª**æ‰©æ•£å¼•å¯¼çš„åé—¨æ”»å‡»æ¡†æ¶**ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯**åœ¨æ„ŸçŸ¥å±‚é¢æ¤å…¥åé—¨**ï¼Œè€Œéç›´æ¥æ“çºµæ§åˆ¶åŠ¨ä½œã€‚é€šè¿‡ç”Ÿæˆä¸€ä¸ªåœ¨çœŸå®ä¸–ç•Œè§†è§‰å˜åŒ–ä¸‹ä»èƒ½å¯é è§¦å‘çš„ç‰©ç†è§†è§‰è§¦å‘å™¨ï¼Œå¹¶ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿å…¶åœ¨è§¦å‘åè¾“å‡ºçš„åŠ¨ä½œåœ¨ç»è¿‡å®‰å…¨æ§åˆ¶æ ˆè¿‡æ»¤åï¼Œä»èƒ½äº§ç”Ÿæ”»å‡»è€…æœŸæœ›çš„â€œæ‰§è¡Œåè¡Œä¸ºâ€ã€‚\n\n## å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¿°\n\nä»¥ä¸‹è¡¨æ ¼è¯¦ç»†è¯´æ˜äº†DGBAæ¡†æ¶ä¸­çš„å…³é”®æŠ€æœ¯ç»„ä»¶åŠå…¶ç»†èŠ‚ï¼š\n\n| æ–¹æ³•ç»„ä»¶ | æŠ€æœ¯ç›®æ ‡ | å…·ä½“æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç† | å…³é”®å‚æ•°ä¸å®ç° |\n| :--- | :--- | :--- | :--- |\n| **1. ç‰©ç†åœ°æ¿è¡¥ä¸è§¦å‘å™¨** | æ„å»ºä¸€ä¸ªå¯åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ã€å¯æ‰“å°çš„ç‰©ç†è§¦å‘å™¨ï¼Œæœ€å°åŒ–å¯¹åœºæ™¯çš„å¹²æ‰°ã€‚ | - **è§¦å‘å™¨å‡½æ•°**ï¼šç»™å®šåŸå§‹è§†è§‰è§‚æµ‹ \\(x_t \\in \\mathbb{R}^{H \\times W \\times C}\\) å’Œä¸€ä¸ªå®šä¹‰è¡¥ä¸åŒºåŸŸçš„äºŒå€¼æ©ç  \\(M \\in \\{0,1\\}^{H \\times W}\\)ï¼Œè§¦å‘åçš„å›¾åƒ \\(\\tilde{x}_t\\) é€šè¿‡åƒç´ çº§æ··åˆç”Ÿæˆï¼š <br> \\(\\tilde{x}_t = (1 - M) \\odot x_t + M \\odot p_t\\) <br> å…¶ä¸­ \\(p_t\\) æ˜¯è¡¥ä¸å›¾æ¡ˆï¼Œ\\(\\odot\\) è¡¨ç¤ºé€å…ƒç´ ç›¸ä¹˜ã€‚ <br> - è§¦å‘åçš„ç­–ç•¥è¾“å…¥ä¸º \\(\\tilde{o}_t = (\\tilde{x}_t, s_t)\\)ï¼Œå…¶ä¸­ \\(s_t\\) ä¸ºä¸å¯æ§çš„è¾…åŠ©ä¼ æ„Ÿå™¨è¯»æ•°ï¼ˆå¦‚æ¿€å…‰é›·è¾¾ï¼‰ã€‚ | - **è¡¥ä¸å°ºå¯¸**ï¼šåœ¨ç­–ç•¥è¾“å…¥åˆ†è¾¨ç‡ï¼ˆ84x84ï¼‰ä¸‹ï¼Œè¡¥ä¸åŒºåŸŸä¸º \\(16 \\times 16\\) åƒç´ ï¼Œä½äºå›¾åƒä¸‹éƒ¨ï¼ˆå¯¹åº”åœ°æ¿åŒºåŸŸï¼‰ã€‚ <br> - **éƒ¨ç½²å°ºå¯¸**ï¼šè¡¥ä¸ä¸Šé‡‡æ ·ä¸º \\(128 \\times 128\\) çš„RGBå›¾åƒè¿›è¡Œæ‰“å°ã€‚ <br> - **æ©ç M**ï¼šå›ºå®šä½ç½®å’Œå¤§å°çš„äºŒå€¼çŸ©é˜µã€‚ |\n| **2. æ¡ä»¶æ‰©æ•£è¡¥ä¸ç”Ÿæˆå™¨** | ç”Ÿæˆä¸€ä¸ª**è¡¥ä¸å¤–è§‚çš„åˆ†å¸ƒ**ï¼Œè€Œéå•ä¸€å›ºå®šå›¾æ¡ˆï¼Œä»¥åº”å¯¹çœŸå®ä¸–ç•Œä¸­ä¸å¯æ§çš„è§†è§‰å˜åŒ–ï¼ˆå¦‚è§†è§’ã€å…‰ç…§ã€èƒŒæ™¯çº¹ç†ï¼‰å’Œè¾…åŠ©çŠ¶æ€ \\(s_t\\) çš„å·®å¼‚ï¼Œç¡®ä¿è§¦å‘å¯é æ€§ã€‚ | - **é‡‡ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹**ã€‚å‰å‘è¿‡ç¨‹é€æ­¥æ·»åŠ é«˜æ–¯å™ªå£°ï¼š <br> \\(q(p_k \\mid p_0) = \\mathcal{N}(p_k; \\sqrt{\\bar{\\alpha}_k} p_0, (1-\\bar{\\alpha}_k)I)\\) <br> å…¶ä¸­ \\(p_0\\) æ˜¯å¹²å‡€è¡¥ä¸ï¼Œ\\(k\\) æ˜¯æ‰©æ•£æ­¥æ•°ï¼Œ\\(\\bar{\\alpha}_k = \\prod_{i=1}^{k} \\alpha_i\\) æ˜¯å™ªå£°è°ƒåº¦è¡¨çš„ç´¯ç§¯ä¹˜ç§¯ã€‚ <br> - **åå‘å»å™ªè¿‡ç¨‹**ç”±ä¸€ä¸ªç¥ç»ç½‘ç»œ \\(\\epsilon_\\psi\\) å­¦ä¹ ï¼Œè¯¥ç½‘ç»œä»¥å¸¦å™ªè¡¥ä¸ \\(p_k\\)ã€å½“å‰è§†è§‰è§‚æµ‹ \\(x_t\\) å’Œæ­¥æ•° \\(k\\) ä¸ºæ¡ä»¶ï¼Œé¢„æµ‹æ³¨å…¥çš„å™ªå£° \\(\\epsilon\\)ã€‚è®­ç»ƒç›®æ ‡ä¸ºæ ‡å‡†å™ªå£°é¢„æµ‹æŸå¤±ï¼š <br> \\(\\mathcal{L}_{\\text{diff}}(\\psi) = \\mathbb{E}_{k, p_0, \\epsilon} \\| \\epsilon - \\epsilon_\\psi(p_k, x_t, k) \\|^2\\) <br> - **åœ¨è®­ç»ƒä¸­**ï¼Œå¸¦å™ªè¡¥ä¸é€šè¿‡é‡å‚æ•°åŒ–æŠ€å·§å¾—åˆ°ï¼š \\(p_k = \\sqrt{\\bar{\\alpha}_k} p_0 + \\sqrt{1-\\bar{\\alpha}_k} \\epsilon\\)ã€‚ <br> - **åœ¨å¾®è°ƒä¸­**ï¼Œåé—¨æ¤å…¥çš„ç›®æ ‡æ˜¯ä¼˜åŒ–ï¼š <br> \\(\\mathbb{E}_{p_k \\sim p_\\psi(p \\mid x_t)} [ \\mathcal{L}_{\\text{tar}}(\\theta; p_k) ]\\) <br> è¿™ä½¿å¾—ç­–ç•¥å­¦ä¹ å°†**ä¸€ç»„è¡¥ä¸å¤–è§‚**ä¸ç›®æ ‡è¡Œä¸ºå…³è”ã€‚ | - **ç½‘ç»œç»“æ„**ï¼šæ¡ä»¶U-Netã€‚ <br> - **è¾“å…¥æ¡ä»¶**ï¼šå½“å‰è§‚æµ‹å›¾åƒ \\(x_t\\) å’Œæ‰©æ•£æ­¥æ•° \\(k\\)ã€‚ <br> - **è¾“å‡º**ï¼š\\(16 \\times 16\\) çš„è¡¥ä¸å›¾æ¡ˆã€‚ <br> - **å…³é”®æ€æƒ³**ï¼šæ‰©æ•£æ¨¡å‹ä¼šä¸ºé‚£äº›åœ¨**ç»è¿‡é»‘ç›’æ§åˆ¶æ ˆ \\(\\mathcal{C}\\) æ‰§è¡Œå**ä»èƒ½**ä¸€è‡´**å¼•å‘ç›®æ ‡è¡Œä¸ºçš„è¡¥ä¸å¤–è§‚ \\(p_t\\) åˆ†é…æ›´é«˜çš„æ¦‚ç‡ï¼Œä»è€Œéšå¼åœ°å…‹æœäº†â€œè¡°å‡â€ã€‚ |\n| **3. ç‰©ç†é£æ ¼è¡¥ä¸å¢å¼º** | å¼¥è¡¥ä»¿çœŸä¸ç°å®ä¹‹é—´çš„è§†è§‰å·®å¼‚ï¼ˆsim-to-real gapï¼‰ï¼Œæé«˜è¡¥ä¸åœ¨çœŸå®ç‰©ç†ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚ | - å¯¹æ‰©æ•£é‡‡æ ·å¾—åˆ°çš„è¡¥ä¸ \\(p_t\\) åº”ç”¨å¢å¼ºç®—å­ \\(\\mathcal{A}\\)ï¼š <br> \\(p_t^{aug} = \\mathcal{A}(p_t)\\) <br> - \\(\\mathcal{A}\\) æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å›¾åƒå½¢æˆè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š <br> 1. **å‡ ä½•å˜æ¢**ï¼šéšæœºç¼©æ”¾ï¼ˆæ¨¡æ‹Ÿè·ç¦»å˜åŒ–ï¼‰ã€é€è§†æ‰­æ›²ï¼ˆæ¨¡æ‹Ÿè§†è§’å˜åŒ–ï¼‰ã€å¹³é¢å†…æ—‹è½¬ï¼ˆæ¨¡æ‹Ÿè´´ç‰‡æœªå¯¹é½ï¼‰ã€‚ <br> 2. **å…‰åº¦å˜æ¢**ï¼šäº®åº¦å’Œå¯¹æ¯”åº¦æŠ–åŠ¨ï¼ˆæ¨¡æ‹Ÿå…‰ç…§å˜åŒ–ï¼‰ã€é¢œè‰²æ‰°åŠ¨ï¼ˆæ¨¡æ‹Ÿä¼ æ„Ÿå™¨å·®å¼‚ï¼‰ã€‚ <br> 3. **éƒ¨åˆ†é®æŒ¡**ï¼šæ¨¡æ‹Ÿæ±¡æ¸ã€ç£¨æŸæˆ–ç¯å¢ƒæ‚ç‰©ã€‚ | - **å¢å¼ºç±»å‹**ï¼šç»„åˆä½¿ç”¨ä¸Šè¿°å‡ ä½•ä¸å…‰åº¦å˜æ¢ã€‚ <br> - **ç›®çš„**ï¼šå¼ºåˆ¶æ¨¡å‹å¯¹è¿™äº›æ‰°åŠ¨å…·æœ‰ä¸å˜æ€§ï¼Œå¢åŠ çœŸå®æ‰“å°è¡¥ä¸è¢«ä¸€è‡´è¯†åˆ«ä¸ºè§¦å‘å™¨çš„æ¦‚ç‡ã€‚ |\n| **4. åŸºäºä¼˜åŠ¿çš„æ¯’åŒ–ç­–ç•¥** | åœ¨æœ‰é™çš„æ¯’åŒ–é¢„ç®—ï¼ˆä»…èƒ½ä¿®æ”¹å°‘é‡è®­ç»ƒæ•°æ®ï¼‰ä¸‹ï¼Œæœ€å¤§åŒ–åé—¨æ¤å…¥çš„æ•ˆç‡ã€‚ | - **æ ¸å¿ƒ**ï¼šä»…æ¯’åŒ–å¯¹ç­–ç•¥æ›´æ–°å½±å“æœ€å¤§çš„â€œå†³ç­–å…³é”®çŠ¶æ€â€ã€‚ <br> - **é€‰æ‹©æ ‡å‡†**ï¼šä½¿ç”¨PPOç®—æ³•ä¸­è®¡ç®—çš„**ä¼˜åŠ¿å‡½æ•°ç»å¯¹å€¼** \\(|A_t|\\)ã€‚ä¼˜åŠ¿å‡½æ•° \\(A_t = \\hat{R}_t - V_\\phi(o_t)\\) è¡¡é‡åœ¨çŠ¶æ€ \\(o_t\\) é‡‡å–æŸä¸ªåŠ¨ä½œç›¸å¯¹äºå¹³å‡æ°´å¹³çš„ä¼˜åŠ£ã€‚\\(|A_t|\\) è¶Šå¤§ï¼Œè¯¥çŠ¶æ€-åŠ¨ä½œå¯¹ç­–ç•¥æ›´æ–°çš„å½±å“è¶Šå¤§ã€‚ <br> - **é€‰æ‹©è¿‡ç¨‹**ï¼šåœ¨æ¯ä¸ªè®­ç»ƒå›åˆä¸­ï¼Œå¯¹æ‰€æœ‰æ—¶é—´æ­¥ \\(t\\) çš„ \\(|A_t|\\) è¿›è¡Œæ’åºï¼Œä»…é€‰æ‹©æ’åå‰ \\(K\\) çš„æ­¥éª¤è¿›è¡Œæ¯’åŒ–ã€‚ \\(K = \\lfloor \\beta H \\rfloor\\)ï¼Œå…¶ä¸­ \\(H\\) æ˜¯å›åˆé•¿åº¦ï¼Œ\\(\\beta\\) æ˜¯æ¯’åŒ–æ¯”ç‡ã€‚ | - **å…³é”®å‚æ•°**ï¼šæ¯’åŒ–æ¯”ç‡ \\(\\beta\\)ã€‚è®ºæ–‡ä¸­ä¸»è¦è®¾ç½®ä¸º **5%**ï¼ˆå³ä»…æ¯’åŒ– top 5% çš„å…³é”®çŠ¶æ€ï¼‰ã€‚ <br> - **æ•ˆæœ**ï¼šå°†æ¯’åŒ–æ ·æœ¬é›†ä¸­åœ¨é«˜æ æ†çš„å­¦ä¹ ä¿¡å·ä¸Šï¼Œåœ¨ä¿æŒæ­£å¸¸ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œé«˜æ•ˆåœ°æ¤å…¥åé—¨ã€‚ |\n| **5. é»‘ç›’å®‰å…¨çº¦æŸæ§åˆ¶æ ˆå¤„ç†** | åœ¨ä¸çŸ¥é“ã€ä¹Ÿæ— æ³•ä¿®æ”¹å®‰å…¨æ§åˆ¶å™¨ \\(\\mathcal{C}\\) å†…éƒ¨é€»è¾‘çš„æƒ…å†µä¸‹ï¼Œè®¾è®¡èƒ½æŠµæŠ—å…¶â€œè¡°å‡â€æ•ˆåº”çš„æ”»å‡»ã€‚ | - **æ ¸å¿ƒå‡è®¾**ï¼šå°†æ§åˆ¶æ ˆ \\(\\mathcal{C}\\) è§†ä¸ºä¸€ä¸ª**é»‘ç›’ç³»ç»Ÿ**ï¼Œæ”»å‡»è€…æ— æ³•å¯¹å…¶æ±‚å¯¼æˆ–å»ºæ¨¡ã€‚ <br> - **æ”»å‡»ç›®æ ‡é‡æ–°å®šä¹‰**ï¼šæ”»å‡»æˆåŠŸä¸å¦ï¼Œä¸æ˜¯çœ‹ç­–ç•¥è¾“å‡ºçš„åŸå§‹åŠ¨ä½œ \\(a_t\\)ï¼Œè€Œæ˜¯çœ‹ç»è¿‡æ§åˆ¶å™¨å¤„ç†åçš„**å®é™…æ‰§è¡ŒåŠ¨ä½œ** \\(\\tilde{a}_t\\)ï¼š <br> \\(\\tilde{a}_t = \\mathcal{C}(\\pi_\\theta(\\tilde{o}_t))\\) <br> - **æ¯’åŒ–æŸå¤±å‡½æ•°**ï¼šå› æ­¤ï¼Œæ¯’åŒ–æ—¶çš„ç›®æ ‡æŸå¤±å‡½æ•°æ˜¯åŸºäºæ‰§è¡ŒåŠ¨ä½œå®šä¹‰çš„ï¼š <br> \\(\\mathcal{L}_{\\text{tar}}(\\theta) = \\mathbb{E}_{t \\in \\mathcal{I}} [ \\ell(\\tilde{a}_t, a_t^{\\text{target}}) ]\\) <br> å…¶ä¸­ \\(\\ell\\) æ˜¯è·ç¦»åº¦é‡ï¼ˆå¦‚MSEï¼‰ï¼Œ\\(\\mathcal{I}\\) æ˜¯è¢«æ¯’åŒ–çš„æ—¶é—´æ­¥é›†åˆã€‚ | - **æ— éœ€**å¯¹ \\(\\mathcal{C}\\) è¿›è¡Œæ˜¾å¼å»ºæ¨¡æˆ–å¯å¾®è¿‘ä¼¼ã€‚ <br> - **è®­ç»ƒè¿‡ç¨‹**ï¼šåœ¨ä»¿çœŸä¸­ï¼Œé€šè¿‡è°ƒç”¨ä¸çœŸå®æ§åˆ¶å™¨è¡Œä¸ºè¿‘ä¼¼çš„å®‰å…¨æ¨¡å—æ¥è®¡ç®— \\(\\tilde{a}_t\\)ï¼Œå¹¶ä»¥æ­¤è®¡ç®— \\(\\mathcal{L}_{\\text{tar}}\\) æ¥æ›´æ–°ç­–ç•¥ã€‚è¿™ä½¿å¾—æ‰©æ•£æ¨¡å‹å’Œç­–ç•¥çš„ä¼˜åŒ–è¿‡ç¨‹éƒ½**éšå¼åœ°**å­¦ä¼šäº†ç”Ÿæˆèƒ½ç»•è¿‡æˆ–é€‚åº”å®‰å…¨è¿‡æ»¤çš„è§¦å‘å™¨å’Œç­–ç•¥ã€‚ |\n| **6. ç»Ÿä¸€è®­ç»ƒä¸éƒ¨ç½²æµç¨‹** | å°†ä¸Šè¿°ç»„ä»¶æ•´åˆä¸ºä¸€ä¸ªå¯æ“ä½œçš„ç«¯åˆ°ç«¯æ”»å‡»æµç¨‹ã€‚ | **é˜¶æ®µ1ï¼ˆæ¸…æ´é¢„è®­ç»ƒï¼‰**ï¼šåœ¨ä»¿çœŸä¸­è®­ç»ƒä¸€ä¸ªæ­£å¸¸çš„PPOç­–ç•¥ \\(\\pi_{\\theta_0}\\)ï¼Œè·å¾—è‰¯å¥½çš„",
            "timestamp": "2026-01-27T15:57:40.455034",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡\nè®ºæ–‡ä½¿ç”¨ä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡æ¥è¯„ä¼°åé—¨æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œéšè”½æ€§ï¼Œå‡åœ¨**çœŸå®ç‰©ç†æœºå™¨äººï¼ˆTurtleBot3ï¼‰éƒ¨ç½²**ç¯å¢ƒä¸‹æµ‹é‡ã€‚\n\n| æŒ‡æ ‡åç§° | è‹±æ–‡å…¨ç§° | å®šä¹‰ä¸è®¡ç®—ç»†èŠ‚ | è¯„ä¼°ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **æ¸…æ´ä»»åŠ¡æˆåŠŸç‡** | Clean Success Rate (CSR) | åœ¨**æ²¡æœ‰è§¦å‘è¡¥ä¸**çš„æƒ…å†µä¸‹ï¼Œæœºå™¨äººä»å›ºå®šèµ·ç‚¹æˆåŠŸå¯¼èˆªè‡³ç›®æ ‡ç‚¹ä¸”é¿å…ç¢°æ’çš„è¯•éªŒæ¬¡æ•°å æ€»è¯•éªŒæ¬¡æ•°çš„ç™¾åˆ†æ¯”ã€‚ | è¯„ä¼°åé—¨æ”»å‡»çš„**éšè”½æ€§**ã€‚é«˜CSRè¡¨æ˜æ¤å…¥çš„åé—¨æ²¡æœ‰å½±å“ç­–ç•¥åœ¨æ­£å¸¸æƒ…å†µä¸‹çš„æ€§èƒ½ï¼Œæ”»å‡»ä¸æ˜“è¢«å¯Ÿè§‰ã€‚ |\n| **æ”»å‡»æˆåŠŸç‡** | Attack Success Rate (ASR) | åœ¨**å­˜åœ¨è§¦å‘è¡¥ä¸**çš„æƒ…å†µä¸‹ï¼Œæœºå™¨äººæ‰§è¡Œæ”»å‡»è€…æŒ‡å®šçš„**ç›®æ ‡è¡Œä¸º**çš„è¯•éªŒæ¬¡æ•°å æ€»è¯•éªŒæ¬¡æ•°çš„ç™¾åˆ†æ¯”ã€‚ç›®æ ‡è¡Œä¸ºåŒ…æ‹¬å³è½¬ã€å·¦è½¬ã€ç›´è¡Œæ’å‘éšœç¢ç‰©æˆ–å®Œå…¨åœæ­¢ã€‚ | è¯„ä¼°åé—¨æ”»å‡»çš„**æœ‰æ•ˆæ€§**ã€‚é«˜ASRè¡¨æ˜åœ¨å®‰å…¨çº¦æŸæ§åˆ¶æ ˆçš„è¿‡æ»¤ä¸‹ï¼Œåé—¨ä»èƒ½è¢«å¯é è§¦å‘å¹¶äº§ç”Ÿé¢„æœŸçš„æ¶æ„è¡Œä¸ºã€‚ |\n\n**æŒ‡æ ‡ç»†èŠ‚ä¸åˆ›æ–°ç‚¹ï¼š**\n1.  **æ‰§è¡Œå±‚å®šä¹‰**ï¼šASRçš„å…³é”®åœ¨äºï¼Œç›®æ ‡è¡Œä¸ºæ˜¯åŸºäº**å®‰å…¨çº¦æŸæ§åˆ¶æ ˆæ‰§è¡Œåçš„å®é™…åŠ¨ä½œ** `aÌƒ_t = C(Ï€_Î¸(oÌƒ_t))` æ¥åˆ¤å®šçš„ï¼Œè€Œéç­–ç•¥è¾“å‡ºçš„åŸå§‹åŠ¨ä½œ `a_t`ã€‚è¿™ç›´æ¥é’ˆå¯¹è®ºæ–‡æå‡ºçš„â€œè¡°å‡â€ç°è±¡ï¼Œç¡®ä¿äº†è¯„ä¼°æ ‡å‡†ä¸çœŸå®ä¸–ç•Œéƒ¨ç½²çš„å®‰å…¨çº¦æŸç›¸åŒ¹é…ã€‚\n2.  **ç‰©ç†ä¸–ç•ŒéªŒè¯**ï¼šæ‰€æœ‰æŒ‡æ ‡æ•°æ®å‡æ¥è‡ª**çœŸå®æœºå™¨äººå®éªŒ**ï¼Œè€Œéä»¿çœŸã€‚å®éªŒåœ¨å®¤å†…é™æ€éšœç¢ç‰©ç¯å¢ƒä¸­è¿›è¡Œï¼Œé€šè¿‡éšæœºåŒ–æœºå™¨äººçš„åˆå§‹æœå‘è¿›è¡Œå¤šæ¬¡è¯•éªŒï¼Œä»¥ä¿è¯ç»“æœçš„å¯é æ€§å’Œç»Ÿè®¡æ„ä¹‰ã€‚\n3.  **å¯¹æ¯”åŸºå‡†**ï¼šè®ºæ–‡å°†DGBAä¸TrojDRLã€BadRLã€SleeperNetsç­‰ç°æœ‰RLåé—¨æ”»å‡»æ–¹æ³•åœ¨**ç›¸åŒçš„CSRå’ŒASRæŒ‡æ ‡**ä¸‹è¿›è¡Œå¯¹æ¯”ï¼Œå‡¸æ˜¾äº†å…¶åœ¨çœŸå®ä¸–ç•Œçº¦æŸä¸‹çš„ä¼˜è¶Šæ€§ã€‚\n\n### äºŒã€ å®éªŒæ•°æ®é›†ä¸å¹³å°\nè®ºæ–‡**æ²¡æœ‰ä½¿ç”¨**ä¼ ç»Ÿçš„é™æ€æ•°æ®é›†ï¼ˆå¦‚ImageNetã€Atari ROMï¼‰ï¼Œè€Œæ˜¯é‡‡ç”¨**ä»¿çœŸåˆ°çœŸå®ï¼ˆSim-to-Realï¼‰çš„è®­ç»ƒæµç¨‹**ï¼Œå¹¶åœ¨**æ ‡å‡†æœºå™¨äººå¹³å°**ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°ã€‚\n\n| åç§° | ç±»å‹ | ç»†èŠ‚æè¿° | ä½œç”¨ä¸ç‰¹ç‚¹ |\n| :--- | :--- | :--- | :--- |\n| **Gazeboä»¿çœŸç¯å¢ƒ** | è®­ç»ƒç¯å¢ƒ | ä½¿ç”¨Gazeboæœºå™¨äººä»¿çœŸå™¨æ„å»ºï¼Œå¹¶ç²¾ç¡®é…ç½®ä»¥åŒ¹é…çœŸå®TurtleBot3 Burgerçš„ä¼ æ„Ÿå™¨å‚æ•°ï¼ˆç›¸æœºç„¦è·ã€è§†åœºã€LiDARç‰¹æ€§ï¼‰ã€‚ | 1. **å®‰å…¨é«˜æ•ˆåœ°é¢„è®­ç»ƒ**ï¼šåœ¨æ­¤ç¯å¢ƒä¸­è®­ç»ƒåˆå§‹çš„æ¸…æ´ç­–ç•¥å’Œè¿›è¡Œåé—¨å¾®è°ƒã€‚<br>2. **ä¿è¯ä»¿çœŸçœŸå®æ€§**ï¼šä¼ æ„Ÿå™¨å»ºæ¨¡é™ä½äº†Sim-to-Realçš„å·®è·ï¼Œä¸ºç­–ç•¥è¿ç§»åˆ°ç‰©ç†ä¸–ç•Œæ‰“ä¸‹åŸºç¡€ã€‚ |\n| **TurtleBot3 Burgeræœºå™¨äººå¹³å°** | çœŸå®ä¸–ç•Œè¯„ä¼°å¹³å° | é…å¤‡å‰å‘RGBæ‘„åƒå¤´ï¼ˆ640x480åˆ†è¾¨ç‡ï¼‰å’Œ360Â° 2D LiDARã€‚è¿è¡Œæ ‡å‡†çš„ROSå¯¼èˆªæ ˆï¼Œè¯¥æ ˆé›†æˆäº†**é€Ÿåº¦é™åˆ¶ã€åŠ¨ä½œå¹³æ»‘å’Œç¢°æ’é¿å…**ç­‰å®‰å…¨çº¦æŸæ§åˆ¶å™¨ã€‚ | 1. **æ ¸å¿ƒè¯„ä¼°è½½ä½“**ï¼šæ‰€æœ‰CSRå’ŒASRæŒ‡æ ‡å‡åœ¨æ­¤ç‰©ç†å¹³å°ä¸Šæµ‹å¾—ã€‚<br>2. **ä½“ç°ç°å®çº¦æŸ**ï¼šå…¶å†…ç½®çš„**å®‰å…¨çº¦æŸæ§åˆ¶æ ˆ**æ˜¯å¯¼è‡´ä¼ ç»Ÿåé—¨â€œè¡°å‡â€çš„å…³é”®ï¼Œä¹Ÿæ˜¯æœ¬è®ºæ–‡ç ”ç©¶çš„å‡ºå‘ç‚¹ã€‚<br>3. **é€šç”¨æ€§éªŒè¯**ï¼šè¯¥å¹³å°åœ¨RLç ”ç©¶ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå¢å¼ºäº†å®éªŒç»“æœçš„å¯æ¯”æ€§å’Œè¯´æœåŠ›ã€‚ |\n| **è‡ªå®šä¹‰å®¤å†…å¯¼èˆªåœºæ™¯** | è¯„ä¼°ä»»åŠ¡åœºæ™¯ | ä¸€ä¸ªåŒ…å«é™æ€éšœç¢ç‰©çš„å®¤å†…ç‰©ç†ç¯å¢ƒã€‚ä»»åŠ¡è¦æ±‚æœºå™¨äººä»å›ºå®šèµ·ç‚¹å¯¼èˆªè‡³ç›®æ ‡ç‚¹ã€‚è§¦å‘è¡¥ä¸è¢«æ‰“å°å‡ºæ¥å¹¶æ”¾ç½®åœ¨æœºå™¨äººè·¯å¾„ä¸Šçš„ç‰¹å®šä½ç½®ï¼ˆå¦‚éšœç¢ç‰©å‰ï¼‰ã€‚ | 1. **å®šä¹‰å…·ä½“ä»»åŠ¡**ï¼šæ¸…æ´ä»»åŠ¡æ˜¯ç‚¹å¯¹ç‚¹é¿éšœå¯¼èˆªï¼›æ”»å‡»ä»»åŠ¡æ˜¯è§¦å‘åæ‰§è¡Œç‰¹å®šé”™è¯¯åŠ¨ä½œã€‚<br>2. **æä¾›ç‰©ç†è§¦å‘**ï¼šè¯„ä¼°çš„è§¦å‘æ˜¯**å¯æ‰“å°ã€å¯æ”¾ç½®åœ¨çœŸå®åœ°æ¿ä¸Šçš„è§†è§‰è¡¥ä¸**ï¼ŒéªŒè¯äº†æ”»å‡»çš„ç‰©ç†å¯å®ç°æ€§ã€‚ |\n\n**æ•°æ®é›†ä¸è¯„ä¼°æ–¹æ³•çš„æ ¸å¿ƒä»·å€¼ï¼š**\nè®ºæ–‡çš„è¯„ä¼°ä½“ç³»ç´§å¯†å›´ç»•å…¶**æŠ€æœ¯åˆ›æ–°ç‚¹**æ„å»ºï¼š\n- **æŒ‡æ ‡è®¾è®¡**ç›´æ¥éªŒè¯äº†å…¶è§£å†³â€œ**è¡°å‡**â€é—®é¢˜çš„èƒ½åŠ›ï¼ˆASRåŸºäºæ‰§è¡ŒååŠ¨ä½œï¼‰ã€‚\n- **å¹³å°é€‰æ‹©**ç¡®ä¿äº†è¯„ä¼°åœ¨**çœŸå®ã€å®‰å…¨çº¦æŸå®Œå¤‡**çš„æœºå™¨äººç³»ç»Ÿä¸Šè¿›è¡Œï¼Œè€Œéç†æƒ³çš„ä»¿çœŸç¯å¢ƒã€‚\n- **æµç¨‹è®¾è®¡**ï¼ˆä»¿çœŸè®­ç»ƒâ†’çœŸå®éƒ¨ç½²ï¼‰ç¬¦åˆç°å®ä¸–ç•Œçš„RLç³»ç»Ÿå¼€å‘æµç¨‹ï¼Œä½¿å¾—æ”»å‡»å¨èƒæ¨¡å‹æ›´å…·å®é™…æ„ä¹‰ã€‚\n\nè¿™ç§æ–¹æ³•è¡¨æ˜ï¼Œå¯¹RLå®‰å…¨æ€§çš„ç ”ç©¶å¿…é¡»ä»ä»¿çœŸçš„â€œç©å…·ç¯å¢ƒâ€è½¬å‘åŒ…å«**æ„ŸçŸ¥å˜å¼‚**å’Œ**æ§åˆ¶çº¦æŸ**çš„å¤æ‚ç°å®ä¸–ç•Œç³»ç»Ÿã€‚",
            "timestamp": "2026-01-27T15:58:19.051381",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡åˆ†æ\n\n### ä¸€ã€å®éªŒæ•°æ®é›†ä¸å¹³å°\nè®ºæ–‡**æœªä½¿ç”¨**ä¼ ç»Ÿçš„å…¬å¼€æ•°æ®é›†ï¼ˆå¦‚Atariã€ImageNetç­‰ï¼‰ï¼Œè€Œæ˜¯é‡‡ç”¨**çœŸå®ç‰©ç†æœºå™¨äººå¹³å°**å’Œ**ä»¿çœŸç¯å¢ƒ**è¿›è¡Œå®éªŒï¼š\n\n1.  **ä¸»è¦å®éªŒå¹³å°**ï¼š\n    - **TurtleBot3 Burgerç§»åŠ¨æœºå™¨äºº**ï¼šé…å¤‡å‰å‘RGBæ‘„åƒå¤´å’Œ2D LiDARä¼ æ„Ÿå™¨ã€‚\n    - **ä»¿çœŸç¯å¢ƒ**ï¼šä½¿ç”¨**Gazeboä»¿çœŸå™¨**è¿›è¡Œç­–ç•¥é¢„è®­ç»ƒå’Œæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œä¼ æ„Ÿå™¨è®¾ç½®ä¸çœŸå®æœºå™¨äººåŒ¹é…ã€‚\n\n2.  **å®éªŒåœºæ™¯**ï¼š\n    - **å®¤å†…å¯¼èˆªä»»åŠ¡**ï¼šæœºå™¨äººåœ¨é™æ€éšœç¢ç‰©ç¯å¢ƒä¸­ä»èµ·ç‚¹ç§»åŠ¨åˆ°ç›®æ ‡ç‚¹ã€‚\n    - **è§¦å‘è®¾ç½®**ï¼šå°†ç”Ÿæˆçš„æ‰“å°è¡¥ä¸æ”¾ç½®åœ¨æœºå™¨äººè¡Œè¿›è·¯å¾„çš„åœ°é¢ä¸Šã€‚\n\n### äºŒã€ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨ä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡è¯„ä¼°æ”»å‡»æ•ˆæœï¼š\n\n1.  **Clean Success Rate (CSR)**ï¼š\n    - **å®šä¹‰**ï¼šåœ¨**æ— è§¦å‘è¡¥ä¸**æ—¶ï¼Œæœºå™¨äººæˆåŠŸé¿éšœå¹¶åˆ°è¾¾ç›®æ ‡ç‚¹çš„æˆåŠŸç‡ã€‚\n    - **æ„ä¹‰**ï¼šè¡¡é‡åé—¨æ”»å‡»å¯¹æ­£å¸¸ä»»åŠ¡æ€§èƒ½çš„ä¿ç•™ç¨‹åº¦ã€‚\n\n2.  **Attack Success Rate (ASR)**ï¼š\n    - **å®šä¹‰**ï¼šåœ¨**è§‚å¯Ÿåˆ°è§¦å‘è¡¥ä¸**æ—¶ï¼Œæœºå™¨äººæ‰§è¡Œç›®æ ‡æ¶æ„è¡Œä¸ºï¼ˆå¦‚å³è½¬ã€å·¦è½¬ã€ç›´è¡Œæ’éšœç¢ç‰©ã€åœæ­¢ï¼‰çš„æˆåŠŸç‡ã€‚\n    - **æ„ä¹‰**ï¼šè¡¡é‡åé—¨æ”»å‡»åœ¨å®‰å…¨çº¦æŸæ§åˆ¶ä¸‹çš„å®é™…æœ‰æ•ˆæ€§ã€‚\n\n### ä¸‰ã€æ€§èƒ½æå‡ä¸å¯¹æ¯”ç»“æœ\nè®ºæ–‡åœ¨**PPO**å’Œ**TRPO**ä¸¤ç§RLç®—æ³•ä¸Šè¯„ä¼°äº†DGBAä¸ç°æœ‰åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ï¼š\n\n#### 1. **åœ¨PPOå—å®³è€…æ¨¡å‹ä¸Šçš„ç»“æœ**\n| æ–¹æ³• | CSR (%) | ASR (%) | æ€§èƒ½æå‡åˆ†æ |\n| :--- | :--- | :--- | :--- |\n| **Clean PPO (æ— æ”»å‡»)** | 91.1 | â€“ | åŸºçº¿æ­£å¸¸æ€§èƒ½ |\n| **TrojDRL** (Kiourti et al., 2019) | 85.6 | 34.5 | ä¼ ç»Ÿæ”»å‡»åœ¨çœŸå®æ§åˆ¶ä¸‹ä¸¥é‡è¡°å‡ |\n| **BadRL** (Cui et al., 2024) | 87.3 | 57.0 | é€šè¿‡å…³é”®çŠ¶æ€é€‰æ‹©æœ‰æ‰€æå‡ï¼Œä½†ä»å—è§†è§‰å˜åŒ–å’Œæ§åˆ¶å™¨æŠ‘åˆ¶ |\n| **SleeperNets** (Rathbun et al., 2024) | 88.7 | 21.3 | è¡¨ç¤ºçº§åé—¨å¯¹çœŸå®ä¸–ç•Œå˜åŒ–æ•æ„Ÿ |\n| **DGBA (Ours)** | **89.1** | **83.5** | **ASRç›¸å¯¹æœ€ä½³åŸºçº¿(BadRL)æå‡çº¦46.5%** |\n\n#### 2. **åœ¨TRPOå—å®³è€…æ¨¡å‹ä¸Šçš„ç»“æœ**\n| æ–¹æ³• | CSR (%) | ASR (%) | æ€§èƒ½æå‡åˆ†æ |\n| :--- | :--- | :--- | :--- |\n| **Clean TRPO (æ— æ”»å‡»)** | 93.4 | â€“ | åŸºçº¿æ­£å¸¸æ€§èƒ½ |\n| **TrojDRL** | 82.5 | 43.9 | æ”»å‡»æ•ˆæœæœ‰æ‰€æå‡ä½†ä»æœ‰é™ |\n| **BadRL** | 88.2 | 46.5 | ä¸PPOåœºæ™¯ç±»ä¼¼ï¼Œæ•ˆæœä¸­ç­‰ |\n| **SleeperNets** | 87.9 | 11.9 | æ•ˆæœæœ€å·®ï¼Œè¡¨æ˜æ–¹æ³•æ³›åŒ–æ€§å¼± |\n| **DGBA (Ours)** | **90.7** | **76.3** | **ASRç›¸å¯¹æœ€ä½³åŸºçº¿(BadRL)æå‡çº¦64.1%** |\n\n#### 3. **å…³é”®æ€§èƒ½æå‡æ€»ç»“**\n- **æ”»å‡»æˆåŠŸç‡æ˜¾è‘—æå‡**ï¼šåœ¨PPOä¸Šï¼ŒDGBAçš„ASRè¾¾åˆ°83.5%ï¼Œè¿œè¶…æœ€ä½³åŸºçº¿BadRLçš„57.0%ã€‚åœ¨æ›´é²æ£’çš„TRPOä¸Šï¼ŒDGBAä»ä¿æŒ76.3%çš„ASRï¼Œè€ŒåŸºçº¿æ–¹æ³•å‡ä½äº50%ã€‚\n- **æ­£å¸¸ä»»åŠ¡æ€§èƒ½ä¿æŒè‰¯å¥½**ï¼šDGBAçš„CSRä¸å¹²å‡€ç­–ç•¥ç›¸å·®æ— å‡ ï¼ˆPPO: 89.1% vs 91.1%ï¼‰ï¼Œè¯æ˜å…¶ç¨€ç–æŠ•æ¯’ç­–ç•¥æœ‰æ•ˆä¿ç•™äº†åŸæœ‰å¯¼èˆªèƒ½åŠ›ã€‚\n- **è·¨ç®—æ³•æœ‰æ•ˆæ€§**ï¼šDGBAåœ¨PPOå’ŒTRPOä¸Šå‡è¡¨ç°æœ€ä½³ï¼Œè¯æ˜äº†å…¶æ–¹æ³•å¯¹ä¸åŒRLç®—æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n#### 4. **æ¶ˆèå®éªŒéªŒè¯æ ¸å¿ƒç»„ä»¶è´¡çŒ®**\n| æ¶ˆèè®¾ç½® | CSR (%) | ASR (%) | æ€§èƒ½ä¸‹é™åˆ†æ |\n| :--- | :--- | :--- | :--- |\n| **DGBA (å®Œæ•´)** | 89.1 | 83.5 | åŸºå‡† |\n| **w/o Diffusion (æ— æ‰©æ•£æ¨¡å‹)** | 86.0 | 43.4 | ASR**ä¸‹é™çº¦48%**ï¼Œè¯æ˜**å­¦ä¹ è¡¥ä¸åˆ†å¸ƒ**å¯¹åº”å¯¹çœŸå®ä¸–ç•Œè§†è§‰å˜åŒ–è‡³å…³é‡è¦ |\n| **w/o Physical Augmentation (æ— ç‰©ç†å¢å¼º)** | 88.5 | 56.7 | ASR**ä¸‹é™çº¦32%**ï¼Œè¯æ˜**å‡ ä½•ä¸å…‰åº¦å¢å¼º**å¯¹å¼¥åˆä»¿çœŸ-çœŸå®å·®è·å…³é”® |\n| **w/o Advantage Poisoning (æ— ä¼˜åŠ¿æŠ•æ¯’)** | 90.2 | 51.6 | ASR**ä¸‹é™çº¦38%**ï¼Œè¯æ˜**é’ˆå¯¹å†³ç­–å…³é”®çŠ¶æ€æŠ•æ¯’**èƒ½æå¤§æå‡æ”»å‡»æ•ˆç‡ |\n\n### å››ã€å®é™…ä»·å€¼ä¸åˆ›æ–°ç‚¹\n1.  **é¦–æ¬¡ç³»ç»Ÿç ”ç©¶çœŸå®ä¸–ç•ŒRLåé—¨æ”»å‡»**ï¼šæ­ç¤ºäº†å®‰å…¨çº¦æŸæ§åˆ¶æ ˆä¼šå¯¼è‡´ä¼ ç»Ÿåé—¨æ”»å‡»**ä¸¥é‡è¡°å‡**çš„ç°è±¡ï¼Œå°†å®‰å…¨è¯„ä¼°ä»ä»¿çœŸæ¨å‘ç‰©ç†éƒ¨ç½²ã€‚\n2.  **æå‡ºæ„ŸçŸ¥å±‚æ”»å‡»æ–°èŒƒå¼**ï¼šé€šè¿‡**æ‰©æ•£æ¨¡å‹ç”Ÿæˆç‰©ç†å¯æ‰“å°çš„è§†è§‰è¡¥ä¸è§¦å‘å™¨**ï¼Œç»•è¿‡å¯¹æ§åˆ¶åŠ¨ä½œçš„ç›´æ¥æ“çºµï¼Œä»è€ŒæŠµå¾¡å®‰å…¨è¿‡æ»¤ã€‚\n3.  **é«˜æ•ˆæŠ•æ¯’ç­–ç•¥**ï¼š**åŸºäºä¼˜åŠ¿çš„å…³é”®çŠ¶æ€é€‰æ‹©**ï¼Œä»…éœ€æ±¡æŸ“5%çš„è®­ç»ƒæ•°æ®å³å¯å®ç°é«˜æ”»å‡»æˆåŠŸç‡ï¼Œæå‡äº†æ”»å‡»çš„éšè”½æ€§å’Œå¯è¡Œæ€§ã€‚\n4.  **é»‘ç›’æ”»å‡»è®¾å®š**ï¼šæ— éœ€å¯¹å®‰å…¨æ§åˆ¶æ ˆè¿›è¡Œå»ºæ¨¡æˆ–è®¿é—®ï¼Œç¬¦åˆç°å®æ”»å‡»åœºæ™¯ï¼Œå¢å¼ºäº†æ–¹æ³•çš„å®ç”¨æ€§å’Œå¨èƒæ€§ã€‚\n\n**ç»“è®º**ï¼šè¯¥è®ºæ–‡é€šè¿‡åœ¨çœŸå®æœºå™¨äººå¹³å°ä¸Šçš„å®éªŒï¼Œè¯æ˜äº†DGBAæ¡†æ¶èƒ½æœ‰æ•ˆå…‹æœå®‰å…¨çº¦æŸæ§åˆ¶å¸¦æ¥çš„è¡°å‡ï¼Œåœ¨ä¿æŒæ­£å¸¸æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°é«˜æˆåŠŸç‡çš„åé—¨æ”»å‡»ï¼Œä¸ºRLç³»ç»Ÿçš„ç°å®å®‰å…¨è¯„ä¼°æä¾›äº†é‡è¦åŸºå‡†å’Œæ–°å¨èƒè§†è§’ã€‚",
            "timestamp": "2026-01-27T15:59:05.900975",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼Œæœ¬æ–‡æå‡ºçš„æ‰©æ•£å¼•å¯¼åé—¨æ”»å‡»ï¼ˆDGBAï¼‰æ–¹æ³•è™½ç„¶åœ¨å®é™…æœºå™¨äººç³»ç»Ÿä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **æ”»å‡»åœºæ™¯ä¸ä»»åŠ¡ç›¸å¯¹å—é™**\n   - **ç¯å¢ƒä¸ä»»åŠ¡å•ä¸€æ€§**ï¼šå®éªŒä»…åœ¨**å®¤å†…é™æ€ç¯å¢ƒ**ä¸­è¿›è¡Œï¼Œä½¿ç”¨TurtleBot3å®Œæˆ**ç‚¹å¯¹ç‚¹å¯¼èˆªé¿éšœ**ä»»åŠ¡ã€‚æœªéªŒè¯åœ¨åŠ¨æ€ç¯å¢ƒã€å¤æ‚äº¤äº’ä»»åŠ¡ï¼ˆå¦‚æœºæ¢°è‡‚æ“ä½œï¼‰æˆ–å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚\n   - **è§¦å‘å½¢å¼å›ºå®š**ï¼šè§¦å‘å™¨ä¸º**åœ°é¢æ‰“å°æ–‘å—**ï¼Œä¾èµ–äºæœºå™¨äººæ‘„åƒå¤´å¯¹åœ°é¢çš„è§‚æµ‹ã€‚å¯¹äºéåœ°é¢æœºå™¨äººï¼ˆå¦‚æ— äººæœºã€æœºæ¢°è‡‚ï¼‰æˆ–éè§†è§‰ä¸»å¯¼çš„ä»»åŠ¡ï¼Œè¯¥è§¦å‘æ–¹å¼å¯èƒ½ä¸é€‚ç”¨ã€‚\n\n### 2. **å¯¹å®‰å…¨æ§åˆ¶æ ˆçš„å‡è®¾è¾ƒä¸ºç†æƒ³åŒ–**\n   - **â€œé»‘ç›’â€ä½†å·²çŸ¥å­˜åœ¨**ï¼šè®ºæ–‡å°†å®‰å…¨æ§åˆ¶æ ˆè§†ä¸ºé»‘ç›’ï¼Œä½†**å‡è®¾å…¶å§‹ç»ˆå­˜åœ¨ä¸”åŠŸèƒ½å·²çŸ¥**ï¼ˆå¦‚é€Ÿåº¦é™åˆ¶ã€å¹³æ»‘æ»¤æ³¢ï¼‰ã€‚åœ¨å®é™…ä¸­ï¼Œæ§åˆ¶æ ˆçš„å®ç°å¯èƒ½æ›´åŠ å¤æ‚æˆ–åŒ…å«ä¸å¯é¢„æµ‹çš„ç´§æ€¥å¹²é¢„æœºåˆ¶ï¼Œå¯èƒ½è¿›ä¸€æ­¥å‰Šå¼±æ”»å‡»æ•ˆæœã€‚\n   - **æœªè€ƒè™‘è‡ªé€‚åº”æˆ–å­¦ä¹ å‹å®‰å…¨æ§åˆ¶å™¨**ï¼šç°ä»£æœºå™¨äººå¯èƒ½é‡‡ç”¨åŸºäºå­¦ä¹ çš„è‡ªé€‚åº”å®‰å…¨è¿‡æ»¤å™¨ï¼Œå…¶è¡Œä¸ºå¯èƒ½æ›´éš¾ä»¥é¢„æµ‹å’Œç»•è¿‡ï¼Œæœ¬æ–‡æœªå¯¹æ­¤ç±»åœºæ™¯è¿›è¡Œæµ‹è¯•ã€‚\n\n### 3. **æ”»å‡»ä¾èµ–ä¸¥æ ¼çš„è®­ç»ƒæ¡ä»¶ä¸æƒé™**\n   - **éœ€è¦æ±¡æŸ“è®­ç»ƒæ•°æ®çš„èƒ½åŠ›**ï¼šæ”»å‡»å±äº**è®­ç»ƒæ—¶æŠ•æ¯’**ï¼Œè¦æ±‚æ”»å‡»è€…èƒ½å¤Ÿè®¿é—®å¹¶ä¿®æ”¹éƒ¨åˆ†è®­ç»ƒæ•°æ®æµã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œå¦‚æœè®­ç»ƒæµç¨‹æ˜¯å°é—­æˆ–å—ä¸¥æ ¼ç›‘æ§çš„ï¼Œè¿™ç§è®¿é—®æƒé™å¯èƒ½éš¾ä»¥è·å¾—ã€‚\n   - **éœ€è¦ä»¿çœŸåˆ°å®ç‰©çš„è®­ç»ƒç®¡é“**ï¼šDGBAä¾èµ–äºå…ˆåœ¨ä»¿çœŸä¸­è®­ç»ƒå¹²å‡€ç­–ç•¥ï¼Œå†è¿›è¡Œåé—¨å¾®è°ƒã€‚å¦‚æœç­–ç•¥æ˜¯å®Œå…¨åœ¨çœŸå®ä¸–ç•Œä¸­åœ¨çº¿å­¦ä¹ çš„ï¼Œè¯¥æ–¹æ³•å¯èƒ½ä¸ç›´æ¥é€‚ç”¨ã€‚\n\n### 4. **ç‰©ç†å®ç°çš„æ½œåœ¨æŒ‘æˆ˜**\n   - **æ–‘å—æ”¾ç½®çš„å®æ“æ€§**ï¼šæ”»å‡»æˆåŠŸä¾èµ–äºå°†æ‰“å°æ–‘å—**ç²¾ç¡®æ”¾ç½®**åœ¨æœºå™¨äººè·¯å¾„ä¸Šçš„å…³é”®å†³ç­–ç‚¹ã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè¿™å¯èƒ½å—åˆ°ç¯å¢ƒç®¡ç†ã€äººå‘˜å¹²é¢„æˆ–ç›‘æ§çš„é™åˆ¶ã€‚\n   - **ç¯å¢ƒå˜åŒ–çš„æç«¯æƒ…å†µ**ï¼šå°½ç®¡ä½¿ç”¨äº†æ‰©æ•£æ¨¡å‹å’Œç‰©ç†å¢å¼ºï¼Œä½†**æç«¯å…‰ç…§ï¼ˆå¦‚å¼ºåå…‰ã€é»‘æš—ï¼‰ã€ä¸¥é‡é®æŒ¡ã€æˆ–æ–‘å—ä¸¥é‡ç£¨æŸ**ç­‰æƒ…å†µä»å¯èƒ½å½±å“è§¦å‘çš„å¯é æ€§ã€‚\n\n### 5. **è¯„ä¼°ç»´åº¦ä¸å¤Ÿå…¨é¢**\n   - **éšè”½æ€§è¯„ä¼°ä¸è¶³**ï¼šè®ºæ–‡ä¸»è¦è¯„ä¼°äº†æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰å’Œæ¸…æ´ä»»åŠ¡æˆåŠŸç‡ï¼ˆCSRï¼‰ï¼Œä½†æœªæ·±å…¥åˆ†æåé—¨ç­–ç•¥çš„**éšè”½æ€§**ï¼ˆä¾‹å¦‚ï¼Œå…¶æ¿€æ´»è§¦å‘å™¨çš„è¡Œä¸ºæ¨¡å¼æ˜¯å¦å®¹æ˜“è¢«å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿå‘ç°ï¼‰ã€‚\n   - **é•¿æœŸä¸å¤šä»»åŠ¡å½±å“æœªçŸ¥**ï¼šå®éªŒåœ¨å•æ¬¡ã€çŸ­æ—¶ä»»åŠ¡ä¸­è¿›è¡Œã€‚åé—¨åœ¨**é•¿æœŸè¿è¡Œ**æˆ–**å¤šä»»åŠ¡åˆ‡æ¢**ä¸­æ˜¯å¦ä¼šå¯¼è‡´ç­–ç•¥ä¸ç¨³å®šæˆ–æ€§èƒ½æ¼‚ç§»ï¼Œå°šæœªéªŒè¯ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹å‘è¿›è¡Œæ”¹è¿›ï¼š\n\n### 1. **æ‰©å±•æ”»å‡»åœºæ™¯ä¸æ³›åŒ–èƒ½åŠ›**\n   - **å¼€å‘æ›´é€šç”¨çš„è§¦å‘æœºåˆ¶**ï¼šç ”ç©¶ä¸ä¾èµ–äºç‰¹å®šä¼ æ„Ÿå™¨ï¼ˆå¦‚æ‘„åƒå¤´ï¼‰æˆ–åœºæ™¯ï¼ˆå¦‚åœ°é¢ï¼‰çš„è§¦å‘å™¨ï¼Œä¾‹å¦‚**å£°å­¦è§¦å‘ã€ç”µç£å¹²æ‰°æˆ–å¯¹æœ¬ä½“æ„ŸçŸ¥ä¿¡å·çš„è½»å¾®æ‰°åŠ¨**ã€‚\n   - **æ¢ç´¢åŠ¨æ€ç¯å¢ƒä¸å¤æ‚ä»»åŠ¡**ï¼šåœ¨åŠ¨æ€éšœç¢ç‰©ã€äººæœºå…±å­˜ç¯å¢ƒä»¥åŠæ›´å¤æ‚çš„æ“ä½œä»»åŠ¡ä¸­éªŒè¯å’Œå¢å¼ºæ”»å‡»æ–¹æ³•çš„é²æ£’æ€§ã€‚\n\n### 2. **æå‡å¯¹å¤æ‚å®‰å…¨æœºåˆ¶çš„ç»•è¿‡èƒ½åŠ›**\n   - **é’ˆå¯¹è‡ªé€‚åº”å®‰å…¨æ§åˆ¶å™¨çš„æ”»å‡»**ï¼šç ”ç©¶å¦‚ä½•å¯¹æŠ—åŸºäºå¼ºåŒ–å­¦ä¹ æˆ–ç¥ç»ç½‘ç»œçš„å®‰å…¨è¿‡æ»¤å™¨ï¼Œå¯èƒ½éœ€è¦åœ¨æ”»å‡»ä¼˜åŒ–ä¸­**æ˜¾å¼åœ°å¯¹å®‰å…¨æ§åˆ¶å™¨çš„è¡Œä¸ºè¿›è¡Œå»ºæ¨¡æˆ–å¯¹æŠ—æ€§é€‚åº”**ã€‚\n   - **æ¢ç´¢æµ‹è¯•æ—¶æ”»å‡»**ï¼šåœ¨æ— æ³•æ¥è§¦è®­ç»ƒè¿‡ç¨‹çš„æƒ…å†µä¸‹ï¼Œç ”ç©¶å¦‚ä½•é€šè¿‡**æµ‹è¯•æ—¶çš„è¾“å…¥æ‰°åŠ¨ï¼ˆå¯¹æŠ—æ€§ç¤ºä¾‹ï¼‰** æ¥æ¿€æ´»æ¶æ„è¡Œä¸ºï¼Œé™ä½æ”»å‡»é—¨æ§›ã€‚\n\n### 3. **å¢å¼ºæ”»å‡»çš„éšè”½æ€§ä¸é²æ£’æ€§**\n   - **ä¼˜åŒ–è§¦å‘å™¨çš„éšè”½æ€§**ï¼šè®¾è®¡**æ›´è‡ªç„¶ã€ä¸æ˜“å¯Ÿè§‰çš„è§¦å‘å™¨**ï¼ˆå¦‚èå…¥ç¯å¢ƒçš„çº¹ç†ã€ç‰¹å®šå½¢çŠ¶çš„æ—¥å¸¸ç‰©ä½“ï¼‰ï¼Œå¹¶è¯„ä¼°å…¶å¯¹æŠ—äººå·¥å®¡æŸ¥å’Œè‡ªåŠ¨å¼‚å¸¸æ£€æµ‹çš„èƒ½åŠ›ã€‚\n   - **æé«˜å¯¹æç«¯ç‰©ç†æ¡ä»¶çš„é²æ£’æ€§**ï¼šåœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸­å¼•å…¥**æ›´å¹¿æ³›å’Œæ›´æç«¯çš„ç‰©ç†å¢å¼º**ï¼Œæ¨¡æ‹Ÿå„ç§æ¶åŠ£æ¡ä»¶ï¼Œä½¿ç”Ÿæˆçš„è§¦å‘å™¨åˆ†å¸ƒè¦†ç›–æ›´å¹¿çš„çœŸå®ä¸–ç•Œå˜å¼‚ã€‚\n\n### 4. **å‘å±•æ›´å…¨é¢çš„è¯„ä¼°æ¡†æ¶ä¸é˜²å¾¡æ‰‹æ®µ**\n   - **å»ºç«‹å¤šç»´åº¦çš„å®‰å…¨åŸºå‡†æµ‹è¯•**ï¼šä¸ºçœŸå®ä¸–ç•ŒRLç³»ç»Ÿè®¾è®¡åŒ…å«**éšè”½æ€§ã€é•¿æœŸç¨³å®šæ€§ã€å¤šä»»åŠ¡ä¸€è‡´æ€§**ç­‰ç»´åº¦çš„å®‰å…¨è¯„ä¼°åŸºå‡†ã€‚\n   - **æ¢ç´¢é’ˆå¯¹æ€§çš„é˜²å¾¡æ–¹æ³•**ï¼šæœ¬æ–‡æ­ç¤ºäº†ä¼ ç»Ÿä»¿çœŸåé—¨æ”»å‡»çš„â€œè¡°å‡â€ç°è±¡ï¼Œè¿™æœ¬èº«ä¸ºé˜²å¾¡æä¾›äº†æ€è·¯ã€‚æœªæ¥å¯ç ”ç©¶ï¼š\n     - **åˆ©ç”¨å®‰å…¨æ§åˆ¶æ ˆä½œä¸ºä¸»åŠ¨é˜²å¾¡å±‚**çš„å¢å¼ºæ–¹æ³•ã€‚\n     - **é’ˆå¯¹ç‰©ç†è§¦å‘å™¨çš„æ£€æµ‹æŠ€æœ¯**ï¼ˆå¦‚åœºæ™¯å¼‚å¸¸åˆ†æï¼‰ã€‚\n     - **å¯¹è®­ç»ƒæ•°æ®ä¸­æ½œåœ¨ç‰©ç†è§¦å‘æ¨¡å¼çš„é²æ£’å­¦ä¹ ç®—æ³•**ã€‚\n\n### 5. **æ·±å…¥ç†è®ºåˆ†æ**\n   - **ç†è®ºè§£é‡Šè¡°å‡ç°è±¡**ï¼šå¯¹å®‰å…¨æ§åˆ¶æ ˆå¼•èµ·çš„åé—¨â€œè¡°å‡â€æ•ˆåº”è¿›è¡Œæ›´å½¢å¼åŒ–çš„**ç†è®ºå»ºæ¨¡ä¸åˆ†æ**ï¼Œé‡åŒ–ä¸åŒæ§åˆ¶çº¦æŸå¯¹å„ç±»åé—¨æ”»å‡»çš„æŠ‘åˆ¶èƒ½åŠ›ã€‚\n   - **åˆ†ææ”»å‡»çš„å¯è½¬ç§»æ€§**ï¼šæ·±å…¥ç ”ç©¶DGBAç”Ÿæˆçš„è§¦å‘å™¨åœ¨ä¸åŒç­–ç•¥æ¶æ„ã€ä¸åŒæœºå™¨äººå¹³å°ä¹‹é—´çš„**å¯è½¬ç§»æ€§**ï¼Œè¿™å¯¹äºè¯„ä¼°æ”»å‡»çš„å®é™…å¨èƒèŒƒå›´è‡³å…³é‡è¦ã€‚\n\n**æ€»ç»“**ï¼šæœ¬æ–‡å¼€åˆ›æ€§åœ°ç ”ç©¶äº†å®‰å…¨çº¦æŸä¸‹çœŸå®ä¸–ç•ŒRLçš„åé—¨æ”»å‡»ï¼ŒæŒ‡å‡ºäº†ç°æœ‰ä»¿çœŸæ”»å‡»çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æœªæ¥çš„å·¥ä½œåº”åœ¨**æ³›åŒ–æ€§ã€éšè”½æ€§ã€å¯¹æŠ—æ›´å¤æ‚å®‰å…¨æœºåˆ¶**ç­‰æ–¹é¢æ·±å…¥æ¢ç´¢ï¼ŒåŒæ—¶ä»¥æ­¤ä¸ºå¥‘æœºï¼Œæ¨åŠ¨å»ºç«‹æ›´åšå®çš„çœŸå®ä¸–ç•ŒRLç³»ç»Ÿå®‰å…¨è¯„ä¼°ä¸é˜²å¾¡ä½“ç³»ã€‚",
            "timestamp": "2026-01-27T15:59:51.839846",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T16:03:51Z",
    "created_at": "2026-01-27T15:53:12.751860",
    "updated_at": "2026-01-27T15:53:12.751867"
}