{
    "id": "2601.15486v1",
    "title": "A Universal Large Language Model -- Drone Command and Control Interface",
    "authors": [
        "Javier N. Ramos-Silva",
        "Peter J. Burke"
    ],
    "abstract": "将人工智能（AI）应用于无人机控制可显著提升无人机能力，尤其在现实世界信息能与无人机感知、指挥及控制系统融合时——这属于物理人工智能这一新兴领域的重要组成部分。大规模语言模型（LLMs）若经过海量通用知识训练将具备显著优势，特别是当训练数据涵盖全球详细地理拓扑信息，并能实时获取天气等动态态势数据时。然而，无人机与LLMs的交互接口仍存在普遍性挑战，每个应用场景都需要耗费大量人力将LLMs习得的知识与无人机指挥控制系统进行连接。\n\n本研究通过一种独立于LLMs与无人机型号的接口策略解决了该问题，首次构建出通用、多功能、全面且易用的无人机控制接口。我们采用新型模型上下文协议（MCP）标准实现这一目标——该开放标准为AI系统访问外部数据、工具和服务提供了通用途径。我们开发并部署了基于云端的Linux主机，其上运行的MCP服务器支持Mavlink协议（该通用无人机控制语言已被数百万台无人机广泛采用，包括Ardupilot和PX4框架），并成功实现了真实无人机的飞行控制验证。\n\n在进一步测试中，我们通过集成谷歌地图MCP服务器获取实时导航信息，在仿真无人机上展示了全面的飞行规划与控制能力。这项工作为LLMs与无人机指挥控制的融合提供了通用解决方案，构建出将自然语言转化为无人机指令的易用接口范式，从而有效整合现代AI产业与无人机技术。",
    "url": "https://arxiv.org/abs/2601.15486v1",
    "html_url": "https://arxiv.org/html/2601.15486v1",
    "html_content": "A Universal Large Language Model - Drone Command and Control Interface\nJavier N. Ramos-Silva, Peter J. Burke\nJavier N. Ramos-Silva and Peter J. Burke are with the Department of Electrical Engineering and Computer Science, University of California, Irvine, as well as BME, MSE, CBEMS, Physics (Burke). e-mail: pburke@uci.eduManuscript dated Dec. 15, 2025.\nAbstract\nThe use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework. We demonstrate interface with multiple commercial (including OpenAI’s ChatGPT, Anthropic’s Claude, and Google’s Gemini) and open source LLMs. We demonstrate flight control of a real unmanned aerial vehicle (UAV, drone), decided by the LLM based on dynamic unpredictable events. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.\nIndex Terms:\nLarge language model (LLM), drone, unmanned aerial vehicles (UAVs), unmanned aircraft systems (UAS).\nI\nIntroduction\nAutonomy in drones is usually based on onboard image and state processing from onboard sensors\n[\n11\n]\n. An outstanding example is the 2023 AI controlled racing drone\n[\n24\n]\nthat beat a human drone pilot, similar to the IBM supercomputer beating the world chess master in the 1990s. However, challenges remain in the amount of compute power onboard due to weight and energy restrictions for all drone classes and sizes, especially small drones. With the coming of age of internet connected drones, using AI in the cloud opens new opportunities to harness the virtually unlimited compute power of data centers deployed globally. LLMs are one such technology, trained at scale. To date, the interface between the AI LLM and the drone has been an unsolved challenge for general use cases. Here we show, using the model context protocol (MCP) standard, a comprehensive and complete drone control interface using the Mavlink protocol for drone communication. We demonstrate autonomous control of a real world internet connected drone (with real-time dynamic decision making by the LLM), and demonstrate more comprehensive missions with a virtual (simulated) drone. This approach is key to unlocking the power of AI from the virtual world to the physical world.\nFigure 1:\nArchitecture.\nAny LLM that supports the MCP standard can connect to the MCP server, which in turn provides a low level interface with a drone using Mavlink protocol.\nII\nArchitecture\nFig.\n1\nshows the architecture, and Fig.\n2\nthe concept of operations, designed to bridge the virtual world of AI with the material (physical) world of drones.\nII-A\nConcept of operations\nAny LLM (private proprietary or open source, cloud or locally hosted) that supports the MCP standard can connect to the MCP server developed in this work, which we call ”DroneServer”.\nFigure 2:\nConcept of operations.\nAn LLM has access to multiple services, tools, and MCP servers. One of them is this drone control server, but the LLM can access any of thousands of other MCP servers. An example mission is shown integrating both a map (Google maps) and drone control server.\nA list of LLMs as of the writing of this paper that support the MCP server architecture is:\n•\nAnthropic Claude\n– Native support, including Claude 3.5 Sonnet, Claude Opus 4, and Claude Sonnet 4.5. Claude Desktop app has built-in MCP client support.\n•\nOpenAI\n– Adopted MCP in March 2025. Works with GPT-4, GPT-4 Turbo, and the GPT-5 series models. The OpenAI Agents SDK includes MCP support.\n•\nGoogle Gemini\n– Announced support in April 2025. Works with Gemini 2.0 Flash, Gemini 2.5 Pro, and Gemini 2.5 Flash. Google launched production-ready managed MCP servers in December 2025.\n•\nLocal/Open Source Models\n– Any LLM that supports function calling can work with MCP, including:\n–\nLlama 3.2 and Llama 3.3 70B\n–\nQwen 2.5 72B\n–\nModels running on Ollama (such as qwen2.5:7b)\nIn turn, the MCP server can connect to any drone that supports the Mavlink protocol and has an internet connection. This includes PX4 and Ardupilot, the two largest drone software packages, already used on millions of drones. The MCP server handles all drone communications and provides LLMs with a description of capabilities suitable for the context. We describe each component of the architecture in detail next.\nII-B\nMavlink protocol\nMavlink\n[\n31\n]\nis the protocol used by open source autonomous drones running Ardupilot and PX4. This includes millions of drones (land, air, sea) and the vast majority of the global fleet of open source drones.\nMavlink supports hundreds of commands for drone control and telemetry, and is supported by libraries in most common modern high level programming languages such as C, Python, etc.\nII-C\nModel context protocol: From manual tools to universal standard LLM interface\nII-C\n1\nTools: Tedious and manual agentic AI programming\nA large language model (LLM) supports inference and text prediction based on tokens, but does not otherwise perform any function. Recently, LLMs have been programmed with the ability to call ”tools” in order to interact outside the context of pure text. This enables LLMs to perform as agents, accessing internet content, reading and modifying files and databases, and even writing code. Typically, each tool is hand coded or formalized as an API, a process that requires manual attention for each set of tools and each variation of an LLM.\nII-C\n2\nModel Context Protocol\nIn order to enable more seamless connection between LLMs and tools, the model context protocol concept was introduced by Anthropic in Nov. 2024\n[\n4\n]\n. In December 2025, Anthropic donated the MCP standard the Linux Foundation, in order to secure it as an open, industry wide standard\n[\n40\n]\n.\nThe MCP serves as a layer between LLMs and the rest of the virtual world by standardizing the LLM communication protocol. An MCP server can connect any LLM to any tool. MCP is now supported by all the major LLM providers (OpenAI, Anthropic, Gemini). The prime use cases initially envisioned included interaction between LLMs and (broadly) the virtual world, such as banking, finance, files, databases, websites, and social media, even to write and modify code. Fig.\n3\nshows the detailed specification of the MCP standard.\nFigure 3:\nModel context protocol (MCP) standard.\nThe MCP server exposes resources, prompts, and tools to the LLM. The LLM does not need to know the details of the implementation of these, and uses them based on the context of the prompt.\nIII\nDevelopment method\nThe development method was based primarily on Cursor IDE, a fork of Microsoft’s Visual Studio Code, which allows access to modern coding tools including Anthropic’s Claude, Google Gemini, and OpenAI ChatGPT. All of the code was generated this way. This is an extension of the concept that we recently developed and published\n[\n10\n]\n. This work is forked as a major extension, test and demonstration of the original project basic MavlinkMCP (\ngithub.com/ion-g-ion/MAVLinkMCP\n), which had 10 tools. This project has 15,000 lines of code, 45 tools, detailed install instructions, as well as systemd service files to enable the MCPserver to run continuously in the background without the need for manual start. Github was used for continuous development (as recommended in\n[\n10\n]\n), and provides detailed usage instructions. Testing was performed on both a virtual and a real drone, described in more detail below.\nIV\nServer architecture\nFig.\n4\nshows the detailed architecture (the ”tech stack”) used in this paper. The entire server is hosted on a cloud Linux Ubuntu instance. An MCP server (the main contribution of this work) is coded using primarly the Python language, deployed on a cloud linux instance, and connected to a drone. The MCP server tells the LLM what support it provides for the LLM to control and get information about the drone. This is high level, in the next section we dive deep into the MCP server design and implementation.\nThe MCP server consists of a set of code run on a cloud Linux instance with internet connectivity. The codebase is custom for this work and is available on github at\ngithub.com/PeterJBurke/droneserver\n. The codebase is moslty written in Python, because of all of the packages available both for the drone command and control interface, and the LLM interface. Fig.\n4\nshows in detail the tech stack used by the MCP server. We describe each below.\nIV-A\nInternals\nThe internal code provides custom, hand coded interface between the drone and the LLM. This write once, use multiple times provides drone pilots and LLM context engineers with a pre-configured interface, so the hand coding is not necessary. Natural language prompts are automatically translated into drone command and control interactions. This represents the first versatile, universal interface between any drone supporting Mavlink (which is most of them), and any LLM supporting MCP. Thus, the impact is expected to be vast for applications of individual single drones with the significant resources of LLMs, as well as at scale with coordinated swarms of drones and swarms of agents.\nIV-B\nLLL-MCP server link\nThe MCP Python SDK\n[\n5\n]\nis the official Python implementation of the Model Context Protocol, serving as a framework for building servers that expose tools, resources, and prompts to AI applications like Claude.\nIt enables developers to build MCP servers that expose resources, tools, and prompts to Large Language Models in a standardized way. Once deployed, it provides a standardized way to create integrations that allow AI assistants and agents to interact with external systems, databases, APIs, and services through a well-defined protocol. It uses standard transports like stdio, SSE, and Streamable HTTP for communication. (Here, we use HTTP). It is the primary machine that provides context and LLM interaction.\nThe package includes base classes and utilities for defining server capabilities, handling client connections, and managing the request/response lifecycle. Developers can use it to build custom MCP servers that extend an AI’s capabilities - for example, connecting to databases, file systems, or third-party APIs - while maintaining a consistent interface that MCP-compatible clients can consume.\nIV-C\nDrone to MCP server link\nIV-C\n1\nLow level Mavlink and TCP/IP are abstracted from the LLM\nStarting from the bottom, the low level bit movement from the MCP server to the drone is through TCP/IP. At the next level up, the Mavlink protocol is used. Mavlink messages are very low level messages, and there are hundreds of messages defined. Although it would be possible to code all of them up into the server, the LLM may not need or be able to handle such a fine grained control of the drone detailed state and configuration. In addition, it may be too taxing on the context window (see below) to define hundreds of tools, one for each Mavlink message. This is discussed in more detail below in the section called “Number of tools vs context size”. The other reason is that having a tool for each Mavlink message type would be much lower level than a typical use case for an LLM, which we envision as integrating at a higher level other training data and even other MCP servers and tools for integrative systems meta-engineering rather than low level control such as throttle setting, bank angle, etc. Therefore, in this work, we did not use Mavlink as the base set of commands to expose to the LLM.\nIV-C\n2\nMavSDK is a higher level set of commands used for this work\nWhile Mavlink is the protocol, there are two Python packages that handle communications, links, and provide higher level commands and methods within the Python application. These are Pymavlink and MavSDK. Pymavlink provides a direct python implementation of the MAVLink protocol. For example, PyMavlink can be used to set the throttle or read the attitude and IMU. MavSDK is higher level abstraction, which enables more mission oriented commands, such as “go to xyz location” “take off” “land”. MavSDK also handles establishing and maintaining the communications links under the hood. Therefore, we used MavSDK in this work. Our model exposes the key MavSDK methods as tools to the LLM via the MCP server, discussed next.\nMavSDK provides several high level commands such as “take off to xxx meters”, or “fly to xyz position”, commands which are not available as simple Mavlink commands. Of the 155 MavSDK methods, we chose a subset to implement in this initial work. Implementing all of the methods was not deemed necessary at this time, and this would likely consume too many tokens in the LLM context (see below). Table\nLABEL:tab:mavsdk_methods\nin the appendix lists all 155 of the methods in MavSDK (grouped by major function), whether they are exposed as tools to the LLM, and a brief description of their tasks. Of the 155, 40 are exposed. Table\nI\nbelow shows as a summary how many of each class are exposed. We discuss the selection process in more detail below.\nFigure 4:\nTech stack.\nThe tech stack of the MCP server developed in this work. The drone communicates over TCP/IP using Mavlink protocol, while the LLM communications over HTTP using the MCP protocol. The server contains custom code to coordinate all the interactions to provide seemless integration between the LLM and the drone.\nTABLE I:\nMAVSDK Python Method Implementation Summary\nCategory\nTotal\nImplemented\nCoverage\nAction\n22\n10\n45%\nTelemetry\n31\n17\n55%\nMission\n10\n6\n60%\nMissionRaw\n7\n2\n29%\nParam\n7\n5\n71%\nCamera\n21\n0\n0%\nGimbal\n8\n0\n0%\nOffboard\n10\n0\n0%\nFollowMe\n7\n0\n0%\nGeofence\n2\n0\n0%\nManualControl\n3\n0\n0%\nInfo\n5\n0\n0%\nCalibration\n6\n0\n0%\nLogFiles\n3\n0\n0%\nFTP\n9\n0\n0%\nTune\n1\n0\n0%\nShell\n2\n0\n0%\nTransponder\n1\n0\n0%\nTOTAL\n∼\n\\sim\n155\n40\n26%\nIV-D\nBeyond one tool per MavSDK method\nIn developing the MCP server and testing it on real world and simulated real world scenarios, we found that the LLM was not well suited to handle simply MavSDK methods presented as tools. For example, the LLM would immediately assume that the drone was at its new location after sending a goto command via the goto tool. In response to a prompt “Take off, fly to xyz location and land”, the LLM would send all commands simultaneously, resulting in the drone landing before it reached the target location. This could also lead to dangerous situations resulting in a crash. For example, if prompt was takeoff to 100 m and fly to xyz, the LLM may send the take off and fly to command in rapid succession, and the drone would not have time to ascend to the initial take off height, resulting in a low altitude flight towards the final destination, which may be below the level of obstacles, resulting in a crash. Thus, a simple “one tool per MavSDK” method is not advisable for an MCP drone command and control server.\nTherefore, we decided to manually add some additional tools, such as “wait for xxx” to be enabled. Several additional tools were defined, based on the author’s extensive experience with programming drones\n[\n22\n,\n10\n]\n. In the future, it would be a good idea to automate this or even use AI to improve the tool set based on a database of mission profiles. For now, the tools were manually curated and tested. Table\nII\nshows a summary of how many custom and MavSDK commands were developed.\nIV-E\nOne off vs. continuous drone control\nModern LLMs are designed for prompt/response operation, which does not fit well with continuous command and control of drones for long missions. The architect of modern LLMs is “fire and forget”. However, for drone flight, there should in many cases be continuous monitoring, which LLMs are not good it. For example, the LLM may say it checks the progress every once it a while but it does not do it. The LLM also told the drone to take off and then immediately told it to fly to xyz location, before the takeoff maneviur was complete, causing the (virtual) drone to crash into an obstacle. Also, the monitor location method we used during development did not provide the user with realtime feedback of the drone location or status. In fact one option was to just wait for the set_location to return complete, which could block the LLM for the length of the mission, which might be as long as 30 minutes or longer. Therefore, we had to implement some logic of real time monitoring of the drone into the MCP server itself. This made the MCP server a kind of ground control station with it’s own internal memory and logic, which is not the goal. Ideally, all the memory, logic, real time, long term monitoring would be done by the LLM, and the MCP server would just be an interpreter between the drone and the LLM. We do not know what the correct long term solution to this is. LLM technology with real time, long term situational awareness and memory needs to be developed for this approach to scale and reach its full potential.\nThe specific timing and coordination is specific to the LLM model used, and the chatbot and/or agentic wrapper. For closed source providers, these details are usually not exposed to the public. For example, ChatGPT’s agentic mode works like this: Run ALL tool calls in sequence. Only show output to the user AFTER the turn is complete. It does not pause between tool calls to show intermediate results. We attempted to work around this with the DISPLAY_TO_USER tool in order to give the user incremental progress reports on the status of the flight, but ChatGPT waited until the end to show them in some cases\nIt is an important topic for future research to develop agentic LLM systems for continuous drone command and control. Once developed, the interface here can serve as a continuous, real time, standardized, and easy to use bridge between the virtual LLM world and the real world.\nTABLE II:\nSummary of Exposed MCP Tools\nCategory\nMAVSDK Eq.\nCustom\nTotal\nFlight Control\n4\n1\n5\nSafety\n3\n0\n3\nNavigation\n4\n5\n9\nMission Management\n6\n4\n10\nTelemetry\n14\n0\n14\nParameter Management\n3\n0\n3\nOther\n0\n1\n1\nTOTAL\n34\n11\n45\nV\nDemonstration and testing\nDemonstration and testing was performed on a real drone and a virtual drone. The real drone provided real world testing, while the virtual drone provided more extensive testing in situations not suitable, safe, or allowed with a real drone.\nIn the main text, we use OpenAI. Claude Desktop was also demonstrated (not shown). In the appendix, we show and discuss demonstrated with open-source, locally run LLMs using LMStudio.\nFigure 5:\nPicture of drone used in this work. A LIDAR and optical flow sensor is used for GPS-denied flights, for example in the drone cage.\nV-A\nReal drone\nV-A\n1\nDrone design\nThe drone design was a sub-250g 4 inch quad running Ardupilot, as described in ref.\n[\n8\n]\n. Some modifications were needed for stable flight in our small (10 x 10 x 10 foot) drone cage. A lidar and optical flow sensor was used for position stabilization with 1 cm accuracy. This enabled indefinite stable hover, despite the weak or no GPS signal. As GPS hold stability is not enough, even in the presence of a good lock in our small cage, the GPS functionality was completely disabled in the software. A picture of the drone and the LIDAR is shown in Fig.\n5\n.\nV-A\n2\nDemo in cage: Initial flight test\nThe drone was equipped with a Raspberry Pi Zero W, connected to the drone via UART, and providing WiFi connection to the internet. Mavlink Router\n[\n30\n]\npassed all Mavlink traffic from the internet to the flight controller. A connection to the drone with a laptop running Mission Planner ground control station (using TCP connectivity) was initially used to confirm the drone could take off, loiter, and land autonomously in the cage safely before the LLM control was tested.\nV-A\n3\nDemo in cage: LLM controlled flight test\nOnce stable hover and flight was demonstrated under internet control, the LLM was then connected to the drone via the MCP server. The MCP server has the IP address of the drone as an environmental parameter, and runs as a service on a cloud Linux instance. During the flight, the drone was also monitored by a simultaneous connection to Mission Planner, still connected from the first test above.\nV-A\n4\nDetails of MCP server deployment\nIn order to handle firewalls, we used a Tailscale VPN to allow transparent, secure communication from the MCP server in the cloud to the drone. The MCP server runs continuously without the need for user monitoring. However, for debugging purposes, MCP activities are programmed to be logged to the terminal for status monitoring. Since the MCP server runs as a systemd service/daemon, it is always on at boot. The demands on the Linux instance are nominal, so the cheapest Linux cloud instances provide plenty of processing and I/O power. These are readily available from multiple service providers for a low cost.\nV-A\n5\nDemo in cage: Actual flight tests\nFig.\n6\nshows the actual flight test. The drone was on the ground, without the propellers spinning. The LLM was asked to flip a coin, and take off if it came up heads. Then, the LLM was asked a question about movies, and if it was true, to land the drone. Both maneuvers were executed flawlessly by the LLM, as shown in Fig.\n6\n. This demonstrates for the first time LLM natural language control of a real drone in response to unpredictable or pre-trained world knowledge using a universal MCP interface.\nFigure 6:\nLLM control of a real drone.\n(\nA\n) Demonstration of LLM controlled take off in a drone cage. The LLM decides based on a virtual coin flip if it should command the drone to take off.\n(\nB\n) Demonstration of LLM controlled landing. The LLM is asked a question based on its trained data, and uses the answer to that question to decide autonomously whether to command the drone to land.\nV-B\nVirtual drone\nV-B\n1\nSITL instance\nA SITL Software in the loop\n[\n6\n,\n9\n]\ninstance of a virtual drone was deployed on a cloud hosted Ubuntu 22.04 instance. The virtual drone thus had an IP address. Again, we used Tailscale VPN for easy secure connection between firewalled systems. A ground control station (QGroundControl) was used to continuously monitor the status and position of the virtual drone on a map.\nV-B\n2\nLLM virtual drone control\nExtensive (virtual) test flights were performed to confirm the full functionality of the software MCP server during development. The project was able to control the drone take off, land, fly to, arm, disarm, and other basic functionalities. This demonstrated the ability of an LLM to fly the drone virtually anywhere in the world.\nV-B\n3\nLLM virtual drone control: Hicups\nOne of the disadvantages to this approach is the LLM was only willing to do a certain number of tool calls while monitoring the drone flight. Also, the LLM would not always follow the prompt instruction to loop between tool calls to check the drone status. For this reason, at this juncture, the MCP server is not able to follow the drone on long missions (longer than about 5-10 minutes). This is a limitation of the LLM model used, and not of this work.\nV-B\n4\nIntegration with other MCPs servers: Google maps\nDuring testing, we would ask the drone to fly to the nearest grocery store, but the LLM model did not have up to date information about the world map. By a series of fortunate events, during the writing of this paper, Google decided to open up Google Maps to an MCP server on Dec. 10, 2025\n[\n7\n]\n. Therefore, we used this as an opportunity to demonstrate multiple MCP servers in a single agent for drone control. Shown in Fig.\n7\nis a demo where google maps was used to provide up to date real time information about local store, and fly the drone there. This is a major milestone. Thus, we have demonstrated MCP LLM drone control, and other MCP LLM integration for real time, global navigation information for drone command and control.\nFigure 7:\nLLM control of a virtual drone.\n(\nA\n) Browser interface, with inset showing some of the exposed tools for the DroneServer MCP, and Google maps MCP, as well as an example prompt for a drone mission.\n(\nB\n) Drone mission flown by the virtual drone, under control of the LLM, monitored in real time by a separate connection to the drone using QGroundControl.\nVI\nDiscussion\nVI-A\nNumber of tools vs context size\nA typical maximum number of tokens for a commercial LLM is around 100k-1M tokens. Local LLMs with less compute power have smaller limits. Anthropic has noted that MCP servers can consume a large number of tokens\n[\n3\n]\n, and has suggested strategies to address this. In this work, we used about 5k tokens for the tool definitions (45 tools). For this application, this is acceptable. However, this needs to be taken into account for future agentic systems with many different MCP servers.\nVI-B\nLimits and extensions of this approach\nThis approach is the first demonstration of LLM control of a drone through a scalable, industry standard interface platform MCP. It abstracts away from the LLM the need to know the details of how the drone operates or communicates. As such, it represents the first step towards physical AI, where the LLM has knowledge of the physical world and, in this case, control of it.\nHowever, it only has a small amount of information in this realization. The amount of information about the drone in this work is only the gps location, orientation, velocity. In principle additional data such as temperature, humidity, wind, etc, could be beamed down from the drone’s sensors.\nHowever, the work does not provide enough information for the LLM to enable the LLM to provide a more sophisticated 3d model of the physical world. A possible extension of this project would be to enable this capability. One such strategy could be to equip swarms of drones with Lidar, Radar, sonar, or other 3d mapping abilities, and to give dynamic information into the LLM directly. This would be a step towards cyber-physical integration at scale: Similar to how google earth is a computer map of the world, one could envision an extension of this work to an LLM model of the entire physical world, with dynamic as well as static, high resolution 3d dimensional representations of objects and their interactions. Such a technology would be transformative and bring AI from the virtual to the physical world for applications we cannot yet even imagine.\nVI-C\nSafety\nWe did our test flights in a drone cage. Obviously, there are safety issues. For one, we firmly believe that there should always be a human in the loop for possible manual override.\nIn addition, there should be some reliable way to ensure the LLM does not break out of any firewall rules. For example, the MCP should not allow the AI to override the override, e.g. lock the human out of the loop problematically. If the MCP server is properly configured, the LLM will not be able to do this.\nThere is another important question: LLM behavior is not deterministic and therefore not predictable exactly. How should the safety of AI drone control be tested? What standards should be used? What should the test protocols be? These are questions beyond the scope of this paper.\nVII\nSignificance and impact\nThe significance of this work is the ability to integrate with other LLM models, as well as real-time, real-world data using additional MCP servers. The following are concepts enabled by this work.\nVII-A\nIntegrated autonomous missions\nReal time, real world data could be included in separate MCP servers. The LLMs could be trained to incorporate this for specific missions, which will be discussed next.\nVII-A\n1\nFirefighting\nFirefighting requires response to dynamic and rapidly changing situations. Here, LLM models integrated with drones could be used for rapid assessment and planning to strategically deploy precious resources for maximum impact.\nVII-A\n2\nSAR\nThe search and rescue of people, ground vehicles, sea vehicles, and downed aircraft could be enabled using agentic AI for planning, search grid optimization, and image processing.\nVII-A\n3\nBVLOS\nBeyond visual line of sight could benefit from LLMs for enhanced planning and prediction for collision avoidance and air traffic control.\nVII-B\nScaling to multiple drones\nFinally, scaling from one to multiple drones could enable agent-AI coordination of swarms of drones for coordinated, complex missions. Swarms of agents could control swarms of drones.\nVII-C\nPrior art confirms and affirms vast potential of LLMs in drones in general\nMany of these advantages of LLM use in drones have also been discussed conceptually in\n[\n2\n,\n36\n,\n12\n,\n45\n,\n41\n,\n26\n,\n15\n,\n44\n,\n37\n,\n17\n,\n48\n,\n47\n,\n12\n,\n23\n,\n46\n,\n19\n,\n42\n,\n25\n,\n21\n,\n20\n,\n38\n]\n, and references therein. Some attempts have been made to hand-code the tools for LLM-drone integration\n[\n35\n,\n32\n,\n27\n,\n28\n,\n1\n,\n16\n,\n47\n,\n13\n,\n49\n,\n18\n,\n34\n,\n33\n,\n43\n,\n29\n,\n50\n,\n14\n,\n39\n]\nwhich, as mentioned above, lack universal and versatile ability to interact with all LLMs supporting MCP and all drones supporting Mavlink, and they are also not open source like this project, which is made freely available to the entire drone community under the liberal GNU license. We are not discounting those works; some of the themes are extremely significant, involved, complex, and comprehensive. However, each is only usable for that particular use case. What we are aiming to do with this paper is to leverage catalytically all of the excellent technical talent and hard work that has gone into LLM-drone integration into a new, standard, universal interface, that would prevent write once, use once, publish once use cases, no matter how significant or complex, into a larger ecosystem of drone-LLM technology, broadly available, and easily deployable, to the whole world, not just LLM experts, matrix programmers, wealthy companies, or wealthy institutions with high end hardware, but the drone community of users (industry, academia, government) and tinkerers alike, and at scale. In our opinion, that is the only way drone and AI technology integration will make a global impact on humanity and allow drones to evolve to their true potential. Thus, regardless of this editorial, and independent of the reader’s opinion of it, there is broad, general consensus of the significance of using LLMs for a variety of drone use cases.\nPrior art clearly establishes the need for a universal, easy to use interface between LLMs and drones in general, which we provide in this paper for the first time. With regard to prior art, then, our work takes a significant step to enable in practice what has been proposed mathematically or in conceptual form in extensive literature on the subject to date.\nVIII\nConclusion\nWe have demonstrated a comprehensive, versatile, easy to use drone command and control interface, connecting LLMs with drones. The method applies to a large class of drones and LLMs, and can be cheaply and easily implemented for a broad variety of use cases. This brings drones into the world of LLM agentic AI for a large class of future applications.\nAcknowledgments\nThis work was done as a demonstration for the EECS 195 Drone course during Fall 2025\n[\n8\n]\n. We thank lab manager Shawn Davis for help maintaining the lab and drone cage necessary for this work, and the UC Irvine EECS department for allowing us to develop and teach this hands on drone class with AI and autonomy as a feature.\nReferences\n[1]\nS. Ahmmad, Z. A. Aditto, M. M. Hossain, N. Yeasmin, and S. Hossain\n(2025)\nAutonomous navigation of cloud-controlled quadcopters in confined spaces using multi-modal perception and llm-driven high semantic reasoning\n.\narXiv preprint arXiv:2508.07885\n.\nCited by:\n§\nVII-C\n.\n[2]\nN. Ak Kanigur, M. Mert, and I. Duru\n(2025)\nLeveraging large language models and artificial intelligence for uavs in 6g-enabled non-terrestrial networks\n.\nIn\n2025 9th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)\n,\npp. 1–7\n.\nExternal Links:\nDocument\n,\nISBN 979-8-3315-9753-5\nCited by:\n§\nVII-C\n.\n[3]\nAnthropic\n(2024-11-04)\nCode execution with MCP: building more efficient AI agents\n(Website)\nAnthropic Engineering\n.\nNote:\nAccessed: Dec. 19, 2025\nExternal Links:\nLink\nCited by:\n§\nVI-A\n.\n[4]\nAnthropic\n(2024)\nModel context protocol (mcp) specification\n(Website)\nModel Context Protocol\n.\nNote:\nAccessed: Dec. 19, 2025\nExternal Links:\nLink\nCited by:\n§\nII-C\n2\n.\n[5]\nAnthropic\n(2024)\nModel context protocol python sdk\n.\nGitHub\n.\nNote:\nhttps://github.com/modelcontextprotocol/python-sdk\nAccessed: Dec. 19, 2025\nCited by:\n§\nIV-B\n.\n[6]\nArduPilot Dev Team\n(2025)\nSITL Simulator (Software in the Loop)\n.\nNote:\nhttps://ardupilot.org/dev/docs/sitl-simulator-software-in-the-loop.html\nAccessed: 2025-08-01\nCited by:\n§\nV-B\n1\n.\n[7]\nM. Bachman and A. Berenberg\n(2025-12-10)\nAnnouncing official MCP support for google services\n(Website)\nGoogle Cloud\n.\nExternal Links:\nLink\nCited by:\n§\nV-B\n4\n.\n[8]\nP. Burke, P. Wei, M. Fakih, and D. Burke\n(2025)\nDrones from a to z: experiential learning at its finest\n.\nhttps://doi.org/10.36227/techrxiv.175695608.82891552/v1\n.\nCited by:\n§\nV-A\n1\n,\nAcknowledgments\n.\n[9]\nP. Burke\n(2025)\nA simulated, virtual drone for testing and development\n.\nNote:\nhttps://github.com/PeterJBurke/CreateSITLenv\nAccessed: 2025-08-01\nCited by:\n§\nV-B\n1\n.\n[10]\nP. Burke\n(2025)\nRobot builds a robot’s brain: ai generated drone command and control station hosted in the sky\n.\narXiv preprint arXiv:2508.02962\n.\nCited by:\n§III\n,\n§\nIV-D\n.\n[11]\nD. Caballero-Martin, J. M. Lopez-Guede, J. Estevez, and M. Graña\n(2024)\nArtificial intelligence applied to drone control: a state of the art\n.\nDrones\n8\n(\n7\n),\npp. 296\n.\nCited by:\n§I\n.\n[12]\nF. S. Chagas, N. Ruseno, and A. A. A. Bechina\n(2025)\nArtificial intelligence approaches for uav deconfliction: a comparative review and framework proposal\n.\nAutomation\n6\n(\n4\n),\npp. 54\n.\nCited by:\n§\nVII-C\n.\n[13]\nG. Chen, X. Yu, N. Ling, and L. Zhong\n(2023)\nTypefly: flying drones with large language model\n.\narXiv preprint arXiv:2312.14950\n.\nCited by:\n§\nVII-C\n.\n[14]\nG. Chen, X. Yu, N. Ling, and L. Zhong\n(2025)\nChatFly: low-latency drone planning with large language models\n.\nIEEE Transactions on Mobile Computing\n.\nCited by:\n§\nVII-C\n.\n[15]\nY. Chen, X. Que, J. Zhang, T. Chen, G. Li, and J. Chen\n(2025)\nWhen large language models meet uavs: how far are we?\n.\narXiv preprint arXiv:2509.12795\n.\nCited by:\n§\nVII-C\n.\n[16]\nK. Choutri, S. Fadloun, A. Khettabi, M. Lagha, S. Meshoul, and R. Fareh\n(2025)\nLeveraging large language models for real-time uav control\n.\nElectronics\n14\n(\n21\n),\npp. 4312\n.\nCited by:\n§\nVII-C\n.\n[17]\nD. D. Cidjeu, J. L. K. E. Fendji, V. C. Kamla, and I. Tchappi\n(2025)\nUAV leveraging genai/llms, a brief survey\n.\nProcedia Computer Science\n265\n,\npp. 382–389\n.\nCited by:\n§\nVII-C\n.\n[18]\nJ. Cleland-Huang, P. A. A. Granadeno, A. M. R. Bernal, D. Hernandez, M. Murphy, M. Petterson, and W. Scheirer\n(2025)\nCognitive guardrails for open-world decision making in autonomous drone swarms\n.\narXiv preprint arXiv:2505.23576\n.\nCited by:\n§\nVII-C\n.\n[19]\nV. S. A. Duvvuru, B. Zhang, M. Vierhauser, and A. Agrawal\n(2025)\nLLM-agents driven automated simulation testing and analysis of small uncrewed aerial systems\n.\narXiv preprint arXiv:2501.11864\n.\nCited by:\n§\nVII-C\n.\n[20]\nE. M. Eumi, H. Abbass, and N. Marcus\n(2025)\nSwarmChat: an llm-based, context-aware multimodal interaction system for robotic swarms\n.\nIn\nInternational Conference on Swarm Intelligence\n,\npp. 181–192\n.\nCited by:\n§\nVII-C\n.\n[21]\nB. Han, Y. Chen, J. Li, J. Li, and J. Su\n(2025)\nSwarmChain: collaborative llm inference for uav swarm control\n.\nIEEE Internet of Things Magazine\n.\nCited by:\n§\nVII-C\n.\n[22]\nL. Hu, O. Pathak, Z. He, H. Lee, M. Bedwany, J. Mica, and P. J. Burke\n(2020)\n“CloudStation:” a cloud-based ground control station for drones\n.\nIEEE Journal on Miniaturization for Air and Space Systems\n2\n(\n1\n),\npp. 36–42\n.\nCited by:\n§\nIV-D\n.\n[23]\nS. Javaid, H. Fahim, B. He, and N. Saeed\n(2024)\nLarge language models for uavs: current state and pathways to the future\n.\nIEEE Open Journal of Vehicular Technology\n.\nCited by:\n§\nVII-C\n.\n[24]\nE. Kaufmann, L. Bauersfeld, A. Loquercio, M. Müller, V. Koltun, and D. Scaramuzza\n(2023)\nChampion-level drone racing using deep reinforcement learning\n.\nNature\n620\n(\n7976\n),\npp. 982–987\n.\nCited by:\n§I\n.\n[25]\nA. Khan, I. U. Rehman, N. Saeed, D. Sobnath, F. Khan, and M. A. K. Khattak\n(2025)\nContext-aware autonomous drone navigation using large language models (llms)\n.\nIn\nProceedings of the AAAI Symposium Series\n,\nVol.\n6\n,\npp. 102–107\n.\nCited by:\n§\nVII-C\n.\n[26]\nH. Kheddar, Y. Habchi, M. C. Ghanem, M. Hemis, and D. Niyato\n(2025)\nRecent advances in transformer and large language models for uav applications\n.\narXiv preprint arXiv:2508.11834\n.\nCited by:\n§\nVII-C\n.\n[27]\nA. Koubaa and K. Gabr\n(2025)\nAgentic uavs: llm-driven autonomy with integrated tool-calling and cognitive reasoning\n.\narXiv preprint arXiv:2509.13352\n.\nCited by:\n§\nVII-C\n.\n[28]\nS. K. Lim, M. J. Y. Chong, J. H. Khor, and T. Y. Ling\n(2025)\nTaking flight with dialogue: enabling natural language control for px4-based drone agent\n.\narXiv preprint arXiv:2506.07509\n.\nCited by:\n§\nVII-C\n.\n[29]\nS. Majumdar, S. E. Kirkley, and B. B. Mallik\n(2025)\nLLM-guided hybrid architecture for autonomous fire response: dialog-driven planning in space and disaster missions\n.\nIn\n2025 IEEE World AI IoT Congress (AIIoT)\n,\npp. 1049–1054\n.\nCited by:\n§\nVII-C\n.\n[30]\nMAVLink Router Project\n(2024)\nMavlink-router\n.\nGitHub\n.\nNote:\nhttps://github.com/mavlink-router/mavlink-router\nAccessed: Dec. 19, 2025\nCited by:\n§\nV-A\n2\n.\n[31]\nL. Meier\n(2013)\nMAVLink: micro air vehicle communication protocol\n.\nNote:\nhttps://mavlink.io\nAccessed: August 1, 2025\nCited by:\n§\nII-B\n.\n[32]\nS. Mishra, R. D. Yadav, A. Das, S. Gupta, W. Pan, and S. Roy\n(2025)\nAERMANI-vlm: structured prompting and reasoning for aerial manipulation with vision language models\n.\narXiv preprint arXiv:2511.01472\n.\nCited by:\n§\nVII-C\n.\n[33]\nÁ. Moraga, J. de Curtò, I. de Zarzà, and C. T. Calafate\n(2025)\nAI-driven uav and iot traffic optimization: large language models for congestion and emission reduction in smart cities\n.\nDrones\n9\n(\n4\n),\npp. 248\n.\nCited by:\n§\nVII-C\n.\n[34]\nA. Navarro, C. de Quinto, and J. A. Hernández\n(2025)\nBeyond visual line of sight: uavs with edge ai, connected llms, and vr for autonomous aerial intelligence\n.\narXiv preprint arXiv:2507.15049\n.\nCited by:\n§\nVII-C\n.\n[35]\nD. Nunes, R. Amorim, P. Ribeiro, A. Coelho, and R. Campos\n(2025)\nA framework leveraging large language models for autonomous uav control in flying networks\n.\narXiv preprint arXiv:2506.04404\n.\nCited by:\n§\nVII-C\n.\n[36]\nY. Ping, T. Liang, H. Ding, G. Lei, J. Wu, X. Zou, K. Shi, R. Shao, C. Zhang, W. Zhang,\net al.\n(2025)\nMultimodal large language models-enabled uav swarm: towards efficient and intelligent autonomous aerial systems\n.\narXiv preprint arXiv:2506.12710\n.\nCited by:\n§\nVII-C\n.\n[37]\nR. Sapkota, K. I. Roumeliotis, and M. Karkee\n(2025)\nUAVs meet agentic ai: a multidomain survey of autonomous aerial intelligence and agentic uavs\n.\narXiv preprint arXiv:2506.08045\n.\nCited by:\n§\nVII-C\n.\n[38]\nM. Schuck, D. O. Dahanaggamaarachchi, B. Sprenger, V. Vyas, S. Zhou, and A. P. Schoellig\n(2025)\nSwarmGPT: combining large language models with safe motion planning for drone swarm choreography\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\n§\nVII-C\n.\n[39]\nM. L. Tazir, M. Mancas, and T. Dutoit\n(2023)\nFrom words to flight: integrating openai chatgpt with px4/gazebo for natural language-based drone control\n.\nIn\nInternational Workshop on Computer Science and Engineering\n,\nCited by:\n§\nVII-C\n.\n[40]\nThe Linux Foundation\n(2025-12-09)\nLinux foundation announces the formation of the agentic AI foundation (AAIF), anchored by new project contributions including model context protocol (MCP), goose and AGENTS.md\n.\nNote:\nPress ReleaseAccessed: Dec. 19, 2025\nExternal Links:\nLink\nCited by:\n§\nII-C\n2\n.\n[41]\nY. Tian, F. Lin, Y. Li, T. Zhang, Q. Zhang, X. Fu, J. Huang, X. Dai, Y. Wang, C. Tian,\net al.\n(2025)\nUAVs meet llms: overviews and perspectives towards agentic low-altitude mobility\n.\nInformation Fusion\n122\n,\npp. 103158\n.\nCited by:\n§\nVII-C\n.\n[42]\nH. Wang, Z. Chen, G. Li, B. Ma, and C. Li\n(2025)\nChat with uav–human-uav interaction based on large language models\n.\narXiv preprint arXiv:2512.08145\n.\nCited by:\n§\nVII-C\n.\n[43]\nL. Wassim, K. Mohamed, and A. Hamdi\n(2024)\nLlm-daas: llm-driven drone-as-a-service operations from text user requests\n.\nIn\nThe International Conference of Advanced Computing and Informatics\n,\npp. 108–121\n.\nCited by:\n§\nVII-C\n.\n[44]\nJ. Wu, H. You, B. Sun, and J. Du\n(2025)\nLLM-driven pareto-optimal multi-mode reinforcement learning for adaptive uav navigation in urban wind environments\n.\nIEEE Access\n.\nCited by:\n§\nVII-C\n.\n[45]\nZ. Yang, Y. Zhang, J. Zeng, Y. Yang, Y. Jia, H. Song, T. Lv, Q. Sun, and J. An\n(2025)\nAI-driven safety and security for uavs: from machine learning to large language models\n.\nDrones\n9\n(\n6\n),\npp. 392\n.\nCited by:\n§\nVII-C\n.\n[46]\nF. Yao, Y. Yue, Y. Liu, X. Sun, and K. Fu\n(2024)\nAeroverse: uav-agent benchmark suite for simulating, pre-training, finetuning, and evaluating aerospace embodied world models\n.\narXiv preprint arXiv:2408.15511\n.\nCited by:\n§\nVII-C\n.\n[47]\nL. Yuan, C. Deng, D. Han, I. Hwang, S. Brunswicker, and C. G. Brinton\n(2025)\nNext-generation llm for uav: from natural language to autonomous flight\n.\narXiv preprint arXiv:2510.21739\n.\nCited by:\n§\nVII-C\n.\n[48]\nX. Zhang, Y. Tian, F. Lin, Y. Liu, J. Ma, K. S. Szatmáry, and F. Wang\n(2025)\nLogisticsVLN: vision-language navigation for low-altitude terminal delivery based on agentic uavs\n.\narXiv preprint arXiv:2505.03460\n.\nCited by:\n§\nVII-C\n.\n[49]\nJ. Zhao and X. Lin\n(2025)\nGeneral-purpose aerial intelligent agents empowered by large language models\n.\narXiv preprint arXiv:2503.08302\n.\nCited by:\n§\nVII-C\n.\n[50]\nQ. Zhou, J. Wu, M. Zhu, Y. Zhou, F. Xiao, and Y. Zhang\n(2025)\nLLM-ql: a llm-enhanced q-learning approach for scheduling multiple parallel drones\n.\nIEEE Transactions on Knowledge and Data Engineering\n.\nCited by:\n§\nVII-C\n.\nJavier N. Ramos-Silva\nreceived his M.Sc. degree in telecommunications engineering and B.Eng. degree in electronics and telecommunications from the National Polytechnic Institute (IPN), México, in 2020 and 2018, respectively. He was a visiting researcher at Universitat Autònoma de Barcelona (UAB) in 2019. He is currently working toward the\nPh.D. in EECS at the University of California, Irvine, USA. His main research areas are quantum sensing, compact modeling, antennas and microwave circuit design.\nPeter J. Burke\n(M’02–SM’17-F’20) received the Ph.D. degree in physics from Yale University, New Haven, CT, USA, in 1998. From 1998 to 2001, he was a Sherman Fairchild Postdoctoral Scholar in physics with the California Institute of Technology, Pasadena, CA, USA. Since 2001, he has been a Faculty Member with the Department of Electrical Engineering and Computer Science, University of California at Irvine, Irvine, CA, USA.\nSupplementary Materials\nVIII-A\nOpen-source local LLM demo\nIn Fig.\n8\n, we use LM Studio running on a Macbook Air M4 with 16 GBytes of RAM. (Thus this can be done on a local machine with cost around one thousand dollars.) The model used is the open-source model qwen2.5-7b-instruct. Note that the nominal context window size of 4k tokens was too small to fit the MCP server. Therefore, a context window of size 32k tokens was used.\nFigure 8:\nLLM control of a virtual drone.\n(\nA\n) LM Studio interface, showing the chat window and the list of tools for the MCP.\n(\nB\n) Drone mission flown by the virtual drone, under control of the LLM, using LM Studio.\nVIII-B\nTable of complete list of tools exposed to the LLM\nA comparison of all MavSDK methods with MCP tools is given in Table\nLABEL:tab:mavsdk_methods\n.\nTABLE III:\nMAVSDK Python Methods and MCP Implementation Status\nMethod\nImpl.\nMCP Tool\nDescription\nACTION Plugin - Basic Flight Commands (10/22 implemented)\narm()\nYes\narm_drone\nArm motors for flight\ndisarm()\nYes\ndisarm_drone\nDisarm motors\ntakeoff()\nYes\ntakeoff\nAutonomous takeoff\nland()\nYes\nland\nLand at current position\nreturn_to_launch()\nYes\nreturn_to_launch\nReturn to home/launch position\ngoto_location()\nYes\ngo_to_location\nFly to GPS coordinates\nhold()\nYes\nhold_position\nHold/hover at current position\nkill()\nYes\nkill_motors\nEmergency motor cutoff\nset_takeoff_altitude()\nYes\ntakeoff\nSet takeoff target altitude\nset_maximum_speed()\nYes\nset_max_speed\nSet max flight speed\nget_takeoff_altitude()\nNo\n–\nGet current takeoff altitude setting\nget_maximum_speed()\nNo\n–\nGet current max speed setting\nset_return_to_launch_altitude()\nNo\n–\nSet RTL altitude\nget_return_to_launch_altitude()\nNo\n–\nGet RTL altitude setting\ntransition_to_fixedwing()\nNo\n–\nVTOL: switch to fixed-wing mode\ntransition_to_multicopter()\nNo\n–\nVTOL: switch to multicopter mode\ndo_orbit()\nNo\n–\nOrbit around a point\nreboot()\nNo\n–\nReboot autopilot\nshutdown()\nNo\n–\nShutdown autopilot\nterminate()\nNo\n–\nFlight termination\nset_actuator()\nNo\n–\nDirect actuator/servo control\nset_current_speed()\nNo\n–\nSet current target speed\nTELEMETRY Plugin - Sensor Data & Status (17/31 implemented)\nposition()\nYes\nget_position\nGPS position (lat, lon, alt)\nhome()\nYes\nget_home_position\nHome/launch position\nattitude_euler()\nYes\nget_attitude\nRoll, pitch, yaw angles\nvelocity_ned()\nYes\nget_speed\nVelocity (North, East, Down)\nbattery()\nYes\nget_battery\nBattery voltage & percentage\ngps_info()\nYes\nget_gps_info\nSatellite count, fix type\nflight_mode()\nYes\nget_flight_mode\nCurrent flight mode\nhealth()\nYes\nget_health\nSystem health checks\nin_air()\nYes\nget_in_air\nIs drone flying?\narmed()\nYes\nget_armed\nAre motors armed?\nstatus_text()\nYes\nprint_status_text\nStatus messages stream\nhealth_all_ok()\nYes\nget_health_all_ok\nAll health checks passed?\nlanded_state()\nYes\nget_landed_state\nOn ground / taking off / in air / landing\nrc_status()\nYes\nget_rc_status\nRC controller status & signal\nheading()\nYes\nget_heading\nCompass heading (degrees)\nimu()\nYes\nget_imu\nRaw IMU data (accel, gyro)\nodometry()\nYes\nget_odometry\nPosition + velocity + orientation\nattitude_quaternion()\nNo\n–\nAttitude as quaternion\nattitude_angular_velocity_body()\nNo\n–\nAngular velocity (body frame)\nground_speed_ned()\nNo\n–\nGround speed (NED frame)\nfixedwing_metrics()\nNo\n–\nAirspeed, climb rate (fixed-wing)\nscaled_imu()\nNo\n–\nScaled IMU readings\nraw_imu()\nNo\n–\nUnprocessed IMU data\ndistance_sensor()\nNo\n–\nRangefinder/lidar distance\nscaled_pressure()\nNo\n–\nBarometer pressure\nactuator_control_target()\nNo\n–\nCommanded actuator values\nactuator_output_status()\nNo\n–\nActual actuator outputs\nvtol_state()\nNo\n–\nVTOL mode (MC/FW/transition)\nunix_epoch_time()\nNo\n–\nSystem time\nposition_velocity_ned()\nNo\n–\nCombined position & velocity\nground_truth()\nNo\n–\nSimulation ground truth\nMISSION Plugin - Waypoint Missions High-Level (6/10 implemented)\nstart_mission()\nYes\ninitiate_mission\nStart uploaded mission\nmission_progress()\nYes\nprint_mission_progress\nCurrent waypoint progress\nclear_mission()\nYes\nclear_mission\nClear all waypoints\nset_current_mission_item()\nYes\nset_current_waypoint\nJump to specific waypoint\nis_mission_finished()\nYes\nis_mission_finished\nCheck if mission complete\nset_return_to_launch_after_mission()\nYes\ninitiate_mission\nRTL after mission ends\nupload_mission()\nNo\n–\nUpload mission plan (high-level API)\ndownload_mission()\nNo\n–\nDownload mission from drone (high-level)\npause_mission()\nNo\n–\nPause current mission\nget_return_to_launch_after_mission()\nNo\n–\nGet RTL-after-mission setting\nMISSION_RAW Plugin - Waypoint Missions Low-Level (2/7 implemented)\nupload_mission()\nYes\nupload_mission\nUpload raw mission items\ndownload_mission()\nYes\ndownload_mission\nDownload raw mission items\nstart_mission()\nNo\n–\nStart mission (raw API)\npause_mission()\nNo\n–\nPause mission (raw API)\nclear_mission()\nNo\n–\nClear mission (raw API)\nset_current_mission_item()\nNo\n–\nSet current waypoint (raw)\nimport_qgroundcontrol_mission()\nNo\n–\nImport QGC mission file\nPARAM Plugin - Parameter Management (5/7 implemented)\nget_param_int()\nYes\nget_parameter\nGet integer parameter\nget_param_float()\nYes\nget_parameter\nGet float parameter\nset_param_int()\nYes\nset_parameter\nSet integer parameter\nset_param_float()\nYes\nset_parameter\nSet float parameter\nget_all_params()\nYes\nlist_parameters\nList all parameters\nget_param_custom()\nNo\n–\nGet custom parameter type\nset_param_custom()\nNo\n–\nSet custom parameter type\nCAMERA Plugin - Photo & Video Control (0/21 implemented)\ntake_photo()\nNo\n–\nCapture single photo\nstart_photo_interval()\nNo\n–\nStart time-lapse capture\nstop_photo_interval()\nNo\n–\nStop time-lapse\nstart_video()\nNo\n–\nStart video recording\nstop_video()\nNo\n–\nStop video recording\nstart_video_streaming()\nNo\n–\nStart video stream\nstop_video_streaming()\nNo\n–\nStop video stream\nset_mode()\nNo\n–\nSet photo/video mode\nset_setting()\nNo\n–\nAdjust camera setting\nget_setting()\nNo\n–\nGet camera setting\nset_zoom_level()\nNo\n–\nSet zoom level\nzoom_in()\nNo\n–\nIncrease zoom\nzoom_out()\nNo\n–\nDecrease zoom\nformat_storage()\nNo\n–\nFormat SD card\nselect_camera()\nNo\n–\nSelect camera by index\ninformation()\nNo\n–\nGet camera info\nstatus()\nNo\n–\nGet camera status\ncapture_info()\nNo\n–\nLast capture info\ncurrent_settings()\nNo\n–\nCurrent camera settings\npossible_setting_options()\nNo\n–\nAvailable setting options\nlist_photos()\nNo\n–\nList captured photos\nGIMBAL Plugin - Camera Gimbal Control (0/8 implemented)\nset_pitch_and_yaw()\nNo\n–\nSet gimbal angles\nset_pitch_rate_and_yaw_rate()\nNo\n–\nSet gimbal angular rates\nset_mode()\nNo\n–\nSet yaw follow/lock mode\nset_roi_location()\nNo\n–\nPoint at GPS location\ntake_control()\nNo\n–\nTake gimbal control\nrelease_control()\nNo\n–\nRelease gimbal\ncontrol()\nNo\n–\nGimbal control stream\nattitude()\nNo\n–\nGet gimbal attitude\nOFFBOARD Plugin - Direct Control Mode (0/10 implemented)\nstart()\nNo\n–\nEnter offboard mode\nstop()\nNo\n–\nExit offboard mode\nis_active()\nNo\n–\nCheck if offboard active\nset_position_ned()\nNo\n–\nSet position (NED frame)\nset_position_global()\nNo\n–\nSet position (GPS)\nset_velocity_ned()\nNo\n–\nSet velocity (NED frame)\nset_velocity_body()\nNo\n–\nSet velocity (body frame)\nset_attitude()\nNo\n–\nSet attitude angles\nset_attitude_rate()\nNo\n–\nSet attitude rates\nset_actuator_control()\nNo\n–\nDirect actuator control\nFOLLOW_ME Plugin - Target Following (0/7 implemented)\nstart()\nNo\n–\nStart follow mode\nstop()\nNo\n–\nStop follow mode\nis_active()\nNo\n–\nCheck if following\nset_config()\nNo\n–\nSet follow behavior\nget_config()\nNo\n–\nGet follow config\nset_target_location()\nNo\n–\nUpdate target GPS\nget_last_location()\nNo\n–\nGet last target location\nGEOFENCE Plugin - Flight Boundaries (0/2 implemented)\nupload_geofence()\nNo\n–\nUpload geofence polygons\nclear_geofence()\nNo\n–\nClear all geofences\nMANUAL_CONTROL Plugin - Joystick Control (0/3 implemented)\nstart_position_control()\nNo\n–\nStart position control\nstart_altitude_control()\nNo\n–\nStart altitude control\nset_manual_control_input()\nNo\n–\nSend joystick inputs\nINFO Plugin - System Information (0/5 implemented)\nget_version()\nNo\n–\nGet firmware version\nget_product()\nNo\n–\nGet product/vendor info\nget_flight_information()\nNo\n–\nFlight time, distance\nget_identification()\nNo\n–\nSystem identification\nget_speed_factor()\nNo\n–\nSimulation speed factor\nCALIBRATION Plugin - Sensor Calibration (0/6 implemented)\ncalibrate_gyro()\nNo\n–\nCalibrate gyroscope\ncalibrate_accelerometer()\nNo\n–\nCalibrate accelerometer\ncalibrate_magnetometer()\nNo\n–\nCalibrate compass\ncalibrate_level_horizon()\nNo\n–\nLevel horizon calibration\ncalibrate_gimbal_accelerometer()\nNo\n–\nCalibrate gimbal accel\ncancel()\nNo\n–\nCancel calibration\nLOG_FILES Plugin - Flight Logs (0/3 implemented)\nget_entries()\nNo\n–\nList available logs\ndownload_log_file()\nNo\n–\nDownload specific log\nerase_all_log_files()\nNo\n–\nDelete all logs\nFTP Plugin - File Transfer (0/9 implemented)\nreset()\nNo\n–\nReset FTP server\ndownload()\nNo\n–\nDownload file from drone\nupload()\nNo\n–\nUpload file to drone\nlist_directory()\nNo\n–\nList directory contents\ncreate_directory()\nNo\n–\nCreate directory\nremove_directory()\nNo\n–\nRemove directory\nremove_file()\nNo\n–\nDelete file\nrename()\nNo\n–\nRename file/directory\nare_files_identical()\nNo\n–\nCompare files (CRC)\nTUNE Plugin - Audio Feedback (0/1 implemented)\nplay_tune()\nNo\n–\nPlay buzzer tune\nSHELL Plugin - MAVLink Shell (0/2 implemented)\nsend()\nNo\n–\nSend shell command\nsubscribe_receive()\nNo\n–\nReceive shell output\nTRANSPONDER Plugin - ADS-B (0/1 implemented)\ntransponder()\nNo\n–\nGet nearby aircraft data\nCORE Plugin - Connection Management (1/2 implemented)\nconnection_state()\nYes\n(internal)\nConnection status\nset_mavlink_timeout()\nNo\n–\nSet connection timeout\nThe complete set of tools exposed to LLM and their description are given in Table\nLABEL:tab:mcp_tools\n.\nTABLE IV:\nComplete MCP Tools Reference (45 Tools)\nMCP Tool\nMAVSDK Method\nDescription\nFlight Control (5 tools)\narm_drone\naction.arm()\nArm motors for flight\ndisarm_drone\naction.disarm()\nDisarm motors\ntakeoff\naction.set_takeoff_altitude()\n+\ntakeoff()\nAutonomous takeoff to specified altitude\nland\naction.land()\nLand at current position\nhold_position\nCustom\nHold current position in GUIDED mode\nSafety (3 tools)\nreturn_to_launch\naction.return_to_launch()\nReturn to home/launch position\nkill_motors\naction.kill()\nEmergency motor cutoff\nget_battery\ntelemetry.battery()\nBattery voltage & percentage\nNavigation (9 tools)\nget_position\ntelemetry.position()\nCurrent GPS position (lat, lon, alt)\nget_home_position\ntelemetry.home()\nHome/launch position\ngo_to_location\naction.goto_location()\nFly to GPS coordinates\nmove_to_relative\nCustom\nMove relative distance (N/E/D meters)\nset_max_speed\naction.set_maximum_speed()\nSet maximum flight speed\nset_yaw\nCustom\nRotate to specified heading\nreposition\nCustom\nMove to location and loiter\ncheck_arrival\nCustom\nCheck if drone arrived at destination\nmonitor_flight\nCustom\nMonitor flight progress with auto-land\nMission Management (10 tools)\ninitiate_mission\nmission_raw.upload()\n+\nmission.start()\nUpload and start mission\nupload_mission\nmission_raw.upload_mission()\nUpload mission waypoints\ndownload_mission\nmission_raw.download_mission()\nDownload mission from drone\nprint_mission_progress\nmission.mission_progress()\nCurrent waypoint progress\npause_mission\nDeprecated\nPause mission (unsafe, do not use)\nhold_mission_position\nCustom\nSafe pause alternative (GUIDED mode)\nresume_mission\nmission.start_mission()\nResume paused mission\nclear_mission\nmission.clear_mission()\nClear all waypoints\nset_current_waypoint\nmission.set_current_mission_item()\nJump to specific waypoint\nis_mission_finished\nmission.is_mission_finished()\nCheck if mission complete\nTelemetry (14 tools)\nget_flight_mode\ntelemetry.flight_mode()\nCurrent flight mode\nget_health\ntelemetry.health()\nDetailed system health checks\nget_health_all_ok\ntelemetry.health_all_ok()\nQuick health check (boolean)\nget_speed\ntelemetry.velocity_ned()\nVelocity (North, East, Down)\nget_attitude\ntelemetry.attitude_euler()\nRoll, pitch, yaw angles\nget_gps_info\ntelemetry.gps_info()\nSatellite count, fix type\nget_in_air\ntelemetry.in_air()\nIs drone flying? (boolean)\nget_armed\ntelemetry.armed()\nAre motors armed? (boolean)\nget_landed_state\ntelemetry.landed_state()\nLanded state (ground/air/landing)\nget_rc_status\ntelemetry.rc_status()\nRC controller status & signal\nget_heading\ntelemetry.heading()\nCompass heading (degrees)\nget_imu\ntelemetry.imu()\nIMU data (accel, gyro)\nget_odometry\ntelemetry.odometry()\nCombined position, velocity, orientation\nprint_status_text\ntelemetry.status_text()\nStatus messages from autopilot\nParameter Management (3 tools)\nget_parameter\nparam.get_param_int/float()\nGet parameter value (auto-detects type)\nset_parameter\nparam.set_param_int/float()\nSet parameter value (auto-detects type)\nlist_parameters\nparam.get_all_params()\nList all parameters\nOther (1 tool)\nset_flight_mode\nCustom\nSet flight mode by name (HOLD, RTL, LAND, GUIDED)",
    "preview_text": "The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.\n\nA Universal Large Language Model - Drone Command and Control Interface\nJavier N. Ramos-Silva, Pete",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "Large Language Model",
        "drone control",
        "interface",
        "MCP",
        "Mavlink",
        "natural language"
    ],
    "one_line_summary": "这篇论文提出了一种基于大型语言模型的通用无人机控制接口，利用MCP协议实现自然语言到无人机指令的转换，但与强化学习、VLA、扩散模型等关键词相关性较低。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T21:37:54Z",
    "created_at": "2026-01-27T15:53:26.618110",
    "updated_at": "2026-01-27T15:53:26.618117"
}