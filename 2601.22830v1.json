{
    "id": "2601.22830v1",
    "title": "A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions",
    "authors": [
        "Ji Zhou",
        "Yilin Ding",
        "Yongqi Zhao",
        "Jiachen Xu",
        "Arno Eichberger"
    ],
    "abstract": "å¯é çš„ç¯å¢ƒæ„ŸçŸ¥ä»æ˜¯è‡ªåŠ¨é©¾é©¶è½¦è¾†å®‰å…¨è¿è¡Œçš„ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚é¢„æœŸåŠŸèƒ½å®‰å…¨å…³æ³¨æ„ŸçŸ¥ä¸è¶³å¸¦æ¥çš„å®‰å…¨é£é™©ï¼Œå°¤å…¶åœ¨ä¼ ç»Ÿæ£€æµ‹å™¨å¸¸å¤±æ•ˆçš„æ¶åŠ£æ¡ä»¶ä¸‹ã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å±•ç°å‡ºæœ‰å‰æ™¯çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨å®‰å…¨å…³é”®å‹äºŒç»´ç‰©ä½“æ£€æµ‹ä¸­çš„é‡åŒ–æ•ˆèƒ½å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä½¿ç”¨ä¸“é—¨é’ˆå¯¹é•¿å°¾äº¤é€šåœºæ™¯å’Œç¯å¢ƒé€€åŒ–é—®é¢˜æ„å»ºçš„åŸºå‡†æ•°æ®é›†PeSOTIFï¼Œå¯¹åç§ä»£è¡¨æ€§å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå¹¶ä¸åŸºäºYOLOæ£€æµ‹å™¨çš„ç»å…¸æ„ŸçŸ¥æ–¹æ³•è¿›è¡Œäº†é‡åŒ–å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ä¸€ä¸ªå…³é”®æƒè¡¡ï¼šåœ¨å¤æ‚è‡ªç„¶åœºæ™¯ä¸­ï¼Œæ€§èƒ½æœ€ä¼˜çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Gemini 3ã€è±†åŒ…ï¼‰çš„å¬å›ç‡æ¯”YOLOåŸºçº¿é«˜å‡º25%ä»¥ä¸Šï¼Œå±•ç°å‡ºå¯¹è§†è§‰é€€åŒ–çš„å“è¶Šé²æ£’æ€§ï¼›è€ŒåŸºçº¿æ¨¡å‹åœ¨åˆæˆæ‰°åŠ¨åœºæ™¯çš„å‡ ä½•ç²¾åº¦æ–¹é¢ä»ä¿æŒä¼˜åŠ¿ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†è¯­ä¹‰æ¨ç†ä¸å‡ ä½•å›å½’çš„äº’è¡¥ç‰¹æ€§ï¼Œæ”¯æŒå°†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºé¢å‘é¢„æœŸåŠŸèƒ½å®‰å…¨çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„é«˜å±‚çº§å®‰å…¨éªŒè¯å™¨ã€‚",
    "url": "https://arxiv.org/abs/2601.22830v1",
    "html_url": "https://arxiv.org/html/2601.22830v1",
    "html_content": "A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions\nJi Zhou\nâˆ—\n, Yilin Ding\nâˆ—\n, Yongqi Zhao, Jiachen Xu, and Arno Eichberger\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\nâˆ—\nThe first two authors contribute equally to this work.Â (\nCorresponding author: Yongqi Zhao)\nJi Zhou, Yilin Ding, Yongqi Zhao, and Arno Eichberger are with the Institute of Automotive Engineering, Graz University of Technology, 8010, Graz, Austria (e-mail: ji.zhou@student.tugraz.at; yilin.ding@student.tugraz.at; yongqi.zhao@tugraz.at; arno.eichberger@tugraz.at).Jiachen Xu is an Independent Researcher, 200000, Shanghai, China (e-mail: jcxu97@gmail.com).The code of this work is available via the following link:\nhttps://github.com/ftgTUGraz/LLM-SOTIF\nAbstract\nReliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.\nI\nIntroduction\nAutomated vehicles rely heavily on onboard perception systems to sense and interpret their surroundings, with vision-based sensing playing a crucial role in environment understanding\n[\n9\n]\n. Beyond failure caused by hardware or software malfunctions, safety risks may also arise when correctly functioning perception algorithms encounter conditions that exceed their operational capabilities, such as severe glare or low visibility. To address such non-malfunction-related safety risks, Safety of the Intended Functionality (SOTIF), formalized in ISO 21448, focuses on hazards caused by functional insufficiency rather than component faults\n[\n4\n]\n. For example, as shown in\nFigureÂ 1\n, severe glare from oncoming traffic can significantly degrade visibility. In such scenarios, an automated vehicle may fail to detect a pedestrian or another vehicle even though the perception systems operate as specified. Such failure modes highlight systematic perception limitations that are not attributable to component malfunctions.\nFigure 1:\nAn example of potential perception functional insufficiency under severe glare from oncoming traffic, as provided in the PeSOTIF dataset\n[\n10\n]\n.\nConventional object detection benchmarks in autonomous driving, such as KITTI\n[\n2\n]\n, nuScenes\n[\n1\n]\nand BDD100K\n[\n16\n]\n, are primarily designed around common traffic scenarios and standard detection conditions, and therefore often underrepresent perception failures in adverse environments. This discrepancy motivates SOTIF-oriented datasets that explicitly target non-malfunction-related perception limitations. To this end, PeSOTIF dataset\n[\n10\n]\nwas introduced to curate challenging scenarios, such as the one illustrated in\nFigureÂ 1\n, characterized by adverse conditions and atypical objects, which are prone to inducing perception failures. By systematically capturing these corner cases, PeSOTIF serves as a dedicated testbed for identifying the operational boundaries of perception systems and evaluating their robustness against functional insufficiencies.\nMotivated by the SOTIF-oriented perception failures discussed above, existing algorithmic efforts for object detection can be broadly grouped into two paradigms. One line of research enhances conventional detectors by explicitly enlarging safety margins under perceptual uncertainty, thereby improving robustness within the paradigm of detector-centric architectures. For example, Peng et al.\n[\n11\n]\nincorporate probabilistic inference mechanisms into the YOLO architecture to quantify uncertainty and identify perception risk in complex SOTIF scenarios. Similarly, Wang et al.\n[\n14\n]\naugment YOLO\n[\n12\n]\nwith enhanced geometric constraints, achieving improved robustness and higher detection accuracy on KITTI.\nIn parallel, the emergence of Large Vision-Language Models (LVLMs) offers a new perspective: leveraging their vast world knowledge and semantic reasoning capabilities to handle open-set concepts and interpret complex scenarios. Representative studies focus on high-level reasoning, interpretation, or decision-making tasks using models such as GPT-4 and Video-LLaVA\n[\n5\n]\n, or propose SOTIF-oriented frameworks for risk interpretation specifically under adverse conditions\n[\n3\n]\n. Other efforts employ LLMs as auxiliary tools, for instance by guiding diffusion-based data augmentation to improve few-shot object detection\n[\n7\n]\n. However, these studies predominantly operate at the semantic level. It remains unverified whether the high-level reasoning of LVLMs can be effectively translated into precise geometric localization, a prerequisite for safe motion planning. Consequently, it is unclear how their detection performance compares systematically against established detectors such as YOLO on dedicated benchmarks like PeSOTIF.\nTo bridge this gap, the presented work conducts a comprehensive benchmark of ten mainstream LVLMs for object detection under SOTIF conditions, employing established YOLO detectors as a comparative baseline. The main contributions are summarized as follows:\n1.\nA unified evaluation pipeline is developed to enable LVLMs to perform 2D object detection via visual grounding under SOTIF conditions, effectively bridging the gap between high-level semantic reasoning and precise geometric localization without task-specific fine-tuning.\n2.\nA systematic benchmark of ten representative large foundation models is conducted for SOTIF-oriented object detection, using PeSOTIF as a dedicated testbed and YOLO-based detectors as reference baselines.\n3.\nA quantitative analysis of detection performance and failure patterns is provided, revealing a critical trade-off where LVLMs exhibit superior robustness in semantic recall but lag behind conventional detectors in geometric precision.\nBy establishing this benchmark, it is aimed to provide a quantitative reference for the safe deployment of foundation models in automated driving. The insights derived from the performance trade-offs are expected to guide the design of future hybrid perception systems, which combine the semantic robustness of LVLMs with the geometric precision of specialized detectors to address the safety challenges of SOTIF.\nII\nMethodology\nThe overall workflow of the presented work is depicted in\nFigureÂ 2\n. The process initiates with image preprocessing, where raw samples from the PeSOTIF dataset\n[\n10\n]\nare standardized through resizing, border annotation, and the superimposition of scale markers. Subsequently, the processed inputs are fed into ten LVLMs for object detection, with the generated responses parsed into a structured format consistent with the dataset annotations. In the final stage\n1\n1\n1\nThe metrics are explained in\nsubsection\nII-E\n.\n, performance evaluation is conducted by quantitatively comparing the predicted bounding boxes against ground-truth labels to measure comprehensive detection performance.\nFigure 2:\nOverall workflow of the LVLM-based 2D object recognition in PeSOTIF dataset.\nII-A\nPeSOTIF Dataset\nThis work employs the PeSOTIF dataset\n[\n10\n]\nas the testbed. As a dedicated benchmark for SOTIF-oriented perception, PeSOTIF comprises 1126 frames capturing long-tail traffic scenarios. Distinct from conventional datasets, it prioritizes scenes characterized by perception-degrading conditions and atypical road anomalies, which serve as stress tests for vision-based systems.\nAs depicted in\nFigureÂ 3\n, PeSOTIF is structured into two subsets. The environment subset categorizes visual degradation into natural conditions (e.g., rain) and handcrafted perturbations (e.g., synthetic glare). Complementing this, the object subset distinguishes between common vehicles and atypical anomalies (e.g., overturned trucks).\nFigure 3:\nOverview of PeSOTIF dataset\n[\n10\n]\n.\nII-B\nImage Preprocessing\nII-B\n1\nImage Resize\nAs shown in the first stage of the preprocessing pipeline (\nFigureÂ 2\n), all images are resized using a standardized procedure to ensure a consistent input format across the dataset and to facilitate LVLM inference. Each raw image\nI\nI\n, with original width\nW\norig\nW_{\\text{orig}}\nand height\nH\norig\nH_{\\text{orig}}\n, was scaled to fit within bounds\nW\nmax\n=\n800\nW_{\\text{max}}=800\nand\nH\nmax\n=\n600\nH_{\\text{max}}=600\npixels, thereby providing a uniform coordinate space for subsequent evaluation. The resizing operation preserved the original aspect ratio to avoid geometric distortion of traffic objects. A global scaling factor\nS\nS\nwas computed as\nS\n=\nmin\nâ¡\n(\nW\nmax\nW\norig\n,\nH\nmax\nH\norig\n)\n.\nS=\\min\\left(\\frac{W_{\\max}}{W_{\\text{orig}}},\\frac{H_{\\max}}{H_{\\text{orig}}}\\right).\n(1)\nThe resized dimensions were then obtained by\nW\nnew\n=\nS\nâ‹…\nW\norig\nW_{\\text{new}}=S\\cdot W_{\\text{orig}}\nand\nH\nnew\n=\nS\nâ‹…\nH\norig\nH_{\\text{new}}=S\\cdot H_{\\text{orig}}\n. To reduce aliasing\n2\n2\n2\nAliasing describes distortions introduced during image resizing or downsampling, where fine spatial details are not correctly preserved.\nduring downsampling and maintain visual quality, the Lanczos resampling filter\n[\n13\n]\nwas applied.\nII-B\n2\nDraw Border\nAfter resizing, a clear visual boundary was added to define the spatial extent of the input. Specifically, a continuous rectangular border with a linewidth of 2 pixels was drawn along the image perimeter, spanning from\n(\n0\n,\n0\n)\n(0,0)\nto\n(\nW\nâˆ’\n1\n,\nH\nâˆ’\n1\n)\n(W-1,H-1)\n. This boundary serves two purposes: it provides an explicit visual cue that delineates the Region of Interest (ROI) for the LVLM, and it helps separate scene content from the canvas edges, which may reduce coordinate estimation errors near image boundaries.\nII-B\n3\nDraw Scale\nBecause LVLMs may be limited in precise spatial localization\n3\n3\n3\nThis limitation is closely related to the concept of\nvisual grounding\n, which describes the alignment between language expressions and image regions in computer vision.\n, an explicit visual coordinate reference was embedded into the image. A visual coordinate reference in the form of ruler ticks was overlaid along the top (horizontal) and left (vertical) margins of the image. The coordinate range was divided into ten equal intervals, with tick marks placed every 10% of the image width (\nW\nW\n) and height (\nH\nH\n), respectively. Normalized numeric labels (e.g., 0.1, 0.2, â€¦, 0.9) were displayed next to each tick. This augmentation provides a direct spatial reference within the input. By acting as explicit visual landmarks, these ticks allow the model to\nâ€œreadâ€\ncoordinates directly from the image rather than implicitly, thereby improving the precision of bounding-box regression in the normalized coordinate domain\n[\n0\n,\n1\n]\n[0,1]\n.\nII-C\nLVLMs Under Test\nII-C\n1\nLVLMs Specification\nTo ensure a comprehensive evaluation, ten state-of-the-art foundation models were selected from major developers, including Google, OpenAI, xAI, Anthropic, Alibaba, ByteDance, and Tencent. These models represent the forefront of multimodal capabilities as of late 2025. The selection covers a broad spectrum of parameters scales where publicly disclosed, ranging from lightweight models to trillion-parameter giants. The candidate models include: Grok 2, Grok 4, Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini 3, GPT-5, Claude 4.5, Qwen 3-Max, Doubao and Hunyuan 2.0. Key specifications, including developer, release note, and parameter count, are summarized in\nTableÂ I\n.\nTABLE I:\nSpecifications of LVLMs under test\nModel\nDeveloper\nRelease Date\nParameter\nGrok 2\nxAI\nAug. 2024\n270B\nGrok 4\nxAI\nJuly 2025\n1.7T\nGemini 2.5 Pro\nGoogle\nJune 2025\n128B\nGemini 2.5 Flash\nGoogle\nMay 2025\n5B\nGemini 3\nGoogle\nNov. 2025\n-\nGPT-5\nOpenAI\nAug. 2025\n-\nQwen 3-Max\nAlibaba\nSep. 2025\n1T\nClaude 4.5\nAnthropic\nNov. 2025\n-\nDoubao\nByteDance\nMay 2024\n-\nHunyuan 2.0\nTencent\nDec. 2025\n32B\nII-C\n2\nPrompt Engineering\nAs depicted in\nFigureÂ 4\n, the prompt is designed with a structured chain-of-thought\n[\n15\n]\napproach, comprising four strategic components to maximize inference reliability:\n(a) Role Definition\nestablishes an expert persona for image-based perception, aiming to activate the modelâ€™s domain-specific latent knowledge;\n(b) Task Specification\nexplicitly details the 11-class taxonomy and instructs the model to use the visual rulers grounding, thereby aligning semantic understanding with the normalized coordinate space\n[\n0\n,\n1\n]\n[0,1]\n;\n(c) Output Format\nenforces a strict JSON schema for class IDs and bounding boxes, facilitating automated parsing;\n(d) Constraints\nrestrict the response to the specified fields to ensure consistent, machine-readable outputs.\nAs an image recognition expert, your task is to analyze images from dashcam footage and provide output in JSON format with the following keys only: class, x_center, y_center, width, length and safety critical.\n(a)\n(a)\n- class represents the class of an object which divided like this: 0 car, 1 bus, 2 truck, 3 train, 4 bike, 5 motor, 6 person, 7 rider, 8 traffic sign, 9 traffic light, and 10 traffic cone\n- the coordinate of object in the given picture should be normalized to 0-1, with the reference point [0,0] at the top left corner and the reference point [1,1] at the bottom right corner of the picture\n- x center and y center should represent the coordinates of the center of the detected object within the image\n- width and length represent a bounding box that frames exactly the outline of one object\n- safety critical represents whether the object is safety critical for the driver proceed\n- to give you reference of coordinates, rulers with a marker every 1/10th of width and height are drawn on top and left of image and the whole picture is being framed\n(b)\n(b)\nEach {} represents an object in the picture. Please adhere strictly to this output structure:\n[\n{\nâ€classâ€: int value,\nâ€x_centerâ€: float value to two decimal places,\nâ€y_centerâ€: float value to two decimal places,\nâ€widthâ€: float value to two decimal places,\nâ€lengthâ€: float value to two decimal places,\nâ€safety_criticalâ€: int value\n}\n]\n(c)\n(c)\nNote: Do not include any additional data or keys outside of what has been specified.\n(d)\n(d)\nFigure 4:\nStructure of the LLM prompt for 2D object detection, consisting of (a) role definition, (b) task specification with class taxonomy and coordinate reference, (c) output format specification in JSON, and (d) constraints to enforce format consistency.\nII-D\nLVLM Output Parsing\nThe unstructured, verbose textual responses of LVLMs were transformed into structured data for quantitative analysis. Using regular expressions, the system extracted the JSON-formatted detection result from the raw outputs while removing additional text (e.g., explanations). The extracted JSON was then parsed to obtain the required object attributes, including class labels, normalized bounding-box coordinates, and safety-critical indicators. This procedure ensures that only valid and standardized detection records are passed to the downstream Intersection-Over-Union (IoU) evaluation module.\nFigure 5:\nVisualization of ten LVLMsâ€™ performance in PeSOTIF dataset.\nII-E\nPerformance Evaluation\nIn this study, YOLOv5 is adopted as the comparative baseline. While newer versions such as YOLOv11 exist, YOLOv5 is selected for its recognized stability, extensive validation in safety-critical applications, and proven maturity as an industrial standard\n[\n6\n]\n. This ensures a reliable benchmark against a well-established production-grade detector. To assess LVLM performance in traffic-scene perception, standard object detection metrics based on IoU are adopted following the COCO benchmark protocols\n[\n8\n]\n. Detection precision is quantified using Mean Average Precision (\nmAP\n50\n\\text{mAP}_{50}\n) at an IoU threshold of 0.50, which is the mean of Average Precision (AP) across all\nN\nc\nN_{c}\nobject classes:\nmAP\n50\n=\n1\nN\nc\nâ€‹\nâˆ‘\nc\n=\n1\nN\nc\nAP\nc\nâ€‹\n(\n0.50\n)\n,\n\\text{mAP}_{50}=\\frac{1}{N_{c}}\\sum_{c=1}^{N_{c}}\\text{AP}_{c}(0.50),\n(2)\nwhere\nAP\nc\nâ€‹\n(\n0.50\n)\n\\text{AP}_{c}(0.50)\ndenotes the average precision for class\nc\nc\n. Similarly, the detection completeness is measured using Mean Average Recall (\nmAR\n50\n\\text{mAR}_{50}\n), calculated as the mean of maximum recall values across classes:\nmAR\n50\n=\n1\nN\nc\nâ€‹\nâˆ‘\nc\n=\n1\nN\nc\nAR\nc\nâ€‹\n(\n0.50\n)\n,\n\\text{mAR}_{50}=\\frac{1}{N_{c}}\\sum_{c=1}^{N_{c}}\\text{AR}_{c}(0.50),\n(3)\nwhere\nAR\nc\nâ€‹\n(\n0.50\n)\n\\text{AR}_{c}(0.50)\nis the average recall for class\nc\nc\n. Finally, to evaluate localization robustness across diverse overlap requirements, the component-averaged Mean Average Precision (\nmmAP\n50\n:\n95\n\\text{mmAP}_{50:95}\n) is computed by averaging the mAP over multiple IoU thresholds\nğ’¯\n=\n{\n0.50\n,\n0.55\n,\nâ€¦\n,\n0.95\n}\n\\mathcal{T}=\\{0.50,0.55,\\dots,0.95\\}\n:\nmmAP\n50\n:\n95\n=\n1\n|\nğ’¯\n|\nâ€‹\nâˆ‘\nt\nâˆˆ\nğ’¯\nmAP\nt\n.\n\\text{mmAP}_{50:95}=\\frac{1}{|\\mathcal{T}|}\\sum_{t\\in\\mathcal{T}}\\text{mAP}_{t}.\n(4)\nIII\nResult\nFigureÂ 5\nvisualizes the detection predictions across five representative SOTIF scenarios in the PeSOTIF dataset. As shown in the first three rows, general-purpose LVLMs demonstrate remarkable robustness under weather-induced degradation, effectively localizing objects even in rain, snow, and dense fog. Furthermore, these models exhibit strong semantic generalization in the last row, successfully grounding uncommon objects that challenge traditional detectors. These qualitative results suggest that LVLMs can translate high-level semantic understanding into reasonable spatial predictions, maintaining operation where traditional systems might fail.\nIII-A\nOverall Performance\nFigureÂ 6\nreports the overall object-detection performance of ten LVLMs relative to YOLOv5\n60e\nbaseline. The results indicate that LVLMs can reach competitive accuracy in SOTIF-related traffic scenes. In particular, Gemini 3 (â‘¤) achieves the highest mAP\n50\nscore, followed by Doubao (â‘¨) and Gemini 2.5 Pro (â‘¢), all exceeding the YOLOv5\n60e\nbaseline. Moreover, the ranking is not fully explained by model parameter scale, as Gemini 2.5 Flash (â‘£, 5B) outperforms larger models, such as Grok 4 (1.7T) and Qwen 3-Max (1T), in terms of mAP\n50\n, highlighting the role of architecture and training in spatially grounded prediction.\n\\begin{overpic}[width=411.93767pt]{figure/overall_performance_heatmap.png}\n\\par\\put(-2.0,-3.0){\\tiny YOLOv5${}_{60e}$}\n\\put(11.0,-3.0){\\footnotesize\\char 172}\n\\put(20.0,-3.0){\\footnotesize\\char 173}\n\\put(28.0,-3.0){\\footnotesize\\char 174}\n\\put(36.0,-3.0){\\footnotesize\\char 175}\n\\put(44.0,-3.0){\\footnotesize\\char 176}\n\\put(51.0,-3.0){\\footnotesize\\char 177}\n\\put(60.0,-3.0){\\footnotesize\\char 178}\n\\put(68.0,-3.0){\\footnotesize\\char 179}\n\\put(76.0,-3.0){\\footnotesize\\char 180}\n\\put(84.0,-3.0){\\footnotesize\\char 181}\n\\par\\put(-3.0,5.0){\\footnotesize\\char 182}\n\\put(-3.0,16.0){\\footnotesize\\char 183}\n\\put(-3.0,28.0){\\footnotesize\\char 184}\n\\par\\end{overpic}\nFigure 6:\nOverall object-detection performance of LVLMs in PeSOTIF dataset. LVLMs:Â â‘  Grok 2;Â â‘¡ Grok 4Â â‘¢ Gemini 2.5 Pro;Â â‘£ Gemini 2.5 Flash;Â â‘¤ Gemini 3;Â â‘¥ GPT 5;Â â‘¦ Qwen;Â â‘§ Claude;Â â‘¨ Doubao;Â â‘© Hunyuan. Metrics:Â â¶ mmAP\n50:95\n;Â â· mAR\n50\n;Â â¸ mAP\n50\n.\nIII-B\nPerformance in Environment Subset\nFigureÂ 7\nsummarizes detection performance on the environment subset of PeSOTIF, covering perception-degrading conditions such as rain, snow, particulates, and challenging illumination. Overall, several LVLMs remain competitive under these conditions. Gemini 3 (â‘¤) achieves the best performance and surpasses the YOLOv5\n60e\nbaseline in both precision- and recall-oriented metrics (e.g.,\nm\nâ€‹\nA\nâ€‹\nP\n50\nmAP_{50}\nof 0.690 vs. 0.441). Doubao (â‘¨) and Gemini 2.5 Pro (â‘¢) also show strong results, indicating that LVLM-based approaches can better tolerate environmental interference in long-tail traffic scenes.\n\\begin{overpic}[width=411.93767pt]{figure/env_subset_performance_heatmap.png}\n\\par\\put(-2.0,-3.0){\\tiny YOLOv5${}_{60e}$}\n\\put(12.0,-3.0){\\footnotesize\\char 172}\n\\put(20.0,-3.0){\\footnotesize\\char 173}\n\\put(28.0,-3.0){\\footnotesize\\char 174}\n\\put(36.0,-3.0){\\footnotesize\\char 175}\n\\put(44.0,-3.0){\\footnotesize\\char 176}\n\\put(52.0,-3.0){\\footnotesize\\char 177}\n\\put(60.0,-3.0){\\footnotesize\\char 178}\n\\put(68.0,-3.0){\\footnotesize\\char 179}\n\\put(76.0,-3.0){\\footnotesize\\char 180}\n\\put(84.0,-3.0){\\footnotesize\\char 181}\n\\par\\put(-3.0,5.0){\\footnotesize\\char 182}\n\\put(-3.0,16.0){\\footnotesize\\char 183}\n\\put(-3.0,28.0){\\footnotesize\\char 184}\n\\par\\end{overpic}\nFigure 7:\nPerformance of LVLMs in environment subset of PeSOTIF. LVLMs:Â â‘  Grok 2;Â â‘¡ Grok 4;Â â‘¢ Gemini 2.5 Pro;Â â‘£ Gemini 2.5 Flash;Â â‘¤ Gemini 3;Â â‘¥ GPT 5;Â â‘¦ Qwen;Â â‘§ Claude;Â â‘¨ Doubao;Â â‘© Hunyuan. Metrics:Â â¶ mmAP\n50:95\n;Â â· mAR\n50\n;Â â¸ mAP\n50\n.\nIII-B\n1\nPerformance in Natural Subset\nFigureÂ 8\nsummarizes detection performance on the natural subset, which consists of real-world footage with complex and unstructured visual noise. The strongest LVLM outperforms the YOLOv5\n60e\nbaseline across the IoU-based metrics. In particular, Gemini 3 (â‘¤) achieves the best results and exceeds the baseline in both precision and recall (e.g.,\nm\nâ€‹\nA\nâ€‹\nP\n50\nmAP_{50}\nof 0.717 vs. 0.422). Doubao (â‘¨) and Gemini 2.5 Pro (â‘¢) also perform consistently well, indicating that several LVLMs maintain competitive detection quality in authentic driving scenes.\n\\begin{overpic}[width=411.93767pt]{figure/natural_subset_performance_heatmap.png}\n\\par\\put(-2.0,-3.0){\\tiny YOLOv5${}_{60e}$}\n\\put(11.0,-3.0){\\footnotesize\\char 172}\n\\put(20.0,-3.0){\\footnotesize\\char 173}\n\\put(28.0,-3.0){\\footnotesize\\char 174}\n\\put(36.0,-3.0){\\footnotesize\\char 175}\n\\put(44.0,-3.0){\\footnotesize\\char 176}\n\\put(52.0,-3.0){\\footnotesize\\char 177}\n\\put(60.0,-3.0){\\footnotesize\\char 178}\n\\put(68.0,-3.0){\\footnotesize\\char 179}\n\\put(76.0,-3.0){\\footnotesize\\char 180}\n\\put(84.0,-3.0){\\footnotesize\\char 181}\n\\par\\put(-3.0,5.0){\\footnotesize\\char 182}\n\\put(-3.0,16.0){\\footnotesize\\char 183}\n\\put(-3.0,28.0){\\footnotesize\\char 184}\n\\par\\end{overpic}\nFigure 8:\nLVLMs performance in natural subset. LVLMs:Â â‘  Grok 2;Â â‘¡ Grok 4;Â â‘¢ Gemini 2.5 Pro;Â â‘£ Gemini 2.5 Flash;Â â‘¤ Gemini 3;Â â‘¥ GPT 5;Â â‘¦ Qwen;Â â‘§ Claude;Â â‘¨ Doubao;Â â‘© Hunyuan. Metrics:Â â¶ mmAP\n50:95\n;Â â· mAR\n50\n;Â â¸ mAP\n50\n.\nIII-B\n2\nPerformance in Handcraft Subset\nFigureÂ 9\nreports the detection performance in the handcraft subset, which is composed of real-world images with manually constructed environment degradations. A different performance pattern is observed compared with the natural subset. The YOLOv5\n60e\nbaseline achieves the highest accuracy in this subset, outperforming all LVLMs in terms of\nm\nâ€‹\nA\nâ€‹\nP\n50\nmAP_{50}\n.\nIn contrast, LVLMs retain an advantage in object coverage. Doubao (â‘¨) achieves higher recall than the baseline, suggesting fewer missed detections in this setting. Overall, these results point to a trade-off: the baseline shows stronger localization on the handcraft subset, whereas LVLMs provide better coverage of the annotated objects.\n\\begin{overpic}[width=411.93767pt]{figure/handcraft_subset_performance_heatmap.png}\n\\par\\put(-2.0,-3.0){\\tiny YOLOv5${}_{60e}$}\n\\put(12.0,-3.0){\\footnotesize\\char 172}\n\\put(20.0,-3.0){\\footnotesize\\char 173}\n\\put(28.0,-3.0){\\footnotesize\\char 174}\n\\put(36.0,-3.0){\\footnotesize\\char 175}\n\\put(44.0,-3.0){\\footnotesize\\char 176}\n\\put(52.0,-3.0){\\footnotesize\\char 177}\n\\put(60.0,-3.0){\\footnotesize\\char 178}\n\\put(68.0,-3.0){\\footnotesize\\char 179}\n\\put(76.0,-3.0){\\footnotesize\\char 180}\n\\put(84.0,-3.0){\\footnotesize\\char 181}\n\\par\\put(-3.0,5.0){\\footnotesize\\char 182}\n\\put(-3.0,16.0){\\footnotesize\\char 183}\n\\put(-3.0,28.0){\\footnotesize\\char 184}\n\\par\\end{overpic}\nFigure 9:\nLVLMs performance in handcraft subset. LVLMs:Â â‘  Grok 2;Â â‘¡ Grok 4;Â â‘¢ Gemini 2.5 Pro;Â â‘£ Gemini 2.5 Flash;Â â‘¤ Gemini 3;Â â‘¥ GPT 5;Â â‘¦ Qwen;Â â‘§ Claude;Â â‘¨ Doubao;Â â‘© Hunyuan. Metrics:Â â¶ mmAP\n50:95\n;Â â· mAR\n50\n;Â â¸ mAP\n50\n.\nIII-C\nPerformance in Object Subset\nFigureÂ 10\nillustrates the detection performance heatmap on the object subset. Among all evaluated LVLMs, Gemini 3 (â‘¤) achieves the strongest performance, attaining the highest\nm\nâ€‹\nm\nâ€‹\nA\nâ€‹\nP\n50\n:\n95\nmmAP_{50:95}\n,\nm\nâ€‹\nA\nâ€‹\nR\n50\nmAR_{50}\n, and\nm\nâ€‹\nA\nâ€‹\nP\n50\nmAP_{50}\n. In comparison, the YOLOv5\n60e\nbaseline exhibits competitive precision in terms of\nm\nâ€‹\nA\nâ€‹\nP\n50\nmAP_{50}\nbut substantially lower recall, highlighting its limited coverage under object-centric challenges. Other LVLMs display diverse precision-recall trade-offs, whereas Gemini 3 (â‘¤) demonstrates a more balanced capability across localization accuracy and detection coverage in the object subset.\n\\begin{overpic}[width=411.93767pt]{figure/object_subset_performance_heatmap.png}\n\\par\\put(-2.0,-3.0){\\tiny YOLOv5${}_{60e}$}\n\\put(11.0,-3.0){\\footnotesize\\char 172}\n\\put(20.0,-3.0){\\footnotesize\\char 173}\n\\put(28.0,-3.0){\\footnotesize\\char 174}\n\\put(36.0,-3.0){\\footnotesize\\char 175}\n\\put(44.0,-3.0){\\footnotesize\\char 176}\n\\put(52.0,-3.0){\\footnotesize\\char 177}\n\\put(60.0,-3.0){\\footnotesize\\char 178}\n\\put(68.0,-3.0){\\footnotesize\\char 179}\n\\put(76.0,-3.0){\\footnotesize\\char 180}\n\\put(84.0,-3.0){\\footnotesize\\char 181}\n\\par\\put(-3.0,5.0){\\footnotesize\\char 182}\n\\put(-3.0,16.0){\\footnotesize\\char 183}\n\\put(-3.0,28.0){\\footnotesize\\char 184}\n\\par\\end{overpic}\nFigure 10:\nLVLMs performance in object subset of PeSOTIF. LVLMs:Â â‘  Grok 2;Â â‘¡ Grok 4;Â â‘¢ Gemini 2.5 Pro;Â â‘£ Gemini 2.5 Flash;Â â‘¤ Gemini 3;Â â‘¥ GPT 5;Â â‘¦ Qwen;Â â‘§ Claude;Â â‘¨ Doubao;Â â‘© Hunyuan. Metrics:Â â¶ mmAP\n50:95\n;Â â· mAR\n50\n;Â â¸ mAP\n50\n.\nIII-D\nInference Time\nFigureÂ 11\ncompares the average inference time per image across the ten evaluated LVLMs. Substantial efficiency differences are observed, with GPT-5 and Gemini 3 exhibiting the highest latency (exceeding\n50\ns\n50\\text{\\,}\\mathrm{s}\n), while Claude and Doubao achieve markedly lower inference time (approximately\n5\ns\n5\\text{\\,}\\mathrm{s}\n). When jointly considered with the detection results in\nFigureÂ 7\nto\nFigureÂ 10\n, Doubao offers a more favorable trade-off between detection performance and computational efficiency, making it better suited for latency-constrained SOTIF applications.\nFigure 11:\nAverage inference time per image for the evaluated LVLMs.\nIV\nConclusion and Discussion\nThis study conducted a comprehensive evaluation of LVLMs for 2D object detection within SOTIF-relevant traffic scenarios. Utilizing the PeSOTIF dataset and a visual-prompting methodology, we demonstrated that general-purpose LVLMs can effectively translate high-level semantic understanding into structured spatial output. The experimental results reveal that top-performing LVLMs, particularly Gemini 3 and Doubao, significantly outperform the YOLOv5\n60e\nbaseline in adverse weather conditions and rare object scenarios. While the baseline detector retains a slight advantage in geometric bounding box precision for clear, synthetic targets, LVLMs exhibit superior robustness in the\nâ€œnaturalâ€\nsubsets, surpassing traditional methods in recall by a substantial margin (over 25%). This indicates that the global contextual reasoning of LVLMs offers greater resilience against the degradation of local visual features, which typically impairs conventional detectors like the YOLOv5\n60e\nbaseline.\nDespite the promising detection capabilities, practical deployment faces challenges regarding computational efficiency. Our analysis identified a critical trade-off between performance and latency; while Doubao achieves an optimal balance with leading accuracy in this study and lower inference time, the current processing speed of LVLMs generally lags behind the real-time requirements of automated driving. Consequently, LVLMs are currently best positioned as a high-level\nâ€œsafety validatorâ€\nor a redundant perception branch to handle long-tail corner cases that baffle conventional sensors. Future research should target the distillation of these LVLMs into lightweight architectures and the refinement of end-to-end spatial alignment to further bridge the gap between semantic reasoning and real-time execution.\nReferences\n[1]\nH. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom\n(2020-06)\nnuScenes: a multimodal dataset for autonomous driving\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§I\n.\n[2]\nA. Geiger, P. Lenz, and R. Urtasun\n(2012)\nAre we ready for autonomous driving? the KITTI vision benchmark suite\n.\nIn\n2012 IEEE Conference on Computer Vision and Pattern Recognition\n,\nVol.\n,\npp.Â 3354â€“3361\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[3]\nS. Huang, F. Shi, C. Sun, J. Zhong, M. Ning, Y. Yang, Y. Lu, H. Wang, and A. Khajepour\n(2025)\nDriveSOTIF: advancing SOTIF through multimodal large language models\n.\nIEEE Transactions on Vehicular Technology\n.\nCited by:\nÂ§I\n.\n[4]\nCited by:\nÂ§I\n.\n[5]\nS. Jain, S. Thapa, K. Chen, A. L. Abbott, and A. Sarkar\n(2024)\nSemantic understanding of traffic scenes with large vision language models\n.\nIn\n2024 IEEE Intelligent Vehicles Symposium (IV)\n,\nVol.\n,\npp.Â 1580â€“1587\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[6]\nN. Jegham, C. Y. Koh, M. Abdelatti, and A. Hendawi\n(2025)\nYOLO evolution: a comprehensive benchmark and architectural review of yolov12, yolo11, and their previous versions\n.\nExternal Links:\n2411.00201\n,\nLink\nCited by:\nÂ§\nII-E\n.\n[7]\nY. Jiang, S. Qiang, W. Li, and Y. Liang\n(2025)\nLLM-DiffAug: enhancing few-shot object detection via LLM-guided diffusion augmentation\n.\nKnowledge-Based Systems\n326\n,\npp.Â 114066\n.\nCited by:\nÂ§I\n.\n[8]\nT. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. DollÃ¡r, and C. L. Zitnick\n(2014)\nMicrosoft COCO: common objects in context\n.\nIn\nComputer Vision â€“ ECCV 2014\n,\nD. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars (Eds.)\n,\nCham\n,\npp.Â 740â€“755\n.\nExternal Links:\nISBN 978-3-319-10602-1\nCited by:\nÂ§\nII-E\n.\n[9]\nE. Marti, M. A. de Miguel, F. Garcia, and J. Perez\n(2019)\nA review of sensor technologies for perception in automated driving\n.\nIEEE Intelligent Transportation Systems Magazine\n11\n(\n4\n),\npp.Â 94â€“108\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[10]\nL. Peng, J. Li, W. Shao, and H. Wang\n(2023)\nPeSOTIF: a challenging visual dataset for perception SOTIF problems in long-tail traffic scenarios\n.\nIn\n2023 IEEE Intelligent Vehicles Symposium (IV)\n,\npp.Â 1â€“8\n.\nCited by:\nFigure 1\n,\nÂ§I\n,\nFigure 3\n,\nÂ§\nII-A\n,\nÂ§II\n.\n[11]\nL. Peng, H. Wang, and J. Li\n(2021)\nUncertainty evaluation of object detection algorithms for autonomous vehicles\n.\nAutomotive Innovation\n4\n(\n3\n),\npp.Â 241â€“252\n.\nCited by:\nÂ§I\n.\n[12]\nJ. Redmon, S. Divvala, R. Girshick, and A. Farhadi\n(2016-06)\nYou only look once: unified, real-time object detection\n.\nIn\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§I\n.\n[13]\nK. Turkowski\n(1990)\nFilters for common resampling tasks\n.\nIn\nGraphics gems\n,\npp.Â 147â€“165\n.\nCited by:\nÂ§\nII-B\n1\n.\n[14]\nS. Wang, Z. Wang, S. Hong, P. Wang, and S. Zhang\n(2025)\nEnsuring SOTIF: enhanced object detection techniques for autonomous driving\n.\nAccident Analysis & Prevention\n218\n,\npp.Â 108094\n.\nCited by:\nÂ§I\n.\n[15]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou\n(2022)\nChain-of-thought prompting elicits reasoning in large language models\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.)\n,\nVol.\n35\n,\npp.Â 24824â€“24837\n.\nCited by:\nÂ§\nII-C\n2\n.\n[16]\nF. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell\n(2020-06)\nBDD100K: a diverse driving dataset for heterogeneous multitask learning\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§I\n.",
    "preview_text": "Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.\n\nA Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions\nJi Zhou\nâˆ—\n, Yilin Ding\nâˆ—\n, Yongqi Zhao, Jiachen Xu, and Arno Eichberger\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\nâˆ—\nThe first two authors contribute equally to this work.Â (\nCorresponding author: Yongqi Zhao)\nJi Zhou, Yilin Ding, Yongqi Zhao, and Arno Eichberger are with the Institute of Automotive Engineering, Graz University of Technology, 8010, Graz, Austria (e-mail: ji.zhou@student.tugraz.at; yilin.ding@student.tugraz.at; yongqi.zhao@tugraz.at; arno.eichberger@",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "VLM",
        "Vision-Language-Action Model",
        "fine tune"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡ç³»ç»Ÿè¯„ä¼°äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨SOTIFæ¡ä»¶ä¸‹è¿›è¡Œ2Dç‰©ä½“æ£€æµ‹çš„æ€§èƒ½ï¼Œä¸YOLOåŸºå‡†å¯¹æ¯”ï¼Œå‘ç°LVLMsåœ¨å¤æ‚è‡ªç„¶åœºæ™¯ä¸­å…·æœ‰æ›´é«˜çš„å¬å›ç‡å’Œé²æ£’æ€§ï¼Œä½†å‡ ä½•ç²¾åº¦è¾ƒä½ï¼Œæ”¯æŒå…¶ä½œä¸ºé«˜çº§å®‰å…¨éªŒè¯å™¨ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T10:58:24Z",
    "created_at": "2026-02-03T15:53:07.319905",
    "updated_at": "2026-02-03T15:53:07.319912"
}