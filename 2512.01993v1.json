{
    "id": "2512.01993v1",
    "title": "RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies",
    "authors": [
        "Guillermo Garcia-Cobo",
        "Maximilian Igl",
        "Peter Karkus",
        "Zhejun Zhang",
        "Michael Watson",
        "Yuxiao Chen",
        "Boris Ivanovic",
        "Marco Pavone"
    ],
    "abstract": "è‡ªåŠ¨é©¾é©¶ç­–ç•¥é€šå¸¸é€šè¿‡äººç±»æ¼”ç¤ºçš„å¼€ç¯è¡Œä¸ºå…‹éš†è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œæ­¤ç±»ç­–ç•¥åœ¨é—­ç¯éƒ¨ç½²æ—¶ä¼šé­é‡åå˜é‡åç§»é—®é¢˜ï¼Œå¯¼è‡´è¯¯å·®ä¸æ–­ç´¯ç§¯ã€‚æˆ‘ä»¬æå‡º\"æ»šåŠ¨æ¼”ç¤º\"æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åˆ©ç”¨ç­–ç•¥è‡ªèº«é—­ç¯æ»šåŠ¨ç”Ÿæˆçš„æ•°æ®ä½œä¸ºé¢å¤–è®­ç»ƒæ ·æœ¬ï¼Œä»è€Œç¼“è§£åå˜é‡åç§»çš„ç®€æ´é«˜æ•ˆæ–¹æ¡ˆã€‚åœ¨æ»šåŠ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œè¯¥æ–¹æ³•èå…¥ä¸“å®¶å¼•å¯¼æœºåˆ¶ï¼Œä½¿è½¨è¿¹åå‘é«˜è´¨é‡è¡Œä¸ºæ¨¡å¼ï¼Œä¸ºå¾®è°ƒé˜¶æ®µæä¾›ä¿¡æ¯ä¸°å¯Œä¸”çœŸå®åº¦é«˜çš„æ¼”ç¤ºæ•°æ®ã€‚ç›¸è¾ƒäºå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ¡ˆä»…éœ€æ•°é‡çº§æ›´å°‘çš„æ•°æ®å³å¯å®ç°ç¨³å¥çš„é—­ç¯é€‚åº”ï¼ŒåŒæ—¶è§„é¿äº†å…ˆå‰é—­ç¯ç›‘ç£å¾®è°ƒæ–¹æ³•çš„é™åˆ¶æ€§å‡è®¾ï¼Œå¯æ‹“å±•åº”ç”¨äºåŒ…æ‹¬ç«¯åˆ°ç«¯é©¾é©¶åœ¨å†…çš„æ›´å¹¿æ³›é¢†åŸŸã€‚æˆ‘ä»¬åœ¨WOSACå¤§è§„æ¨¡äº¤é€šä»¿çœŸåŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå…¶è¡¨ç°ä¸ç°æœ‰é—­ç¯ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼›åœ¨åŸºäºç¥ç»é‡å»ºçš„é«˜ä¿çœŸç«¯åˆ°ç«¯é©¾é©¶ä»¿çœŸå™¨AlpaSimä¸­ï¼Œè¯¥æ–¹æ³•å°†é©¾é©¶è¯„åˆ†æå‡41%ï¼Œç¢°æ’ç‡é™ä½54%ã€‚",
    "url": "https://arxiv.org/abs/2512.01993v1",
    "html_url": "https://arxiv.org/html/2512.01993v1",
    "html_content": "RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies\nGuillermo Garcia-Cobo\nâˆ—1\nMaximilian Igl\nâˆ—1\nPeter Karkus\nâˆ—1\nZhejun Zhang\nâˆ—2â€ \nMichael Watson\n1\nYuxiao Chen\n1\nBoris Ivanovic\n1\nMarco Pavone\n1,3\n1\nNVIDIA Research\n2\nHuawei VN Research Center\n3\nStanford University\n{guillermog, migl, pkarkus}@nvidia.com\nAbstract\nAutonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors.\nWe introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policyâ€™s own closed-loop rollouts as additional training data.\nDuring rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning.\nThis approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving.\nWe demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstructionâ€“based simulator for end-to-end driving, where it improves driving score by 41% and reduces collisions by 54%.\n1\nIntroduction\n*\n*\nfootnotetext:\nEqual contribution, alphabetically sorted.\n$\\dagger$\n$\\dagger$\nfootnotetext:\nWork performed during an internship at NVIDIA Research.\nAutonomous vehicle (AV) policies are typically trained with behavior cloning\n(BC) of human demonstrations, which is scalable but inherently open loop: it\nassumes i.i.d. inputs and optimizes one-step accuracy under the dataset\ndistribution. Deployed in closed loop, policies influence their own\nobservations, creating a train-test mismatch that induces covariate shift,\ncompounds errors, and reduces robustness to long-tail and interactive scenarios.\nOn the other hand, End-to-end (E2E) policies are becoming the new norm of AV policy learning,\nwhich map sensor inputs directly to trajectories or\ncontrols. By coupling perception,\nprediction, and planning, they offer data efficiency, simpler deployment, and\nbetter long-horizon coordination than hand-engineered\nstacks\n[\nchen2024end\n,\nhwang2024emma\n,\ntian2024drivevlm\n,\nwayve2025\n,\nNVIDIASafety2025\n,\nwang2025alpamayo\n]\n.\nWhile RL directly optimizes closed-loop behavior, it remains impractical for\nend-to-end driving\n[\nkiran2021deep\n]\ndue to brittle reward design and\nthe cost of safe exploration and high-fidelity simulation. This leaves a gap for\na scalable closed-loop training recipe for E2E driving that retains supervised simplicity and\ndata efficiency.\nFigure 1\n:\nRoaD closed-loop SFT with expert-guided rollout\n.\nLeft:\nExpert-guided rollouts in simulation yield additional training data that is incorporated into the dataset and used to fine-tune the policy, improving subsequent rollouts.\nRight:\nExpert-guided trajectories are sampled from the policy and biased toward the expert to produce higher-quality demonstrations.\nClosed-loop supervised fine-tuning (CL-SFT) has recently emerged as a promising\nalternative to RL (see\nFig.\n1\n). The core idea of CL-SFT is to generate expert-biased on-policy\nrollouts in simulation and use them as additional demonstrations for\nfine-tuning, combining the simplicity of supervised learning with the benefits\nof closed-loop training. The key challenge is how to bias the rollouts towards\nhigh-quality behavior such that the fine-tuning step improves the policy. In\ntraffic simulation, Closest Among Top-K (CAT-K)\n[\nzhang2025closed\n]\ninstantiates this idea by selecting, at each step, the closest among a small set\nof policy-proposed candidate actions to the ground-truth trajectory. On these\ngenerated trajectories, CAT-K derives fine-tuning action targets using an\ninverse dynamics model that chooses the action bringing the agent closest to\nground truth.\nWhile effective for traffic modeling, CAT-K is poorly suited to modern E2E\npolicies because it assumes: (i) discrete actions; (ii) deterministic dynamics and known inverse dynamics;\n(iii) a diverse pretrained policy where at least one action sample lies close to the ground-truth\ntrajectory at each time step; and (iv) that fresh on-policy trajectories can be generated\ncontinuously during training. In E2E driving, none of these assumptions usually hold: policies\nmay output multi-token plans or continuous trajectories, as in the case of diffusion policies;\nthe dynamics (including downstream controllers) are stochastic without closed-form inverse dynamics;\nthe action distribution is typically less diverse due to safety- and,\npredictability-oriented training (unlike traffic agents that deliberately\npromote diversity); and regenerating closed-loop rollouts at every optimization\nstep is prohibitively expensive.\nIn this work we introduce\nRollouts as Demonstrations (RoaD)\n, a novel\nCL-SFT approach that addresses all limitations above (\nFig.\n2\n).\nFirst, RoaD retains the expert-biased rollout principle, but removes the need for a recovery action\nby treating the policyâ€™s closed-loop rollouts directly as additional\ndemonstrations for SFT. Empirically we find that this strategy achieves performance on par with, or\nbetter than CAT-K on large-scale traffic-simulation benchmarks.\nSecond, we replace the Top-K selection with\nsampling K action candidates so that RoaD can be applied to a more general class of policies.\nThird, when limited action diversity prevents naive\nRoaD from being guided by the ground truth, we introduce a lightweight\nrecovery-mode policy output that enables following the ground-truth trajectory even when it is not close to any of the top-k most likely actions. Finally, to reduce\ncollection cost, we show that reusing rollout datasets across multiple\noptimization steps results in only marginal performance degradation, greatly\nimproving data efficiency.\nFigure 2\n:\nRoaD method overview.\nA pretrained policy generates rollouts in simulation via expert-guided sampling and an optional recovery mode. The collected trajectories form the CL-SFT dataset used to fine-tune the policy. The cycle can be repeated.\nIn experiments, we first validate RoaD for traffic simulation using the Waymo Open Simulation Agent Challenge (WOSAC). RoaD outperforms or matches CAT-K, even when it generates CL experiences only once with the base policy, and the performance improves further the more frequently the data is updated.\nWe then apply RoaD to an E2E driving task to fine-tune a VLM-based policy deployed in AlpaSim\n[\nnvidia2025alpasim\n]\n, an E2E AV simulator that reconstructs real-world 3D scenes using SOTA 3D Gaussian splatting, 3DGUT\n[\nwu20253dgut\n]\n. RoaD fine-tuning improves driving scores by 41% and reduces collisions by 54% over the base model in previously unseen scenarios.\nIn summary, our conclusions are as follows.\nâ€¢\nWe introduce a novel CL-SFT algorithm, RoaD, that removes restrictive assumptions made by prior work.\nâ€¢\nWe apply RoaD to traffic simulation and match or outperform the previous SOTA CL-SFT method.\nâ€¢\nWe apply RoaD to E2E driving and achieve substantial improvement in closed-loop driving metrics.\n2\nRelated work\nClosed-loop training in AV.\nClassic closed-loop training methods such as DAgger\n[\nross2011reduction\n]\nand DART\n[\nlaskey2017dart\n]\niteratively collect states induced by the learner and query the expert to label them, thus adapting the training distribution to the learnerâ€™s roll-out distribution.\nHowever, in driving applications expert interventions are expensive, unsafe, or infeasible during interaction. Therefore, despite efforts on reducing expert burden in variants of the algorithm\n[\nross2014reinforcement\n,\nsun2017deeplyaggrevated\n,\nzhang2017queryefficient\n,\nkahn2018self\n]\n, repeated expert relabeling remains impractical in autonomous driving.\nReinforcement learning presents a natural closed-loop training paradigm with a large body of literature in autonomous driving\n[\nisele2018navigating\n,\nzhu2020safe\n,\nsaxena2020driving\n,\nma2021reinforcement\n]\n. From early works such as\n[\nsallab2017deep\n]\nto more recent studies applying hierarchical RL\n[\nchen2021interpretable\n]\n, curriculum-based RL\n[\ncodevilla2019exploring\n]\n, or model-based RL\n[\nxu2020guided\n]\n, they face two major challenges. (1) reward design that captures the diverse and often conflicting requirements; (2) training stability, compute cost, and safety constraints for E2E driving systems. These practical issues often lead to slow convergence, brittle behavior, and poor transfer to rare scenarios when applying RL to AV.\nInspired by recent work on CAT-K\n[\nzhang2025closed\n]\n, our method performs closed-loop training\nwithout\nrelying\non reinforcement rewards or on-demand expert relabeling. It derives training targets from expert demonstrations\nand performs supervised updates in the closed-loop setting, thereby achieving the stability and data-efficiency benefits of\nsupervised learning. Importantly, unlike CAT-K, our method does not require a discrete action space and deterministic dynamics, which extends its applicability to domains such as E2E driving.\nEnd-to-end driving.\nModern AV systems increasingly favor end-to-end designs over modular pipelines to reduce information loss and simplify training\n[\nkarkus2022diffstack\n]\n. Work in this direction was pioneered by, among others, DAVE-2\n[\nbojarski2016end\n]\n. Subsequent approaches, such as ChauffeurNet\n[\nbansal2019chauffeurnet\n]\n, UniAD\n[\nhu2022planning\n]\n, and PARA-Drive\n[\nweng2024paradrive\n]\nintroduced intermediate BEV representations, and integrated perception, prediction, and planning within unified multi-task networks.\nIn recent years, the emergent class of visionâ€languageâ€action (VLA) models has adapted large visionâ€“language models (VLMs) to driving, combining visual input, language instructions, and trajectory generation\n[\njiang2025survey\n,\nzhou2024vlmsurvey\n,\nhwang2024emma\n,\ntian2024drivevlm\n,\nzhou2025autovla\n,\nwang2025alpamayo\n]\n. These models promise richer semantic understanding and human-aligned decision making. In particular, systems like EMMA\n[\nhwang2024emma\n]\nand Alpamayo-R1\n[\nwang2025alpamayo\n]\ndemonstrate how VLM-based architectures can ingest camera imagery (and optionally language navigation cues) and output trajectories in a unified framework. While equipped with strong semantic understanding capabilities, these models are largely trained in open-loop, and thus remain prone to covariate shift.\n3\nBackground\nOur goal is to finetune a pretrained driving policy in closed-loop\nto minimize the covariate shift between open-loop pre-training and closed-loop\ndeployment. To achieve this without access to a reward function (which is hard\nto define for this task), we use closedâ€‘loop supervised fineâ€‘tuning (CLâ€‘SFT).\nIn the following, we first formalize the problem and review CAT-K\n[\nzhang2025closed\n]\n, a recent CLâ€‘SFT instantiation.\n3.1\nProblem formulation\nWe are given a policy\nÏ€\nÎ¸\n=\np\nâ€‹\n(\na\nt\n|\no\n<\nt\n)\n\\pi_{\\theta}=p(a_{t}|o_{<t})\nthat maps a history of observation inputs\no\n<\nt\no_{<t}\nto action outputs\na\nt\na_{t}\n. The policy is pre-trained with behavior cloning\n(BC), using a dataset of expert demonstrations\nğ’Ÿ\n=\n{\n(\no\n0\n:\nT\nE\n,\ni\n,\na\n0\n:\nT\nE\n,\ni\n)\n}\ni\n=\n1\n|\nğ’Ÿ\n|\n\\mathcal{D}=\\{({o}^{E,i}_{0:T},{a}^{E,i}_{0:T})\\}_{i=1}^{|\\mathcal{D}|}\n. Our\ngoal is to perform closed-loop finetuning of\nÏ€\nÎ¸\n\\pi_{\\theta}\nto minimize the\ncovariate shift between open-loop training and closed-loop deployment. We\nassume access to a stochastic simulator that generates a next observation given\naction, previous observation and some internal state\nğ’«\nâ€‹\n(\no\nt\n+\n1\nâˆ£\no\nt\n,\na\nt\n;\nâ‹…\n)\n\\mathcal{P}(o_{t+1}\\mid{o}_{t},a_{t};\\cdot)\n, but no access to an on-demand expert nor to a reward\nfunction.\nThe contents of the observation\no\nt\no_{t}\nare domain specific. In\ntraffic simulation,\no\nt\no_{t}\nincludes the positions, velocities, and orientations\nof all nearby agents, together with a vectorized map (lane markings, wait lines, etc.);\nin E2E driving, it includes the ego vehicleâ€™s sensor inputs (e.g., multi-view\ncamera images) and estimated egomotion (e.g., pose, steering angle, velocity,\nand acceleration).\nWe denote by\ns\nt\nâˆˆ\nâ„\nD\ns\ns_{t}\\in\\mathbb{R}^{D_{s}}\nthe pose of the controlled agent.\nFor notational simplicity, we assume\ncontrol of a single agent. Since RoaD operates per agent, extending to\nmulti-agent control, as in our traffic simulation experiments, is\nstraightforward.\nUnlike prior work, we make no strong assumptions on the structure of\na\nt\na_{t}\n: it\nmay represent a state delta, as is common in\ntraffic simulation; a continuous control\nsignal, a waypoint, or a trajectory. Single-step control inputs are\nexecuted through forward dynamics, respecting\nvehicle motion constraints, while waypoints and trajectories are tracked by\nlow-level controllers. Predicting\nT\npred\nT_{\\mathrm{pred}}\n-step trajectories\nis common because such\naction chunking\nencourages longâ€‘horizon reasoning\nand often improves accuracy with open-loop training. For\nnotational simplicity, we refer to all these outputs uniformly as\na\nt\na_{t}\nand use\ns\nt\n+\n1\n=\nf\nâ€‹\n(\ns\nt\n,\na\nt\n)\ns_{t+1}=f(s_{t},a_{t})\nto denote the agent state evolution over time.\nFurther, prior work assumed a policy with discrete modes to select top\nK\nK\npredictions, such as next-token-prediction (NTP) traffic simulation models that\nencode actions in a single token. In contrast, we only assume that\nÏ€\nÎ¸\n\\pi_{\\theta}\ncan generate\nK\nK\nindependent action samples, allowing for modern E2E driving\npolicies such as Transformers with simple Gaussian outputs, NTP models with\nmultiple tokens per action, or diffusion and flow-matching\npolicies\n[\nwang2025alpamayo\n]\n.\n3.2\nClosed-loop supervised fine-tuning and CatK\nCL-SFT adapts a pretrained policy by behavior cloning on states encountered under its own closed-loop rollouts,\naligning the training distribution with deployment and mitigating covariate\nshift. CAT-K\n[\nzhang2025closed\n]\nprovides a practical instantiation with two complementary\ncomponents: (i) recovery supervision, which defines action\ntargets that move the rollout back toward the expert trajectory at the visited on-policy rollout states; and\n(ii) expert-proximal rollouts using top\nK\nK\npredictions, which bias action selection during rollouts to\nremain close to the expert, so that the recovery supervision remains valid.\nIntuitively, CAT-K learns â€œhow to get back on trackâ€ while ensuring it never\ndrifts too far from the track in the first place.\nFormally, the algorithm assumes a tokenized model or a distribution with discrete modes, which can be generally written as\nÏ€\nâ€‹\n(\na\nt\nâˆ£\no\n<\nt\n)\n=\nâˆ‘\nm\n=\n1\nM\nÏ€\nâ€‹\n(\na\nâˆ£\nm\n)\nâ€‹\nÏ€\nâ€‹\n(\nm\nâˆ£\no\n<\nt\n)\n\\pi(a_{t}\\mid o_{<t})=\\sum_{m=1}^{M}\\pi(a\\mid m)\\,\\pi(m\\mid o_{<t})\n,\nwhere\nM\nâˆˆ\nâ„•\nM\\in\\mathbb{N}\ndenotes the vocabulary size or number of modes,\nÏ€\nâ€‹\n(\nm\nâˆ£\no\n<\nt\n)\n\\pi(m\\mid o_{<t})\nthe token prediction or mode-selection distribution, and\nÏ€\nâ€‹\n(\na\nâˆ£\nm\n)\n\\pi(a\\mid m)\nthe action decoder or action distribution within each mode.\nIn each rollout step, the algorithm selects the top\nK\nK\npredictions,\nÎ\nt\n=\ntop\nK\n[\nÏ€\nÎ¸\nâ€‹\n(\na\nâˆ£\no\n<\nt\n)\n]\n\\Xi_{t}=\\mathop{\\mathrm{top}^{K}}[\\pi_{\\theta}(a\\mid o_{<t})]\n, where\ntop\nK\n[\nÏ€\n]\n\\mathop{\\mathrm{top}^{K}}[\\pi]\nrepresents finding the\nK\nK\nmost likely tokens/modes under\nÏ€\nâ€‹\n(\nm\nâˆ£\no\n<\nt\n)\n\\pi(m\\mid o_{<t})\nand decoding/sampling the associated action.\nTo bias rollouts toward the expert, the action â€œclosestâ€ to the expert is selected,\na\nt\n=\narg\nâ¡\nmin\na\nâˆˆ\nÎ\nt\nâ¡\nd\nâ€‹\n(\nf\nâ€‹\n(\ns\nt\n,\na\n)\n,\ns\nt\n+\n1\nE\n)\n,\na_{t}=\\arg\\min_{a\\in\\Xi_{t}}d\\big(f(s_{t},a),s^{E}_{t+1}\\big),\n(1)\nwhere\ns\nt\ns_{t}\nis the current agent state,\nf\nf\nare the deterministic dynamics,\ns\nt\n+\n1\nE\ns^{E}_{t+1}\nis the next expert state, and\nd\nâ€‹\n(\nâ‹…\n,\nâ‹…\n)\nd(\\cdot,\\cdot)\nis a distance metric on states, e.g., a weighted\nâ„“\n2\n\\ell_{2}\nover position, heading, and speed.\nFor each state, recovery actions are defined by projecting the expert continuation onto the action vocabulary,\na\n^\nt\n=\narg\nâ¡\nmin\na\nâˆˆ\n{\n1\n,\nâ€¦\n,\n|\nM\n|\n}\nâ¡\nd\nâ€‹\n(\nf\nâ€‹\n(\ns\nt\n,\na\n)\n,\ns\nt\n+\n1\nE\n)\n,\n\\hat{a}_{t}=\\arg\\min_{a\\in\\{1,\\dots,|M|\\}}d\\big(f(s_{t},a),s^{E}_{t+1}\\big),\n(2)\nand\nÎ¸\n\\theta\nis updated with behavior cloning on the rollout states:\nâ„’\nBC\nâ€‹\n(\nÎ¸\n)\n=\nâˆ’\n1\nN\nâ€‹\nT\nâ€‹\nâˆ‘\nt\n=\n0\nT\nâˆ’\n1\nâˆ‘\ni\n=\n1\nN\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\n^\nt\ni\nâˆ£\no\n<\nt\n)\n,\n\\mathcal{L}_{\\mathrm{BC}}(\\theta)=-\\frac{1}{NT}\\sum_{t=0}^{T-1}\\sum_{i=1}^{N}\\log\\pi_{\\theta}(\\hat{a}_{t}^{i}\\mid o_{<t}),\nwhere\nN\nN\nrepresents the number of controlled agents.\nCAT-K is highly effective in achieving this goal for traffic\nsimulation, but is limited when applied to E2E driving, due to the assumptions of deterministic dynamics and known inverse dynamics to construct recovery action targets, and itâ€™s reliance on single-step, discrete policies to efficiently compute the top-K operator.\nWOSAC leaderboard, test split\nMethod\n# model\nparams\nRMM\nâ†‘\n\\uparrow\nKinematic\nmetrics\nâ†‘\n\\uparrow\nInteractive\nmetrics\nâ†‘\n\\uparrow\nMap-based\nmetrics\nâ†‘\n\\uparrow\nmin\nADE\nâ†“\n\\downarrow\nSMART-tiny RoaD (ours)\n7\n7\nM\n0.7847\n0.4932\n0.8106\n0.9178\n1.3042\nSMART-tiny CAT-K\n[\nzhang2025closed\n]\n7\n7\nM\n0.7846\n{0.7846}\n0.4931\n0.8106\n0.9177\n1.3065\nSMART-large\n[\nwu2025smart\n]\n102\n102\nM\n0.7614\n0.7614\n0.4786\n0.4786\n0.8066\n0.8066\n0.8648\n0.8648\n1.3728\n1.3728\nSMART-tiny\n[\nwu2025smart\n]\n7\n7\nM\n0.7591\n0.7591\n0.4759\n0.4759\n0.8039\n0.8039\n0.8632\n0.8632\n1.4062\n1.4062\nTable 1\n:\nWOSAC leaderboard\n[\nwosac2024\n]\nfor traffic simulation comparing CL-SFT approaches\n. RMM stands for Realism Meta Metric, the key metric used for ranking.\nRoaD fine-tuning significantly improves over the base model (SMART-tiny), it outperforms a much larger model from the same model family (SMART-large), and it is on par with the SOTA CL-SFT method, CAT-K.\nWOSAC local val. split\nMethod\nData update\nfrequency\nRMM\nâ†‘\n\\uparrow\nKinematic\nmetrics\nâ†‘\n\\uparrow\nInteractive\nmetrics\nâ†‘\n\\uparrow\nMap-based\nmetrics\nâ†‘\n\\uparrow\nmin\nADE\nâ†“\n\\downarrow\nRoaD fine-tuning\nalways\n0.7673\n0.4871\n0.8107\n0.8715\n{0.8715}\n1.3004\n{1.3004}\nRoaD fine-tuning\nevery 2 epochs\n0.7669\n0.7669\n0.4865\n0.4865\n0.8098\n0.8098\n0.8720\n1.2893\nRoaD fine-tuning\none-off\n0.7664\n0.7664\n0.4865\n0.4865\n0.8093\n0.8093\n0.8712\n0.8712\n1.2983\n1.2983\nSMART-tiny base model\nâ€“\n0.7653\n0.7653\n0.4831\n0.4831\n0.8081\n0.8081\n0.8716\n0.8716\n1.3240\n1.3240\nCAT-K fine-tuning\n[\nzhang2025closed\n]\nalways\n0.7616\n0.7616\n0.4583\n0.4583\n0.8105\n0.8105\n0.8720\n1.3105\n1.3105\nSMART-tiny base model (from\n[\nzhang2025closed\n]\n)\nâ€“\n0.7581\n0.7581\n0.4512\n0.4512\n0.8076\n0.8076\n0.8697\n0.8697\n1.3152\n1.3152\nTable 2\n:\nAblation of data collection frequency for traffic simulation, WOSAC 2% validation split\n. RoaD fine-tuning leads to significant improvement even when closed-loop data is only collected once, achieving similar levels of improvements over the base model as through CAT-K fine-tuning. The more frequently the data is updated the larger the performance gain. Note that results for CAT-K were taken from\n[\nzhang2025closed\n]\n, where likely a different SMART-tiny checkpoint was used as a base model.\n4\nMethod\nAlgorithm 1\nRoaD\n1:\nInput\n: policy\nÏ€\nÎ¸\n\\pi_{\\theta}\n, dataset\nğ’Ÿ\n\\mathcal{D}\n, candidate action set size\nK\nK\n, number of rollouts\nN\nroll\nN_{\\mathrm{roll}}\n, number of training steps\nN\ntrain\nN_{\\mathrm{train}}\n, recovery parameters\n(\nÎ´\nrec\n,\nN\nrec\n)\n(\\delta_{\\mathrm{rec}},N_{\\text{rec}})\n2:\nInitialize dataset\nğ’Ÿ\ngen\n=\n{\n}\n\\mathcal{D}_{\\text{gen}}=\\{\\}\n3:\nfor\nj\n=\n0\n,\nâ€¦\n,\nN\nroll\nj=0,\\dots,N_{\\mathrm{roll}}\ndo\n4:\nStart simulation with scenario\ns\n0\n:\nT\nE\nâˆ¼\nğ’Ÿ\ns^{E}_{0:T}\\sim\\mathcal{D}\n5:\nfor\nt\n=\n1\n,\nâ€¦\n,\nT\nâˆ’\n1\nt=1,\\dots,T-1\ndo\n6:\nSample candidates (\nEq.\n4\n)\n7:\nChoose closest to expert (\nEq.\n5\n)\n8:\nif\ntrigger (\nEq.\n7\n)\nthen\n9:\nUse recovery mode output\na\nt\nâ†\na\nt\nâ€²\na_{t}\\leftarrow a^{\\prime}_{t}\n(\nEq.\n8\n)\n10:\nend\nif\n11:\nStep simulator\no\nt\n+\n1\nâˆ¼\nğ’«\nâ€‹\n(\no\nt\n+\n1\n|\na\nt\n,\no\nt\n;\nâ‹…\n)\no_{t+1}\\sim\\mathcal{P}(o_{t+1}|a_{t},o_{t};\\cdot)\n12:\nend\nfor\n13:\nAdd rollout to dataset\nğ’Ÿ\ngen\nâ†\n(\no\n0\n:\nT\n,\na\n0\n:\nT\n)\n\\mathcal{D}_{\\text{gen}}\\leftarrow(o_{0:T},a_{0:T})\n14:\nend\nfor\n15:\nfor\nj\n=\n0\n,\nâ€¦\n,\nN\ntrain\nj=0,\\dots,N_{\\mathrm{train}}\ndo\n16:\nUpdate\nÎ¸\n\\theta\nwith\n(\no\nt\n,\na\nt\n)\nâˆ¼\nğ’Ÿ\ngen\n(o_{t},a_{t})\\sim\\mathcal{D}_{\\text{gen}}\nand the RoaD loss (\nEq.\n3\n)\n17:\nend\nfor\nOur goal is a CL-SFT recipe that works with modern E2E driving\npolicies and reduces the covariate shift between open-loop training and\nclosed-loop deployment without requiring a reward function.\nOur proposed method, rollouts as demonstrations (RoaD), keeps CAT-Kâ€™s bias-toward-expert\nidea but removes its main constraints while remaining simple and data-efficient.\n4.1\nRollouts as demonstrations (RoaD)\nThe key idea of RoaD is to treat the policyâ€™s own expertâ€‘guided, closedâ€‘loop\nrollouts as additional supervision for fineâ€‘tuning.\nFormally, let\nâ„›\ns\n0\n:\nT\nE\nğ’«\nâ€‹\n[\nÏ€\nÎ¸\n]\n\\mathcal{R}_{{s}^{E}_{0:T}}^{\\mathcal{P}}[\\pi_{\\theta}]\ndenote the expertâ€‘guided\nrollout operator for\nÏ€\nÎ¸\n\\pi_{\\theta}\ngiven the simulator\nğ’«\n\\mathcal{P}\nand expert (GT) trajectory\ns\n0\n:\nT\nE\n{s}^{E}_{0:T}\n.\nWe accumulate generated rollouts in a dataset,\nğ’Ÿ\ngen\n=\n{\n(\no\n0\n:\nT\n,\na\n0\n:\nT\n)\n|\no\n0\n:\nT\n,\na\n0\n:\nT\nâˆ¼\nâ„›\ns\n0\n:\nT\nE\nğ’«\nâ€‹\n[\nÏ€\nÎ¸\n]\n,\ns\n0\n:\nT\nE\nâˆ¼\nğ’Ÿ\n}\n\\mathcal{D}_{\\text{gen}}=\\Big\\{(o_{0:T},a_{0:T})\\,\\Big|\\,o_{0:T},a_{0:T}\\sim\\mathcal{R}_{s^{E}_{0:T}}^{\\mathcal{P}}[\\pi_{\\theta}],\\,\\,{s}^{E}_{0:T}\\sim\\mathcal{D}\\Big\\}\nand fineâ€‘tune the policy by behavior cloning:\nâ„’\nRoaD\nâ€‹\n(\nÎ¸\n)\n=\nâˆ’\nâˆ‘\n(\no\ni\n,\na\ni\n)\nâˆˆ\nğ’Ÿ\ngen\nâˆ‘\nt\nâˆ‘\ni\n=\n1\nN\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nt\ni\nâˆ£\no\n<\nt\n)\n.\n\\mathcal{L}_{\\mathrm{RoaD}}(\\theta)=-\\sum_{(o^{i},a^{i})\\in\\mathcal{D}_{\\text{gen}}}\\sum_{t}\\sum_{i=1}^{N}\\log\\pi_{\\theta}(a_{t}^{i}\\mid o_{<t}).\n(3)\nThe expert guidance is designed to produce trajectories that are\nsimultaneously near onâ€‘policy (i.e. sampled from\nÏ€\nÎ¸\n\\pi_{\\theta}\n), but also higherâ€‘quality than\nunassisted rollouts. Because this data is collected on-policy, it\ncovers states the policy is likely to encounter,\nreducing covariate shift between training and deployment.\nIn practice,\nâ„›\ns\n0\n:\nT\nE\nâ€‹\n[\nÏ€\nÎ¸\n]\n\\mathcal{R}_{{s}^{E}_{0:T}}[\\pi_{\\theta}]\ncan be implemented by\nbiasing the policyâ€™s output toward the expert continuation, for example using\nTopâ€‘\nK\nK\nselection (\nEq.\n1\n) or Sampleâ€‘\nK\nK\n(see\nSec.\n4.2\n).\nCrucially, compared to CAT-K, RoaD does not require the construction of target\nrecovery actions which are challenging to construct under stochastic or non-invertible dynamics, and are often low-quality for policies that output future trajectories rather than single-step actions.\nInstead, it uses the future trajectory itself as the target.\nIn the following, we discuss three further modifications which makes\nRoaD applicable to E2E driving.\n4.2\nSample-\nK\nK\nexpert-guided rollouts\nTo preserve expert guidance without discrete top-\nK\nK\nenumeration, we draw\nK\nK\naction candidates from the current policy distribution (e.g., trajectory samples or\ndiffusion/flow-matching draws),\nÎ\nt\n=\n{\na\nt\n(\nk\n)\n}\nk\n=\n1\nK\nâˆ¼\ni.i.d.\nÏ€\nÎ¸\n(\nâ‹…\nâˆ£\no\n<\nt\n)\n\\Xi_{t}=\\{a_{t}^{(k)}\\}_{k=1}^{K}\\sim\\text{i.i.d. }\\pi_{\\theta}(\\cdot\\mid o_{<t})\n(4)\nand select the candidate closest to the expert continuation under a generalized distance metric (\nEq.\n6\n):\na\nt\n=\narg\nâ¡\nmin\na\nâˆˆ\nÎ\nt\nâ¡\nd\ng\nâ€‹\n(\na\n,\ns\nt\n:\nT\nE\n)\n.\na_{t}=\\arg\\min_{a\\in\\Xi_{t}}d^{\\mathrm{g}}\\!\\big(a,\\,s^{E}_{t:T}\\big).\n(5)\nThis Sample-K relaxation maintains the â€œclosest-to-expertâ€ bias while accommodating continuous policy outputs\nand large vocabularies.\nWhen\na\nt\na_{t}\nrepresent trajectories, the distance function\nd\nâ€‹\n(\nâ‹…\n,\nâ‹…\n)\nd(\\cdot,\\cdot)\ncan be implemented as a trajectory-level (generalized) distance between predicted trajectories and the future expert trajectory. A concrete choice is a weighted step-wise distance over a comparison horizon\nH\nt\nH_{t}\n,\nd\ng\nâ€‹\n(\na\nt\n,\ns\nt\n:\nT\nE\n)\n=\nâˆ‘\nk\n=\n1\nH\nt\nw\nk\nâ€‹\nd\nâ€‹\n(\ns\n~\nt\n+\nk\nâ€‹\n(\na\nt\n)\n,\ns\nt\n+\nk\nE\n)\n,\nd^{\\mathrm{g}}\\!\\big(a_{t},\\,{s}^{E}_{t:T}\\big)=\\sum_{k=1}^{H_{t}}w_{k}\\;d\\big(\\tilde{s}_{t+k}(a_{t}),\\,{s}^{E}_{t+k}\\big),\n(6)\nwhere\ns\n~\nt\n+\nk\nâ€‹\n(\na\nt\n)\n\\tilde{s}_{t+k}(a_{t})\ndenotes the predicted state at step\nt\n+\nk\nt+k\nimplied by action\na\nt\na_{t}\n,\nand\nw\nk\nâ‰¥\n0\nw_{k}\\!\\geq\\!0\nare arbitrary weights.\n4.3\nRecovery-mode policy output\nE2E driving policies often exhibit limited action diversity as they are trained\nto drive safely and predictably, preventing naive sampling from reliably\nproducing a candidate near the expert. To address this, we introduce an optional\nrecovery-mode policy output that nudges the policy toward the expert when all\nsampled actions are too far from the expert.\nConcretely, when the chosen action\na\nt\na_{t}\nis a trajectory, we linearly\ninterpolate between\na\nt\na_{t}\nand the expert continuation, acting as guidance\nrather than a discrete override. Let the prediction horizon be\nF\nF\n. We reuse the\nnotation\ns\n~\nt\n+\nk\nâ€‹\n(\na\nt\n)\n\\tilde{s}_{t+k}(a_{t})\nfrom\nEq.\n6\nto denote the\npredicted state at step\nt\n+\nk\nt+k\nimplied by\na\nt\na_{t}\n.\nRecovery is triggered when the generalized distance to the expert exceeds a\nthreshold:\nd\ng\nâ€‹\n(\na\nt\n,\ns\nt\n:\nT\nE\n)\n>\nÎ´\nrec\n.\nd^{\\mathrm{g}}\\!\\big(a_{t},\\,s^{E}_{t:T}\\big)>\\delta_{\\mathrm{rec}}.\n(7)\nUpon triggering, we define a weight vector\nÎ»\nâˆˆ\n[\n0\n,\n1\n]\nF\n\\lambda\\in[0,1]^{F}\n(e.g., a linear\nschedule\nÎ»\nk\n=\nmin\nâ¡\n(\n1\n,\nk\n/\nN\nrec\n)\n\\lambda_{k}=\\min(1,k/N_{\\text{rec}})\n) and blend the trajectories as\ns\n~\nt\n+\nk\nâ€‹\n(\na\nt\nâ€²\n)\n=\n(\n1\nâˆ’\nÎ»\nk\n)\nâ€‹\ns\n~\nt\n+\nk\nâ€‹\n(\na\nt\n)\n+\nÎ»\nk\nâ€‹\ns\nt\n+\nk\nE\n,\nk\n=\n1\n,\nâ€¦\n,\nF\n,\n\\tilde{s}_{t+k}(a^{\\prime}_{t})=(1-\\lambda_{k})\\,\\tilde{s}_{t+k}(a_{t})+\\lambda_{k}\\,{s}^{E}_{t+k},\\quad k=1,\\dots,F,\n(8)\nwhich defines the recovery trajectory\na\nt\nâ€²\na^{\\prime}_{t}\n. The weight vector\nÎ»\n\\lambda\nsubsumes all parameters of the schedule; in practice we use a simple linear\nramp over the first\nN\nrec\nN_{\\text{rec}}\nsteps.\nAV NuRec dataset\nMethod\nDriving\nscore\nâ†‘\n\\uparrow\nCollision\nrate\nâ†“\n\\downarrow\nOffroad\nrate\nâ†“\n\\downarrow\nDistance\ntraveled (m)\nâ†‘\n\\uparrow\nFine-tuning with RoaD (ours)\n0.6300\nÂ±\n\\pm{}\n0.0090\n0.0239\nÂ±\n\\pm{}\n0.0000\n0.2098\nÂ±\n\\pm{}\n0.0029\n147.2\nÂ±\n\\pm{}\n0.54\nFine-tuning with re-rendered expert trajectories\n0.4985\nÂ±\n\\pm{}\n0.0046\n0.0464\nÂ±\n\\pm{}\n0.0051\n0.2583\nÂ±\n\\pm{}\n0.0044\n151.9\nÂ±\n\\pm{}\n0.36\nContinued large-scale training with BC\n0.4215\nÂ±\n\\pm{}\n0.0092\n0.0627\nÂ±\n\\pm{}\n0.0033\n0.2783\nÂ±\n\\pm{}\n0.0039\n143.7\nÂ±\n\\pm{}\n0.76\nBase model pre-trained with BC\n0.4443\nÂ±\n\\pm{}\n0.0210\n0.0525\nÂ±\n\\pm{}\n0.0051\n0.2833\nÂ±\n\\pm{}\n0.0084\n149.0\nÂ±\n\\pm{}\n1.08\nTable 3\n:\nEnd-to-end simulation results over the AV NuRec dataset.\nRoaD fine-tuning significantly increases the driving score, and it outperforms both continued open-loop training with real data, as well as fine-tuning with re-rendered expert trajectories.\nAV NuRec dataset\nMethod\nDriving\nscore\nâ†‘\n\\uparrow\nCollision\nrate\nâ†“\n\\downarrow\nOffroad\nrate\nâ†“\n\\downarrow\nDistance\ntraveled (m)\nâ†‘\n\\uparrow\nRoaD\n0.6300\nÂ±\n\\pm{}\n0.0090\n0.0239\nÂ±\n\\pm{}\n0.0000\n0.2098\nÂ±\n\\pm{}\n0.0029\n147.2\nÂ±\n\\pm{}\n0.54\nRoaD (no expert guidance)\n0.4847\nÂ±\n\\pm{}\n0.0027\n0.0576\nÂ±\n\\pm{}\n0.0022\n0.2543\nÂ±\n\\pm{}\n0.0011\n151.2\nÂ±\n\\pm{}\n0.24\nRoaD (no recovery)\n0.5030\nÂ±\n\\pm{}\n0.0107\n0.0518\nÂ±\n\\pm{}\n0.0044\n0.2493\nÂ±\n\\pm{}\n0.0013\n151.4\nÂ±\n\\pm{}\n0.42\nRoaD (1 rollout)\n0.5898\nÂ±\n\\pm{}\n0.0108\n0.0341\nÂ±\n\\pm{}\n0.0006\n0.2091\nÂ±\n\\pm{}\n0.0045\n143.4\nÂ±\n\\pm{}\n0.29\nRoaD (3 rollouts, default)\n0.6300\nÂ±\n\\pm{}\n0.0090\n0.0239\nÂ±\n\\pm{}\n0.0000\n0.2098\nÂ±\n\\pm{}\n0.0029\n147.2\nÂ±\n\\pm{}\n0.54\nRoaD (9 rollouts)\n0.6317\nÂ±\n\\pm{}\n0.0070\n0.0264\nÂ±\n\\pm{}\n0.0027\n0.2083\nÂ±\n\\pm{}\n0.0054\n148.3\nÂ±\n\\pm{}\n0.41\nRoaD (fine-tune once, default)\n0.6300\nÂ±\n\\pm{}\n0.0090\n0.0239\nÂ±\n\\pm{}\n0.0000\n0.2098\nÂ±\n\\pm{}\n0.0029\n147.2\nÂ±\n\\pm{}\n0.54\nRoaD (fine-tune twice)\n0.6613\nÂ±\n\\pm{}\n0.0130\n0.0420\nÂ±\n\\pm{}\n0.0035\n0.1967\nÂ±\n\\pm{}\n0.0043\n157.8\nÂ±\n\\pm{}\n0.37\nRoaD (1k steps)\n0.6171\nÂ±\n\\pm{}\n0.0124\n0.0246\nÂ±\n\\pm{}\n0.0035\n0.2152\nÂ±\n\\pm{}\n0.0011\n148.0\nÂ±\n\\pm{}\n0.66\nRoaD (4.2k steps, default)\n0.6300\nÂ±\n\\pm{}\n0.0090\n0.0239\nÂ±\n\\pm{}\n0.0000\n0.2098\nÂ±\n\\pm{}\n0.0029\n147.2\nÂ±\n\\pm{}\n0.54\nRoaD (10k steps)\n0.6095\nÂ±\n\\pm{}\n0.0245\n0.0424\nÂ±\n\\pm{}\n0.0029\n0.2043\nÂ±\n\\pm{}\n0.0066\n150.2\nÂ±\n\\pm{}\n0.15\nRoaD (K=16)\n0.5789\nÂ±\n\\pm{}\n0.0039\n0.0322\nÂ±\n\\pm{}\n0.0023\n0.2207\nÂ±\n\\pm{}\n0.0029\n146.4\nÂ±\n\\pm{}\n0.18\nRoaD (K=32)\n0.5898\nÂ±\n\\pm{}\n0.0060\n0.0290\nÂ±\n\\pm{}\n0.0006\n0.2196\nÂ±\n\\pm{}\n0.0033\n146.6\nÂ±\n\\pm{}\n0.16\nRoaD (K=64, default)\n0.6300\nÂ±\n\\pm{}\n0.0090\n0.0239\nÂ±\n\\pm{}\n0.0000\n0.2098\nÂ±\n\\pm{}\n0.0029\n147.2\nÂ±\n\\pm{}\n0.54\nRoaD (K=128)\n0.6396\nÂ±\n\\pm{}\n0.0119\n0.0304\nÂ±\n\\pm{}\n0.0039\n0.2047\nÂ±\n\\pm{}\n0.0045\n150.4\nÂ±\n\\pm{}\n0.10\nBase model\n0.4443\nÂ±\n\\pm{}\n0.0210\n0.0525\nÂ±\n\\pm{}\n0.0051\n0.2833\nÂ±\n\\pm{}\n0.0084\n149.0\nÂ±\n\\pm{}\n1.08\nTable 4\n:\nAblation study for E2E driving.\nThe setting is identical to the main experiments apart from the ablated property.\nsteps\nrefer to the number of optimization steps for fine-tuning. K denotes the number of trajectory samples for expert-guided rollouts.\nResults indicate that both expert guidance and recovery mode are important in the algorithm; and performance gains are observed over a wide range of hyperparameters.\n4.4\nCL-SFT with off-policy data\nCAT-K regenerates rollouts at each gradient step, which is feasible in BEV traffic\nsimulation, but prohibitive for E2E driving due to the high cost of rendering\nsensor inputs. To reduce\nthis collection cost, we evaluate reusing the same rollout dataset across\nmultiple optimization steps, similar to a replay buffer in off-policy RL,\nincluding the extreme case of generating only a single dataset at the start of\nfine-tuning. Empirically, we find that rollout data reuse incurs only small degradation,\nmaking RoaD practical when highâ€‘fidelity rollouts are expensive to obtain.\n5\nExperiments\nWe validate our method for traffic simulation on the WOSAC benchmark\n[\nmontali2023waymo\n]\n, and for E2E driving using the AlpaSim simulator\n[\nnvidia2025alpasim\n]\nand the NVIDIA Physical AI - AV NuRec Dataset\n[\nnvidia2025nurecavdata\n]\n.\n5.1\nTraffic simulation\nWe first validate RoaD for traffic simulation using the WOSAC benchmark.\nNote that our primary goal here is not to outperform CAT-K, but to show that the\nsimplified RoaD approach can achieve comparable performance to CAT-K while also\nbeing applicable to E2E driving due to fewer restrictive assumptions.\n5.1.1\nExperimental setup\nWe follow the experimental setup of\n[\nzhang2025closed\n]\nand use RoaD to\nfine-tune the SMART-tiny model on the WOMD dataset. The model receives 1 second\nof trajectory history for all agents, it outputs delta x-y actions, and at test\ntime it is rolled out for 8 seconds with 0.1s time steps. Note that for this experiment we do not use the recovery mode as traffic models\nare naturally diverse enough.\nMetrics.\nEvaluation follows the WOSAC protocol. For each scenario, we generate 32 rollouts for all agents in the scene and compare the resulting joint behavior distribution to human driven trajectories.\nWe report the following metrics which, excluding minADE, measure the distributional similarity between the policy and the data.\nThe principal metric on the leaderboard is\nRealism Meta Metric (RMM)\n, which combines three distributional metrics:\nkinematic metrics\n, e.g. velocities and accelerations;\ninteractive metrics\n, e.g., collisions; and\nmap-based metrics\n, e.g. off-road driving. For the exact definition of the metrics we refer to\n[\nmontali2024waymo\n]\n.\nAdditionally we also report\nminADE\n, i.e., minimum Average Displacement Error, a widely used metric for trajectory prediction.\n5.1.2\nResults\nMain results on WOSAC.\nTab.\n1\nprovides results on the public WOSAC leaderboard. RoaD fine-tuning significantly improves over the base model (SMART-tiny), it outperforms a much larger model from the same model family (SMART-large), and it is on-par with the SOTA CL-SFT method, CAT-K\n[\nzhang2025closed\n]\n. We note that multiple works on the leaderboard, concurrently developed with ours, such as SMART-R1\n[\npei2025advancing\n]\nachieve higher RMM using a combination of CL-SFT with CAT-K, and RL fine-tuning. However, these approaches require highly specialized rewards derived from the WOSAC evaluation metrics, and a large number of environment interactions, making them unsuitable for E2E driving. A snapshot of the complete leaderboard at the time of submission is included in the Appendix.\nRe-using CL experience.\nTo assess the effect of reusing previously\ngenerated CL-SFT data, we ablate the data refresh frequency in\nTab.\n2\n, evaluating locally on 2% of the WOMD validation\nset following\n[\nzhang2025closed\n]\n. As expected, more frequent refreshes yield\nhigher performance, though the incremental gains are modest. Importantly, even\nwhen closed-loop data is generated only once at the start of fine-tuning, RoaD\nalready delivers a substantial improvement. Given the high cost of data\nrendering, this motivates our default E2E setup (\nSec.\n5.2\n) of generating CL-SFT data only once. For completeness, we also ablate repeated data generation and observe\nadditional, albeit smaller, improvements in the E2E experiment.\nLocal scene set\nMethod\n3D-GS\nDriving score\nâ†‘\n\\uparrow\nNeRF\nDriving score\nâ†‘\n\\uparrow\nRoaD (ours)\n0.75\nÂ±\n\\pm{}\n0.23\n0.58\nÂ±\n\\pm{}\n0.09\nRe-rendered expert trajectories\n0.42\nÂ±\n\\pm{}\n0.07\n0.35\nÂ±\n\\pm{}\n0.05\nBase model\n0.28\nÂ±\n\\pm{}\n0.05\n0.33\nÂ±\n\\pm{}\n0.04\nTable 5\n:\nSim2sim transfer results.\nPolicies are fine-tuned with 3DGS generated data, and evaluated in previously unseen 75 scenarios reconstructed either as 3DGS (default setting) or as a NeRF (sim2sim transfer). As expected, performance reduces when transferring fine-tuned policies to a new simulation environment, but fine-tuning with RoaD improves over the base model even in the transfer setting.\n5.2\nEnd-to-end driving\nOur main result is that CL-SFT with RoaD can significantly improve closed-loop performance in E2E driving.\n5.2.1\nExperimental setup.\nEnd-to-end VLA policy\nWe employ a VLA-based policy structured similar to\n[\nwang2025alpamayo\n]\n. The policy takes in 1.6s ego motion history, and a sequence of timestamped images from two onboard cameras (front facing wide-angle and tele camera), and generates 6.4s trajectory sample output, which is then tracked by a downstream controller when executed in closed-loop. The policy is trained with a large-scale dataset comprising of 20,000 hours of human driving data from 25 countries, covering a variety of scenarios including highway and urban driving, weather conditions, day and night times. A 1700+ hour subset of this dataset is publicly available\n[\nnvidia2025avdata\n]\n.\nSimulation environment.\nFor E2E driving experiments, we employ AlpaSim\n[\nnvidia2025alpasim\n]\n, a closed-loop simulator built on SOTA neural scene reconstruction\n[\nwu20253dgut\n]\n. The system reconstructs real-world driving logs as temporal 3D Gaussian Splatting (3D-GS) scenes and renders novel camera views when the ego vehicle diverges from the recorded path. We employ custom controllers that track predicted trajectories with separate lateral and longitudinal control, using a 200 ms control delay and ego-motion noise. The vehicle dynamics is governed by a dynamically extended bicycle model.\nThe controller, control delay and dynamics model are designed to imitate real-word driving as closely as possible.\nAll other traffic participants, including vehicles and pedestrians, replay their logged trajectories.\nQualitative examples from AlpaSim\n[\nnvidia2025alpasim\n]\nare shown in\nFig.\n4\n.\nFine-tuning.\nTo fine-tune the VLA policy we generate 20s long simulated CL data using 8251 3D-GS scenes reconstructed from real-world driving logs, 3x rollouts per scene by default.\nWe fine-tune for 4.2k steps (approximately one epoch of non-overlapping trajectory data) with frozen encoders to mitigate overfitting to visual artifacts.\nFigure 3\n:\nThe impact of generating multiple rollouts per scene.\nIn this experiment the same set of AV NuRec scenes are used for training and evaluation. RoaD performance improves monotonically as more rollouts are added to its SFT dataset (orange), while fine-tuning with resimulated expert demonstrations cannot make use of multiple rollouts.\nt\n=\n0\nâ€‹\ns\nt=0s\nt\n=\n3\nâ€‹\ns\nt=3s\nt\n=\n5\nâ€‹\ns\nt=5s\nt\n=\n6\nâ€‹\ns\nt=6s\nPre-trained VLA policy\nRoaD fine-tuned VLA policy\nFigure 4\n:\nQualitative comparison of policy rollouts in our E2E simulator\n.\nTop row:\nbefore fine-tuning, the policy navigates this intersection poorly, ends up in a wrong lane and fails to avoid a collision with a stationary vechicle.\nBottom row:\nafter fine-tuning with RoaD, the policy handles the intersection correctly and avoids any collision.\nMetrics\nTo evaluate models, we use 920 openly accessible challenging 3D-GS scenes from the NVIDIA Physical AI - AV NuRec Dataset\n[\nnvidia2025nurecavdata\n]\n, and generate 3 rollouts per scenes. In\nTabs.\n3\n,\n4\nand\n5\n, mean values are computed over all scenes and rollouts, standard deviations are computed by taking the mean across scenes and computing the standard deviation across rollouts, estimating the evaluation uncertainty.\nThe standard deviation over the full RoaD training, including data generation, fine-tuning, and evaluation with three rollouts per scene, is too expensive to perform for every model. For the driving score of our main result in\nTab.\n3\nwe found it to be\nÂ±\n0.0057\n\\pm 0.0057\n.\nWe report the following metrics.\nThe\ndriving score\nmeasures the average distance traveled (in kilometers) between incident events, where an incident corresponds to either off-road driving or a collision.\nThe\ncollision rate\ndenotes the proportion of scenarios in which the ego vehicle is involved in a close encounter or collision for which it is deemed responsible, i.e., excluding rear-end and side contacts.\nThe\noff-road rate\ncaptures the fraction of scenarios where the ego vehicle leaves the drivable area; this value appears relatively high because in the AV NuRec Dataset only the region bounded by lane markings is considered drivable.\nFinally, the\ndistance traveled\ndenotes the distance traveled by the ego vehicle in meters.\nEach simulation terminates after the first incident. Evaluation in reconstructed scenes is inherently sensitive to rendering artifacts, particularly when the ego vehicle diverges from the logged path. To reduce the impact of such artifacts, we exclude any events where the ego deviates by more than 4â€‰m from the original trajectory. Nonetheless, a portion of recorded incidents remain attributable to visual artifacts or imperfect scene reconstructions.\n5.2.2\nResults\nMain results.\nThe main results for E2E driving are reported in\nTab.\n3\n. RoaD fine-tuning increases the driving score in previously unseen scenarios by 41% and reduces collisions by 54%.\nRoAD outperform fine-tuning with expert demonstrations re-rendered in the same simulation environment, indicating that the performance gains of RoaD are not only from adjusting to simulation artifacts.\nRoaD also outperforms continued large-scale open-loop training using real-world driving data, indicating that the performance gains are not simply due to further training steps.\nFig.\n4\nshows a qualitative example: while the base policy encounters a collision, after fine-tuning with RoaD, the policy handles the intersection correctly without collision.\nAblation studies.\nAblation results in\nTab.\n4\nindicate that both expert guidance during rollouts and recovery-mode policy outputs are important for best performance in E2E driving.\nFurthermore, RoaD is not strongly sensitive to its hyperparameters, including the number of rollouts generated per scene, re-collecting CL data and further fine-tuning the policy, changing the number of optimization steps used for fine-tuning, or the number of trajectory samples (\nK\nK\n) during CL data generation. In all alternative settings, RoaD improves upon the base model.\nWhile some hyper-parameter choices can further increase performance, in particular, increasing\nK\nK\nand re-collecting CL data for additional fine-tuning, these also increase the computational costs of RoaD.\nOn the other hand, we found that fine-tuning for too many optimization steps can slightly reduce performance, likely due to a lack of co-training with the original large-scale training data in our experiments.\nData scaling.\nGiven the high cost of scene reconstruction for E2E CL-SFT (i.e. generating 3D-GS artifacts), scalability of RoaD with the number of rollouts per scene is an important question. To this end, in\nFig.\n3\nwe vary the number of rollouts generated per scene for CL-SFT. RoaD performance improves monotonically as more rollouts are added to its SFT dataset, while fine-tuning with re-simulated expert demonstrations cannot make use of multiple rollouts.\nSim2sim transfer.\nFinally, in our experiments so far we have generated CL fine-tuning data in the same simulation environment where the policy is evaluated. Given a gap between simulation and the real-world, the policy may overfit to artifact of the simulation and in turn it may degrade in real-world deployment. While addressing sim2real gap is not in the scope of this work, to shed some light on this issue, we perform a sim2sim transfer experiment, where the policies are fine-tuned with 3DGS generated data, and evaluated in either 3DGS (default setting) or NeRF reconstructions (sim2sim transfer). For this experiment, we use an in-house scenarios set consisting of 75 scenarios cureted for dense ego-agent interactions. Results are reported in\nTab.\n5\n. As expected, performance reduces when transferring fine-tuned policies to a new simulation environment, but fine-tuning with RoaD improves over the base model even in the transfer setting, indicating that RoaD has potential to improve real-world driving performance, despite possible sim2real gaps.\n6\nConclusions\nWe presented RoaD, a simple closed-loop supervised fine-tuning (CL-SFT) method\nthat treats the policyâ€™s own expert-guided rollouts as additional\ndemonstrations. By avoiding discrete recovery targets and introducing a\nlightweight recovery mode, RoaD removes key assumptions that limit prior\nCL-SFT approaches and makes the recipe applicable to modern E2E driving\npolicies, allowing closed-loop training without the need for reward functions.\nAcross vectorized traffic simulation and high-fidelity E2E driving,\nRoaD consistently improves closed-loop performance over strong baselines.\nBecause RoaD can achieve substantial improvements even when closed-loop data\nis only collected once, it is a promising, data-efficient, approach for\ntraining E2E driving policies in closed-loop.\nLimitation of all CL-SFT approaches include the reliance of a pre-trained policy with sufficiently high performance, the assumption that the expert trajectory remains good behavior despite small deviations by the actor, and a distance metric for expert-guided rollouts. Further, our method relies on a high-fidelity simulator such as AlpaSim. Results on sim2sim transfer suggest that RoaD has potential to improve real-world driving performance, despite possible sim2real gaps. Future work may more explicitly address sim-to-real transfer and reduce overfitting to simulation, e.g., by co-training on simulated and real images, or introducing feature similarity bottlenecks\n[\nfeng2025rap\n]\n.\n\\thetitle\nSupplementary Material\nFigure 5\n:\nSnapshot of the WOSAC leaderboard with our SMART-tiny-CLSFT-RoaD entry highlighted (red box).\nAppendix A\nLeaderboard Snapshot\nIn\nFig.\n5\nwe show a snapshot of the nuPlan WOSAC\nleaderboard, with our\nSMART-tiny-CLSFT-RoaD\nentry highlighted (red box).\nWe briefly discuss the other high-performing methods to clarify how they are\neither orthogonal to our approach or specialized to the WOSAC task, and thus do\nnot directly transfer to E2E driving.\nSMART-tiny-DecompGAIL\n[\nguo2025decomp\n]\n,\nSMART-tiny-RLFTSim\n[\nahmadi2024rlftsim\n]\n, and\nSMART-R1\n[\npei2025advancing\n]\nuse\nreinforcement learning (RL) to optimize policies for the WOSAC task of matching\nthe data distribution.\nSMART-tiny-DecompGAIL\ndoes so by using GAIL with\nPPO, while\nSMART-tiny-RLFTSim\nand\nSMART-R1\ndirectly optimize the\nWOSAC metric using RL (with small differences in implementation). However, these\nmethods do not translate well to E2E driving, where we typically lack a\nwell-defined reward function and RL tends to be too data-inefficient given the\nhigh cost of high-fidelity simulation.\nBy contrast,\nTrajTok\n[\nzhang2025trajtok\n]\nproposes an improved\ntokenizer for traffic models. This contribution is orthogonal to our approach.\nHowever, it is only applicable to tokenizing short actions (e.g., one-step\nactions), and hence does not translate to E2E driving, where policies typically\npredict multiple seconds into the future.\nFinally,\nunimotion\n[\nlin2025revisit\n]\nproposes an alternative to the\nCatK rollout approach, whereby it finds the closest action to the ground truth\nnot only among the top-\nK\nK\nactions, but among all actions. As a result, it does\nnot require additional recovery actions (since it already tracks the\nground-truth trajectory as closely as possible), which yields a setup more\nsimilar to our RoaD approach, where rollouts are taken directly as\ndemonstrations. However, this exhaustive search makes the method unsuitable for\nE2E driving: it requires predicting and evaluating\nall\npossible actions\nof the policy, which is only feasible for small action spaces. This excludes\nmulti-token trajectory predictions, whose action space grows exponentially with\nthe horizon length, and flow-matching policies, whose action space is\ncontinuous.\nIn their work, focussing on traffic models, they use either a fixed set of up to\n2024 actions or a set of up to 16 actions predicted from action queries.\nAppendix B\nExperimental details: end-to-end driving\nB.1\nSimulation\nFor data generation, we run AlpaSim\n[\nnvidia2025alpasim\n]\nat 30 Hz to match the frequency of the\nground-truth logs and reuse the same dataloader as for pre-training.\nFor evaluation, we run the simulator at 10 Hz, which matches the modelâ€™s training frequency.\nAt each step, we render two camera views (front-facing wide-angle and telephoto).\nThe policy predicts 6.4s trajectories, which are tracked by a downstream controller.\nAs in the main text, the controller models a 200 ms control delay and ego-motion noise,\nand uses a dynamically extended bicycle model for the ego-vehicle dynamics.\nScenes are reconstructed using 3D Gaussian Splatting (3D-GS). Reconstruction quality\ndegrades with distance from the recorded trajectory. This is negligible for rollout\ngeneration, where expert-guided rollouts remain close to the log, but can reduce\nvisual fidelity during evaluation if the ego vehicle deviates too far. To avoid such\nartifacts, we discard rollouts whose ego trajectory deviates by more than 4 m from\nthe recorded trajectory.\nAll other traffic participants, including vehicles and pedestrians, replay their logged\ntrajectories and do not react to the ego vehicle. Consequently, they cannot avoid rear-end\ncollisions if the ego drives more slowly than in the recording. Following prior work\n[\nnuplan\n,\ncao2025pseudo\n,\nwang2025alpamayo\n]\n, we therefore count only â€œat-faultâ€ collisions for the ego:\nrear-end collisions caused by following vehicles are ignored, while lateral collisions are\nstill included.\nB.2\nRoaD fine-tuning\nFor RoaD rollout generation, we sample\nK\n=\n64\nK{=}64\ncandidate trajectories from the\npolicy at a temperature of 0.8. To select the executed trajectory, we compute\nthe ground-truth distance\nd\ng\nd^{\\mathrm{g}}\nas the average distance between the\nfour corners of the ego vehicle along the predicted and ground-truth trajectories\nover the first 20 time steps (2s). The same distance metric is used to decide\nwhether to trigger the recovery mode, with a threshold of\nÎ´\nrec\n=\n3\n\\delta_{\\text{rec}}{=}3\nm.\nWhen recovery is triggered, we linearly interpolate between the predicted and\nground-truth trajectories over\nN\nrec\n=\n30\nN_{\\text{rec}}{=}30\ntime steps and follow the\nground-truth trajectory thereafter. Recovery is disabled in the last 4s of the\nepisode because the controller requires at least 4s of input trajectory.",
    "preview_text": "Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\\% and reduces collisions by 54\\%.\n\nRoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies\nGuillermo Garcia-Cobo\nâˆ—1\nMaximilian Igl\nâˆ—1\nPeter Karkus\nâˆ—1\nZhejun Zhang\nâˆ—2â€ \nMichael Watson\n1\nYuxiao Chen\n1\nBoris Ivanovic\n1\nMarco Pavone\n1,3\n1\nNVIDIA Research\n2\nHuawei VN Research Center\n3\nStanford University\n{guillermog, migl, pkarkus}@nvidia.com\nAbstract\nAutonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors.\nWe introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policyâ€™s own closed-loop rollouts as additional training data.\nDuring rollout generation, RoaD incorpora",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "autonomous driving",
        "closed-loop fine-tuning",
        "rollouts",
        "supervised learning",
        "covariate shift"
    ],
    "one_line_summary": "RoaDæ˜¯ä¸€ç§é€šè¿‡åˆ©ç”¨ç­–ç•¥è‡ªèº«é—­ç¯rolloutsä½œä¸ºé¢å¤–è®­ç»ƒæ•°æ®æ¥ç¼“è§£è‡ªåŠ¨é©¾é©¶ç­–ç•¥ä¸­åå˜é‡åç§»çš„é«˜æ•ˆç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T18:52:03Z",
    "created_at": "2026-01-10T10:50:34.381751",
    "updated_at": "2026-01-10T10:50:34.381759",
    "flag": true
}