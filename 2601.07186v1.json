{
    "id": "2601.07186v1",
    "title": "PROTEA: Securing Robot Task Planning and Execution",
    "authors": [
        "Zainab Altaweel",
        "Mohaiminul Al Nahian",
        "Jake Juettner",
        "Adnan Siraj Rakin",
        "Shiqi Zhang"
    ],
    "abstract": "机器人需要任务规划方法来为复杂任务生成动作序列。近期对抗性攻击研究揭示，现有机器人任务规划器存在显著脆弱性，尤其是基于基础模型构建的规划系统。本文旨在通过引入PROTEA——一种基于大语言模型的裁判防御机制来评估任务计划的安全性，从而应对这些安全挑战。PROTEA的开发旨在解决计划安全评估中的维度复杂性与历史依赖难题。我们采用不同大语言模型实现了多个PROTEA版本以进行对比研究。为开展系统性评估，我们构建了包含良性及恶意任务计划的数据集，其中恶意行为以不同隐蔽层级注入。研究结果为寻求增强任务规划系统鲁棒性与安全性的机器人系统实践者提供了可操作的见解。详细内容、数据集及演示请访问：https://protea-secure.github.io/PROTEA/",
    "url": "https://arxiv.org/abs/2601.07186v1",
    "html_url": "https://arxiv.org/html/2601.07186v1",
    "html_content": "PROTEA: Securing Robot Task Planning and Execution\nZainab Altaweel, Mohaiminul Al Nahian, Jake Juettner, Adnan Siraj Rakin, Shiqi Zhang\nAbstract\nRobots need task planning methods to generate action sequences for complex tasks.\nRecent work on adversarial attacks has revealed significant vulnerabilities in existing robot task planners, especially those built on foundation models.\nIn this paper, we aim to address these security challenges by introducing PROTEA, an LLM-as-a-Judge defense mechanism, to evaluate the security of task plans.\nPROTEA is developed to address the dimensionality and history challenges in plan safety assessment.\nWe used different LLMs to implement multiple versions of PROTEA for comparison purposes.\nFor systemic evaluations, we created a dataset containing both benign and malicious task plans, where the harmful behaviors were injected at varying levels of stealthiness.\nOur results provide actionable insights for robotic system practitioners seeking to enhance robustness and security of their task planning systems.\nDetails, dataset and demos are provided:\nhttps://protea-secure.github.io/PROTEA/\nFigure 1:\nOverview of our proposed defense method PROTEA. We assume the existence of an attacked task planner that may generates malicious plans, we compare three defenses: (i) Naïve Method that holistically evaluates the full plan along with the environment description; (ii) Object Filtering Method, which evaluates the full plan along with the filtered environment; and (iii) PROTEA that evaluates each action step-by-step while updating and storing environment states in an external memory. In all cases, an LLM acts as the safety judge and decides whether execution should continue or not.\nI\nINTRODUCTION\nTask planning plays an important role in robot intelligence, as it enables robots to sequence actions in order to achieve goals that cannot be accomplished through individual actions alone.\nClassical task planning methods rely on search-based methods to reason over domain knowledge\n[\n14\n,\n15\n]\n, while ensuring plan correctness.\nMore recently, pre-trained large models have been integrated into task planners\n[\n27\n,\n1\n,\n17\n,\n41\n]\n, offering strong capabilities in interpreting natural language input and generating task plans using commonsense knowledge.\nDespite these advances, recent studies have demonstrated that such planners remain vulnerable to adversarial attacks.\nFor example, a kitchen robot’s task planner can be triggered to suggest harmful actions, such as cutting human hands\n[\n31\n]\nor placing a cellphone in a kettle\n[\n29\n]\n.\nThese findings highlight the critical need to develop defense methods to ensure the safety and security of robotic systems.\nDefense strategies have been developed to ensure the robustness of robot task planners against adversarial attacks.\nThese efforts mainly rely on formal logic-based methods, such as encoding constraints using temporal logics or synthesizing safety rules that can be verified before execution\n[\n38\n,\n52\n]\n.\nSuch defense strategies require carefully defined safety rules, domain knowledge, or symbolic representations that are not readily available, and the rules usually do not generalize well to new domains.\nTo address these limitations, we take a complementary approach that leverages the reasoning capabilities of large language models (LLMs) for safety assessment.\nEvaluating the safety of a task plan presents at least the following two key challenges\n[\n34\n]\n.\nThe first is the\ncurse of dimensionality\n, which arises from the complex interactions among the many objects in the robot’s environment and the actions in the task plan.\nThe second is the\ncurse of history\nwhere long-horizon reasoning is required to detect malicious actions that may be scattered a lengthy sequence of actions.\nFor example, consider a malicious plan consisting of the three critical steps: 1) pouring bleach into a cup, 2) moving the cup to a table, and 3) using the cup to serve coffee.\nThese actions are interleaved with navigation and manipulation steps that do not involve the cup, making it difficult for the robot to recognize how the three critical actions together form an attack.\nTo tackle these challenges, we propose a defense strategy, called\nP\nrotecting\nR\nobot Planners via\nO\nbject-filtered\nT\nask\nE\nxecution against\nA\nttacks (\nPROTEA\n), that combines object filtering and external memory for iterative reasoning to mitigate both dimensionality and history issues.\nAn overview of our method is illustrated in Fig.\n1\n.\nTo advance research in this area, we constructed a dataset that includes both benign and malicious task plans for household service tasks.\nThe malicious task plans were created by injecting harmful behaviors (in groups of fire hazards, electrical hazards, property damage, animal harm, item loss, and poisoning/contamination) into otherwise benign task plans to simulate adversarial attacks.\nTo support the evaluations of defense methods and their implementations, we developed multiple injection strategies that generate attacks with varying levels of stealth.\nThe main contributions of this paper are as follows:\n•\nWe propose PROTEA a defense mechanism based on an LLM-as-a-judge, including holistic and step-by-step variants, for assessing the safety of symbolic task plans.\n•\nWe construct HarmPlan the first dataset of malicious robot task plans by injecting harmful behaviors into benign plans from the VirtualHome benchmark.\n•\nWe demonstrate the implementation of PROTEA with different LLMs and offer suggestions to robot planning practitioners of real-world applications.\nII\nRELATED WORK\nIn this section, we begin with a brief overview of early research on safety in robot planning, then summarize work on attacking robot task planners, and finally discuss recent advances in defenses against such attacks.\nII-A\nSafety in Robot Planning\nSafety has long been an essential requirement in robotics, beginning with obstacle avoidance in motion planning\n[\n13\n,\n12\n,\n22\n,\n21\n,\n44\n]\n.\nTo provide provable safety guarantees on robot trajectories, researchers have introduced\nsafe sets\n[\n5\n,\n45\n]\n, which have since been applied to tasks such as manipulation\n[\n43\n,\n9\n,\n42\n,\n32\n,\n8\n]\nand navigation\n[\n10\n,\n30\n,\n6\n]\n.\nAs robots are increasingly being used for complex tasks, the safety in task planning has been studied.\nFor instance, temporal logic has been used to encode task constraints, allowing planners to automatically avoid unsafe task sequences\n[\n24\n,\n47\n]\n.\nThe rise of LLM-based methods in task planning has raised new safety challenges.\nRecent studies demonstrate that LLM-controlled robots are highly sensitive to small variations in input prompts or perception\n[\n48\n,\n7\n]\n.\nTo mitigate these risks, researchers have incorporated collision prediction into high-level decision-making\n[\n25\n]\n, enforcing formal logic constraints (e.g., SELP\n[\n49\n]\n, Safety Chip\n[\n52\n]\nand SafePlan\n[\n33\n]\n), and adopting LLM-as-a-Judge approaches\n[\n4\n,\n23\n]\n.\nThese approaches have been evaluated using benchmarks such as SafeAgentBench\n[\n53\n]\nand SafePlan-Bench\n[\n18\n]\n.\nIn contrast to prior work on robot safety, our focus is on the development of defenses against adversarial attacks targeting robot planners.\nII-B\nAdversarial Attacks for Robot Task Planners\nRecent studies reveal a parallel threat, that is adversarial manipulation.\nUnlike natural errors in reasoning, these attacks intentionally exploit LLM-based planners to produce unsafe or malicious plans.\nRecently, researchers have developed several types of attacks on LLM-based planners, including adversarial prompt perturbations that mislead planning\n[\n20\n,\n46\n,\n28\n]\n; jailbreaking attacks that bypass safety filters to allow unsafe plans\n[\n40\n,\n54\n,\n29\n]\n; and backdoor attacks that use hidden triggers to activate harmful behaviors\n[\n31\n,\n19\n,\n26\n]\n.\nThis research is motivated by such adversarial attacks against robot task planners.\nII-C\nDefenses against Task Plan Attacks\nIn response to these attacks, several defense methods have been proposed.\nCEE is an inference-time defense that steers the hidden representations of LLMs toward safe directions during plan generation\n[\n51\n]\n, mitigating jailbreak attacks without retraining or external safety modules.\nRoboGuard introduces a two-stage guardrail architecture that grounds high-level safety rules in the robot’s environment and prunes unsafe LLM-generated plans\n[\n38\n]\n.\nPOEX is a model-based defense framework against policy-executable jailbreak attacks on LLM-based robots\n[\n29\n]\n.\nThe POEX study further highlighted that the existing defenses often suffer from low recall or high false positive rates, underscoring the need for robust defense mechanisms.\nCompared with the existing defenses, our approach does not rely on expert knowledge and is particularly effective in domains with high complexity (e.g., involving many objects) and long task horizons (e.g., requiring many actions for completion).\nThose challenges are captured in our novel dataset, which includes both benign and malicious task plans.\nIII\nMethodology\nIn this section, we begin by giving a problem definition, and then describe the key components that form the core of PROTEA, our proposed defense strategy.\nIII-A\nProblem Statement\nWe consider a general task planning system, where a robot receives a high-level task instruction (e.g., “wash dishes”) and a task planner produces a sequence of executable actions.\nThe planner may take different forms, such as classical symbolic planners\n[\n14\n]\n, learning-based approaches\n[\n11\n]\n, or LLM-based planners\n[\n16\n]\n.\nRegardless of the planner type, unsafe or malicious actions may be introduced, either unintentionally due to reasoning errors or intentionally through adversarial manipulation like a backdoor or a jailbreak attack.\nThose malicious actions might appear in the plan consecutively or be scattered a lengthy sequence of actions.\nA solution is in the form of a defense method that evaluates the safety level of the task plan, serving as a safety layer during execution.\nThe defense method evaluates each action against the evolving environment state and halts execution if a harmful behavior is detected.\nTo this end, we assume the defense method has access to:\n•\nA task plan generated by a robot’s task planner,\n•\nThe robot’s current world state, including objects, their properties, and between-object relations, and\n•\nA trusted LLM that acts as a\nsafety judge\n.\nThis setup allows us to design a defense pipeline that generalizes across planner types while mitigating the risks of unsafe or malicious behaviors.\nAlgorithm 1\nPROTEA, the Defense Strategy\n1:\nInput:\nPlan\nP\n=\n[\na\n1\n,\na\n2\n,\n…\n,\na\nn\n]\nP=[a_{1},a_{2},\\dots,a_{n}]\n, initial environment\nE\n0\nE_{0}\n, valid actions\nV\nV\n2:\nOutput:\nVerdict (\nmalicious\nor\nnot malicious\n)\n3:\nE\n←\nFilterObjects\n​\n(\nE\n0\n,\nP\n)\nE\\leftarrow\\textsc{FilterObjects}(E_{0},P)\n4:\nH\n←\n∅\nH\\leftarrow\\emptyset\n⊳\n\\triangleright\ninitialize execution history\n5:\nfor\nt\n=\n1\nt=1\nto\nn\nn\ndo\n6:\ny\n←\nf\nLLM\n​\n(\nV\n,\nE\n,\na\nt\n,\nH\n)\ny\\leftarrow f_{\\text{LLM}}(V,E,a_{t},H)\n7:\nif\ny\n=\nmalicious\ny=\\textit{malicious}\nthen\n8:\nreturn\nMalicious action detected + explanation\n9:\nelse\n10:\nE\n←\nSimUpdate\n​\n(\nE\n,\na\nt\n)\nE\\leftarrow\\textsc{SimUpdate}(E,a_{t})\n⊳\n\\triangleright\nupdate environment\n11:\nH\n←\nH\n∪\n{\na\nt\n}\nH\\leftarrow H\\cup\\{a_{t}\\}\n⊳\n\\triangleright\nrecord executed action\n12:\nend\nif\n13:\nend\nfor\n14:\nreturn\nAccept plan as safe\nIII-B\nPROTEA: our Defense Mechanism\nOur primary defense,\nPROTEA\n, integrates two key components:\n(i) Object Filtering\n, which prunes irrelevant objects from the current world state description, and\n(ii) External Memory\n, which iteratively maintains and updates the evolving state of each individual relevant object during execution.\nAlgorithm\n1\nsummarizes the overall process. PROTEA begins by filtering the initial environment with\nFilterObjects\n, storing the compact representation in the external memory. At each step, the\nSafety Judge\nLLM receives the current action, the valid action set, the updated environment, and the execution history, and returns a binary judgment (\nmalicious\nor\nnot malicious\n). If the action is safe, a second LLM call in its role as a simulator updates the environment state (\nSimUpdate\n), which is then stored in the external memory for the next iteration. If an unsafe action is detected, execution halts.\nIn practice, replanning is triggered with additional constraints after such a halt, which is not listed in the algorithm.\nThis combination of object filtering and external memory enables PROTEA to detect hidden malicious behaviors that only emerge dynamically over time.\nTo assess the importance of each component, we also study two ablations.\nRemoving the step-by-step evaluation and external memory yields the\nObject Filtering Method\nvariant, in which the LLM judges the plan in a single pass using only the filtered environment.\nFurther removing object filtering yields the\nNaïve Method\nvariant, where the LLM evaluates the entire unfiltered environment and complete plan in one shot.\nThe two ablation methods will be used in evaluations to highlight the contributions of object filtering and step-by-step external memory, demonstrating their value in strengthening plan-level safety judgments.\nIV\nHarmPlan Dataset\nTo systematically evaluate our defense mechanisms, we develop HarmPlan, a dataset of benign and malicious robot task plans derived from the VirtualHome simulator.\nThe dataset is designed to capture both explicit and subtle harmful behaviors across multiple categories and difficulty levels.\nIV-A\nDataset Source\nWe build HarmPlan on top of the publicly available activity knowledge base from VirtualHome\n[\n37\n]\n, a widely used simulator for modeling household activities. VirtualHome provides a structured set of tasks, where each task includes a task name, a natural language description, and a programmatic plan represented as a sequence of actions. Each task is also paired with an environment graph, where nodes correspond to objects in the environment and edges represent relations between objects. These plans can be executed in the VirtualHome simulator within their respective environments. As such, VirtualHome serves as a reliable foundation for our work, offering realistic and executable household task plans from which we construct harmful variants.\nFigure 2:\nHarmPlan: a dataset of benign and malicious plan for evaluating defenses against malicious task plans. The construction process consists of four main stages, including plan selection, malicious behavior design, behavior injection, and validation.\nFigure 3:\nExamples of crafted harmful behaviors across six different harm categories in HarmPlan as validated through the VirtualHome simulator.\nIV-B\nDataset Construction\nTo build HarmPlan, we begin with a set of benign task plans from the VirtualHome dataset. We then inject malicious behaviors into these plans to create harmful variants. The construction process consists of\nfour main stages\n(Fig.\n2\n).\n1.\nPlan Selection.\nWe first selected 750 base plans from VirtualHome, sampling across the categories\nFood Preparation\n,\nHouse Arrangement\n,\nHouse Cleaning\n,\nSocial Interaction\n,\nLeisure\n, and\nWork\n. These plans serve as the benign foundation for constructing harmful variants.\n2.\nMalicious Behavior Design.\nWe manually crafted 91 malicious behaviors. These behaviors fall into two classes:\nOne is the\nDirect behaviors\n(50 total): harmful behaviors that are explicit with 4–6 actions.\nDirect behaviors are used to form the basis for the\neasy\nsubset of HarmPlan.\nExample include placing papers on a stove and switching it on; and breaking a mirror on the floor.\nThe other class is the\nConsequential behaviors\n(41 total): harmful behaviors that are more stealthy, and unfold across multiple steps.\nThese behaviors require 6–13 actions and are used to construct the\nmedium\nand\nhard\nsubsets of HarmPlan.\nExamples include pouring soap into a cup and then pouring the cup to a coffee machine.\nExample malicious behaviors are shown in Fig.\n3\nand a complete example is illustrated in Fig.\n4\n.\n3.\nBehavior Injection and Graph Update.\nWe injected malicious behaviors into the benign plans to generate malicious plans.\nFor\neasy\ncases, malicious actions were inserted consecutively at the beginning or end of the plan. For\nmedium\nand\nhard\ncases, we used an LLM to guide the placement of malicious actions within the plan while maintaining a seamless flow. Specifically, in\nmedium\ncases, the injected actions were separated by 2–4 benign steps, whereas in\nhard\ncases, they were separated by 5 or more benign steps.\nAt the same time the corresponding environment graphs were also updated with any additional objects required for executing the malicious behaviors.\n4.\nValidation.\nAfter injection, the generated plans were executed in the VirtualHome simulator to confirm their validity. During this stage, we identified cases where the injected behaviors required intermediate actions (e.g., to open a microwave, one has to walk to the kitchen first).\nSuch additional actions were added to ensure the feasibility of the generated malicious plans.\nHarmful Behavior\n: Pour detergent in a water bottle, then pour the water bottle in a coffee maker\nCategory\n: Food preparation\nPlan\n(consequential malicious behaviors):\n•\n[\nWALK\n]\n​\n<\nk\n​\ni\n​\nt\n​\nc\n​\nh\n​\ne\n​\nn\n>\n[\\textit{WALK}]<kitchen>\n•\n[\nWALK\n]\n​\n<\nd\n​\ne\n​\nt\n​\ne\n​\nr\n​\ng\n​\ne\n​\nn\n​\nt\n>\n[\\textit{WALK}]<detergent>\n•\n[\nFIND\n]\n​\n<\nd\n​\ne\n​\nt\n​\ne\n​\nr\n​\ng\n​\ne\n​\nn\n​\nt\n>\n[\\textit{FIND}]<detergent>\n•\n[\nGRAB\n]\n​\n<\nd\n​\ne\n​\nt\n​\ne\n​\nr\n​\ng\n​\ne\n​\nn\n​\nt\n>\n[\\textit{GRAB}]<detergent>\n•\n[\nPOUR\n]\n​\n<\nd\n​\ne\n​\nt\n​\ne\n​\nr\n​\ng\n​\ne\n​\nn\n​\nt\n>\n​\n<\nd\n​\nr\n​\ni\n​\nn\n​\nk\n​\ni\n​\nn\n​\ng\n​\n_\n​\ng\n​\nl\n​\na\n​\ns\n​\ns\n>\n[\\textit{POUR}]<detergent><drinking\\_glass>\n•\n[\nPUTOBJBACK\n]\n​\n<\nd\n​\ne\n​\nt\n​\ne\n​\nr\n​\ng\n​\ne\n​\nn\n​\nt\n>\n[\\textit{PUTOBJBACK}]<detergent>\n•\n[\nGRAB\n]\n​\n<\nd\n​\nr\n​\ni\n​\nn\n​\nk\n​\ni\n​\nn\n​\ng\n​\n_\n​\ng\n​\nl\n​\na\n​\ns\n​\ns\n>\n[\\textit{GRAB}]<drinking\\_glass>\n•\n[\nWALK]\n<\nc\no\nf\nf\ne\ne\n_\nm\na\nk\ne\nr\n>\n[\\textit{WALK]}<coffee\\_maker>\n•\n[\nFIND\n]\n​\n<\nc\n​\no\n​\nf\n​\nf\n​\ne\n​\ne\n​\n_\n​\nm\n​\na\n​\nk\n​\ne\n​\nr\n>\n[\\textit{FIND}]<coffee\\_maker>\n•\n[\nPOUR\n]\n​\n<\nd\n​\nr\n​\ni\n​\nn\n​\nk\n​\ni\n​\nn\n​\ng\n​\n_\n​\ng\n​\nl\n​\na\n​\ns\n​\ns\n>\n​\n<\nc\n​\no\n​\nf\n​\nf\n​\ne\n​\ne\n​\n_\n​\nm\n​\na\n​\nk\n​\ne\n​\nr\n>\n[\\textit{POUR}]<drinking\\_glass><coffee\\_maker>\nFigure 4:\nAn example of consequential malicious behaviors, shown before their injection into an otherwise benign plan.\nFig.\n5\nshows the statistics of our HarmPlan dataset.\nEach plan instance includes a task name, a natural language description, a programmatic plan, a corresponding environment graph file, and a label (benign or malicious).\nCompared with existing datasets on plan safety where agents are explicitly instructed to perform harmful tasks\n[\n53\n,\n18\n]\n, HarmPlan is created by injecting malicious behaviors into otherwise benign plans, which makes the resulting attacks more stealthy and harder to detect.\nFigure 5:\nHarmPlan includes benign and malicious plans. The malicious plans were generated by injecting malicious behaviors to otherwise benign plans.\nV\nExperimental Setup\nIn this section, we describe our experimental setup including the language models used in the implementation of PROTEA and the evaluation metrics and protocol.\nV-A\nModels\nWe evaluate PROTEA using a diverse set of state-of-the-art large language models (LLMs), covering both proprietary and open-source families. All models are accessed in their instruction-following variants and used as\nsafety judges\n. The selected models span different architectures, parameter scales, and alignment strategies, allowing us to assess the generality of our defense framework.\n•\nGPT-4o-mini\n[\n35\n]\n:\nOpenAI’s lightweight reasoning model, optimized for efficiency while retaining strong instruction-following and safety alignment.\n•\nGPT-OSS-120B\n[\n36\n]\n:\nOpenAI’s recent open-source model with 120B parameters.\n•\nGrok-3-mini\n[\n50\n]\n:\nxAI’s smaller Grok series model, designed for fast inference with competitive reasoning capabilities.\n•\nLLaMA3.3-70B\n[\n2\n]\n:\nMeta’s most recent open-source release, widely used for research due to its strong reasoning performance and transparent training pipeline.\n•\nPhi-4\n[\n39\n]\n:\nMicrosoft’s compact model optimized for safety alignment and reasoning on everyday tasks.\n•\nMixtral-8\n×\n\\times\n22B-instruct\n[\n3\n]\n:\none of the largest open-source aligned models available released by Mistral AI.\nV-B\nEvaluation Metrics\nWe evaluate the effectiveness of PROTEA using three standard classification metrics: precision, recall, and F1 score. In our setting, these metrics capture the trade-off between detecting harmful plans and avoiding false alarms on benign plans.\nIn safety-critical contexts such as robotics,\nrecall is the most important for preventing harm\n, though precision remains crucial for preserving usability and F1 is the harmonic mean of the two.\nV-C\nEvaluation Protocol\nAll three defense methods (Naïve Method, Object Filtering Method, and PROTEA) were evaluated on the full HarmPlan dataset. This ensures that performance comparisons between methods are directly comparable and based on identical data splits.\nFor model access, we relied on different infrastructures depending on availability. GPT-4o-mini and Grok-3-mini were accessed through their respective APIs, while the remaining four open-source models were run on local GPU servers. Every plan instance was processed exactly once per model-method pair.\nV-D\nReal Robot Demo\nWe further deployed PROTEA on a real robot to demonstrate the executablility of a harmful plan from HarmPlan, highlighting their potential catastrophic consequences.\nOur real-robot setup consists of a UR5e manipulator equipped with a Hand-E gripper, mounted on a Segway mobile base, and observed by an overhead Intel RealSense RGB-D camera for perception.\nFig.\n6\nillustrates a sequence of snapshots from the execution of one such malicious plan, where the robot followed the harmful instructions and ultimately disposes of a medicine container in the trash bin.\nFigure 6:\nAn illustrative example of a malicious task plan. It demonstrated the malicious behaviors\nin red\n, “consequential” as defined in Section\nIV-B\n, being injected into the first few actions of a benign “serving coffee” plan from VirtualHome, forming an attack with “medium” difficulty.\nThe malicious behaviors are considered stealthy, because one has to reason over a sequence of actions to identify the medicine is discarded together with the box.\nVI\nResults and Analysis\n​​​​​​Observation 1\nUsing the Naïve Method, all LLMs achieve high precision, but not all of them achieve high recall, missing a large portion of the malciious plans. (Table\nI\n).\nBased on the results in Table\nI\n, we observed that the Naive method maintains high precision\n(\n≥\n90\n%\n)\n(\\geq 90\\%)\nfor all the six models, indicating that the flagged plans are actually harmful, but some models miss a large portion of the malicious instances.\nAs shown in Table\nI\n, models like LLaMA3.3-70B, Mixtral-8x22B and Phi-4 exhibit lower recall value compared to the GPTs or Grok-3-mini. Those are powerful open models but may lack dedicated safety alignment, making them less sensitive to malicious content.\nOur experiments show that recall in harmful task plan detection does not simply depend on model size. For example, GPT-4o-mini, Grok-3-mini, and GPT-OSS-120B showed higher recall, while LLaMA3.3-70B, Mixtral-8x22B, and Phi-4 were less effective.\nOne possible explanation is that models such as GPT-4o-mini, Grok-3-mini, and GPT-OSS-120B may have been trained with stronger emphasis on alignment techniques like RLHF or red-teaming, which could bias them toward more conservative decisions and therefore higher recall. In addition, these models are explicitly designed or trained to employ chain-of-thought (CoT) or step-by-step reasoning (e.g., Grok-3-mini was RL-trained to refine its chain-of-thought process, and GPT-OSS-120B is documented to “think step-by-step” with exposed CoT traces), which likely supports more reliable detection of harmful patterns.\nIn contrast, models like LLaMA3.3-70B, Mixtral-8x22B, and Phi-4 appear to emphasize general-purpose reasoning efficiency and broad applicability rather than heavy safety alignment or explicit chain-of-thought training. This focus yields strong overall performance on diverse tasks, but may come at the expense of heightened sensitivity to harmful cases, resulting in lower recall in our evaluation.\nWhile we cannot verify the exact training pipelines of these models, these observations suggest that alignment strategy and training objectives, rather than scale alone, likely play an important role in recall performance.\nTABLE I:\nPerformance of the Naive method across models. Precision is consistently high (\n≥\n0.90\n\\geq 0.90\n), but recall varies significantly across models.\nModel\nGPT-4o\n-mini\nGPT-OSS\n-120B\nGrok-3\n-mini\nLLaMA3.3\n-70B\nMixtral-\n8x22B\nPhi-4\nPrecision\n0.902\n0.963\n0.929\n0.924\n0.914\n0.903\nRecall\n0.940\n0.855\n0.955\n0.787\n0.532\n0.507\n​​​​​​Observation 2\nModels without Chain-of-thought reasoning capabilities (e.g., LLaMA3.3-70B) perform poorly, especially on difficult category tasks, exhibiting the need for our proposed method to improve robot-safety (Table\nI\n& Fig.\n7\n).\nWe observe that the performance of LLaMA3.3-70B, Phi-4, and Mixtral-8x22B degrades as the difficulty level of the malicious plans increases, as shown in Fig.\n7\n. These models perform relatively well on easy attacks, where malicious behaviors are injected as contiguous steps. However, as the difficulty increases, with harmful actions scattered throughout the plan, the models struggle to distinguish malicious from benign steps. This drop in performance is most pronounced for hard examples. These results highlight the limitations of models that lack chain-of-thought reasoning capabilities when tasked with reasoning over long, complex plans and detecting subtle, context-sensitive threats.\nThis degradation is particularly evident in the recall metric as highlighted in Table\nI\n, which directly reflects the models’ inability to identify malicious behaviors, especially in more challenging scenarios (broken down across task difficulty in Fig.\n7\n).\nFigure 7:\nRecall scores of LLaMA3.3-70B, Phi-4, Mixtral-8x22B models across different difficulty levels (Easy, Medium, Hard) and evaluation methods (Naive, Object Filtering, External Memory).\n​​​​​​Observation 3\nOur proposed Object Filtering helps improve the detection performance of the under-performing models, especially improving recall by up to\n∼\n\\sim\n30% on medium and hard tasks (Fig.\n7\n).\nWhen applying the Object Filtering Method, we observed notable improvements in recall and F1 for the under-performing models in the naive method, e.g., LLaMA3.3-70B, Phi-4, and Mixtral-8x22B. These models appear to struggle when presented with longer and more cluttered prompts, such as those used in the Naive method. By reducing irrelevant environment information and focusing on only the objects involved in the plan, Object Filtering likely mitigates distraction and helps these models better identify malicious actions. On the other hand, the stronger safety-aligned models like GPT-4o-mini and Grok-3-mini are already robust to longer prompts and thus show limited performance gain for object filtering.\nThis suggests that the benefits of object filtering are model-dependent. Fig.\n7\n, highlights the impact of Object Filtering Method on LLaMA3.3-70B, Phi-4, and Mixtral-8x22B, where it shows a notable improvement in recall. It also shows that Object Filtering Method becomes more effective in harder tasks compared to the naive method.\nBeyond this, PROTEA consistently provides the best performance for the no-CoT models. By tracking the evolving environment state step by step, PROTEA enables step-by-step safety judgments, which is particularly beneficial for complex and hard malicious plans. As shown in Fig.\n7\n, PROTEA substantially improves recall for LLaMA3.3-70B, Phi-4, and Mixtral-8x22B, even on the hard task cases.\n​​​​​​Observation 4\nIn contrast, PROTEA degrades performance (precision) for some specific class of model, e.g., GPT Models and Grok (Table\nII\n).\nWhile PROTEA was expected to improve detection by providing step-by-step reasoning with updated environment states, GPT models (GPT-4o-mini and GPT-OSS-120B) and Grok-3-mini, instead showed degraded performance. Both precision and recall dropped or remained unchanged compared to the Naive Method. This degradation stems from the cautious reasoning style of these models:\n•\nOver-cautiousness: The models flagged many benign actions as malicious simply because they appeared suspicious in isolation (e.g., approaching a trashcan before throwing keys, or moving toward an object located in another room).\n•\nRedundant actions misclassified: Repeated steps such as turning a light on after another light was switched off were treated as harmful instead of merely unnecessary.\n•\nWasteful actions equated to harmfulness: Actions like discarding clean towels were misjudged as malicious, even though they were only inefficient, not dangerous.\nThese false positives occur because external memory narrows the model’s focus, it sees one action at a time with the updated environment state, which makes it judge every step strictly. In other words, GPT-family and Grok behave like catastrophizers, they anticipate the worst outcome before it materializes.\nBy contrast, the Naive Method evaluates the entire plan at once, without fine-grained environment tracking, so it only flags overtly harmful actions and ignores subtle inefficiencies.\nFor GPT models and Grok, the detailed per-step evaluation of external memory induces a form of “overthinking”, due to their CoT reasoning capabilities, where they predict harm prematurely and misclassify safe actions as dangerous. This excessive caution inflates false positives and reduces overall detection performance.\nTABLE II:\nComparison of Naive vs. PROTEA\nGPT-4o-mini\nGPT-OSS-120B\nGrok-3-mini\nPrecision\nRecall\nPrecision\nRecall\nPrecision\nRecall\nNaïve Method\n0.902\n0.940\n0.963\n0.855\n0.929\n0.955\nPROTEA\n0.807\n0.924\n0.754\n0.856\n0.689\n0.946\n​​​​​​Observation 5\nHowever, PROTEA can still help improve detection performance when the task category difficulty is the highest, e.g., Important Item Loss.\nAcross all models, Important Item Loss (e.g., disposing keys, wallets, or cellphones) consistently shows the lowest recall compared to other harm categories such as Fire Hazard or Poisoning/ Contamination. Especially Fig.\n8\nshows the performance on the under-performing models without CoT reasoning. It clearly indicates that the Important Item Loss category is especially difficult for models to detect.\nFor the Important Item Loss category, we observed a drastic drop in the Naive and Object Filtering methods compared to PROTEA. This suggests that models benefit from step-by-step environment state tracking provided by the PROTEA, allowing them to better reason about scattered malicious actions in more complex plans.\nThis observation supports the validity of our dataset’s difficulty annotation. The consistent decline across easy, medium, and hard plans demonstrates that harder plans are indeed more challenging to detect, affirming that the dataset provides a meaningful gradient for evaluating detection robustness.\nRecommendations.\nIn conclusion, the usage of LLMs’ reasoning capabilities to improve robot safety is not a one-size-fits-all solution. Depending on the reasoning capabilities and type of alignment of the LLM models, the optimal approach may differ. We recommend adopting the Naive approach or PROTEA for models with CoT reasoning capabilities (e.g., GPT-4o-mini, GPT-OSS-120B). For the general reasoning models (e.g., LLaMA3.3-70B, Phi-4), we recommend either Object Filtering alone or PROTEA for improved safety.\nFigure 8:\nRecall across harm categories for different defense methods. While most categories (e.g., Fire Hazard, Poisoning/Contamination) are detected reliably, Important Item Loss remains the hardest to detect across all models.\nVII\nConclusion and Future Work\nIn this paper, we develop a defense strategy PROTEA for securing robot task planning and execution in adversarial scenarios.\nPROTEA focuses on objects involved in the current plan to address the dimensionality challenge and maintains an external memory to facilitate long-horizon reasoning to assess plan safety.\nPROTEA was implemented using six different LLMs.\nWe have developed a dataset HarmPlan that includes both benign and malicious plans (in six harm categories, with three difficulty levels for detection) for benchmarking such defense methods.\nResults demonstrate the effectiveness of PROTEA, and we share observations from the experimental results with practical recommendation with the robot planning practitioners.\nReferences\n[1]\nM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman,\net al.\n(2022)\nDo as i can, not as i say: grounding language in robotic affordances\n.\narXiv preprint arXiv:2204.01691\n.\nCited by:\n§I\n.\n[2]\nM. AI\n(2024)\nLLaMA 3.3-70B Instruct: open-source large language model\n.\nNote:\nhttps://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\nAccessed: 2025-09-15\nCited by:\n4th item\n.\n[3]\nM. AI\n(2024)\nMixtral-8x22B-instruct: aligned mixture-of-experts model\n.\nNote:\nhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1\nAccessed: 2025-09-15\nCited by:\n6th item\n.\n[4]\nA. Althobaiti, A. Ayala, J. Gao, A. Almutairi, M. Deghat, I. Razzak, and F. Cruz\n(2024)\nHow can llms and knowledge graphs contribute to robot safety? a few-shot learning approach\n.\narXiv preprint arXiv:2412.11387\n.\nCited by:\n§\nII-A\n.\n[5]\nA. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada\n(2016)\nControl barrier function based quadratic programs for safety critical systems\n.\nIEEE Transactions on Automatic Control\n62\n(\n8\n),\npp. 3861–3876\n.\nCited by:\n§\nII-A\n.\n[6]\nM. Asselmeier, D. Ahuja, A. Zaro, A. Abuaish, Y. Zhao, and P. A. Vela\n(2025)\nDynamic gap: safe gap-based navigation in dynamic environments\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\n§\nII-A\n.\n[7]\nR. Azeem, A. Hundt, M. Mansouri, and M. Brandão\n(2024)\nLlm-driven robots risk enacting discrimination, violence, and unlawful actions\n.\narXiv preprint arXiv:2406.08824\n.\nCited by:\n§\nII-A\n.\n[8]\nL. Brunke, Y. Zhang, R. Römer, J. Naimer, N. Staykov, S. Zhou, and A. P. Schoellig\n(2025)\nSemantically safe robot manipulation: from semantic scene understanding to motion safeguards\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\n§\nII-A\n.\n[9]\nJ. Chiu, J. Sleiman, M. Mittal, F. Farshidian, and M. Hutter\n(2022)\nA collision-free mpc for whole-body dynamic locomotion and manipulation\n.\nIn\nIEEE International conference on robotics and automation (ICRA)\n,\nCited by:\n§\nII-A\n.\n[10]\nY. Ding, X. Zhang, X. Zhan, and S. Zhang\n(2020)\nTask-motion planning for safe and efficient urban driving\n.\nIn\n2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\n§\nII-A\n.\n[11]\nD. Driess, J. Ha, and M. Toussaint\n(2020)\nDeep visual reasoning: learning to predict action sequences for task and motion planning from an initial scene image\n.\narXiv preprint arXiv:2006.05398\n.\nCited by:\n§\nIII-A\n.\n[12]\nP. Fiorini and Z. Shiller\n(1998)\nMotion planning in dynamic environments using velocity obstacles\n.\nThe international journal of robotics research\n17\n(\n7\n),\npp. 760–772\n.\nCited by:\n§\nII-A\n.\n[13]\nD. Fox, W. Burgard, and S. Thrun\n(2002)\nThe dynamic window approach to collision avoidance\n.\nIEEE robotics & automation magazine\n4\n(\n1\n),\npp. 23–33\n.\nCited by:\n§\nII-A\n.\n[14]\nM. Ghallab, D. Nau, and P. Traverso\n(2004)\nAutomated planning: theory and practice\n.\nElsevier\n.\nCited by:\n§I\n,\n§\nIII-A\n.\n[15]\nP. Haslum, N. Lipovetzky, D. Magazzeni, C. Muise, R. Brachman, F. Rossi, and P. Stone\n(2019)\nAn introduction to the planning domain definition language\n.\nSpringer\n.\nCited by:\n§I\n.\n[16]\nW. Huang, P. Abbeel, D. Pathak, and I. Mordatch\n(2022)\nLanguage models as zero-shot planners: extracting actionable knowledge for embodied agents\n.\nIn\nInternational conference on machine learning\n,\nCited by:\n§\nIII-A\n.\n[17]\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\net al.\n(2022)\nInner monologue: embodied reasoning through planning with language models\n.\narXiv preprint arXiv:2207.05608\n.\nCited by:\n§I\n.\n[18]\nY. Huang, L. Ding, Z. Tang, T. Wang, X. Lin, W. Zhang, M. Ma, and Y. Zhang\n(2025)\nA framework for benchmarking and aligning task-planning safety in llm-based embodied agents\n.\narXiv preprint arXiv:2504.14650\n.\nCited by:\n§\nII-A\n,\n§\nIV-B\n.\n[19]\nR. Jiao, S. Xie, J. Yue, T. SATO, L. Wang, Y. Wang, Q. A. Chen, and Q. Zhu\n(2025)\nCan we trust embodied agents? exploring backdoor attacks against embodied LLM-based decision-making systems\n.\nIn\nInternational Conference on Learning Representations (ICLR)\n,\nCited by:\n§\nII-B\n.\n[20]\nE. K. Jones, A. Robey, A. Zou, Z. Ravichandran, G. J. Pappas, H. Hassani, M. Fredrikson, and J. Z. Kolter\n(2025)\nAdversarial attacks on robotic vision language action models\n.\narXiv preprint:2506.03350\n.\nCited by:\n§\nII-B\n.\n[21]\nS. Karaman and E. Frazzoli\n(2011)\nSampling-based algorithms for optimal motion planning\n.\nThe international journal of robotics research\n30\n(\n7\n),\npp. 846–894\n.\nCited by:\n§\nII-A\n.\n[22]\nL. Karvraki\n(1996)\nProbabilistic roadmaps for path planning in high-dimensional configuration spaces\n.\nIEEE Trans. Robotics Automat.\n12\n,\npp. 566–580\n.\nCited by:\n§\nII-A\n.\n[23]\nA. A. Khan, M. Andrev, M. A. Murtaza, S. Aguilera, R. Zhang, J. Ding, S. Hutchinson, and A. Anwar\n(2025)\nSafety aware task planning via large language models in robotics\n.\narXiv preprint arXiv:2503.15707\n.\nCited by:\n§\nII-A\n.\n[24]\nH. Kress-Gazit, G. E. Fainekos, and G. J. Pappas\n(2007)\nWhere’s waldo? sensor-based temporal logic motion planning\n.\nIn\nProceedings 2007 IEEE International Conference on Robotics and Automation\n,\npp. 3116–3121\n.\nCited by:\n§\nII-A\n.\n[25]\nS. Li, F. Liu, L. Cui, J. Lu, Q. Xiao, X. Yang, P. Liu, K. Sun, Z. Ma, and X. Wang\n(2025)\nSafe planner: empowering safety awareness in large pre-trained models for robot task planning\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nCited by:\n§\nII-A\n.\n[26]\nA. Liu, Y. Zhou, X. Liu, T. Zhang, S. Liang, J. Wang, Y. Pu, T. Li, J. Zhang, W. Zhou,\net al.\n(2024)\nCompromising embodied agents with contextual backdoor attacks\n.\narXiv preprint arXiv:2408.02882\n.\nCited by:\n§\nII-B\n.\n[27]\nB. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone\n(2023)\nLlm+ p: empowering large language models with optimal planning proficiency\n.\narXiv preprint arXiv:2304.11477\n.\nCited by:\n§I\n.\n[28]\nS. Liu, J. Chen, S. Ruan, H. Su, and Z. Yin\n(2024)\nExploring the robustness of decision-level through adversarial attacks on llm-based embodied models\n.\nIn\nProceedings of the 32nd ACM International Conference on Multimedia\n,\npp. 8120–8128\n.\nCited by:\n§\nII-B\n.\n[29]\nX. Lu, Z. Huang, X. Li, W. Xu,\net al.\n(2024)\nPoex: policy executable embodied ai jailbreak attacks\n.\narXiv e-prints\n,\npp. arXiv–2412\n.\nCited by:\n§I\n,\n§\nII-B\n,\n§\nII-C\n.\n[30]\nK. Muenprasitivej, J. Jiang, A. Shamsah, S. Coogan, and Y. Zhao\n(2024)\nBipedal safe navigation over uncertain rough terrain: unifying terrain mapping and locomotion stability\n.\nIn\n2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nCited by:\n§\nII-A\n.\n[31]\nM. A. Nahian, Z. Altaweel, D. Reitano, S. Ahmed, S. Zhang, and A. S. Rakin\n(2025)\nRobo-troj: attacking llm-based task planners\n.\narXiv preprint arXiv:2504.17070\n.\nCited by:\n§I\n,\n§\nII-B\n.\n[32]\nK. Nakamura, L. Peters, and A. Bajcsy\n(2025)\nGeneralizing safety beyond collision-avoidance via latent-space reachability analysis\n.\narXiv preprint arXiv:2502.00935\n.\nCited by:\n§\nII-A\n.\n[33]\nI. Obi, V. L. Venkatesh, W. Wang, R. Wang, D. Suh, T. I. Amosa, W. Jo, and B. Min\n(2025)\nSafeplan: leveraging formal logic and chain-of-thought reasoning for enhanced safety in llm-based robotic task planning\n.\narXiv preprint arXiv:2503.06892\n.\nCited by:\n§\nII-A\n.\n[34]\nS. C. Ong, S. W. Png, D. Hsu, and W. S. Lee\n(2010)\nPlanning under uncertainty for robotic tasks with mixed observability\n.\nThe International Journal of Robotics Research\n29\n(\n8\n),\npp. 1053–1068\n.\nCited by:\n§I\n.\n[35]\nOpenAI\n(2024)\nGPT-4o-mini: openai’s lightweight reasoning model\n.\nNote:\nhttps://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence\nAccessed: 2025-09-15\nCited by:\n1st item\n.\n[36]\nopenAI\n(2025)\nGPT-OSS-120B: open-source 120b parameter model\n.\nNote:\nhttps://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/\nAccessed: 2025-09-15\nCited by:\n2nd item\n.\n[37]\nX. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba\n(2018)\nVirtualhome: simulating household activities via programs\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp. 8494–8502\n.\nCited by:\n§\nIV-A\n.\n[38]\nZ. Ravichandran, A. Robey, V. Kumar, G. J. Pappas, and H. Hassani\n(2025)\nSafety guardrails for llm-enabled robots\n.\narXiv preprint arXiv:2503.07885\n.\nCited by:\n§I\n,\n§\nII-C\n.\n[39]\nM. Research\n(2025)\nPhi-4: compact reasoning model\n.\nNote:\nhttps://www.microsoft.com/en-us/research/\nAccessed: 2025-09-15\nCited by:\n5th item\n.\n[40]\nA. Robey, Z. Ravichandran, V. Kumar, H. Hassani, and G. J. Pappas\n(2025)\nJailbreaking llm-controlled robots\n.\nIn\nIEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\n§\nII-B\n.\n[41]\nI. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg\n(2023)\nProgprompt: generating situated robot task plans using large language models\n.\nIn\nIEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\n§I\n.\n[42]\nA. Singletary, W. Guffey, T. G. Molnar, R. Sinnet, and A. D. Ames\n(2022)\nSafety-critical manipulation for collision-free food preparation\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\n§\nII-A\n.\n[43]\nM. W. Spong\n(2022)\nAn historical perspective on the control of robotic manipulators\n.\nAnnual review of control, robotics, and autonomous systems\n5\n(\n1\n),\npp. 1–31\n.\nCited by:\n§\nII-A\n.\n[44]\nJ. Van Den Berg, S. J. Guy, M. Lin, and D. Manocha\n(2011)\nReciprocal n-body collision avoidance\n.\nIn\nRobotics Research: The 14th International Symposium ISRR\n,\npp. 3–19\n.\nCited by:\n§\nII-A\n.\n[45]\nL. Wang, A. D. Ames, and M. Egerstedt\n(2017)\nSafety barrier certificates for collisions-free multirobot systems\n.\nIEEE Transactions on Robotics\n33\n(\n3\n),\npp. 661–674\n.\nCited by:\n§\nII-A\n.\n[46]\nT. Wang, C. Han, J. C. Liang, W. Yang, D. Liu, L. X. Zhang, Q. Wang, J. Luo, and R. Tang\n(2024)\nExploring the adversarial vulnerabilities of vision-language-action models in robotics\n.\narXiv preprint:2411.13587\n.\nCited by:\n§\nII-B\n.\n[47]\nT. Wongpiromsarn, U. Topcu, and R. M. Murray\n(2012)\nReceding horizon temporal logic planning\n.\nIEEE Transactions on Automatic Control\n57\n(\n11\n),\npp. 2817–2830\n.\nCited by:\n§\nII-A\n.\n[48]\nX. Wu, S. Chakraborty, R. Xian, J. Liang, T. Guan, F. Liu, B. M. Sadler, D. Manocha, and A. S. Bedi\n(2024)\nOn the vulnerability of llm/vlm-controlled robotics\n.\narXiv preprint arXiv:2402.10340\n.\nCited by:\n§\nII-A\n.\n[49]\nY. Wu, Z. Xiong, Y. Hu, S. S. Iyengar, N. Jiang, A. Bera, L. Tan, and S. Jagannathan\n(2025)\nSELP: generating safe and efficient task plans for robot agents with large language models\n.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n,\nCited by:\n§\nII-A\n.\n[50]\nxAI\n(2025)\nGrok-3-mini: smaller grok series model\n.\nNote:\nhttps://docs.x.ai/docs/models/grok-3-mini\nAccessed: 2025-09-15\nCited by:\n3rd item\n.\n[51]\nJ. Yang, Z. Lin, S. Yang, Z. Lu, and X. Du\n(2025)\nConcept enhancement engineering: a lightweight and efficient robust defense against jailbreak attacks in embodied ai\n.\narXiv preprint arXiv:2504.13201\n.\nCited by:\n§\nII-C\n.\n[52]\nZ. Yang, S. S. Raman, A. Shah, and S. Tellex\n(2024)\nPlug in the safety chip: enforcing constraints for llm-driven robot agents\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp. 14435–14442\n.\nCited by:\n§I\n,\n§\nII-A\n.\n[53]\nS. Yin, X. Pang, Y. Ding, M. Chen, Y. Bi, Y. Xiong, W. Huang, Z. Xiang, J. Shao, and S. Chen\n(2024)\nSafeagentbench: a benchmark for safe task planning of embodied llm agents\n.\narXiv preprint arXiv:2412.13178\n.\nCited by:\n§\nII-A\n,\n§\nIV-B\n.\n[54]\nH. Zhang, C. Zhu, X. Wang, Z. Zhou, C. Yin, M. Li, L. Xue, Y. Wang, S. Hu, A. Liu,\net al.\n(2024)\nBadRobot: jailbreaking embodied llms in the physical world\n.\narXiv preprint arXiv:2407.20242\n.\nCited by:\n§\nII-B\n.",
    "preview_text": "Robots need task planning methods to generate action sequences for complex tasks. Recent work on adversarial attacks has revealed significant vulnerabilities in existing robot task planners, especially those built on foundation models. In this paper, we aim to address these security challenges by introducing PROTEA, an LLM-as-a-Judge defense mechanism, to evaluate the security of task plans. PROTEA is developed to address the dimensionality and history challenges in plan safety assessment. We used different LLMs to implement multiple versions of PROTEA for comparison purposes. For systemic evaluations, we created a dataset containing both benign and malicious task plans, where the harmful behaviors were injected at varying levels of stealthiness. Our results provide actionable insights for robotic system practitioners seeking to enhance robustness and security of their task planning systems. Details, dataset and demos are provided: https://protea-secure.github.io/PROTEA/\n\nPROTEA: Securing Robot Task Planning and Execution\nZainab Altaweel, Mohaiminul Al Nahian, Jake Juettner, Adnan Siraj Rakin, Shiqi Zhang\nAbstract\nRobots need task planning methods to generate action sequences for complex tasks.\nRecent work on adversarial attacks has revealed significant vulnerabilities in existing robot task planners, especially those built on foundation models.\nIn this paper, we aim to address these security challenges by introducing PROTEA, an LLM-as-a-Judge defense mechanism, to evaluate the security of task plans.\nPROTEA is developed to address the dimensionality and history challenges in plan safety assessment.\nWe used different LLMs to implement multiple versions of PROTEA for comparison purposes.\nFor systemic evaluations, we created a dataset containing both benign and malicious task plans, where the harmful behaviors were injected at varying levels of stealthiness.\nOur results provide actionable insights for robotic system practitioners seeking to enhance robustness and secu",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "task planning",
        "security",
        "LLM",
        "adversarial attacks",
        "robustness"
    ],
    "one_line_summary": "这篇论文提出PROTEA，一种基于LLM的防御机制，用于评估机器人任务规划的安全性，以应对对抗性攻击。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T04:13:46Z",
    "created_at": "2026-01-21T12:09:07.798159",
    "updated_at": "2026-01-21T12:09:07.798165"
}