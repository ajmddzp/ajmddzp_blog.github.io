{
  "id": "2601.08454v1",
  "title": "Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees",
  "authors": [
    "Alessandro Adami",
    "Sebastian Zudaire",
    "Ruggero Carli",
    "Pietro Falco"
  ],
  "abstract": "Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.",
  "url": "https://arxiv.org/abs/2601.08454v1",
  "html_url": "https://arxiv.org/html/2601.08454v1",
  "html_content": "Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees\nAlessandro Adami\nâˆ—,â€ \n, Sebastian Zudaire\nÂ§\n, Ruggero Carli\nâˆ—\n, Pietro Falco\nâˆ—\nFunder: Project co-funded by the European Union â€“ Next Generation Eu - under the National Recovery and Resilience Plan (NRRP), Mission 4 Component 2, Investment 3.3 â€“ Decree no. 630 (24th April 2024) of Italian Ministry of University and Research; Concession Decree no. 1956 del 05th December 2024 adopted by the Italian Ministry of University and Research, CUP D93D24000270003, within the national PhD Programme in Autonomous Systems (XL cycle)\nâˆ—\n*\nUniversity of Padova, Dept. of Information Engineering, Italy.\nÂ§\n\\mathsection\nABB Robotics, VÃ¤sterÃ¥s, Sweden.\nâ€ \n\\dagger\nPolytechnic of Bari Dept. of Electrical and Information Engineering, Italy.\nAbstract\nConstructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines.\nGiven a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation.\nExperimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.\nUser\nSystem (fixed) prompt\n+\nUser Input\n(variable)\nâ€We want to\nbuild a simulationâ€¦â€\nFramework\nVLM Scene\nunderstanding\nBT Gen\nRobot task\nexecution\nSensors\nParams\nSim\nFigure 1:\nCompact Real2Sim adaptive framework. The user specifies the desired objective, while the system prompt (fixed and not accessible to the user) explains how to deal with available atomic functions and the desired output. Then the VLM returns the Behavior Tree for the acquisition of the missing parameters. Once all physical parameters are available, a physics-aware replica of the environment is built in MuJoCo.\nI\nIntroduction\nCreating accurate and physically grounded simulations of real-world environments is a central challenge in robotics and automation. Digital twins enable the safe testing of control policies, the study of contact-rich interactions, and the development of reinforcement learning strategies prior to real-world deployment\n[\n15\n]\n. Constructing high-fidelity simulations (e.g., in MuJoCo\n[\n18\n]\n), however, requires precise knowledge of object properties such as mass, center of mass, surface friction, and geometric dimensions. These quantities are often unknown\na priori\nand cannot be reliably inferred from visual observations alone\n[\n13\n]\n.\nExisting real-to-simulation (real2sim) pipelines for physical parameter estimation typically rely on labour-intensive manual measurements or on automated but rigid, pre-programmed interaction routines. While effective in controlled settings, such pipelines are not robust to novel objects, varying task requirements, or unstructured scenes. In particular, they generally estimate a fixed and often exhaustive set of physical parameters, regardless of whether all parameters are required for the downstream simulation objective. As a result, unnecessary interactions are performed, and task relevance is not explicitly considered. Recent work has demonstrated the feasibility of estimating physical parameters through robotic interaction\n[\n14\n,\n5\n]\n. Nevertheless, these approaches assume fixed exploration strategies and do not reason over user intent, semantic scene context, or parameter relevance when deciding\nwhich\nproperties to measure and\nhow\nto acquire them autonomously.\nIn parallel, vision-language models (VLMs) and vision-language-action models (VLAs) have shown promising capabilities in grounding high-level natural-language instructions into robot-executable actions\n[\n9\n,\n4\n]\n. However, most existing systems produce unstructured action sequences or give single pose commands, offering limited support for explicit task logic, safety constraints, or reactive decision-making during contact-rich interaction. Behavior Trees (BTs) provide a complementary formalism for structuring robotic behavior in a modular, interpretable, and reactive manner\n[\n3\n]\n. Recent studies have explored the use of large language models and VLMs to generate BTs from natural language\n[\n12\n,\n20\n]\n, but have not addressed the problem of autonomously selecting and sequencing physical interaction primitives for active perception, real2sim parameter acquisition.\nIn this work, we propose an autonomous active perception real2sim framework (Fig.\n1\n) that combines multi-modal scene understanding, VLM-based reasoning, and behavior-tree automated generation to construct simulation-ready models in an intent-driven manner. Given a high-level user request, a description of the incomplete simulation, and an RGB observation of the scene, the system identifies relevant objects, infers the minimal set of physical parameters required to complete the simulation, and generates an executable BT that specifies a conditioned sequence of exploratory interactions. The robot executes the BT to acquire the required properties through contact-based probing, lifting, pushing, and gripper-based measurements. The estimated parameters are then used to instantiate a physics-aware simulation in MuJoCo\n[\n18\n]\n.\nThe contributions of this work are twofold:\nâ€¢\nActive perception, intent-driven real2sim pipeline.\nWe introduce a real2sim framework that conditions autonomous physical parameter acquisition on user intent, enabling automatic and selective estimation of only the parameters required by the target simulation task. We present a VLM-powered method for generating executable BTs from natural language and visual context, enabling interpretable and reactive interaction strategies for parameter acquisition without requiring robotics expertise from the user.\nâ€¢\nBehavior Tree planning with atomic, physically grounded actions\n. Unlike prior VLM/LLM-based BT generation approaches that rely on high-level manipulation skills (e.g.,\nPickUp\nor\nThrowAway\n[\n20\n]\n), our framework plans directly over a fixed set of low-level, elementary actions (e.g., gripper commands, Cartesian motions, and sensing primitives), enabling fine-grained control, contact-aware interaction, and direct integration of physical measurements into Real2Sim pipelines.\nWe validate the proposed framework on a torque-controlled robot, demonstrating autonomous estimation of object mass, friction-related quantities, and surface height in real-world tabletop scenarios.\nExperimental results across multiple scenarios involving unknown objects, occlusions, and varying user requests demonstrate that the proposed framework enables flexible, interpretable, and task-aware simulation construction. By explicitly coupling high-level reasoning with physically grounded robotic interaction, this work advances toward scalable and autonomous real2sim pipelines for complex real-world environments.\nII\nRelated Works\nII-A\nReal2Sim and Physical Parameter Estimation for Simulation Construction\nAutonomous simulation constructions aim to create high-fidelity virtual replicas of real environments to support simulation-based planning, control, and learning\n[\n17\n]\n. In robotics, digital twins have been applied to predictive maintenance, simulation-based validation, and reinforcement learning under realistic dynamics. Bridging the gap between physical systems and their simulated counterparts has therefore motivated extensive research in real2sim methodologies.\nReal2sim approaches relied on carefully controlled laboratory setups to estimate object properties such as mass, inertia, and geometry through robotic interaction. Authors in\n[\n14\n]\nintroduced a scalable pick-and-place pipeline that estimates object geometry and inertial properties using joint-torque measurements collected during structured manipulation routines. Differentiable physics has also been employed to optimize physical parameters directly from observed trajectories, enabling closed-loop real2sim refinement\n[\n5\n]\n.\nAlthough effective in controlled settings, these methods use rigid, pre-defined exploration and fixed parameter estimation. They lack adaptation to user goals, object semantics, and scene context, estimate unnecessary physical properties, and rarely leverage semantic reasoning to guide what to estimate or how to interact. This prevents parameter acquisition in unstructured environments and task-specific scenarios, highlighting the need for approaches that integrate user intent, semantic understanding, and autonomous interaction planning.\nII-B\nGenerative AI for 3D World Model Construction\nRecent research has explored generative and automated pipelines for constructing 3D world models, addressing limitations of traditional manual modeling and enabling simulation-ready environments. For example,\n[\n10\n]\nproposes an agentic languageâ€“vision framework that integrates LLM planning with vision-guided refinement to generate physically plausible, interactive 3D scenes from text prompts, improving layout realism and spatial coherence. Similarly,\n[\n11\n]\nintroduces structured spatial contextâ€”comprising semantic blueprints, semantically labeled point clouds, and scene hypergraphsâ€”allowing VLMs to iteratively generate and edit coherent 3D environments for downstream spatial tasks such as interactive editing and path planning. Other systems, such as\n[\n21\n]\n, provide scalable generative engines for creating photorealistic and physics-aware 3D assets that can be directly imported into simulators for embodied intelligence research. These works illustrate emerging trends in automated 3D world model construction with generative AI and multimodal reasoning for robotics and embodied AI.\nWhile these approaches advance automated 3D scene generation and graphical realism, they largely ignore the estimation of underlying physical parameters, such as object mass or friction, focusing primarily on visual appearance rather than simulation fidelity. This limitation constrains their direct applicability to real2sim pipelines or contact-rich robotic interaction tasks.\nII-C\nBehavior Trees for Robotic Task Execution and LLM/VLM-Based Generation\nVision-Language Models (VLMs) enable grounding of natural-language instructions in visual observations, supporting multi-step reasoning and general-purpose manipulation planning. Vision-Language-Action (VLA) models\n[\n16\n]\n, including OpenVLA\n[\n9\n]\n, PaLM-E\n[\n4\n]\n, and RT-2\n[\n2\n]\n, extend this capability by directly mapping multi-modal inputs to action proposals or motion primitives, demonstrating strong cross-task and cross-platform generalization.\nBehavior Trees (BTs) provide a modular and compositional formalism for structuring robotic behavior, enabling reactive, hierarchical, and interpretable task execution\n[\n3\n,\n7\n]\n. Due to these properties, BTs have been widely adopted in robotic manipulation, assembly, and exploration tasks, offering improved robustness and transparency compared to finite-state machines or monolithic policies.\nRecent work has investigated the use of large language models and VLMs to generate BTs from natural language descriptions or contextual inputs\n[\n20\n,\n12\n]\n. However, existing approaches primarily focus on task execution and assume predefined, high-level action nodes with fixed semantics. They do not address the problem of autonomously selecting, composing, and sequencing elementary physical interaction primitives for adaptive environment exploration. Furthermore, the application of LLM/VLM-generated BTs to real2sim parameter acquisition and digital twin construction remains largely unexplored. Related approaches integrate VLMs and reactive planners with BTs for real-time failure detection and correction, highlighting the benefits of structured hierarchical planning for robust robotics execution\n[\n1\n]\n.\nIn contrast to prior VLM-based BT generation approaches that assume pre-defined high-level actions such as\nPickUp\nor\nThrowAway\n[\n20\n]\n, our framework plans directly over a set of atomic motion and sensing primitives, enabling physically grounded interaction and parameter estimation.\nIII\nProblem Formulation and Solution\nOur framework constructs a simulation-ready model by autonomously combining human intent, multi-modal scene understanding, behavior-tree-guided robotic exploration, and physics-based parameter estimation. The overall pipeline transforms a high-level user request into a sequence of targeted interaction behaviors that acquire the physical parameters required for simulation, estimating missing parameters only. Notably, the same system prompt is reused unchanged across all experimental scenarios in Sec\nV\n, demonstrating that task adaptation emerges from multi-modal reasoning rather than from manual tuning.\nThis section introduces the overall methodology of the proposed framework. In Sec.\nIII-A\nwe formalize the problem and define the required abstractions; Sec.\nIII-B\nformalizes the BT generation; Sec.\nIII-C\npresents the Prompt in each part and user inputs; and Sec.\nIII-D\ndetails the control of the robot and data acquisition in the real environment, during the exploration phase.\nIII-A\nProblem Formulation\nWe consider a real-world scene containing one or more objects with partially unknown physical properties. A human operator provides an incomplete simulation description\nğ’Ÿ\n\\mathcal{D}\nwith some missing parameters (e.g.,\n.xml\nfile in MuJoCo\n[\n18\n]\n) and/or a high-level request\nâ„›\n\\mathcal{R}\n(e.g.,\nâ€œCreate a simulation of the table with the bottle visible in the cameraâ€\n), which specifies the intended simulation goal rather than the required measurements. The user is considered to be agnostic to robotics, BTs, and VLMs.\nGiven\nâ„›\n\\mathcal{R}\n,\nğ’Ÿ\n\\mathcal{D}\n, and an RGB image\nâ„\n\\mathcal{I}\nof the scene, the model determines the set of physical parameters that have to be estimated to complete the simulation and autonomously plan the necessary interactions to obtain them.\nThe objective is to construct a simulation model\nğ’¯\n\\mathcal{T}\n:\nğ’¯\n=\n{\nğ’Ÿ\n,\nâ„›\n,\nğ’¢\n,\nğ’«\n}\n,\n\\mathcal{T}=\\{\\mathcal{D},\\mathcal{R},\\mathcal{G},\\mathcal{P}\\},\nwhere\nğ’¢\n\\mathcal{G}\ndenotes the object geometries (meshes, collision shapes, RGB renderingâ€¦), and\nğ’«\n\\mathcal{P}\ndenotes the physical parameters:\nğ’«\n=\n{\nmass\n,\ndimensions\n,\nfriction\n}\n\\mathcal{P}=\\{\\text{mass},\\text{dimensions},\\text{friction}\\}\n.\nUnlike traditional real2sim approaches that rely on predefined exploration routines (Sec.\nII-A\n), our system\nautomatically generates\nthe interaction strategy needed to infer the relevant subset of\nğ’«\n\\mathcal{P}\n, conditioned on the userâ€™s simulation goal. This enables task-aware, minimal, and adaptive parameter acquisition.\nWe assume a static workspace during execution, a single robot entity acting in the static scene, object poses\np\ni\np_{i}\nare available as metadata at planning time, and a predefined library of reliable atomic actions\nğ’œ\n\\mathcal{A}\nthat the robot can execute. The object poses are assumed to be available as the focus is on autonomous selection and acquisition of physical parameters rather than on visual perception itself.\nIII-B\nMulti-Modal Scene Understanding and Plan Generation\nThe first stage processes\nğ’®\nâ€‹\n(\nğ’Ÿ\n,\nâ„›\n,\nâ„\n)\n\\mathcal{S}(\\mathcal{D},\\mathcal{R},\\mathcal{I})\nusing a vision-language model (VLM).\nğ’®\n\\mathcal{S}\nis the system prompt (Sec.\nIII-C\n) tailored for physical parameter estimation that embeds variables\nğ’Ÿ\n,\nâ„›\n,\nâ„\n\\mathcal{D},\\mathcal{R},\\mathcal{I}\nin the fixed and priorly engineered part (see Sec.\nIII-C\n1\n).\nğ’®\n\\mathcal{S}\nis fixed before task execution by an expert, then an agnostic user can perform parameter estimations automatically, only providing variables triplet and without knowing\nğ’®\n\\mathcal{S}\n.\nThe VLM autonomously grounds linguistic descriptors in the visual scene and produces a structured interpretation consisting of:\n1.\nObject Identification.\nThe VLM detects scene objects referenced in the simulation (e.g., â€œbottleâ€, â€œtableâ€) and associates them with provided 3D locations. This step establishes the set of\nn\nn\ncandidate objects for parameter estimation\nğ’ª\n=\n{\n(\no\n1\n,\np\n1\n)\n,\nâ€¦\nâ€‹\n(\no\nn\n,\np\nn\n)\n}\n\\mathcal{O}=\\{(o_{1},p_{1}),\\dots(o_{n},p_{n})\\}\nwhere\no\ni\no_{i}\nis the detected object with the provided pose\np\ni\np_{i}\n. As an assumption, pose\np\ni\np_{i}\nis always available as environmental metadata.\n2.\nParameter Discovery.\nFrom the semantics of\nâ„›\n\\mathcal{R}\nand analysis of\nğ’Ÿ\n\\mathcal{D}\n, the VLM infers\nwhich physical parameters are necessary\n.\nFor example, a request involving manipulation requires object mass, while a placement task requires table height and object dimensions.\nÎ¦\n=\n{\nÏ•\n1\n,\nâ€¦\n,\nÏ•\nm\n}\n\\Phi=\\{\\phi_{1},\\dots,\\phi_{m}\\}\nis the set of\nm\nm\nmissing physical parameters inferred as necessary for the user request,\nwhere\nÏ•\nj\nâˆˆ\nğ’«\n\\phi_{j}\\in\\mathcal{P}\n.\nFormally, the VLM produces a set\nÎ¦\nâŠ†\nğ’«\n,\n\\Phi\\subseteq\\mathcal{P},\nwhere\nÎ¦\n\\Phi\nis the minimal set of parameters needed to satisfy the simulation goal.\n3.\nExploration Action Inference.\nThe VLM maps each missing parameter in\nÎ¦\n\\Phi\nto one or more atomic actions from the predefined library\nğ’œ\n\\mathcal{A}\nshown in Tab.\nI\n. Each action consists of an elementary operation that the robot can perform (e.g., open the gripper or reach a target joint configuration) or an elementary data acquisition (e.g,. read forces and torques vector applied on the end effector). The\nğ’œ\n\\mathcal{A}\nset is defined before the execution of the tasks, and each atomic action needs to be engineered. With this set, the model can build as many complex interactions with the environment as the combinations of atomic actions are. All atomic actions, including sensing operations, are represented as leaf nodes in the BT.\nTherefore, the VLM processes the input triplet\n(\nğ’Ÿ\n,\nâ„›\n,\nâ„\n)\n(\\mathcal{D},\\mathcal{R},\\mathcal{I})\nand autonomously produces a structured output BT. This is a hierarchical list specifying the abstract interaction plan.\nIt is of the form:\nâ„¬\nâ€‹\nğ’¯\nâ€‹\n(\nğ’œ\n,\nğ’\n)\n=\nV\nâ€‹\nL\nâ€‹\nM\nâ€‹\n(\nğ’®\n,\nğ’Ÿ\n,\nâ„›\n,\nâ„\n)\n=\n(\n[\nnode\n1\n,\nnode\n2\n,\nâ€¦\n]\n)\n\\mathcal{BT}(\\mathcal{A},\\mathcal{C})=VLM(\\mathcal{S},\\mathcal{D,R,I})=([\\,\\textsf{node}_{1},\\textsf{node}_{2},\\dots\\,])\nwhere each node contains:\nnode\ni\n=\n(\nargs\n,\ntype\n)\n,\n\\textsf{node}_{i}=\\big(\\textsf{args},\\textsf{type}\\big),\nwith\nargs\ninputs of the nodes as showed in Tab.\nI\nand\ntype\nâˆˆ\n{\nğ’\n,\nğ’œ\n}\n\\in\\{\\mathcal{C},\\mathcal{A}\\}\n, where\nğ’\n\\mathcal{C}\nis the set of composites types for Behavior Trees. Composite behaviors manage children and apply some logic to the way they execute and return a result. They do not directly execute any kind of action.\nThis structured output allows deterministic usage of BT libraries and ensures interpretability and reproducibility.\nTo convert the VLMâ€™s abstract plan into a structured and executable policy, we chose BTs for their compositionality, reactivity, and interoperability.\nBTs provide strong reactive execution properties due to their tick-based evaluation. At each control cycle\nt\nt\n, the root node is ticked, and control flows recursively through the children. This ensures immediate response to sensor feedback if an action fails, non-blocking parallel execution, and a composable structure by adding or removing subtrees, which does not affect the behavior of unrelated branches since each node explicitly encodes success/failure conditions. These properties make BTs particularly suitable for contact-rich parameter estimation tasks, where rapid adaptation to physical feedback is required.\nThe resulting BT encodes a structured exploration policy that is reactive to sensor feedback in real-time, interpretable by human operators, and generalizable across tasks without retraining.\nAlong with the BT, we opted to let the model output an explanation text\nâ„°\n\\mathcal{E}\nexplaining the exploration strategy. Furthermore, in the system prompt, it is specified in the behaviors section of\nğ’®\n\\mathcal{S}\nthat: if the task can not be completed, the model has to explain why and, in case it is possible, complete with the given actions enough measurements with which the user can reconstruct the wanted parameter.\nIII-C\nPrompt composition\nThe prompt is fed to the VLM as input and can be divided into four main parts: system prompt\nğ’®\n\\mathcal{S}\n, simulation description\nğ’Ÿ\n\\mathcal{D}\n, user prompt\nâ„›\n\\mathcal{R}\n, and image\nâ„\n\\mathcal{I}\n.\nIII-C\n1\nSystem Prompt\nğ’®\n\\mathcal{S}\nThe fixed part of the prompt is called the system prompt. It gives the model all the necessary information and instructions, of which the final user is assumed to be agnostic, and which are previously defined by an expert. It is built following prompt engineering principles\n[\n19\n,\n20\n]\nand is comprised of the following elements:\nâ€¢\nRole prompt\n: At the beginning of the system prompt, the model is given a role and general behaviors to follow during the BT planning.\nâ€¢\nObjects metadata\n: Object poses\np\ni\np_{i}\n, and specific locations are given in the prompt, assuming they are prior known. They are labelled to be clearly understandable from the model, not a source of confusion, and are given in code format. Poses can be given both numerically or as a placeholder.\nâ€¢\nInstruction following pipeline\n: In this subsection of the prompt, the model is instructed on how to interpret user instructions and how to act while building a plan.\nâ€¢\nExpected behaviors\n: In this section, all the expected behaviors during execution of the BT are included (e.g.\nAlways restore the environment to its original state after each measurement.\n).\nâ€¢\nğ’\n\\mathcal{C}\nand\nğ’œ\n\\mathcal{A}\nlist\n: All possible composites and actions that the VLM can use to build the behavior are listed with a clear and concise explanation of them. Only useful information is given to the model, without entering the details of the implementation. For actions of\nğ’œ\n\\mathcal{A}\n, expected input and obtained output are further detailed.\nâ€¢\nInput description\n: The input\nğ’Ÿ\n\\mathcal{D}\n,\nâ„›\n\\mathcal{R}\nand\nâ„\n\\mathcal{I}\nthat the model should expect from the user is described, and a placeholder for the user prompt is placed for better understandability for the model.\nâ€¢\nOutput description and example\n: The expected output is detailed explained to the model with an example of a full running BT. This will allow the model to return the desired structure, which can then be executed on the robot with BT libraries.\nWhile our approach relies on a fixed, engineered system prompt, this prompt does not encode task-specific exploration strategies or heuristics. Instead, it defines a task-agnostic interface over a set of atomic actions and expected behaviors, enabling the VLM to autonomously infer which parameters are required and how to acquire them based solely on user intent and scene context. As such, prompt engineering is used as a structural prior rather than as a substitute for planning or domain expertise.\nAction set\nğ’œ\n\\mathcal{A}\nInput\nOutput & Utility\nMovePose\ndesired pose\nMoves gripper to a\ndesired cartesian pose\nMoveJoints\ndesired jointsâ€™\npositions\nMoves joints\nin desired\nconfiguration\nOpenGripper\nNone\nOpens the gripper\nCloseGripper\nNone\nCloses the gripper\nMoveDownUntillContact\nNone\nMoves gripper\ndown until a\ncontact is detected\nMeasureGripperPose\nNone\nMeasures the pose\nof the gripper tips\nMeasureForces\nNone\nMeasures the\nforce applied\non gripper\nMeasureMass\nNone\nMeasures the\nmass of the\npicked object\nTABLE I:\nList of atomic actions\nğ’œ\n\\mathcal{A}\nthat the model can use to create the BT, with relative input and description.\nIII-C\n2\nSimulation description\nğ’Ÿ\n\\mathcal{D}\nThis prompt is embedded in the system prompt using a specific place_holder. A simulation description is given, and it autonomously finds missing parameters, autonomously building a BT for their estimation.\nIII-C\n3\nUser Prompt\nâ„›\n\\mathcal{R}\nThis prompt is embedded in the system prompt using a specific place_holder as\nğ’Ÿ\n\\mathcal{D}\nis. It is a given instruction or a sequence from the user. On top of this, the VLM will build the BT needed to satisfy, if it is possible, all the tasks. It is given in natural language, and it is the variable part of the textual prompt.\nIII-C\n4\nImage\nâ„\n\\mathcal{I}\nThe image is fed to VLM with a text prompt; it is variable and gives the model awareness of the environment in which the robot will operate. Our key assumption is that the working space is static and the robot is the only entity that can act on this. Image is RGB and is crucial for BT planning (as shown in Sec.\nV-A\n5\nand Sec.\nV-C\n). As shown in Sec\nV-A\n3\n,\nâ„\n\\mathcal{I}\ncan be both a real image or a synthetic.\nAt the current stage, vision is involved in the pipeline only during behavior planning.\nIII-D\nRobotic Execution and Data Acquisition\nThe generated BT is executed on a Franka Emika Panda robot equipped with torque sensing. Each leaf node triggers a corresponding controller primitive. To enable compliant interaction with the environment, we use Cartesian impedance control, which generates joint torques according to both the end-effector pose error and the robotâ€™s joint posture:\nğ‰\n=\nğ‰\nâŠ¤\nâ€‹\n[\nâˆ’\nğŠ\nâ€‹\n(\nğ±\nâˆ’\nğ±\nd\n)\nâˆ’\nğƒ\nâ€‹\n(\nğ‰\nâ€‹\nğª\nË™\n)\n]\nâŸ\nCartesian impedance\n+\nğ‚\nâ€‹\n(\nğª\n,\nğª\nË™\n)\nâŸ\nCoriolis compensation\n+\n(\nğˆ\nâˆ’\nğ‰\nâŠ¤\nâ€‹\nğ‰\n+\n)\nâ€‹\n[\nk\nns\nâ€‹\n(\nğª\nd\nâˆ’\nğª\n)\nâˆ’\n2\nâ€‹\nk\nns\nâ€‹\nğª\nË™\n]\nâŸ\nNullspace PD\n\\boldsymbol{\\tau}=\\underbrace{\\mathbf{J}^{\\top}\\left[-\\mathbf{K}(\\mathbf{x}-\\mathbf{x}_{d})-\\mathbf{D}(\\mathbf{J}\\dot{\\mathbf{q}})\\right]}_{\\text{Cartesian impedance}}+\\underbrace{\\mathbf{C}(\\mathbf{q},\\dot{\\mathbf{q}})}_{\\text{Coriolis compensation}}+\\\\\n\\underbrace{(\\mathbf{I}-\\mathbf{J}^{\\top}\\mathbf{J}^{+})\\left[k_{\\text{ns}}(\\mathbf{q}_{d}-\\mathbf{q})-2\\sqrt{k_{\\text{ns}}}\\dot{\\mathbf{q}}\\right]}_{\\text{Nullspace PD}}\n(1)\nğ±\nâˆˆ\nâ„\n6\n\\mathbf{x}\\in\\mathbb{R}^{6}\nis the current end-effector pose (position and orientation),\nğ±\nd\nâˆˆ\nâ„\n6\n\\mathbf{x}_{d}\\in\\mathbb{R}^{6}\nis the desired equilibrium pose,\nğª\n,\nğª\nË™\nâˆˆ\nâ„\n7\n\\mathbf{q},\\dot{\\mathbf{q}}\\in\\mathbb{R}^{7}\nare the joint positions and velocities,\nğª\nd\nâˆˆ\nâ„\n7\n\\mathbf{q}_{d}\\in\\mathbb{R}^{7}\nis the desired joint configuration for nullspace stabilization,\nğ‰\nâ€‹\n(\nğª\n)\nâˆˆ\nâ„\n6\nÃ—\n7\n\\mathbf{J}(\\mathbf{q})\\in\\mathbb{R}^{6\\times 7}\nis the geometric Jacobian,\nğŠ\n,\nğƒ\nâˆˆ\nâ„\n6\nÃ—\n6\n\\mathbf{K},\\mathbf{D}\\in\\mathbb{R}^{6\\times 6}\nare the Cartesian stiffness and Cartesian damping matrices,\nk\nns\nk_{\\text{ns}}\nis the nullspace stiffness scalar,\nğ‰\n+\n\\mathbf{J}^{+}\nis the pseudoinverse of\nğ‰\nâŠ¤\n\\mathbf{J}^{\\top}\nand\nğ‚\nâ€‹\n(\nğª\n,\nğª\nË™\n)\n\\mathbf{C}(\\mathbf{q},\\dot{\\mathbf{q}})\ncompensates for Coriolis forces.\nThis formulation ensures compliant and safe interaction with the environment, allowing the robot to adapt to contact forces during pushing, surface exploration, or other manipulation tasks while maintaining a preferred\nposture in its nullspace\n[\n8\n]\n.\nDuring execution, we record the needed parameters for digital twin construction when a proper action is triggered. Therefore, the data are acquired and processed only when needed, augmenting interpretability for the user.\nIV\nExperimental Setup\nWe validated the proposed Real2Sim pipeline through a series of real-robot experiments involving contact-rich interactions, multi-modal scene understanding, and simulation construction. The experimental setup integrates a visual perception system, the VLM-driven autonomous BT generation and execution framework, and the construction of a MuJoCo simulation environment.\nIV-A\nHardware Platform\nExperiments were conducted using a Franka Emika Panda manipulator equipped with 7-DoF torque-controlled arm, built-in joint torque sensing, a parallel-jaw gripper with position feedback, and real-time Cartesian impedance control and joint trajectory controller.\nA single static RGB camera (Intel RealSense D435i) was positioned to observe the tabletop workspace. In the current implementation, 3D object poses are provided in the system prompt (Sec.\nIII-C\n1\n).\nIV-B\nVLMâ€“Driven BT Generation and Execution\nThe RGB image\nâ„\n\\mathcal{I}\nand user request\nâ„›\n\\mathcal{R}\nare processed by a VLM (ChatGPT-5), which produces a structured action plan in the form described in Sec.\nIII\n. The plan is parsed into an executable BT using the Python\npy_trees\nlibrary.\nThe BT is executed in real time on the Panda robot using ROS Noetic. Each leaf node triggers a specific movement primitive, such as moving to a certain Cartesian pose, opening the gripper, or descending until contact, as reported in the Tab.\nI\n. Sensor data generated during BT execution are logged for subsequent parameter estimation.\nThe measurements capture the robotâ€“object interaction dynamics and provide the data necessary to estimate the objectâ€™s mass, friction, and surface height for simulation building.\nIV-C\nPhysical Parameter Estimation\nPhysical properties\nÎ¦\n\\Phi\nare directly estimated from the recorded interaction data.\nIV-C\n1\nMass Estimation\nDuring quasi-static lifting, the external wrench\nğ’˜\next\n\\boldsymbol{w}_{\\text{ext}}\nat the end-effector is recovered from joint torques\nğ‰\n\\boldsymbol{\\tau}\nvia\nğ’˜\nee\n=\nğ‰\nâ€‹\n(\nq\n)\nâˆ’\nâŠ¤\nâ€‹\nğ‰\n,\n\\boldsymbol{w}_{\\text{ee}}=\\mathbf{J}(q)^{-\\top}\\boldsymbol{\\tau},\nwhere\nJ\nâ€‹\n(\nq\n)\nJ(q)\nis the manipulator Jacobian and\nğ’˜\nee\n=\n[\nF\nx\n,\nF\ny\n,\nF\nz\n,\nÏ„\nx\n,\nÏ„\ny\n,\nÏ„\nz\n]\n\\boldsymbol{w}_{\\text{ee}}=[F_{x},F_{y},F_{z},\\tau_{x},\\tau_{y},\\tau_{z}]\n.\nThe vertical force component yields the estimated mass:\nm\n=\nF\nz\ng\nm=\\frac{F_{z}}{g}\n.\nIV-C\n2\nFriction Estimation\nDuring a planar push with measured tangential force\nF\nâˆ¥\nF_{\\parallel}\nfrom (\nIV-C\n1\n), the friction coefficient is:\nÎ¼\n=\nF\nâˆ¥\nm\nâ€‹\ng\n\\mu=\\frac{F_{\\parallel}}{mg}\n, in both static\nÎ¼\ns\n\\mu_{s}\nand dynamic case\nÎ¼\nd\n\\mu_{d}\n.\nIV-C\n3\nSurface Height Estimation\nA torque threshold\nÏ„\nc\n\\tau_{c}\nindicates contact and knowing the end-effector pose\n[\nx\ne\nâ€‹\ne\n,\ny\ne\nâ€‹\ne\n,\nz\ne\nâ€‹\ne\n,\nÎ±\nx\ne\nâ€‹\ne\n,\nÎ±\ny\ne\nâ€‹\ne\n,\nÎ±\nz\ne\nâ€‹\ne\n]\n[x_{ee},y_{ee},z_{ee},\\alpha_{x_{ee}},\\alpha_{y_{ee}},\\alpha_{z_{ee}}]\n:\nh\n=\nz\nee\nâˆ’\nd\noffset\nh=z_{\\text{ee}}-d_{\\text{offset}}\n,\nwhere\nd\noffset\nd_{\\text{offset}}\nis the calibrated base offset and is known.\nIV-D\nSimulation Environment\nThe estimated parameters are directly integrated into a MuJoCo\n[\n18\n]\nenvironment. Objectsâ€™ graphical representations are instantiated using:\nâ€¢\nconvex hulls or simplified meshes for geometry,\nâ€¢\nmeasured\nÎ¦\n\\Phi\nparameters,\nâ€¢\nreconstructed placement and dimensions.\nV\nExperiments and Results\nThree experimental scenarios were implemented to test the depicted framework. We demonstrate that, unlike scripted Real2Sim pipelines based on pre-defined exploration routines and expert-designed measurement procedures, the proposed framework automatically generates task-specific behavior trees from a high-level user request. This eliminates the need for robotics or system identification expertise during execution, while enabling adaptive and interpretable physical parameter acquisition. The evaluation focuses on real-robot experiments that assess estimation accuracy, robustness to occlusions, and the ability to autonomously adapt the interaction strategy to different user intents. Quantitative comparisons against existing Real2Sim pipelines are not directly applicable, as prior approaches rely on fixed, task-agnostic exploration routines and estimate a predefined set of physical parameters, making them not directly comparable to the proposed intent-driven and selective acquisition framework.\nIn the first scenario, the user wants to build a simulation of a scene with objects on top of a table, in which knowledge of object mass, as well as table height, is required. In the second scenario, the user wants to know the friction coefficients, both static and dynamic. In the third, it is asked to estimate the mass of an object, which is occluded by another one, preventing a direct estimate of the mass of the object below. Note that\nğ’œ\n\\mathcal{A}\nis the same for all three scenarios, while specific actions are needed for specific data acquisition. Measurements are reported as mean and standard deviation, computed across repeated measurements.\nV-A\nFirst scenario - Table Height and Object Properties\nIn this scenario, the model is asked to build a simulation in which the object\no\n1\no_{1}\nmass (\nÏ•\n1\n\\phi_{1}\n) is required together with the table\no\n2\no_{2}\nheight (\nÏ•\n2\n\\phi_{2}\n), to perform some tasks in a simulated environment. For this specific case,\no\n1\no_{1}\nis a water bottle filled with different water quantities. The same quantities are measured by hand and compared with true results.\nFigure 2:\nExample of BT generated for the estimation of table height and mass of the blue bottle. First, the robot executes the sub-tree to acquire the table height, then it executes the other to acquire the bottle mass. As it is possible to see from the second subtree, the VLM is instructed to approach objects before moving to their poses for picking.\nNote that each sub-tree is built with a composition of atomic actions from set\nğ’œ\n\\mathcal{A}\n.\nV-A\n1\nBottle mass and table height estimation\nIn this setup, the robot is explicitly asked to estimate\nÏ•\n1\n\\phi_{1}\nand\nÏ•\n2\n\\phi_{2}\n(for this first case,\nğ’Ÿ\n\\mathcal{D}\nis not given in the prompt):\nâ€¢\nUser request\nâ„›\n\\mathcal{R}\n:\nWe want to build a simulation of the environment that you see from the camera, but we donâ€™t know the mass of the bottle and the height of the table on which the object is.\nThe image\nâ„\n\\mathcal{I}\ngiven to the VLM is the one reported in Fig.\n5\non the left. Results obtained from measurements for one bottle filled with water and table height are reported in Tab.\nII\n, and a sequence of performed actions is reported in Fig.\n3\n. The VLM showed the capability of composing actions of the given\nğ’œ\n\\mathcal{A}\nset, to build composite operations for parameter estimation. The generated BT is reported in Fig.\n2\n.\nFigure 3:\nFirst scenario sequence of parameters estimation. The robot first touches the table to acquire its height with the end-effector pose. Then it lifts the bottle to acquire that mass with torque sensors and then puts the object in its original position.\nObject\nParameter\nAcquired Measure\nManual measure\nBottle\nmass [Kg]\nmean=\n0.257\n0.257\n, std=\n0.022\n0.022\nmass=\n0.254\n0.254\nTable\nheight [m]\nmean=\n0.764226\n0.764226\n,\nstd=\n0.000039\n0.000039\nheight=\n0.765\n0.765\nTABLE II:\nObjects and estimated parameters for real2sim.\nWhile engineering the output prompt description (Sec.\nIII-C\n1\n), we saw that it is useful to provide the model with one or more examples of what the desired structured outcome is. These examples showed a strong influence in VLM outcome. If, for instance, the pick-up of an object is included as an example of structured text in\nğ’®\n\\mathcal{S}\n, the VLM reproduces the same behavior explicited in the example while picking an object, anticipating or postponing the gripper opening to the approach phase with respect to what the example shows. This suggests the opportunity to tune robot behavior by feeding useful examples in the VLM prompt, enhancing human-like behaviors.\nV-A\n2\nKnown\nğ’Ÿ\n\\mathcal{D}\nThe\n.xml\nfile (\nğ’Ÿ\n\\mathcal{D}\n) of the MuJoCo simulation is incorporated in the system prompt of the VLM as additive data. In this way, the user is not required to specify which parameters\nÏ•\ni\n\\phi_{i}\nare missing to build the simulation. The model will autonomously derive which strategy should be applied to acquire all of them. As in the previous test, the\nğ’Ÿ\n\\mathcal{D}\nis given without table height and bottle mass. The BT generated is the same as reported in Fig.\n2\n. Adding\nÏ•\n2\n\\phi_{2}\nto the\nğ’Ÿ\n\\mathcal{D}\nfile, the model built a plan to estimate\nÏ•\n1\n\\phi_{1}\nonly, as reported in Fig.\n4\n. This shows the ability of the model to discern available\nÏ•\ni\n\\phi_{i}\nparameters from missing ones, due to\nğ’®\n\\mathcal{S}\ndesign.\nFigure 4:\nBT generated for bottles mass acquisition only, when\nğ’Ÿ\n\\mathcal{D}\nis available, and table height is known.\nV-A\n3\nSynthetic\nâ„\n\\mathcal{I}\ngeneration\nThe model was asked to build a strategy for physical parameters estimation based on a synthetic representation of the environment in image\nâ„\n\\mathcal{I}\n. Real scenario and synthetic image created with MuJoCo simulation are both reported in Fig.\n5\n. The BT generated resulted in the same as the one obtained with the real image. Different scenes are tested and, in all cases, the composition of elementary actions is the same. This suggests the opportunity to build datasets and train models on top of them for high-level planning on synthetic\nâ„\n\\mathcal{I}\nand then deploy them on real word scenario without adaptation. However, atomic actions set\nğ’œ\n\\mathcal{A}\nhave to be implemented in such a way that allows deployment in the real world.\nFigure 5:\nReal environment\nâ„\n\\mathcal{I}\npicture from camera (left) and simulation\nâ„\n\\mathcal{I}\nof it (right). When the simulation image is used as a prompt, only a graphical representation with bottle meshes is used.\nV-A\n4\nMore\nğ’ª\n\\mathcal{O}\nin the setup\nFor the same case, three bottles of different colours (blue\no\n1\no_{1}\n, green\no\n2\no_{2}\n, and pink\no\n3\no_{3}\n) were put on top of the same table\no\n4\no_{4}\n. The user asks in\nâ„›\n\\mathcal{R}\nthe robot to acquire masses of the three bottles (\nÏ•\n1\n\\phi_{1}\n,\nÏ•\n2\n\\phi_{2}\n,\nÏ•\n3\n\\phi_{3}\n) but in\nğ’Ÿ\n\\mathcal{D}\n, only the green bottleâ€™s mass (\nÏ•\n3\n\\phi_{3}\n) is missing. All three poses\np\n1\np_{1}\n,\np\n2\np_{2}\n, and\np\n3\np_{3}\nare given as metadata, while\nğ’Ÿ\n\\mathcal{D}\nand\nâ„›\n\\mathcal{R}\nspecify the task. As it is possible to see from Fig.\n6\n, the BT generated by the model makes the robot lift and weigh the green bottle, without using the others.\nFigure 6:\nThe sequence shows the robot picking up only the bottle whose mass should be estimated.\nV-A\n5\nModel Hallucinations\nThe VLM may have hallucinations\n[\n6\n]\n, pretending to use objects that are not present in the image\nâ„\n\\mathcal{I}\nbut present in the user request\nâ„›\n\\mathcal{R}\n.\nIn\nğ’®\n\\mathcal{S}\n, the model is specifically asked to rely on\nâ„\n\\mathcal{I}\nfor the plan, and then check if the pose is available in metadata; if the object\no\ni\no_{i}\nis available in both cases,\no\ni\nâˆˆ\nğ’ª\no_{i}\\in\\mathcal{O}\nand the plan is then included in the BT. So the system prompt is engineered against hallucinations, and then tested. For instance, the model is asked to pick up a yellow bottle that is not present in the environment picture\nâ„\n\\mathcal{I}\n. If a fictitious pose of the yellow bottle is present or not in\nS\nS\n, the model does not include the task in the BT, basing its analysis on\nâ„\n\\mathcal{I}\n.\nV-B\nSecond scenario - Friction Parameters\nIn the second scenario, the VLM is asked to estimate the friction parameters while moving\no\n1\no_{1}\n. In this setup, the robot is required to acquire measurements while performing an action (pushing the object).\nThis task implies the execution of two different actions in order to estimate the parameter: weigh\nÏ•\n1\n\\phi_{1}\n, and then push\no\n1\no_{1}\nto acquire the sliding force\nÏ•\n2\n\\phi_{2}\n.\nV-B\n1\nUnachievable task & output and explanation text\nAs it is possible to see from Tab.\nI\n, there is no specific action to estimate friction with the robot within\nğ’œ\n\\mathcal{A}\n. The robot can move the objects and, in parallel, acquire the vector of the forces applied on the end effector, but it canâ€™t give the friction parameter to the user. So the model is not able to satisfy the userâ€™s request. In this situation,\nâ„°\n\\mathcal{E}\nbecomes crucial.\nâ€¢\nOutput text\nâ„°\n\\mathcal{E}\n:\nThe BT measures the mass of the blue bottle, captures forces during a sliding motion along x to enable friction estimation, and measures table height. Derived parameters like friction coefficients can be computed by the user from these measurements.\nThis will not allow the model to do a specific measurement, but allows the robot to perform enough actions to let a human operator derive the friction coefficients.\nNumerical results of friction and mass estimation are reported in Tab.\nIII\n. As it is possible to see from the table, friction parameters are consistent but with high variance, while mass estimation is more precise.\nFigure 7:\nThe sequence shows the robot lifting the bottle and sliding it to acquire parameters needed to find friction coefficients.\nObject\nEstimated mass [Kg]\nEstimated friction coeff.\nBlue bottle\nmean=0.598, std=0.017\nÎ¼\ns\n\\mu_{s}\n: mean=0.41, std=0.11\nÎ¼\nd\n\\mu_{d}\n: mean=0.34, std=0.20\nTABLE III:\nBlue bottle estimated parameters.\nV-C\nThird scenario - Object Occlusion\nIn this scenario, the VLM is asked to estimate the mass (\nÏ•\n1\n\\phi_{1}\n) of a small blue box\no\n1\no_{1}\nthat is piled with another red box\no\n2\no_{2}\non top of it. As reported in Sec.\nIII-C\n, the model is given the possibility to place useless objects that prevent the achievement of the goal in a specific temporary pose\nx\n1\nx_{1}\n. The robot is asked through\nâ„›\n\\mathcal{R}\nto estimate the mass of\no\n1\no_{1}\n, without specifying that it is below the red one as text and giving\nğ’Ÿ\n\\mathcal{D}\n. As\np\n1\np_{1}\nand\np\n2\np_{2}\nmay help the model to build the plan with\nz\nz\ncomponent, only for this case, it is specified that the poses are known for both the boxes with a variable, but they were not given numerically. From the sequence of actions shown in Fig.\n8\n, it is possible to see how the model creates a BT that allows the robot to execute a task, even if the object\no\n1\no_{1}\nis placed below\no\n2\no_{2}\n. This enhances the capability of the model of planning complex behaviors for parameter acquisition with a set of atomic actions\nğ’œ\n\\mathcal{A}\n.\nFigure 8:\nThe sequence shows the robot putting the red block in a temporary position first. Then it lifts the blue block to weigh it and finally restores the original setup configuration.\nObject\nEstimated mass [Kg]\nHand measured mass [Kg]\nBlue box\nman=0.022, std=0.015\nmass=0.016\nTABLE IV:\nMass estimate for the blue box.\nVI\nConclusions and Future works\nThis paper presented an active perception Real2Sim framework for automated simulation construction that integrates multi-modal scene understanding, behavior-tree-based planning, and physical interaction with real robotic systems. By leveraging vision-language models to interpret user intent and scene context, the proposed approach autonomously determines which physical parameters are required for simulation and generates a structured, executable plan to acquire them through targeted robot interactions with a limited set of primitive actions.\nExperiments on a torque-controlled Franka Emika Panda robot demonstrate that the framework can accurately estimate physical properties such as object mass, surface height, and friction-related measurements, even in challenging scenarios involving occlusions or incomplete prior models. The use of behavior trees ensures interpretable, reactive, and safe execution, while the reliance on a minimal set of atomic actions enables flexible composition of complex interaction strategies without task-specific reprogramming.\nOverall, this work moves toward autonomous, intent-driven Real2Sim pipelines that tightly couple high-level reasoning with physically grounded automated robotic interaction, supporting scalable simulation generation for robotics automation and learning.\nFuture works will relax known-poses assumptions, in particular through vision-based pose estimation like\n[\n22\n]\n, as RGB-D from the camera feed contains sufficient information for integration.\nFurthermore, a comprehensive quantitative evaluation of digital twin fidelity remains an important direction for the future. This includes systematic realâ€“simulation comparisons across multiple tasks, metrics on trajectory and force prediction errors, and closed-loop policy transfer performance. Finally, future works will integrate works like those discussed in Sec.\nII-B\nto autonomously build a complete simulation, embedding both graphical and physical elements.\nReferences\n[1]\nF. Ahmad and et al\n(2025)\nA unified framework for real-time failure handling in robotics using vision-language models, reactive planner and behavior trees\n.\nExternal Links:\n2503.15202\n,\nLink\nCited by:\nÂ§\nII-C\n.\n[2]\nB. Anthony and et al\n(2023)\nRT-2: vision-language-action models transfer web knowledge to robotic control\n.\nExternal Links:\n2307.15818\n,\nLink\nCited by:\nÂ§\nII-C\n.\n[3]\nM. Colledanchise and P. Ã–gren\n(2018-07)\nBehavior trees in robotics and ai\n.\nCRC Press\n.\nExternal Links:\nISBN 9780429950902\n,\nLink\n,\nDocument\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[4]\nD. Driess and et al\n(2023)\nPaLM-e: an embodied multimodal language model\n.\nIn\narXiv preprint arXiv:2303.03378\n,\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[5]\nE. Heiden, D. Millard, and G. S. Sukhatme\n(2019)\nReal2Sim transfer using differentiable physics\n.\nR:SS Workshop on Closing the Reality Gap in Sim2real Transfer for Robotic Manipulation\n.\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[6]\nL. Huang and et al\n(2025-01)\nA survey on hallucination in large language models: principles, taxonomy, challenges, and open questions\n.\nACM Transactions on Information Systems\n43\n(\n2\n),\npp.Â 1â€“55\n.\nExternal Links:\nISSN 1558-2868\n,\nLink\n,\nDocument\nCited by:\nÂ§\nV-A\n5\n.\n[7]\nM. Iovino, E. Scukins, J. Styrud, P. Ã–gren, and C. Smith\n(2022)\nA survey of behavior trees in robotics and ai\n.\nRobotics and Autonomous Systems\n154\n,\npp.Â 104096\n.\nExternal Links:\nISSN 0921-8890\n,\nDocument\nCited by:\nÂ§\nII-C\n.\n[8]\nO. Khatib\n(1987)\nA unified approach for motion and force control of robot manipulators: the operational space formulation\n.\nIEEE Journal on Robotics and Automation\n3\n(\n1\n),\npp.Â 43â€“53\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nIII-D\n.\n[9]\nM. J. Kim and et al\n(2024)\nOpenVLA: an open-source vision-language-action model\n.\narXiv preprint arXiv:2406.09246\n.\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[10]\nL. Ling and et al\n(2025)\nScenethesis: a language and vision agentic framework for 3d scene generation\n.\nExternal Links:\n2505.02836\n,\nLink\nCited by:\nÂ§\nII-B\n.\n[11]\nX. Liu, Y. Tai, and C. Tang\n(2025)\nAgentic 3d scene generation with spatially contextualized vlms\n.\nExternal Links:\n2505.20129\n,\nLink\nCited by:\nÂ§\nII-B\n.\n[12]\nA. Lykov and D. Tsetserukou\n(2023)\nLLM-brain: ai-driven fast generation of robot behaviour tree based on large language model\n.\nExternal Links:\n2305.19352\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-C\n.\n[13]\nN. Mavrakis, A. Ghalamzan, and R. Stolkin\n(2020-07)\nEstimating an objectâ€™s inertial parameters by robotic pushing: a data-driven approach\n.\nIn\n2020 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n.\n[14]\nN. Pfaff, E. Fu, J. Binagia, P. Isola, and R. Tedrake\n(2025)\nScalable real2sim: physics-aware asset generation via robotic pick-and-place setups\n.\nExternal Links:\n2503.00370\n,\nLink\nCited by:\nÂ§I\n,\nÂ§\nII-A\n.\n[15]\nR. Rosen, G. von Wichert, G. Lo, and K. D. Bettenhausen\n(2015)\nAbout the importance of autonomy and digital twins for the future of manufacturing\n.\nIFAC-PapersOnLine\n48\n(\n3\n),\npp.Â 567â€“572\n.\nNote:\n15th IFAC Symposium onInformation Control Problems inManufacturing\nExternal Links:\nISSN 2405-8963\n,\nDocument\n,\nLink\nCited by:\nÂ§I\n.\n[16]\nR. Sapkota, Y. Cao, K. Roumeliotis, and M. Karkee\n(2025-05)\nVision-language-action models: concepts, progress, applications and challenges\n.\npp.\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-C\n.\n[17]\nF. Tao and et al\n(2019-01)\nFive-dimension digital twin model and its ten applications\n.\nJisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS\n25\n,\npp.Â 1â€“18\n.\nExternal Links:\nDocument\nCited by:\nÂ§\nII-A\n.\n[18]\nE. Todorov, T. Erez, and Y. Tassa\n(2012)\nMuJoCo: a physics engine for model-based control\n.\nIn\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp.Â 5026â€“5033\n.\nExternal Links:\nDocument\nCited by:\nÂ§I\n,\nÂ§I\n,\nÂ§\nIII-A\n,\nÂ§\nIV-D\n.\n[19]\nS. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor\n(2023)\nChatGPT for robotics: design principles and model abilities\n.\nExternal Links:\n2306.17582\n,\nLink\nCited by:\nÂ§\nIII-C\n1\n.\n[20]\nN. Wake and et al\n(2025-01)\nVLM-driven behavior tree for context-aware task planning\n.\nNote:\narXiv\nExternal Links:\nLink\nCited by:\n2nd item\n,\nÂ§I\n,\nÂ§\nII-C\n,\nÂ§\nII-C\n,\nÂ§\nIII-C\n1\n.\n[21]\nX. Wang and et al\n(2025)\nEmbodiedGen: towards a generative 3d world engine for embodied intelligence\n.\nExternal Links:\n2506.10600\n,\nLink\nCited by:\nÂ§\nII-B\n.\n[22]\nB. Wen, W. Yang, J. Kautz, and S. Birchfield\n(2024)\nFoundationPose: unified 6d pose estimation and tracking of novel objects\n.\nIn\nCVPR\n,\nCited by:\nÂ§VI\n.",
  "preview_text": "Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.\n\nReal2Sim based on Active Perception with automatically VLM-generated Behavior Trees\nAlessandro Adami\nâˆ—,â€ \n, Sebastian Zudaire\nÂ§\n, Ruggero Carli\nâˆ—\n, Pietro Falco\nâˆ—\nFunder: Project co-funded by the European Union â€“ Next Generation Eu - under the National Recovery and Resilience Plan (NRRP), Mission 4 Component 2, Investment 3.3 â€“ Decree no. 630 (24th April 2024) of Italian Ministry of Universit",
  "is_relevant": true,
  "relevance_score": 6.0,
  "extracted_keywords": [
    "VLM",
    "whole body control",
    "locomotion"
  ],
  "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆè¡Œä¸ºæ ‘ã€é€šè¿‡ä¸»åŠ¨æ„ŸçŸ¥å®ç°çœŸå®åˆ°ä»¿çœŸçš„æ¡†æ¶ï¼Œç”¨äºç‰©ç†å‚æ•°ä¼°è®¡å’Œä»¿çœŸæ„å»ºã€‚",
  "detailed_summary": "## è®ºæ–‡æ‘˜è¦\n\n**è®ºæ–‡æ ‡é¢˜**ï¼šåŸºäºä¸»åŠ¨æ„ŸçŸ¥ä¸è‡ªåŠ¨VLMç”Ÿæˆè¡Œä¸ºæ ‘çš„Real2Sim\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\næ„å»ºé«˜ä¿çœŸçš„çœŸå®ä¸–ç•Œç¯å¢ƒä»¿çœŸæ¨¡å‹ï¼ˆReal2Simï¼‰æ˜¯æœºå™¨äººå­¦å’Œè‡ªåŠ¨åŒ–é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå…¶å…³é”®åœ¨äºå‡†ç¡®ä¼°è®¡ç‰©ä½“çš„ç‰©ç†å‚æ•°ï¼ˆå¦‚è´¨é‡ã€å‡ ä½•ã€æ‘©æ“¦ç³»æ•°ï¼‰ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººå·¥æµ‹é‡æˆ–é¢„å…ˆç¼–ç¨‹çš„å›ºå®šæ¢ç´¢ç­–ç•¥ï¼Œç¼ºä¹å¯¹**ä¸åŒä»»åŠ¡å’Œç”¨æˆ·æ„å›¾çš„é€‚åº”æ€§**ï¼Œå¾€å¾€è¿›è¡Œä¸å¿…è¦çš„å‚æ•°ä¼°è®¡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®**é«˜å±‚ç”¨æˆ·æ„å›¾**ï¼Œè‡ªä¸»ã€æœ‰é€‰æ‹©æ€§åœ°è·å–ä»¿çœŸæ‰€éœ€ç‰©ç†å‚æ•°çš„è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ä¸»åŠ¨æ„ŸçŸ¥Real2Simæ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯**ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ä¸è¡Œä¸ºæ ‘ï¼ˆBTï¼‰çš„ç»“æ„åŒ–æ‰§è¡Œ**ã€‚\n*   **æ„å›¾é©±åŠ¨çš„å‚æ•°å‘ç°**ï¼šç³»ç»Ÿæ¥æ”¶ç”¨æˆ·è‡ªç„¶è¯­è¨€è¯·æ±‚ã€ä¸å®Œæ•´çš„ä»¿çœŸæè¿°å’Œåœºæ™¯RGBå›¾åƒã€‚VLMé€šè¿‡å¤šæ¨¡æ€æ¨ç†ï¼Œè¯†åˆ«ç›¸å…³ç‰©ä½“ï¼Œå¹¶æ¨æ–­å‡ºå®Œæˆç›®æ ‡ä»¿çœŸæ‰€å¿…éœ€çš„**æœ€å°ç‰©ç†å‚æ•°å­é›†**ã€‚\n*   **åŸå­åŠ¨ä½œçš„è¡Œä¸ºæ ‘ç”Ÿæˆ**ï¼šä¸ä»¥å¾€ä½¿ç”¨é«˜å±‚æŠ€èƒ½ï¼ˆå¦‚â€œæŠ“å–â€ï¼‰çš„æ–¹æ³•ä¸åŒï¼Œæœ¬æ¡†æ¶åŸºäºä¸€ç»„é¢„å®šä¹‰çš„**ä½å±‚åŸå­åŠ¨ä½œ**ï¼ˆå¦‚ç§»åŠ¨åˆ°æŒ‡å®šä½å§¿ã€å¼€åˆå¤¹çˆªã€æµ‹é‡åŠ›/åŠ›çŸ©ï¼‰æ¥è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œçš„è¡Œä¸ºæ ‘ã€‚è¿™å®ç°äº†ç»†ç²’åº¦ã€æ¥è§¦æ„ŸçŸ¥çš„äº¤äº’ï¼Œå¹¶ç›´æ¥å°†ç‰©ç†æµ‹é‡é›†æˆåˆ°æµç¨‹ä¸­ã€‚\n*   **åˆè§„äº¤äº’ä¸å‚æ•°ä¼°è®¡**ï¼šç”Ÿæˆçš„è¡Œä¸ºæ ‘åœ¨åŠ›æ§æœºå™¨äººï¼ˆFranka Emika Pandaï¼‰ä¸Šæ‰§è¡Œï¼Œåˆ©ç”¨ç¬›å¡å°”é˜»æŠ—æ§åˆ¶å®ç°å®‰å…¨çš„æ¥è§¦å¼äº¤äº’ï¼ˆå¦‚æ¨ã€ä¸¾ï¼‰ã€‚é€šè¿‡äº¤äº’ä¸­è®°å½•çš„ä¼ æ„Ÿå™¨æ•°æ®ï¼Œç›´æ¥è®¡ç®—æ‰€éœ€å‚æ•°ï¼ˆå¦‚é€šè¿‡ä¸¾å‡æ—¶çš„åŠ›ä¼°è®¡è´¨é‡ï¼Œé€šè¿‡æ¨åŠ¨æ—¶çš„åŠ›ä¼°è®¡æ‘©æ“¦ç³»æ•°ï¼‰ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨çœŸå®æœºå™¨äººå¹³å°ä¸Šè¿›è¡Œäº†å¤šåœºæ™¯éªŒè¯ï¼Œç»“æœè¡¨æ˜ï¼š\n*   **ä»»åŠ¡é€‚åº”æ€§**ï¼šç³»ç»Ÿèƒ½æ ¹æ®ä¸åŒçš„ç”¨æˆ·è¯·æ±‚ï¼ˆå¦‚â€œä¼°è®¡ç“¶å­è´¨é‡å’Œæ¡Œå­é«˜åº¦â€ã€â€œä¼°è®¡æ‘©æ“¦ç³»æ•°â€ï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆå¹¶æ‰§è¡Œä¸åŒçš„è¡Œä¸ºæ ‘ç­–ç•¥ã€‚\n*   **å‡†ç¡®å‚æ•°ä¼°è®¡**ï¼šæˆåŠŸä¼°è®¡äº†ç‰©ä½“è´¨é‡ã€æ¡Œé¢é«˜åº¦å’Œæ‘©æ“¦ç›¸å…³å‚æ•°ï¼Œä¼°è®¡å€¼ä¸æ‰‹åŠ¨æµ‹é‡å€¼æ¥è¿‘ï¼ˆä¾‹å¦‚ï¼Œç“¶å­è´¨é‡ä¼°è®¡å‡å€¼0.257kg vs. çœŸå®å€¼0.254kgï¼‰ã€‚\n*   **å¤„ç†å¤æ‚åœºæ™¯**ï¼šåœ¨**ç‰©ä½“è¢«é®æŒ¡**çš„åœºæ™¯ä¸­ï¼Œç³»ç»Ÿèƒ½è§„åˆ’å‡ºâ€œå…ˆç§»å¼€é®æŒ¡ç‰©ï¼Œå†æµ‹é‡ç›®æ ‡ç‰©â€çš„å¤æ‚è¡Œä¸ºåºåˆ—ã€‚\n*   **æŠ—å¹»è§‰ä¸è§£é‡Šæ€§**ï¼šé€šè¿‡ç³»ç»Ÿæç¤ºè¯å·¥ç¨‹ï¼Œæœ‰æ•ˆå‡å°‘äº†VLMçš„â€œå¹»è§‰â€é—®é¢˜ã€‚ç³»ç»Ÿè¿˜èƒ½åœ¨æ— æ³•ç›´æ¥å®Œæˆä»»åŠ¡æ—¶ï¼Œè¾“å‡ºè§£é‡Šæ–‡æœ¬å¹¶æ‰§è¡Œå¯è·å¾—é—´æ¥æ•°æ®çš„åŠ¨ä½œï¼Œè¾…åŠ©ç”¨æˆ·è®¡ç®—ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬å·¥ä½œå°†é«˜å±‚è¯­ä¹‰æ¨ç†ä¸åº•å±‚ç‰©ç†æœºå™¨äººäº¤äº’ç´§å¯†ç»“åˆï¼Œæ¨åŠ¨äº†Real2Simæµç¨‹å‘**è‡ªä¸»åŒ–ã€æ„å›¾é©±åŠ¨å’Œå¯è§£é‡Š**æ–¹å‘å‘å±•ã€‚å…¶ä»·å€¼åœ¨äºï¼š\n*   **é™ä½ä½¿ç”¨é—¨æ§›**ï¼šç”¨æˆ·æ— éœ€æœºå™¨äººæˆ–ç³»ç»Ÿè¾¨è¯†ä¸“ä¸šçŸ¥è¯†ï¼Œä»…é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å³å¯é©±åŠ¨ä»¿çœŸæ„å»ºã€‚\n*   **æé«˜æ•ˆç‡ä¸é’ˆå¯¹æ€§**ï¼šåªä¼°è®¡ä»»åŠ¡å¿…éœ€çš„å‚æ•°ï¼Œé¿å…äº†å†—ä½™äº¤äº’ï¼Œæå‡äº†æ„å»ºæ•ˆç‡ã€‚\n*   **å¢å¼ºæ³›åŒ–èƒ½åŠ›**ï¼šåŸºäºåŸå­åŠ¨ä½œçš„ç»„åˆæ–¹å¼ï¼Œä½¿ç³»ç»Ÿèƒ½çµæ´»é€‚åº”æœªè§è¿‡çš„ç‰©ä½“å’Œä»»åŠ¡è¦æ±‚ï¼Œä¸ºåœ¨å¤æ‚ã€éç»“æ„åŒ–ç¯å¢ƒä¸­æ„å»ºç”¨äºæ§åˆ¶æµ‹è¯•ã€å¼ºåŒ–å­¦ä¹ çš„æ•°å­—å­ªç”Ÿæä¾›äº†æ–°é€”å¾„ã€‚",
  "qa_pairs": [
    {
      "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
      "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nä¼ ç»Ÿâ€œä»ç°å®åˆ°ä»¿çœŸâ€æµç¨‹å­˜åœ¨ä¸¤å¤§ç“¶é¢ˆï¼š\n1.  **ç¼ºä¹ä»»åŠ¡é€‚åº”æ€§**ï¼šé€šå¸¸ä¾èµ–**æ‰‹åŠ¨æµ‹é‡**æˆ–**é¢„ç¼–ç¨‹çš„ã€å›ºå®šçš„æ¢ç´¢æµç¨‹**ï¼Œæ— è®ºä¸‹æ¸¸ä»¿çœŸä»»åŠ¡éœ€è¦ä»€ä¹ˆï¼Œéƒ½ä¼°è®¡ä¸€å¥—å›ºå®šçš„ã€å…¨é¢çš„ç‰©ç†å‚æ•°ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚\n2.  **ç¼ºä¹é«˜å±‚æ„å›¾ç†è§£**ï¼šæ— æ³•æ ¹æ®ç”¨æˆ·çš„**é«˜çº§åˆ«ä»¿çœŸç›®æ ‡**ï¼ˆä¾‹å¦‚ï¼Œâ€œåˆ›å»ºä¸€ä¸ªç“¶å­åœ¨æ¡Œä¸Šè¢«æ¨å€’çš„ä»¿çœŸâ€ï¼‰æ¥åŠ¨æ€å†³å®š**éœ€è¦æµ‹é‡å“ªäº›å‚æ•°**ä»¥åŠ**å¦‚ä½•è¿›è¡Œäº¤äº’æµ‹é‡**ã€‚è¿™é™åˆ¶äº†åœ¨éç»“æ„åŒ–åœºæ™¯å’Œé¢å‘ä»»åŠ¡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚\n\n### **äºŒã€ æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ç‚¹**\næœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„ã€**åŸºäºä¸»åŠ¨æ„ŸçŸ¥å’Œæ„å›¾é©±åŠ¨**çš„Real2Simæ¡†æ¶ï¼Œå…¶åˆ›æ–°ä¸»è¦ä½“ç°åœ¨**ç³»ç»Ÿæ¶æ„**å’Œ**æŠ€æœ¯è·¯å¾„**ä¸Šï¼š\n\n1.  **æ„å›¾é©±åŠ¨çš„ä¸»åŠ¨æ„ŸçŸ¥Real2Simç®¡é“**\n    - **åˆ›æ–°**ï¼šå°†ç‰©ç†å‚æ•°è·å–è¿‡ç¨‹**æ¡ä»¶åŒ–äºç”¨æˆ·æ„å›¾**ã€‚ç³»ç»Ÿèƒ½è‡ªåŠ¨åˆ†æä»¿çœŸç›®æ ‡ï¼Œä»…é€‰æ‹©å¹¶ä¼°è®¡å¯¹å®Œæˆè¯¥ç›®æ ‡**å¿…è¦çš„æœ€å°å‚æ•°å­é›†**ï¼Œå®ç°äº†â€œæŒ‰éœ€æµ‹é‡â€ã€‚\n    - **ä»·å€¼**ï¼šé¿å…äº†ä¸å¿…è¦çš„æœºå™¨äººäº¤äº’ï¼Œæé«˜äº†æ•ˆç‡ï¼Œå¹¶ä½¿ç³»ç»Ÿèƒ½é€‚åº”å¤šæ ·åŒ–çš„ç”¨æˆ·è¯·æ±‚ï¼Œæ— éœ€ä¸ºæ¯ä¸ªæ–°ä»»åŠ¡é‡æ–°ç¼–ç¨‹ã€‚\n\n2.  **åŸºäºVLMè‡ªåŠ¨ç”Ÿæˆè¡Œä¸ºæ ‘ï¼Œå¹¶ä½¿ç”¨åŸå­çº§ç‰©ç† grounding åŠ¨ä½œ**\n    - **åˆ›æ–°ç‚¹ A (ç”Ÿæˆæ–¹å¼)**ï¼šåˆ©ç”¨**è§†è§‰-è¯­è¨€æ¨¡å‹**è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼ˆç»“åˆç”¨æˆ·è¯·æ±‚ã€ä¸å®Œæ•´ä»¿çœŸæè¿°ã€RGBå›¾åƒï¼‰ï¼Œè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–çš„**è¡Œä¸ºæ ‘**ä½œä¸ºæ¢ç´¢è®¡åˆ’ã€‚\n        - **ä¼˜åŠ¿**ï¼šBTæä¾›äº†**å¯è§£é‡Šã€æ¨¡å—åŒ–ã€å¯ååº”**çš„ä»»åŠ¡ç»“æ„ï¼Œä¼˜äºVLMç›´æ¥ç”Ÿæˆçš„éç»“æ„åŒ–åŠ¨ä½œåºåˆ—ã€‚\n    - **åˆ›æ–°ç‚¹ B (åŠ¨ä½œå±‚çº§)**ï¼šä¸ä¹‹å‰ä½¿ç”¨é«˜çº§åˆ«æŠ€èƒ½ï¼ˆå¦‚ `PickUp`ï¼‰çš„LLM/BTå·¥ä½œä¸åŒï¼Œæœ¬æ–‡è§„åˆ’åŸºäºä¸€å¥—**å›ºå®šçš„ã€ä½çº§åˆ«çš„åŸå­åŠ¨ä½œé›†**ï¼ˆè§è¡¨ Iï¼‰ã€‚\n        ```\n        ä¾‹å¦‚ï¼šMovePose, OpenGripper, MoveDownUntilContact, MeasureForces\n        ```\n    - **ä»·å€¼**ï¼š\n        - **åŸå­åŠ¨ä½œ**å…è®¸å¯¹æ¥è§¦å¼äº¤äº’è¿›è¡Œ**ç»†ç²’åº¦ã€åˆè§„çš„æ§åˆ¶**ï¼ˆé€šè¿‡é˜»æŠ—æ§åˆ¶å®ç°ï¼‰ï¼Œè¿™å¯¹å®‰å…¨ã€ç²¾ç¡®çš„å‚æ•°ä¼°è®¡è‡³å…³é‡è¦ã€‚\n        - **ç›´æ¥é›†æˆç‰©ç†æµ‹é‡**ï¼šæ„ŸçŸ¥åŸè¯­ï¼ˆå¦‚ `MeasureForces`ï¼‰ä½œä¸ºåŠ¨ä½œèŠ‚ç‚¹ç›´æ¥åµŒå…¥BTï¼Œå®ç°äº†æ„ŸçŸ¥ä¸è¡ŒåŠ¨çš„ç´§å¯†é—­ç¯ã€‚\n        - **ç»„åˆæ€§å¼º**ï¼šæœ‰é™çš„åŸå­åŠ¨ä½œå¯ä»¥ç»„åˆå‡ºåº”å¯¹å¤æ‚åœºæ™¯ï¼ˆå¦‚ç‰©ä½“é®æŒ¡ï¼‰çš„æ¢ç´¢ç­–ç•¥ã€‚\n\n3.  **å›ºå®šç³»ç»Ÿæç¤ºä¸‹çš„é›¶æ ·æœ¬ä»»åŠ¡é€‚é…**\n    - **åˆ›æ–°**ï¼šæ•´ä¸ªæ¡†æ¶ç”±ä¸€ä¸ª**é¢„å…ˆè®¾è®¡å¥½çš„ã€å›ºå®šçš„ç³»ç»Ÿæç¤º**é©±åŠ¨ã€‚ç”¨æˆ·åªéœ€æä¾›å¯å˜çš„è¯·æ±‚ã€æè¿°å’Œå›¾åƒï¼Œè€Œ**æ— éœ€äº†è§£æœºå™¨äººã€BTæˆ–VLMçš„å†…éƒ¨ç»†èŠ‚**ã€‚\n    - **ä»·å€¼**ï¼šå®ç°äº†**ç”¨æˆ·é›¶æœºå™¨äººä¸“ä¸šçŸ¥è¯†**ä¸‹çš„è‡ªåŠ¨åŒ–ï¼Œä¸”ç³»ç»Ÿçš„ä»»åŠ¡é€‚åº”æ€§å®Œå…¨æ¥æºäºVLMå¯¹å¤šæ¨¡æ€è¾“å…¥çš„ç†è§£å’Œæ¨ç†ï¼Œè€Œéé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æç¤ºå·¥ç¨‹ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ¦‚è¦**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„è‡ªä¸»é—­ç¯ç³»ç»Ÿè§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n\n1.  **è¾“å…¥**ï¼šç”¨æˆ·æä¾›**é«˜å±‚è¯·æ±‚ `â„›`**ã€**ä¸å®Œæ•´çš„ä»¿çœŸæè¿° `ğ’Ÿ`**ï¼ˆå¦‚MuJoCo .xmlæ–‡ä»¶ï¼‰å’Œåœºæ™¯**RGBå›¾åƒ `â„`**ã€‚\n2.  **å¤šæ¨¡æ€æ¨ç†ä¸è§„åˆ’**ï¼š\n    - **VLM** åˆ†æè¾“å…¥ï¼Œæ‰§è¡Œï¼ša) è¯†åˆ«ç›¸å…³ç‰©ä½“ï¼›b) æ¨æ–­ç¼ºå¤±çš„å¿…è¦ç‰©ç†å‚æ•° `Î¦`ï¼›c) å°†æ¯ä¸ªå‚æ•°æ˜ å°„åˆ°**åŸå­åŠ¨ä½œé›† `ğ’œ`** ä¸­çš„æ“ä½œåºåˆ—ã€‚\n    - **VLM** è¾“å‡ºä¸€ä¸ªç»“æ„åŒ–çš„ã€å¯æ‰§è¡Œçš„**è¡Œä¸ºæ ‘**ï¼Œä»¥åŠè§£é‡Šæ–‡æœ¬ `â„°`ã€‚\n3.  **æœºå™¨äººæ‰§è¡Œä¸æ•°æ®é‡‡é›†**ï¼š\n    - åœ¨**Franka Panda**ï¼ˆæ‰­çŸ©æ§åˆ¶ï¼‰ä¸Šæ‰§è¡ŒBTã€‚\n    - ä½¿ç”¨**ç¬›å¡å°”é˜»æŠ—æ§åˆ¶**å®ç°å®‰å…¨ã€åˆè§„çš„æ¥è§¦å¼äº¤äº’ï¼ˆæ¨ã€ä¸¾ã€è§¦æ¢ç­‰ï¼‰ã€‚\n    - åœ¨åŠ¨ä½œæ‰§è¡Œä¸­è§¦å‘å¹¶è®°å½•ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå…³èŠ‚æ‰­çŸ©ã€åŠ›ã€ä½å§¿ï¼‰ã€‚\n4.  **å‚æ•°ä¼°è®¡ä¸ä»¿çœŸæ„å»º**ï¼š\n    - ä»ä¼ æ„Ÿå™¨æ•°æ®ä¸­ç›´æ¥è®¡ç®—ç‰©ç†å‚æ•°ï¼ˆè´¨é‡ã€é«˜åº¦ã€æ‘©æ“¦ç³»æ•°ï¼‰ã€‚\n    - å°†ä¼°è®¡å‡ºçš„å‚æ•°è‡ªåŠ¨å¡«å…¥ä»¿çœŸæè¿°ï¼Œåœ¨**MuJoCo**ä¸­æ„å»ºç‰©ç†æ„ŸçŸ¥çš„ä»¿çœŸç¯å¢ƒã€‚\n\n### **å››ã€ å®éªŒéªŒè¯çš„å®é™…ä»·å€¼**\nå®éªŒåœ¨çœŸå®æœºå™¨äººä¸ŠéªŒè¯äº†æ¡†æ¶åœ¨å¤šç§åœºæ™¯ä¸‹çš„èƒ½åŠ›ï¼š\n- **åœºæ™¯1ï¼ˆè´¨é‡ä¸é«˜åº¦ï¼‰**ï¼šæˆåŠŸä¼°è®¡ç“¶å­è´¨é‡å’Œæ¡Œé¢é«˜åº¦ï¼Œå¹¶èƒ½æ ¹æ®å·²çŸ¥çš„ `ğ’Ÿ` è‡ªåŠ¨è°ƒæ•´è®¡åˆ’ï¼Œä»…æµ‹é‡ç¼ºå¤±å‚æ•°ã€‚\n- **åœºæ™¯2ï¼ˆæ‘©æ“¦ï¼‰**ï¼šåœ¨æ— æ³•ç›´æ¥æµ‹é‡æ‘©æ“¦ç³»æ•°æ—¶ï¼ŒBTè§„åˆ’äº†è·å–åŸå§‹åŠ›æ•°æ®çš„åŠ¨ä½œï¼Œå¹¶ç”Ÿæˆè§£é‡Šæ–‡æœ¬æŒ‡å¯¼ç”¨æˆ·è®¡ç®—ï¼Œå±•ç¤ºäº†ç³»ç»Ÿçš„**å®‰å…¨è¾¹ç•Œå’Œå¯è§£é‡Šæ€§**ã€‚\n- **åœºæ™¯3ï¼ˆé®æŒ¡ï¼‰**ï¼šé¢å¯¹è¢«é®æŒ¡çš„ç‰©ä½“ï¼ŒBTè‡ªåŠ¨ç”Ÿæˆäº†â€œç§»å¼€éšœç¢ç‰©-æµ‹é‡-æ¢å¤åŸçŠ¶â€çš„å¤æ‚åºåˆ—ï¼Œè¯æ˜äº†**åŸå­åŠ¨ä½œçš„ç»„åˆèƒ½åŠ›**åº”å¯¹ç°å®æŒ‘æˆ˜ã€‚\n- **å…¶ä»–**ï¼šè¯æ˜äº†å¯ä½¿ç”¨**åˆæˆå›¾åƒ**è¿›è¡Œè§„åˆ’ï¼Œä»¥åŠç³»ç»Ÿå¯¹**VLMå¹»è§‰**å…·æœ‰ä¸€å®šçš„é²æ£’æ€§ã€‚\n\n**æ€»ç»“**ï¼šæœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæå‡ºäº†ä¸€ä¸ª**å°†é«˜å±‚è¯­ä¹‰æ„å›¾é€šè¿‡VLMè½¬åŒ–ä¸ºä½å±‚ç‰©ç†äº¤äº’è¡ŒåŠ¨**çš„è‡ªæ²»Real2Simæ¡†æ¶ã€‚å®ƒé€šè¿‡**VLMç”ŸæˆBT**å’Œ**åŸå­çº§ç‰©ç†åŠ¨ä½œ**è¿™ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•é€‚åº”æ€§å·®ã€æ•ˆç‡ä½çš„é—®é¢˜ï¼Œå‘æ„å»º**å¯æ‰©å±•ã€ä»»åŠ¡æ„ŸçŸ¥ã€ç”¨æˆ·å‹å¥½**çš„è‡ªåŠ¨åŒ–æ•°å­—å­ªç”Ÿç³»ç»Ÿè¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "timestamp": "2026-01-20T17:51:15.500694",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
      "answer": "## è®ºæ–‡åˆ†æï¼šåŸºäºä¸»åŠ¨æ„ŸçŸ¥ä¸VLMç”Ÿæˆè¡Œä¸ºæ ‘çš„Real2Sim\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„**æ„å›¾é©±åŠ¨ã€è‡ªé€‚åº”**çš„Real2Simï¼ˆä»çœŸå®åˆ°ä»¿çœŸï¼‰æ¡†æ¶ã€‚å…¶æ ¸å¿ƒåœ¨äºåˆ©ç”¨**è§†è§‰è¯­è¨€æ¨¡å‹**è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼Œæ ¹æ®ç”¨æˆ·çš„é«˜çº§ä»»åŠ¡ç›®æ ‡ï¼Œè‡ªåŠ¨ç”Ÿæˆ**è¡Œä¸ºæ ‘**æ¥æŒ‡å¯¼æœºå™¨äººæ‰§è¡Œç‰¹å®šçš„ç‰©ç†äº¤äº’ï¼Œä»è€Œ**ä»…è·å–å®Œæˆè¯¥ä»¿çœŸç›®æ ‡æ‰€å¿…éœ€çš„ç‰©ç†å‚æ•°**ï¼Œæœ€ç»ˆè‡ªåŠ¨æ„å»ºé«˜ä¿çœŸç‰©ç†ä»¿çœŸï¼ˆå¦‚MuJoCoï¼‰ã€‚\n\n### æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°\n\n1.  **æ„å›¾é©±åŠ¨çš„å‚æ•°è·å–**ï¼šä¼ ç»Ÿæ–¹æ³•é€šå¸¸é¢„å®šä¹‰ä¸€å¥—å›ºå®šçš„æ¢ç´¢æµç¨‹æ¥ä¼°è®¡æ‰€æœ‰å¯èƒ½çš„ç‰©ç†å‚æ•°ï¼ˆå¦‚è´¨é‡ã€å‡ ä½•ã€æ‘©æ“¦ï¼‰ï¼Œæ— è®ºä¸‹æ¸¸ä»»åŠ¡æ˜¯å¦éœ€è¦ã€‚æœ¬æ–‡æ–¹æ³•é€šè¿‡åˆ†æç”¨æˆ·è¯·æ±‚ï¼ˆ`â„›`ï¼‰å’Œä¸å®Œæ•´çš„ä»¿çœŸæè¿°ï¼ˆ`ğ’Ÿ`ï¼‰ï¼Œ**åŠ¨æ€æ¨æ–­å¹¶ä»…è·å–ä»»åŠ¡ç›¸å…³çš„å‚æ•°å­é›†**ï¼Œé¿å…äº†ä¸å¿…è¦çš„äº¤äº’ï¼Œæé«˜äº†æ•ˆç‡ã€‚\n2.  **åŸºäºåŸå­åŠ¨ä½œçš„VLMè¡Œä¸ºæ ‘ç”Ÿæˆ**ï¼šç°æœ‰VLM/LLMç”Ÿæˆè¡Œä¸ºæ ‘çš„ç ”ç©¶å¤šä¾èµ–äºé¢„å®šä¹‰çš„**é«˜çº§æŠ€èƒ½**ï¼ˆå¦‚â€œæ‹¾å–â€ã€â€œä¸¢å¼ƒâ€ï¼‰ã€‚æœ¬æ–‡åˆ™è§„åˆ’ä¸€ä¸ª**å›ºå®šçš„ã€ä½çº§çš„åŸå­åŠ¨ä½œé›†**ï¼ˆ`ğ’œ`ï¼Œå¦‚ç§»åŠ¨åˆ°æŸä½å§¿ã€å¼€åˆå¤¹çˆªã€æµ‹é‡åŠ›ç­‰ï¼‰ã€‚VLMè´Ÿè´£å°†è¿™äº›åŸå­åŠ¨ä½œç»„åˆæˆå¤æ‚ã€å¯æ‰§è¡Œçš„è¡Œä¸ºæ ‘ï¼Œå®ç°äº†**ç»†ç²’åº¦ã€æ¥è§¦æ„ŸçŸ¥çš„ç‰©ç†äº¤äº’**ï¼Œå¹¶èƒ½ç›´æ¥å°†æµ‹é‡æ•°æ®é›†æˆåˆ°Real2Simæµç¨‹ä¸­ã€‚\n3.  **ä¸»åŠ¨æ„ŸçŸ¥ä¸ç‰©ç†äº¤äº’çš„é—­ç¯**ï¼šå°†**é«˜å±‚è¯­ä¹‰æ¨ç†**ï¼ˆVLMç†è§£åœºæ™¯å’Œæ„å›¾ï¼‰ä¸**åº•å±‚ç‰©ç†äº¤äº’**ï¼ˆæœºå™¨äººæ‰§è¡Œè¡Œä¸ºæ ‘è¿›è¡Œæ¥è§¦å¼æµ‹é‡ï¼‰ç´§å¯†è€¦åˆã€‚ç³»ç»Ÿå½¢æˆäº†ä¸€ä¸ªâ€œæ„ŸçŸ¥-è§„åˆ’-æ‰§è¡Œ-å»ºæ¨¡â€çš„å®Œæ•´è‡ªä¸»é—­ç¯ï¼Œæ— éœ€æœºå™¨äººä¸“å®¶è®¾è®¡å…·ä½“çš„æ¢ç´¢ç­–ç•¥ã€‚\n\n### å®é™…ä»·å€¼\n\n1.  **é™ä½ä½¿ç”¨é—¨æ§›**ï¼šç”¨æˆ·åªéœ€æä¾›è‡ªç„¶è¯­è¨€æè¿°å’Œåœºæ™¯å›¾åƒï¼Œæ— éœ€å…·å¤‡æœºå™¨äººç¼–ç¨‹æˆ–ç³»ç»Ÿè¾¨è¯†ä¸“ä¸šçŸ¥è¯†ï¼Œå³å¯æ„å»ºé¢å‘ä»»åŠ¡çš„ä»¿çœŸæ¨¡å‹ã€‚\n2.  **æé«˜ä»¿çœŸæ„å»ºæ•ˆç‡ä¸é’ˆå¯¹æ€§**ï¼šé€šè¿‡ä»»åŠ¡ç›¸å…³çš„é€‰æ‹©æ€§å‚æ•°ä¼°è®¡ï¼Œå‡å°‘äº†å†—ä½™çš„æœºå™¨äººæ“ä½œå’Œæ•°æ®é‡‡é›†æ—¶é—´ï¼Œä½¿ä»¿çœŸæ„å»ºè¿‡ç¨‹æ›´é«˜æ•ˆã€ç›®æ ‡æ›´æ˜ç¡®ã€‚\n3.  **å¢å¼ºå¯è§£é‡Šæ€§ä¸å®‰å…¨æ€§**ï¼šè¡Œä¸ºæ ‘ä¸ºæœºå™¨äººçš„æ¢ç´¢è®¡åˆ’æä¾›äº†ç»“æ„åŒ–ã€æ¨¡å—åŒ–çš„è¡¨ç¤ºï¼Œæ˜“äºäººç±»ç†è§£å’Œè°ƒè¯•ã€‚ç»“åˆé˜»æŠ—æ§åˆ¶ï¼Œç¡®ä¿äº†ä¸ç¯å¢ƒçš„**æŸ”é¡ºã€å®‰å…¨äº¤äº’**ã€‚\n4.  **æ¨åŠ¨è‡ªåŠ¨åŒ–æ•°å­—å­ªç”Ÿ**ï¼šä¸ºæ„å»ºå¤æ‚ã€åŠ¨æ€ç¯å¢ƒçš„ç‰©ç†æ„ŸçŸ¥æ•°å­—å­ªç”Ÿæä¾›äº†ä¸€ç§å…¨è‡ªåŠ¨åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æœºå™¨äººä»¿çœŸæµ‹è¯•ã€å¼ºåŒ–å­¦ä¹ å’Œé¢„æµ‹æ€§ç»´æŠ¤ç­‰é¢†åŸŸæœ‰é‡è¦ä»·å€¼ã€‚\n\n---\n\n## å‘å±•è„‰ç»œæ¢³ç†ï¼šä»å›ºå®šæµç¨‹åˆ°æ„å›¾é©±åŠ¨çš„è‡ªé€‚åº”Real2Sim\n\nè¯¥ç ”ç©¶æ–¹å‘çš„å‘å±•æ ¸å¿ƒæ˜¯**å¦‚ä½•è®©æœºå™¨äººæ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆã€æ›´è‡ªä¸»åœ°è·å–æ„å»ºç‰©ç†ä»¿çœŸæ‰€éœ€çš„çœŸå®ä¸–ç•Œå‚æ•°**ã€‚å…¶æ¼”è¿›è„‰ç»œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªå‘å±•æ–¹å‘æ¸…æ™°åœ°æ¢³ç†ï¼š\n\n### å‘å±•æ–¹å‘ä¸€ï¼šå‚æ•°ä¼°è®¡æ–¹æ³•çš„æ¼”è¿›â€”â€”ä»â€œå¼€ç¯æµ‹é‡â€åˆ°â€œé—­ç¯ä¼˜åŒ–â€\n\n*   **æ—©æœŸ/ä¼ ç»Ÿæ–¹æ³•**ï¼š\n    *   **ç‰¹ç‚¹**ï¼šä¸¥é‡ä¾èµ–**äººå·¥æµ‹é‡**æˆ–åœ¨é«˜åº¦å—æ§å®éªŒå®¤ç¯å¢ƒä¸‹çš„**é¢„ç¼–ç¨‹ã€å›ºå®šæµç¨‹**çš„æœºå™¨äººäº¤äº’ï¼ˆå¦‚ç‰¹å®šçš„æŠ“å–-æ”¾ç½®åºåˆ—ï¼‰ã€‚\n    *   **æ”¹è¿›ä¸å±€é™**ï¼šå®ç°äº†è‡ªåŠ¨åŒ–ï¼Œä½†æµç¨‹åƒµåŒ–ï¼Œæ— æ³•é€‚åº”æ–°ç‰©ä½“ã€æ–°åœºæ™¯æˆ–ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚é€šå¸¸ä¼°è®¡ä¸€ä¸ªå›ºå®šçš„ã€ exhaustive çš„å‚æ•°é›†ï¼Œç¼ºä¹é’ˆå¯¹æ€§ã€‚\n\n*   **å¼•å…¥æ•°æ®é©±åŠ¨ä¸å¾®åˆ†ç‰©ç†**ï¼š\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šå¦‚ Heiden ç­‰äºº (2019) çš„å·¥ä½œï¼Œåˆ©ç”¨**å¯å¾®åˆ†ç‰©ç†**å¼•æ“ï¼Œé€šè¿‡ä¼˜åŒ–ä½¿ä»¿çœŸè½¨è¿¹é€¼è¿‘çœŸå®è§‚æµ‹è½¨è¿¹ï¼Œä»è€Œåå‘ä¼°è®¡ç‰©ç†å‚æ•°ã€‚\n    *   **æ”¹è¿›**ï¼šå®ç°äº†**é—­ç¯ä¼˜åŒ–**ï¼Œèƒ½åˆ©ç”¨æ›´ä¸°å¯Œçš„äº¤äº’æ•°æ®ï¼ˆè€Œä¸ä»…æ˜¯æœ€ç»ˆçŠ¶æ€ï¼‰æ¥ refine å‚æ•°ï¼Œæé«˜äº†ä¼°è®¡ç²¾åº¦ã€‚\n    *   **å±€é™**ï¼šä¼˜åŒ–è¿‡ç¨‹å¯èƒ½è®¡ç®—æ˜‚è´µï¼Œä¸”ä»éœ€è¦ä¸€ä¸ªåˆå§‹çš„äº¤äº’ç­–ç•¥æ¥ç”Ÿæˆæ•°æ®ï¼Œç­–ç•¥æœ¬èº«æœªå¿…æ˜¯ä»»åŠ¡æœ€ä¼˜çš„ã€‚\n\n*   **æœ¬æ–‡çš„æ¨è¿›**ï¼š\n    *   **æ”¹è¿›**ï¼šå°†å‚æ•°ä¼°è®¡çš„â€œ**ç­–ç•¥ç”Ÿæˆ**â€ä¹Ÿçº³å…¥é—­ç¯ã€‚ä¸æ˜¯ä¼˜åŒ–å›ºå®šäº¤äº’äº§ç”Ÿçš„æ•°æ®ï¼Œè€Œæ˜¯**æ ¹æ®ä»»åŠ¡ç›®æ ‡åŠ¨æ€ç”Ÿæˆæœ€ä¼˜çš„äº¤äº’ç­–ç•¥ï¼ˆè¡Œä¸ºæ ‘ï¼‰**ã€‚å®ƒå…³æ³¨çš„æ˜¯ **â€œä¸ºäº†ä¼°è®¡è¿™ä¸ªç‰¹å®šå‚æ•°ï¼Œæˆ‘åº”è¯¥å¦‚ä½•ä¸ç‰©ä½“äº¤äº’ï¼Ÿâ€** ï¼Œå®ç°äº†ç­–ç•¥å±‚é¢çš„è‡ªé€‚åº”ã€‚\n\n### å‘å±•æ–¹å‘äºŒï¼šä»»åŠ¡ä¸æ„å›¾ç†è§£çš„èå…¥â€”â€”ä»â€œæ— å·®åˆ«ä¼°è®¡â€åˆ°â€œé€‰æ‹©æ€§è·å–â€\n\n*   **å‰æœŸå·¥ä½œ**ï¼š\n    *   **ç‰¹ç‚¹**ï¼šç»å¤§å¤šæ•°Real2Simç ”ç©¶**å¿½ç•¥ç”¨æˆ·æ„å›¾**ã€‚å…¶ç›®æ ‡æ˜¯â€œå°½å¯èƒ½å‡†ç¡®åœ°ä¼°è®¡ç‰©ä½“çš„æ‰€æœ‰ç‰©ç†å±æ€§â€ï¼Œè€Œä¸å…³å¿ƒè¿™äº›å±æ€§å¯¹äºç”¨æˆ·å³å°†è¿›è¡Œçš„ä»¿çœŸä»»åŠ¡æ˜¯å¦å¿…è¦ã€‚\n    *   **å±€é™**ï¼šå¯¼è‡´èµ„æºæµªè´¹ï¼ˆæ—¶é—´ã€ç®—åŠ›ï¼‰ï¼Œä¸”åœ¨ä¸å¿…è¦çš„äº¤äº’ä¸­å¯èƒ½å¢åŠ ç‰©ä½“æŸåçš„é£é™©ã€‚\n\n*   **æœ¬æ–‡çš„çªç ´**ï¼š\n    *   **æ”¹è¿›**ï¼šé¦–æ¬¡æ˜ç¡®å°†**é«˜å±‚ç”¨æˆ·æ„å›¾**ä½œä¸ºæ•´ä¸ªReal2Simæµç¨‹çš„**é©±åŠ¨æ¡ä»¶**ã€‚ç³»ç»Ÿé€šè¿‡VLMè§£æè‡ªç„¶è¯­è¨€è¯·æ±‚ï¼ˆ`â„›`ï¼‰å’Œä»¿çœŸæè¿°ï¼ˆ`ğ’Ÿ`ï¼‰ï¼Œä¸»åŠ¨æ¨ç†å‡ºå®Œæˆè¯¥ä»¿çœŸæ‰€**å¿…éœ€ä¸”ç¼ºå¤±çš„æœ€å°å‚æ•°é›†**ï¼ˆ`Î¦`ï¼‰ã€‚è¿™æ ‡å¿—ç€ä»â€œ**å±æ€§ä¼°è®¡**â€åˆ°â€œ**ä»»åŠ¡æ„ŸçŸ¥çš„å‚æ•°è·å–**â€çš„èŒƒå¼è½¬å˜ã€‚\n\n### å‘å±•æ–¹å‘ä¸‰ï¼šè§„åˆ’ä¸æ‰§è¡Œæ¡†æ¶çš„å‡çº§â€”â€”ä»â€œçŠ¶æ€æœº/è„šæœ¬â€åˆ°â€œå¯è§£é‡Šã€å¯ååº”çš„è¡Œä¸ºæ ‘â€\n\n*   **ä¼ ç»Ÿæ‰§è¡Œæ–¹å¼**ï¼š\n    *   **ç‰¹ç‚¹**ï¼šä½¿ç”¨**æœ‰é™çŠ¶æ€æœº**æˆ–**ç¡¬ç¼–ç è„šæœ¬**æ¥åºåˆ—åŒ–æœºå™¨äººåŠ¨ä½œã€‚è¿™äº›æ–¹æ³•åœ¨å¤æ‚æ€§å’Œå¯ç»´æŠ¤æ€§ä¸Šå­˜åœ¨ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ ¹æ®ä¼ æ„Ÿå™¨åé¦ˆè¿›è¡Œååº”å¼è°ƒæ•´æ—¶ã€‚\n\n*   **è¡Œä¸ºæ ‘çš„å¼•å…¥**ï¼š\n    *   **ç‰¹ç‚¹**ï¼šè¡Œä¸ºæ ‘å› å…¶**æ¨¡å—åŒ–ã€å±‚æ¬¡åŒ–ã€é«˜å¯è¯»æ€§**ä»¥åŠ**å†…ç½®çš„ååº”å¼æœºåˆ¶**ï¼ˆé€šè¿‡èŠ‚ç‚¹è¿”å›æˆåŠŸ/å¤±è´¥çŠ¶æ€ï¼‰è€Œå—åˆ°é’çï¼Œéå¸¸é€‚åˆéœ€è¦æ•…éšœæ¢å¤å’Œæ¡ä»¶åˆ†æ”¯çš„å¤æ‚ä»»åŠ¡ã€‚\n    *   **å‰æœŸç»“åˆLLM/VLMçš„ç ”ç©¶**ï¼šå¦‚ Wake ç­‰äºº (2025) çš„å·¥ä½œï¼Œæ¢ç´¢ç”¨VLMç”Ÿæˆè¡Œä¸ºæ ‘ï¼Œä½†èŠ‚ç‚¹é€šå¸¸æ˜¯é«˜çº§åˆ«ä»»åŠ¡ï¼ˆå¦‚ `PickUp`ï¼‰ã€‚\n    *   **å±€é™**ï¼šè¿™äº›é«˜çº§åŠ¨ä½œæ˜¯â€œé»‘ç›’â€ï¼Œå…¶å†…éƒ¨å…·ä½“å¦‚ä½•æ‰§è¡Œã€å¦‚ä½•ä¸ç‰©ç†ä¼ æ„Ÿå™¨é›†æˆæ˜¯é¢„è®¾çš„ï¼ŒVLMæ— æ³•å¯¹å…¶è¿›è¡Œç»†ç²’åº¦è§„åˆ’ã€‚\n\n*   **æœ¬æ–‡çš„æ·±åŒ–**ï¼š\n    *   **æ”¹è¿›**ï¼šå°†VLMçš„è¡Œä¸ºæ ‘ç”Ÿæˆ**ä¸‹æ²‰åˆ°åŸå­åŠ¨ä½œå±‚**ã€‚VLMç›´æ¥æ“ä½œ `MovePose`, `MeasureForces` ç­‰åº•å±‚åŸè¯­ã€‚è¿™ä½¿å¾—ï¼š\n        1.  **è§„åˆ’æ›´ç²¾ç»†**ï¼šVLMå¯ä»¥æ„æ€å‡ºéå¸¸å…·ä½“ä¸”ç‰©ç† grounded çš„äº¤äº’åºåˆ—ï¼ˆå¦‚â€œå…ˆæ¥è§¦æ¡Œé¢ï¼Œå†æ‹¾å–ç“¶å­ï¼ŒåŒæ—¶è®°å½•åŠ›çŸ©â€ï¼‰ã€‚\n        2.  **ä¸ç‰©ç†æµ‹é‡ç›´æ¥æŒ‚é’©**ï¼šæ¯ä¸ªä¼ æ„ŸåŠ¨ä½œï¼ˆ`MeasureForces`ï¼‰éƒ½ä½œä¸ºè¡Œä¸ºæ ‘çš„å¶å­èŠ‚ç‚¹ï¼Œè§„åˆ’ä¸æ‰§è¡Œã€æ•°æ®é‡‡é›†æ— ç¼è¡”æ¥ã€‚\n        3.  **æ³›åŒ–æ€§æ›´å¼º**ï¼šä¸€å¥—å›ºå®šçš„åŸå­åŠ¨ä½œé›†å¯ä»¥é€šè¿‡ç»„åˆåº”å¯¹å„ç§ä¸åŒçš„å‚æ•°è·å–ä»»åŠ¡ï¼Œæ— éœ€ä¸ºæ¯ä¸ªæ–°ä»»åŠ¡è®¾è®¡æ–°çš„é«˜çº§æŠ€èƒ½ã€‚\n\n### å‘å±•æ–¹å‘å››ï¼šæ„ŸçŸ¥ä¸æ¨¡å‹çš„ç»“åˆâ€”â€”ä»â€œçº¯å‡ ä½•é‡å»ºâ€åˆ°â€œç‰©ç†è¯­ä¹‰åŒ–é‡å»ºâ€\n\n*   **3Dåœºæ™¯ç”Ÿæˆç ”ç©¶**ï¼š\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šå¦‚ SceneSynthesisã€EmbodiedGen ç­‰ï¼Œåˆ©ç”¨ç”Ÿæˆå¼AIå’ŒVLMè‡ªåŠ¨åˆ›å»ºé€¼çœŸçš„3Dåœºæ™¯å’Œèµ„äº§ã€‚\n    *   **ç‰¹ç‚¹ä¸å±€é™**ï¼šé‡ç‚¹åœ¨äº**è§†è§‰å¤–è§‚å’Œå‡ ä½•å¸ƒå±€çš„é€¼çœŸæ€§ä¸åˆç†æ€§**ï¼Œæå¤§æ¨åŠ¨äº†åœºæ™¯æ„å»ºçš„è‡ªåŠ¨åŒ–ã€‚ä½†é€šå¸¸**ç¼ºä¹å¯¹ç‰©ä½“ç‰©ç†å±æ€§**ï¼ˆè´¨é‡ã€æ‘©æ“¦ã€æƒ¯æ€§ï¼‰çš„å»ºæ¨¡ï¼Œå› æ­¤ç”Ÿæˆçš„ä»¿çœŸæ— æ³•ç”¨äºæ¶‰åŠåŠ¨åŠ›å­¦ã€æ¥è§¦åŠ›çš„æœºå™¨äººä»»åŠ¡ã€‚\n\n*   **æœ¬æ–‡çš„å®šä½ä¸å‰ç»**ï¼š\n    *   **å½“å‰é‡ç‚¹**ï¼šæœ¬æ–‡ä¸“æ³¨äºä¸Šè¿°ç ”ç©¶é€šå¸¸ç¼ºå¤±çš„**ç‰©ç†å‚æ•°è·å–**ç¯èŠ‚ï¼Œæ˜¯æ„å»ºé«˜ä¿çœŸã€å¯è¿›è¡Œç‰©ç†ä»¿çœŸçš„æ•°å­—å­ªç”Ÿçš„**å…³é”®è¡¥å……**ã€‚\n    *   **æœªæ¥æ–¹å‘**ï¼šå¦‚æ–‡ç« æ‰€è¿°ï¼Œä¸€ä¸ªè‡ªç„¶çš„å‘å±•æ–¹å‘æ˜¯å°†æœ¬æ–‡çš„ç‰©ç†å‚æ•°è·å–æ¡†æ¶ä¸å…ˆè¿›çš„3Dåœºæ™¯ç”ŸæˆæŠ€æœ¯**èåˆ**ï¼Œå®ç°ä»è§†è§‰åˆ°ç‰©ç†å±æ€§çš„**ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–ä»¿çœŸæ„å»º**ã€‚\n\n### æ€»ç»“è„‰ç»œå›¾\n\n```\nå‘å±•ä¸»çº¿ï¼šè‡ªåŠ¨åŒ–ç¨‹åº¦ä¸æ™ºèƒ½ç¨‹åº¦ä¸æ–­æå‡\n|\n|-- [é˜¶æ®µ1ï¼šå›ºå®šæµç¨‹] é¢„ç¼–ç¨‹æœºå™¨äººäº¤äº’ -> ä¼°è®¡å›ºå®šå‚æ•°é›†\n|    |-- æ”¹è¿›ï¼šå®ç°äº†è‡ªåŠ¨åŒ–æµ‹é‡\n|    `-- å±€é™ï¼šåƒµåŒ–ã€æ— ä»»åŠ¡é’ˆå¯¹æ€§\n|\n|-- [é˜¶æ®µ2ï¼šä¼˜åŒ–ä¼°è®¡] å›ºå®šäº¤äº’ + å¾®åˆ†ç‰©ç†ä¼˜åŒ– -> æå‡å‚æ•°ç²¾åº¦\n|    |-- æ”¹è¿›ï¼šåˆ©ç”¨æ•°æ®é—­ç¯ä¼˜åŒ–ç²¾åº¦\n|    `-- å±€é™ï¼šäº¤äº’ç­–ç•¥ä»æ˜¯å›ºå®šçš„\n|\n|-- [é˜¶æ®µ3ï¼šæ„å›¾é©±åŠ¨] VLMç†è§£ä»»åŠ¡ + ç”ŸæˆåŸå­åŠ¨ä½œè¡Œä¸ºæ ‘ -> é€‰æ‹©æ€§è·å–å‚æ•°\n|    |-- æ”¹è¿›ï¼šä»»åŠ¡æ„ŸçŸ¥ã€ç­–ç•¥è‡ªé€‚åº”ã€å¯è§£é‡Šã€ç»†ç²’åº¦è§„åˆ’\n|    `-- ä»£è¡¨ï¼šæœ¬æ–‡å·¥ä½œ\n|\n`-- [æœªæ¥é˜¶æ®µï¼šå…¨é¢è‡ªåŠ¨åŒ–] 3Dåœºæ™¯ç”Ÿæˆ + ç‰©ç†å‚æ•°è‡ªä¸»è·å– -> å®Œæ•´æ•°å­—å­ªç”Ÿè‡ªåŠ¨æ„å»º\n```\n\n**ç»“è®º**ï¼šæœ¬æ–‡ä»£è¡¨äº†Real2Simé¢†åŸŸä¸€ä¸ªé‡è¦çš„æ¼”è¿›æ–¹å‘â€”â€”**èµ°å‘ä»¥é«˜å±‚è¯­ä¹‰æ„å›¾ä¸ºå¼•å¯¼ã€ç»“åˆå¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ã€é€šè¿‡å¯è§£é‡Šçš„ç»“æ„åŒ–è§„åˆ’ï¼ˆè¡Œä¸ºæ ‘ï¼‰æ‰§è¡Œåº•å±‚ç‰©ç†äº¤äº’çš„ã€é«˜åº¦è‡ªä¸»çš„ä»¿çœŸæ„å»ºèŒƒå¼**ã€‚å®ƒä¸ä»…åœ¨å‚æ•°ä¼°è®¡æ–¹æ³•ä¸Šæ›´æ™ºèƒ½ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå°†æ•´ä¸ª",
      "timestamp": "2026-01-20T17:52:24.515876",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
      "answer": "## è®ºæ–‡å‰ä½œåˆ†æä¸å¯¹æ¯”\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œæœ¬æ–‡ä½œè€…å›¢é˜Ÿï¼ˆAlessandro Adami ç­‰ï¼‰åœ¨ **Real2Sim** å’Œ **VLMç”Ÿæˆè¡Œä¸ºæ ‘** é¢†åŸŸæœ‰æ˜ç¡®çš„å‰æœŸç ”ç©¶åŸºç¡€ã€‚ä»¥ä¸‹è¡¨æ ¼è¯¦ç»†æ¢³ç†äº†æ–‡ä¸­å¼•ç”¨çš„å…³é”®å‰ä½œï¼Œå¹¶é‡ç‚¹å¯¹æ¯”äº†æœ¬æ–‡ä¸è¿™äº›å‰ä½œçš„**æ ¸å¿ƒåŒºåˆ«ä¸ä¸»è¦æ”¹è¿›**ã€‚\n\n| å‰ä½œå¼•ç”¨ (ä½œè€…/å¹´ä»½) | æ ¸å¿ƒè´¡çŒ® / æ–¹æ³• | æœ¬æ–‡ä¸ä¹‹ç›¸æ¯”çš„ä¸»è¦åŒºåˆ«ä¸æ”¹è¿› |\n| :--- | :--- | :--- |\n| **Pfaff, N. et al. (2025) [14]** <br>**Heiden, E. et al. (2019) [5]** | **ä¼ ç»Ÿ/æ—©æœŸ Real2Sim æ–¹æ³•**ï¼š<br>1. ä½¿ç”¨é¢„å®šä¹‰çš„ã€å›ºå®šçš„æ¢ç´¢æµç¨‹ï¼ˆå¦‚ç»“æ„åŒ–çš„æŠ“æ”¾æ“ä½œï¼‰æ¥ä¼°è®¡ç‰©ä½“å‚æ•°ï¼ˆè´¨é‡ã€æƒ¯æ€§ï¼‰ã€‚<br>2. **ç›®æ ‡å›ºå®š**ï¼šä¼°è®¡ä¸€ä¸ª**å®Œæ•´ä¸”å›ºå®š**çš„å‚æ•°é›†ï¼Œæ— è®ºä¸‹æ¸¸ä»»åŠ¡æ˜¯å¦éœ€è¦ã€‚<br>3. **ç¼ºä¹è¯­ä¹‰ä¸æ„å›¾ç†è§£**ï¼šä¸æ ¹æ®ç”¨æˆ·ç›®æ ‡æˆ–åœºæ™¯è¯­ä¹‰æ¥åŠ¨æ€å†³å®šæµ‹é‡å“ªäº›å‚æ•°æˆ–å¦‚ä½•äº¤äº’ã€‚ | **1. ä»â€œå›ºå®šæµç¨‹â€åˆ°â€œæ„å›¾é©±åŠ¨ã€è‡ªé€‚åº”æµç¨‹â€**ï¼š<br>   - **æ ¸å¿ƒåŒºåˆ«**ï¼šæœ¬æ–‡æ¡†æ¶**ä¸ä¾èµ–ä»»ä½•é¢„ç¼–ç¨‹çš„æ¢ç´¢æ¨¡æ¿**ã€‚ç»™å®šä¸€ä¸ªé«˜çº§ç”¨æˆ·è¯·æ±‚ï¼ˆå¦‚â€œåˆ›å»ºè¿™ä¸ªç“¶å­çš„ä»¿çœŸâ€ï¼‰ï¼Œç³»ç»Ÿèƒ½**è‡ªä¸»æ¨ç†**å‡ºå®Œæˆè¯¥ä»¿çœŸæ‰€**å¿…éœ€çš„æœ€å°å‚æ•°å­é›†**ï¼ˆä¾‹å¦‚ï¼Œåªéœ€è´¨é‡ï¼Œè€Œéæ‰€æœ‰æ‘©æ“¦ç³»æ•°ï¼‰ã€‚<br>   - **ä¸»è¦æ”¹å–„**ï¼šå®ç°äº†**ä»»åŠ¡æ„ŸçŸ¥**å’Œ**æœ€å°åŒ–äº¤äº’**ï¼Œé¿å…äº†ä¸å¿…è¦çš„æ“ä½œï¼Œæå‡äº†åœ¨éç»“æ„åŒ–åœºæ™¯ä¸­çš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚ |\n| **Wake, N. et al. (2025) [20]** <br>**Lykov, A. et al. (2023) [12]** | **VLM/LLMç”Ÿæˆè¡Œä¸ºæ ‘ (BT) çš„ç›¸å…³å·¥ä½œ**ï¼š<br>1. ä¸“æ³¨äº**é«˜å±‚ä»»åŠ¡æ‰§è¡Œ**ï¼Œå¦‚ `PickUp`ã€`ThrowAway`ã€‚<br>2. ä½¿ç”¨çš„è¡Œä¸ºæ ‘èŠ‚ç‚¹æ˜¯**é¢„å®šä¹‰çš„é«˜å±‚æŠ€èƒ½**ï¼Œå…¶å†…éƒ¨å®ç°æ˜¯é»‘ç›’ã€‚<br>3. åº”ç”¨åœºæ™¯æ˜¯**é€šç”¨ä»»åŠ¡è§„åˆ’**ï¼Œ**æœªæ¶‰åŠ**ä¸ºç‰©ç†å‚æ•°ä¼°è®¡è€Œè®¾è®¡çš„ä¸»åŠ¨æ„ŸçŸ¥ä¸äº¤äº’ã€‚ | **2. ä»â€œé«˜å±‚æŠ€èƒ½â€åˆ°â€œåŸå­ç‰©ç†åŸºå…ƒåŠ¨ä½œâ€**ï¼š<br>   - **æ ¸å¿ƒåŒºåˆ«**ï¼šæœ¬æ–‡è§„åˆ’ç›´æ¥åŸºäºä¸€å¥—**ä½å±‚ã€åŸå­åŒ–çš„æœºå™¨äººåŠ¨ä½œå’Œæ„ŸçŸ¥åŸºå…ƒ**ï¼ˆå¦‚ `MovePose`, `MeasureForces`, `MoveDownUntilContact`ï¼Œè§è¡¨ Iï¼‰ã€‚<br>   - **ä¸»è¦æ”¹å–„**ï¼š<br>     - **å®ç°äº†ç‰©ç†æ¥åœ°**ï¼šå…è®¸å¯¹æ¥è§¦å¼äº¤äº’è¿›è¡Œ**ç»†ç²’åº¦æ§åˆ¶**ï¼Œå¹¶èƒ½ç›´æ¥å°†åŠ›/åŠ›çŸ©ç­‰ä¼ æ„Ÿå™¨è¯»æ•°æ•´åˆåˆ°å‚æ•°ä¼°è®¡ç®—æ³•ä¸­ï¼ˆå¦‚é€šè¿‡ `MeasureForces` è®¡ç®—è´¨é‡ï¼‰ã€‚<br>     - **æ”¯æŒä¸»åŠ¨æ„ŸçŸ¥**ï¼šèƒ½å¤Ÿç»„åˆè¿™äº›åŸºå…ƒæ¥åˆ›å»ºå¤æ‚çš„æ¢ç´¢ç­–ç•¥ï¼ˆå¦‚å…ˆè§¦ç¢°æ¡Œé¢æµ‹é«˜åº¦ï¼Œå†æŠ“å–ç‰©ä½“æµ‹è´¨é‡ï¼‰ï¼Œè¿™æ˜¯ä¸ºReal2Sim**é‡èº«å®šåˆ¶**çš„ï¼Œè€Œå‰ä½œçš„é«˜å±‚æŠ€èƒ½æ— æ³•ç›´æ¥æ”¯æŒè¿™ç§æµ‹é‡æ€§äº¤äº’ã€‚ |\n| **Ling, L. et al. (2025) [10]** <br>**Liu, X. et al. (2025) [11]** <br>**Wang, X. et al. (2025) [21]** | **ç”Ÿæˆå¼AIç”¨äº3Dä¸–ç•Œæ¨¡å‹æ„å»º**ï¼š<br>1. é‡ç‚¹åœ¨äºä»æ–‡æœ¬æˆ–å›¾åƒ**ç”Ÿæˆè§†è§‰ä¸Šé€¼çœŸã€ç©ºé—´åˆç†çš„3Dåœºæ™¯å’Œèµ„äº§**ã€‚<br>2. **å…³æ³¨å›¾å½¢å¤–è§‚å’Œå¸ƒå±€**ï¼Œè€Œéåº•å±‚ç‰©ç†å‚æ•°ã€‚<br>3. ç›®æ ‡æ˜¯ç”¨äºå¯è§†åŒ–ã€è·¯å¾„è§„åˆ’ç­‰ï¼Œ**ç¼ºä¹å¯¹è´¨é‡ã€æ‘©æ“¦ç­‰åŠ¨åŠ›å­¦å‚æ•°çš„ä¼°è®¡**ï¼Œé™åˆ¶äº†åœ¨æ¥è§¦å¯†é›†å‹æœºå™¨äººä»¿çœŸä¸­çš„åº”ç”¨ã€‚ | **3. ä»â€œå›¾å½¢é‡å»ºâ€åˆ°â€œç‰©ç†æ„ŸçŸ¥ä»¿çœŸæ„å»ºâ€**ï¼š<br>   - **æ ¸å¿ƒåŒºåˆ«**ï¼šæœ¬æ–‡æ¡†æ¶çš„æ ¸å¿ƒè¾“å‡ºæ˜¯**å…·å¤‡å‡†ç¡®ç‰©ç†å±æ€§**çš„ä»¿çœŸæ¨¡å‹ï¼ˆåœ¨MuJoCoä¸­ï¼‰ï¼Œè€Œä¸ä»…ä»…æ˜¯è§†è§‰æ¨¡å‹ã€‚<br>   - **ä¸»è¦æ”¹å–„**ï¼š**å¡«è¡¥äº†å›¾å½¢ä¸–ç•Œç”Ÿæˆä¸ç‰©ç†ä»¿çœŸä¹‹é—´çš„é¸¿æ²Ÿ**ã€‚é€šè¿‡æœºå™¨äººä¸»åŠ¨äº¤äº’æ¥è·å–å¹¶åµŒå…¥å…³é”®ç‰©ç†å‚æ•°ï¼Œä½¿å¾—ç”Ÿæˆçš„æ•°å­—å­ªç”Ÿèƒ½å¤Ÿç”¨äºåŠ¨åŠ›å­¦ä»¿çœŸã€æ§åˆ¶ç­–ç•¥æµ‹è¯•ç­‰å¯¹ç‰©ç†ä¿çœŸåº¦è¦æ±‚é«˜çš„ä»»åŠ¡ã€‚ |\n| **Ahmad, F. et al. (2025) [1]** | **VLMä¸è¡Œä¸ºæ ‘ç»“åˆç”¨äºæœºå™¨äººæ•…éšœå¤„ç†**ï¼š<br>å±•ç¤ºäº†VLMã€ååº”å¼è§„åˆ’å™¨ä¸è¡Œä¸ºæ ‘ç»“åˆå®ç°å®æ—¶æ•…éšœå¤„ç†çš„æ¡†æ¶ï¼Œå¼ºè°ƒäº†BTåœ¨**æé«˜ç³»ç»Ÿé²æ£’æ€§**æ–¹é¢çš„ä»·å€¼ã€‚ | **4. åº”ç”¨é¢†åŸŸä¸“ä¸€åŒ–**ï¼š<br>   - **æ ¸å¿ƒåŒºåˆ«**ï¼šæœ¬æ–‡å°†ç±»ä¼¼çš„â€œVLM+BTâ€æŠ€æœ¯æ ˆ**ä¸“é—¨åº”ç”¨äºReal2Simå‚æ•°é‡‡é›†**è¿™ä¸€ç‰¹å®šä¸”æœªè¢«æ¢ç´¢çš„é¢†åŸŸã€‚<br>   - **ä¸»è¦æ”¹å–„**ï¼š**è¯æ˜äº†è¯¥æŠ€æœ¯æ ˆåœ¨ä¸»åŠ¨æ„ŸçŸ¥å’Œç‰©ç†å‚æ•°ä¼°è®¡ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§**ã€‚æœ¬æ–‡çš„BTç”Ÿæˆæ˜¯é’ˆå¯¹â€œæµ‹é‡â€è€Œéâ€œä»»åŠ¡å®Œæˆâ€æˆ–â€œæ•…éšœæ¢å¤â€è¿›è¡Œä¼˜åŒ–çš„ï¼Œä¾‹å¦‚BTä¸­ä¼šåŒ…å«ç‰¹å®šçš„æ„ŸçŸ¥èŠ‚ç‚¹å’Œç¡®ä¿ç¯å¢ƒçŠ¶æ€æ¢å¤çš„åºåˆ—ã€‚ |\n\n### æ€»ç»“ï¼šæœ¬æ–‡çš„æ ¸å¿ƒåˆ›æ–°ä¸é›†æˆæ€§æ”¹è¿›\n\næœ¬æ–‡å¹¶éç®€å•è¿­ä»£æŸä¸€ç¯‡å‰ä½œï¼Œè€Œæ˜¯è¿›è¡Œäº†ä¸€æ¬¡**å¼€åˆ›æ€§çš„é›†æˆä¸èŒƒå¼è½¬æ¢**ï¼š\n\n1.  **èŒƒå¼åˆ›æ–°**ï¼šé¦–æ¬¡æå‡ºäº† **â€œæ„å›¾é©±åŠ¨çš„ä¸»åŠ¨æ„ŸçŸ¥Real2Simâ€** èŒƒå¼ã€‚å°† **ç”¨æˆ·é«˜çº§ç›®æ ‡** ä½œä¸ºæ•´ä¸ªæµç¨‹çš„è¾“å…¥å’Œçº¦æŸæ¡ä»¶ï¼Œä½¿ç³»ç»Ÿå…·å¤‡â€œä»»åŠ¡æ„è¯†â€ã€‚\n2.  **æŠ€æœ¯è·¯å¾„åˆ›æ–°**ï¼šåˆ›é€ æ€§åœ°å°† **VLMçš„å¤šæ¨¡æ€åœºæ™¯ç†è§£ä¸æ¨ç†èƒ½åŠ›** ä¸ **è¡Œä¸ºæ ‘çš„æ¨¡å—åŒ–ã€å¯ååº”æ‰§è¡Œç‰¹æ€§** ç›¸ç»“åˆï¼Œå¹¶åº”ç”¨äº **ä½å±‚ç‰©ç†äº¤äº’åŸºå…ƒ** çš„è§„åˆ’ï¼Œä»¥è§£å†³Real2Simçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚\n3.  **ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–**ï¼šç›¸è¾ƒäºéœ€è¦ä¸“å®¶è®¾è®¡æµç¨‹çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ–åªè§£å†³éƒ¨åˆ†é—®é¢˜ï¼ˆå¦‚åªåšå›¾å½¢ç”Ÿæˆæˆ–åªåšå›ºå®šå‚æ•°ä¼°è®¡ï¼‰çš„å‰ä½œï¼Œæœ¬æ–‡å®ç°äº†ä¸€ä¸ªä»**ç”¨æˆ·è‡ªç„¶è¯­è¨€è¯·æ±‚**åˆ°**ç‰©ç†å‚æ•°å®Œå¤‡çš„ä»¿çœŸæ¨¡å‹**çš„**å…¨è‡ªåŠ¨ã€å¯è§£é‡Š**ç®¡é“ã€‚\n\n**ç®€è€Œè¨€ä¹‹ï¼Œå‰ä½œè¦ä¹ˆæ˜¯å›ºå®šçš„Real2Simæµç¨‹ï¼Œè¦ä¹ˆæ˜¯ç”¨VLMç”Ÿæˆé«˜å±‚ä»»åŠ¡çš„è¡Œä¸ºæ ‘ï¼Œè¦ä¹ˆæ˜¯ç”Ÿæˆè§†è§‰åœºæ™¯ã€‚æœ¬æ–‡çš„çªå‡ºæ”¹è¿›åœ¨äºï¼šç”¨VLMç”Ÿæˆä¸“é—¨ä¸ºè·å–ç‰©ç†å‚æ•°è€Œè®¾è®¡çš„ã€ç”±åŸå­åŠ¨ä½œæ„æˆçš„è¡Œä¸ºæ ‘ï¼Œå¹¶ä¸”è·å–å“ªäº›å‚æ•°å®Œå…¨ç”±ç”¨æˆ·æ„å›¾å†³å®šï¼Œä»è€Œå®ç°äº†Real2Simæµç¨‹çš„è‡ªä¸»åŒ–ã€è‡ªé€‚åº”åŒ–å’Œæ„å›¾åŒ–ã€‚**",
      "timestamp": "2026-01-20T17:53:12.983450",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
      "answer": "## è®ºæ–‡å…³é”®æŠ€æœ¯æ–¹æ³•åˆ†æ\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸»åŠ¨æ„ŸçŸ¥å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è‡ªåŠ¨ç”Ÿæˆè¡Œä¸ºæ ‘ï¼ˆBTï¼‰çš„Real2Simæ¡†æ¶ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†é«˜å±‚ç”¨æˆ·æ„å›¾ã€å¤šæ¨¡æ€åœºæ™¯ç†è§£ä¸åŸºäºç‰©ç†äº¤äº’çš„æœºå™¨äººå‚æ•°ä¼°è®¡ç›¸ç»“åˆï¼Œå®ç°ä»»åŠ¡é©±åŠ¨çš„ã€è‡ªé€‚åº”çš„ä»¿çœŸæ¨¡å‹æ„å»ºã€‚ä»¥ä¸‹æ˜¯å¯¹å…³é”®æŠ€æœ¯æ–¹æ³•çš„è¯¦ç»†è¯´æ˜ã€‚\n\n### å…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è¡¨\n\n| æ–¹æ³•æ¨¡å— | æŠ€æœ¯ç»†èŠ‚ | æ•°å­¦åŸç†ä¸å…·ä½“å‚æ•° |\n| :--- | :--- | :--- |\n| **1. å¤šæ¨¡æ€åœºæ™¯ç†è§£ä¸æ„å›¾è§£æ** | **è¾“å…¥**ï¼šç”¨æˆ·é«˜å±‚è¯·æ±‚ `â„›`ã€ä¸å®Œæ•´ä»¿çœŸæè¿° `ğ’Ÿ`ã€åœºæ™¯RGBå›¾åƒ `â„`ã€‚<br>**è¿‡ç¨‹**ï¼šä½¿ç”¨VLMï¼ˆå¦‚GPT-4ï¼‰è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚<br>**è¾“å‡º**ï¼š<br>1. **å¯¹è±¡è¯†åˆ«**ï¼šè¯†åˆ«åœºæ™¯ä¸­ä¸ä»»åŠ¡ç›¸å…³çš„ç‰©ä½“ `ğ’ª = {(oâ‚, pâ‚), ..., (oâ‚™, pâ‚™)}`ï¼Œå…¶ä¸­ `páµ¢` ä¸ºå·²çŸ¥çš„ç‰©ä½“ä½å§¿ï¼ˆä½œä¸ºå…ƒæ•°æ®æä¾›ï¼‰ã€‚<br>2. **å‚æ•°å‘ç°**ï¼šæ ¹æ® `â„›` å’Œ `ğ’Ÿ` çš„è¯­ä¹‰ï¼Œæ¨æ–­å®Œæˆä»¿çœŸæ‰€å¿…éœ€çš„**æœ€å°ç‰©ç†å‚æ•°é›†** `Î¦ âŠ† ğ’«`ï¼Œå…¶ä¸­ `ğ’« = {è´¨é‡, å°ºå¯¸, æ‘©æ“¦}`ã€‚<br>3. **è¡Œä¸ºæ ‘ç”Ÿæˆ**ï¼šå°†æ¯ä¸ªç¼ºå¤±å‚æ•° `Ï•â±¼ âˆˆ Î¦` æ˜ å°„åˆ°ä¸€ç³»åˆ—åŸå­åŠ¨ä½œ `ğ’œ`ï¼Œå¹¶ç»„åˆç”Ÿæˆç»“æ„åŒ–çš„ã€å¯æ‰§è¡Œçš„è¡Œä¸ºæ ‘ `â„¬ğ’¯`ã€‚ | **æ— æ˜¾å¼æ•°å­¦å…¬å¼ï¼Œä¾èµ–VLMçš„è¯­ä¹‰æ¨ç†èƒ½åŠ›**ã€‚<br>**å…³é”®å‚æ•°**ï¼š<br>- **åŸå­åŠ¨ä½œé›† `ğ’œ`**ï¼šåŒ…å« `MovePose`, `MoveJoints`, `OpenGripper`, `CloseGripper`, `MoveDownUntilContact`, `MeasureGripperPose`, `MeasureForces`, `MeasureMass` ç­‰8ä¸ªé¢„å®šä¹‰çš„ä½çº§åŠ¨ä½œï¼ˆè§è¡¨Iï¼‰ã€‚<br>- **ç³»ç»Ÿæç¤ºè¯ `ğ’®`**ï¼šå›ºå®šçš„å·¥ç¨‹åŒ–æç¤ºï¼ŒåŒ…å«è§’è‰²å®šä¹‰ã€å¯¹è±¡å…ƒæ•°æ®ã€æŒ‡ä»¤éµå¾ªæµç¨‹ã€é¢„æœŸè¡Œä¸ºã€`ğ’œ`å’Œå¤åˆèŠ‚ç‚¹`ğ’`çš„åˆ—è¡¨ã€è¾“å…¥è¾“å‡ºæè¿°åŠç¤ºä¾‹ã€‚è¿™æ˜¯å®ç°ä»»åŠ¡è‡ªé€‚åº”è€Œæ— éœ€é‡æ–°ç¼–ç¨‹çš„æ ¸å¿ƒã€‚ |\n| **2. è¡Œä¸ºæ ‘ï¼ˆBTï¼‰çš„ç”Ÿæˆä¸æ‰§è¡Œ** | **ç”Ÿæˆ**ï¼šVLMæ ¹æ® `ğ’®`, `ğ’Ÿ`, `â„›`, `â„` è¾“å‡ºä¸€ä¸ªç»“æ„åŒ–çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º `([nodeâ‚, nodeâ‚‚, ...])`ï¼Œå…¶ä¸­ `nodeáµ¢ = (args, type)`ï¼Œ`type âˆˆ {ğ’, ğ’œ}`ã€‚<br>**æ‰§è¡Œ**ï¼šä½¿ç”¨ `py_trees` åº“å°†ç»“æ„åŒ–åˆ—è¡¨è§£æä¸ºå¯æ‰§è¡ŒBTï¼Œåœ¨Franka Emika Pandaæœºå™¨äººä¸Šå®æ—¶è¿è¡Œã€‚BTçš„Tickæœºåˆ¶æä¾›äº†ååº”å¼æ‰§è¡Œå’Œæ•…éšœå¤„ç†èƒ½åŠ›ã€‚ | **æ— ç‰¹å®šæ•°å­¦å…¬å¼ï¼Œä½†ä¾èµ–BTçš„è¯­ä¹‰**ã€‚<br>**å…³é”®å‚æ•°/åŸç†**ï¼š<br>- **èŠ‚ç‚¹ç±»å‹**ï¼š`ğ’œ` ä¸ºåŠ¨ä½œå¶èŠ‚ç‚¹ï¼Œ`ğ’` ä¸ºæ§åˆ¶æµå¤åˆèŠ‚ç‚¹ï¼ˆå¦‚åºåˆ—ã€é€‰æ‹©ã€å¹¶è¡Œï¼‰ã€‚<br>- **æ‰§è¡Œé€»è¾‘**ï¼šæ¯ä¸ªæ§åˆ¶å‘¨æœŸä»æ ¹èŠ‚ç‚¹å¼€å§‹â€œTickâ€ï¼Œä¿¡å·æ²¿æ ‘å‘ä¸‹ä¼ é€’ï¼Œæ ¹æ®å­èŠ‚ç‚¹çš„æˆåŠŸ/å¤±è´¥çŠ¶æ€å†³å®šæ‰§è¡Œæµï¼Œå®ç°ååº”å¼è¡Œä¸ºã€‚ |\n| **3. æŸ”é¡ºæœºå™¨äººæ§åˆ¶ä¸æ•°æ®é‡‡é›†** | **æ§åˆ¶ç­–ç•¥**ï¼šé‡‡ç”¨**ç¬›å¡å°”é˜»æŠ—æ§åˆ¶**ï¼Œä½¿æœºå™¨äººåœ¨æ¥è§¦å¼äº¤äº’ä¸­ä¿æŒæŸ”é¡ºæ€§ã€‚æ§åˆ¶å™¨è®¡ç®—å…³èŠ‚æ‰­çŸ© `ğ›•` ä»¥è·Ÿè¸ªæœŸæœ›çš„æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿ `ğ±_d`ï¼ŒåŒæ—¶å…è®¸ä¸ç¯å¢ƒæ¥è§¦äº§ç”Ÿåå·®ã€‚<br>**æ•°æ®é‡‡é›†**ï¼šåœ¨æ‰§è¡Œç‰¹å®šçš„æ„ŸçŸ¥åŠ¨ä½œï¼ˆå¦‚ `MeasureForces`ï¼‰æ—¶ï¼ŒåŒæ­¥è®°å½•å…³èŠ‚æ‰­çŸ©ã€æœ«ç«¯ä½å§¿ç­‰ä¼ æ„Ÿå™¨æ•°æ®ã€‚ | **æ ¸å¿ƒæ•°å­¦å…¬å¼ï¼ˆè®ºæ–‡å…¬å¼1ï¼‰**ï¼š<br>```<br>ğ›• = ğ‰áµ€[ -ğŠ(ğ± - ğ±_d) - ğƒ(ğ‰ğªÌ‡) ] // ç¬›å¡å°”é˜»æŠ—é¡¹<br>     + ğ‚(ğª, ğªÌ‡)                // ç§‘é‡Œå¥¥åˆ©åŠ›è¡¥å¿é¡¹<br>     + (ğˆ - ğ‰áµ€ğ‰âº)[ k_ns(ğª_d - ğª) - 2âˆš(k_ns)ğªÌ‡ ] // é›¶ç©ºé—´PDæ§åˆ¶é¡¹<br>```<br>**å…·ä½“å‚æ•°**ï¼š<br>- `ğ±, ğ±_d âˆˆ â„â¶`ï¼šå½“å‰/æœŸæœ›æœ«ç«¯ä½å§¿ï¼ˆä½ç½®+æ–¹å‘ï¼‰ã€‚<br>- `ğª, ğªÌ‡ âˆˆ â„â·`ï¼šå…³èŠ‚ä½ç½®ä¸é€Ÿåº¦ã€‚<br>- `ğ‰(ğª) âˆˆ â„â¶Ë£â·`ï¼šå‡ ä½•é›…å¯æ¯”çŸ©é˜µã€‚<br>- `ğŠ, ğƒ âˆˆ â„â¶Ë£â¶`ï¼šç¬›å¡å°”åˆšåº¦ä¸é˜»å°¼çŸ©é˜µï¼ˆéœ€è°ƒå‚ï¼‰ã€‚<br>- `k_ns`ï¼šé›¶ç©ºé—´åˆšåº¦æ ‡é‡ã€‚<br>- `ğ‰âº`ï¼š`ğ‰áµ€` çš„ä¼ªé€†ã€‚<br>- `ğ‚(ğª, ğªÌ‡)`ï¼šç§‘é‡Œå¥¥åˆ©åŠ›å‘é‡ã€‚ |\n| **4. ç‰©ç†å‚æ•°ä¼°è®¡ç®—æ³•** | **è´¨é‡ä¼°è®¡**ï¼šåœ¨å‡†é™æ€æå‡ç‰©ä½“æ—¶ï¼Œé€šè¿‡å…³èŠ‚æ‰­çŸ© `ğ›•` åç®—æœ«ç«¯å¤–åŠ› `ğ’˜_ee`ï¼Œåˆ©ç”¨å‚ç›´æ–¹å‘åŠ› `F_z` è®¡ç®—è´¨é‡ã€‚<br>**æ‘©æ“¦ä¼°è®¡**ï¼šåœ¨å¹³é¢æ¨åŠ¨ç‰©ä½“æ—¶ï¼Œæµ‹é‡åˆ‡å‘åŠ› `F_âˆ¥`ï¼Œç»“åˆå·²çŸ¥è´¨é‡ `m` è®¡ç®—æ‘©æ“¦ç³»æ•°ã€‚<br>**è¡¨é¢é«˜åº¦ä¼°è®¡**ï¼šé€šè¿‡ `MoveDownUntilContact` åŠ¨ä½œï¼Œå½“æ¥è§¦æ‰­çŸ©é˜ˆå€¼ `Ï„_c` è§¦å‘æ—¶ï¼Œè®°å½•æœ«ç«¯é«˜åº¦ `z_ee`ã€‚ | **è´¨é‡ä¼°è®¡**ï¼š<br>`ğ’˜_ee = ğ‰(ğª)â»áµ€ğ›•`ï¼Œ `m = F_z / g`<br>å…¶ä¸­ `g` ä¸ºé‡åŠ›åŠ é€Ÿåº¦ï¼Œ`F_z` ä¸º `ğ’˜_ee` çš„å‚ç›´åˆ†é‡ã€‚<br><br>**æ‘©æ“¦ä¼°è®¡**ï¼š<br>`Î¼ = F_âˆ¥ / (m * g)`<br>å¯åˆ†åˆ«ç”¨äºä¼°ç®—é™æ‘©æ“¦ `Î¼_s` å’ŒåŠ¨æ‘©æ“¦ `Î¼_d`ã€‚<br><br>**é«˜åº¦ä¼°è®¡**ï¼š<br>`h = z_ee - d_offset`<br>å…¶ä¸­ `d_offset` æ˜¯ç»è¿‡æ ¡å‡†çš„æœºå™¨äººåŸºåº§åç§»é‡ã€‚ |\n| **5. ä»¿çœŸç¯å¢ƒæ„å»º** | **æµç¨‹**ï¼šå°†ä¼°è®¡å‡ºçš„ç‰©ç†å‚æ•°é›† `Î¦` ç›´æ¥é›†æˆåˆ°MuJoCoç‰©ç†å¼•æ“çš„ä»¿çœŸæè¿°æ–‡ä»¶ï¼ˆ.xmlï¼‰ä¸­ã€‚<br>**å†…å®¹**ï¼šä¸ºç‰©ä½“èµ‹äºˆä¼°è®¡çš„è´¨é‡ã€æ‘©æ“¦ç³»æ•°ã€å‡ ä½•å°ºå¯¸ï¼ˆé«˜åº¦ç­‰ï¼‰ï¼Œå¹¶ä½¿ç”¨ç®€åŒ–ç½‘æ ¼æˆ–å‡¸åŒ…è¿›è¡Œå‡ ä½•è¡¨ç¤ºï¼Œä»è€Œæ„å»ºå‡ºç‰©ç†æ„ŸçŸ¥çš„æ•°å­—å­ªç”Ÿã€‚ | **æ— å¤æ‚æ•°å­¦ï¼Œå±äºå·¥ç¨‹é›†æˆ**ã€‚<br>**å…³é”®å‚æ•°**ï¼š<br>- ä¼°è®¡çš„ `Î¦` å€¼ï¼ˆè´¨é‡ã€æ‘©æ“¦ã€é«˜åº¦ï¼‰ã€‚<br>- MuJoCoä»¿çœŸæ–‡ä»¶ä¸­çš„å¯¹åº”ç‰©ç†å‚æ•°æ ‡ç­¾ï¼ˆå¦‚ `<geom mass=\"...\" friction=\"...\"/>`ï¼‰ã€‚ |\n\n### æŠ€æœ¯æ–¹æ³•æ€»ç»“ä¸åˆ›æ–°ç‚¹\n\n1.  **æ„å›¾é©±åŠ¨çš„ä¸»åŠ¨æ„ŸçŸ¥ç®¡é“**ï¼šä¸ä¼ ç»ŸReal2Simå›ºå®šã€ exhaustiveçš„å‚æ•°ä¼°è®¡ä¸åŒï¼Œæœ¬æ¡†æ¶é€šè¿‡VLMç†è§£ç”¨æˆ·ç›®æ ‡(`â„›`)å’Œä»¿çœŸçŠ¶æ€(`ğ’Ÿ`)ï¼Œ**åŠ¨æ€æ¨æ–­å¹¶ä»…æµ‹é‡å®Œæˆä»»åŠ¡æ‰€å¿…éœ€çš„æœ€å°å‚æ•°é›†** `Î¦`ï¼Œå®ç°äº†ä»»åŠ¡è‡ªé€‚åº”æ€§ã€‚\n2.  **åŸºäºåŸå­åŠ¨ä½œçš„BTç”Ÿæˆ**ï¼šä¸ç°æœ‰ä½¿ç”¨é«˜çº§æŠ€èƒ½ï¼ˆå¦‚â€œæŠ“å–â€ï¼‰çš„VLM/BTå·¥ä½œä¸åŒï¼Œæœ¬æ–‡è§„åˆ’åŸºäºä¸€ç»„**ä½çº§çš„ã€ç‰©ç† grounded çš„åŸå­åŠ¨ä½œ** `ğ’œ`ã€‚è¿™ä½¿å¾—ç”Ÿæˆçš„BTèƒ½å®ç°æ›´ç²¾ç»†ã€æ¥è§¦æ„ŸçŸ¥çš„äº¤äº’ï¼Œå¹¶ç›´æ¥é›†æˆåŠ›/æ‰­çŸ©ç­‰ç‰©ç†æµ‹é‡ï¼Œæ˜¯è¿æ¥é«˜å±‚æ¨ç†ä¸åº•å±‚ç‰©ç†äº¤äº’çš„å…³é”®ã€‚\n3.  **å›ºå®šæç¤ºè¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›**ï¼šæ•´ä¸ªç³»ç»Ÿçš„é€‚åº”æ€§æ¥æºäºVLMçš„å¤šæ¨¡æ€æ¨ç†ï¼Œè€Œéé’ˆå¯¹æ¯ä¸ªä»»åŠ¡ä¿®æ”¹ç³»ç»Ÿæç¤ºè¯ `ğ’®`ã€‚`ğ’®` ä½œä¸ºå›ºå®šçš„â€œç»“æ„åŒ–å…ˆéªŒâ€ï¼Œå®šä¹‰äº†åŠ¨ä½œæ¥å£å’Œè¡Œä¸ºå‡†åˆ™ï¼Œä½¿å¾—**éæœºå™¨äººä¸“å®¶çš„ç”¨æˆ·ä»…é€šè¿‡è‡ªç„¶è¯­è¨€å’Œå›¾åƒå³å¯é©±åŠ¨å¤æ‚çš„å‚æ•°è·å–æµç¨‹**ã€‚\n4.  **ååº”å¼ä¸å®‰å…¨çš„ç‰©ç†äº¤äº’**ï¼šé€šè¿‡**ç¬›å¡å°”é˜»æŠ—æ§åˆ¶**ä¸**è¡Œä¸ºæ ‘çš„ååº”å¼æ‰§è¡Œ**ç›¸ç»“åˆï¼Œç¡®ä¿äº†åœ¨æ¥è§¦ä¸°å¯Œçš„å‚æ•°ä¼°è®¡ä»»åŠ¡ï¼ˆå¦‚æ¨ã€å‹ã€æŠ¬ï¼‰ä¸­çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚\n\n**å®é™…ä»·å€¼**ï¼šè¯¥æ¡†æ¶æ˜¾è‘—é™ä½äº†æ„å»ºé«˜ä¿çœŸç‰©ç†ä»¿çœŸï¼ˆæ•°å­—å­ªç”Ÿï¼‰çš„é—¨æ§›å’ŒäººåŠ›æˆæœ¬ï¼Œä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°çš„åœºæ™¯å’Œç”¨æˆ·éœ€æ±‚ï¼Œåœ¨æœºå™¨äººç­–ç•¥æµ‹è¯•ã€æ¨¡æ‹Ÿå­¦ä¹ ã€è‡ªåŠ¨åŒ–æµ‹è¯•ç­‰é¢†åŸŸå…·æœ‰ç›´æ¥çš„åº”ç”¨æ½œåŠ›ã€‚",
      "timestamp": "2026-01-20T17:54:14.925585",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
      "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\nè¯¥è®ºæ–‡æå‡ºçš„æ˜¯ä¸€ä¸ª**è‡ªä¸»ã€ä»»åŠ¡é©±åŠ¨çš„ç‰©ç†å‚æ•°ä¼°è®¡ä¸ä»¿çœŸæ„å»ºæ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒè´¡çŒ®åœ¨äº**æ–¹æ³•è®ºçš„åˆ›æ–°**ï¼Œè€Œéåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œæ€§èƒ½æ¯”è¾ƒã€‚å› æ­¤ï¼Œè®ºæ–‡**æ²¡æœ‰ä½¿ç”¨ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å…¬å¼€åŸºå‡†æ•°æ®é›†ï¼ˆå¦‚YCBã€ShapeNetï¼‰æˆ–ç»Ÿä¸€çš„é‡åŒ–è¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚mAPã€RMSEï¼‰**æ¥è¿›è¡Œæ¨ªå‘å¯¹æ¯”ã€‚\n\nå…¶è¯„ä¼°æ–¹å¼æ˜¯åŸºäº**çœŸå®æœºå™¨äººå¹³å°ä¸Šçš„å¤šä¸ªå®šåˆ¶åŒ–åœºæ™¯å®éªŒ**ï¼Œä»¥**è¯æ˜æ¡†æ¶çš„åŠŸèƒ½æ€§ã€é€‚åº”æ€§å’Œæ„å›¾é©±åŠ¨èƒ½åŠ›**ã€‚è¯„ä»·çš„æ ¸å¿ƒæ˜¯ **â€œèƒ½å¦æˆåŠŸå®Œæˆä»»åŠ¡â€** ä»¥åŠ **â€œä¼°è®¡å‚æ•°çš„å‡†ç¡®æ€§â€**ã€‚\n\n### 1. è¯„ä»·æŒ‡æ ‡ï¼ˆåŠŸèƒ½æ€§ä¸å‡†ç¡®æ€§éªŒè¯ï¼‰\n\nè®ºæ–‡é€šè¿‡ä»¥ä¸‹**éæ ‡å‡†åŒ–ä½†å…·ä½“å¯æµ‹çš„æŒ‡æ ‡**æ¥éªŒè¯ç³»ç»Ÿï¼š\n\n| è¯„ä»·ç»´åº¦ | å…·ä½“æŒ‡æ ‡ä¸ç»†èŠ‚ | è¯´æ˜ |\n| :--- | :--- | :--- |\n| **ä»»åŠ¡å®ŒæˆæˆåŠŸç‡** | ç³»ç»Ÿèƒ½å¦æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ï¼ˆ`â„›`ï¼‰å’Œåœºæ™¯å›¾åƒï¼ˆ`â„`ï¼‰**è‡ªåŠ¨ç”Ÿæˆå¹¶æˆåŠŸæ‰§è¡Œ**æ­£ç¡®çš„è¡Œä¸ºæ ‘ï¼ˆBTï¼‰ã€‚ | è¿™æ˜¯**æ ¸å¿ƒåŠŸèƒ½æŒ‡æ ‡**ã€‚è®ºæ–‡é€šè¿‡ä¸‰ä¸ªåœºæ™¯çš„æ¼”ç¤ºï¼Œè¡¨æ˜VLMèƒ½ç”Ÿæˆå¯æ‰§è¡Œä¸”é€»è¾‘æ­£ç¡®çš„BTï¼Œæœºå™¨äººèƒ½æŒ‰è®¡åˆ’å®Œæˆäº¤äº’ã€‚ |\n| **å‚æ•°ä¼°è®¡å‡†ç¡®æ€§** | å°†æœºå™¨äººæµ‹é‡å¾—åˆ°çš„ç‰©ç†å‚æ•°ä¸**äººå·¥æµ‹é‡çœŸå€¼**è¿›è¡Œå¯¹æ¯”ã€‚ | **ä¸»è¦é‡åŒ–æŒ‡æ ‡**ã€‚ç”¨äºéªŒè¯é€šè¿‡æœºå™¨äººäº¤äº’ä¼°è®¡çš„ç‰©ç†å‚æ•°çš„å¯é æ€§ã€‚ |\n| **ä¼°è®¡ä¸€è‡´æ€§ï¼ˆç²¾åº¦ï¼‰** | å¯¹åŒä¸€å‚æ•°è¿›è¡Œ**å¤šæ¬¡é‡å¤æµ‹é‡**ï¼Œè®¡ç®—å…¶**å‡å€¼ï¼ˆmeanï¼‰å’Œæ ‡å‡†å·®ï¼ˆstdï¼‰**ã€‚ | æ ‡å‡†å·®åæ˜ äº†æµ‹é‡è¿‡ç¨‹çš„**å¯é‡å¤æ€§å’Œå™ªå£°æ°´å¹³**ï¼Œè¯„ä¼°äº†æ•°æ®é‡‡é›†çš„ç¨³å®šæ€§ã€‚ |\n| **ç³»ç»Ÿé€‚åº”æ€§** | 1. **åœºæ™¯ç†è§£**ï¼šèƒ½å¦å¤„ç†ç‰©ä½“é®æŒ¡ã€å…ˆéªŒä¿¡æ¯ä¸å…¨ç­‰æƒ…å†µã€‚<br>2. **æ„å›¾ç†è§£**ï¼šèƒ½å¦æ ¹æ®ä¸å®Œæ•´çš„ä»¿çœŸæè¿°ï¼ˆ`ğ’Ÿ`ï¼‰è‡ªåŠ¨æ¨æ–­ç¼ºå¤±å‚æ•°ã€‚<br>3. **ç­–ç•¥ç”Ÿæˆ**ï¼šèƒ½å¦ç”¨åŒä¸€å¥—åŸå­åŠ¨ä½œé›†ï¼ˆ`ğ’œ`ï¼‰ç»„åˆå‡ºä¸åŒä»»åŠ¡çš„æ¢ç´¢ç­–ç•¥ã€‚ | è¿™äº›æ˜¯**å®šæ€§ä½†å…³é”®çš„è¯„ä¼°ç‚¹**ï¼Œé€šè¿‡è®¾è®¡ä¸åŒçš„å®éªŒåœºæ™¯ï¼ˆå¦‚ä¸‹æ–‡æ•°æ®é›†ï¼‰æ¥å±•ç¤ºã€‚ |\n| **æŠ—å¹»è§‰èƒ½åŠ›** | å½“ç”¨æˆ·æŒ‡ä»¤æåŠå›¾åƒä¸­ä¸å­˜åœ¨çš„ç‰©ä½“æ—¶ï¼Œç³»ç»Ÿ**æ˜¯å¦ä¼šåœ¨BTä¸­ç”Ÿæˆç›¸å…³æ“ä½œ**ã€‚ | é€šè¿‡ç‰¹å®šæµ‹è¯•éªŒè¯ç³»ç»Ÿæç¤ºï¼ˆ`ğ’®`ï¼‰å·¥ç¨‹çš„æœ‰æ•ˆæ€§ï¼Œç¡®ä¿è§„åˆ’åŸºäºçœŸå®è§†è§‰è§‚å¯Ÿã€‚ |\n\n**å‚æ•°ä¼°è®¡å‡†ç¡®æ€§çš„è®¡ç®—ç»†èŠ‚ï¼ˆæ¥è‡ªè®ºæ–‡ç¬¬IV-CèŠ‚ï¼‰ï¼š**\n- **è´¨é‡ä¼°è®¡**ï¼š`m = F_z / g`ã€‚é€šè¿‡é˜»æŠ—æ§åˆ¶æŠ¬èµ·ç‰©ä½“ï¼Œä»æœ«ç«¯æ‰§è¡Œå™¨æµ‹å¾—çš„å‚ç›´åŠ› `F_z`ï¼ˆç”±å…³èŠ‚æ‰­çŸ©ç»é›…å¯æ¯”çŸ©é˜µè½¬æ¢å¾—åˆ°ï¼‰é™¤ä»¥é‡åŠ›åŠ é€Ÿåº¦ `g` è®¡ç®—ã€‚\n- **æ‘©æ“¦ç³»æ•°ä¼°è®¡**ï¼š`Î¼ = F_âˆ¥ / (m * g)`ã€‚åœ¨å·²çŸ¥è´¨é‡ `m` åï¼Œé€šè¿‡æ¨åŠ¨ç‰©ä½“å¹¶æµ‹é‡åˆ‡å‘åŠ› `F_âˆ¥` æ¥è®¡ç®—ã€‚\n- **è¡¨é¢é«˜åº¦ä¼°è®¡**ï¼š`h = z_ee - d_offset`ã€‚é€šè¿‡â€œå‘ä¸‹ç§»åŠ¨ç›´è‡³æ¥è§¦â€çš„åŠ¨ä½œï¼Œåœ¨æ£€æµ‹åˆ°æ¥è§¦é˜ˆå€¼æ—¶è®°å½•æœ«ç«¯æ‰§è¡Œå™¨çš„Zè½´ä½ç½® `z_ee`ï¼Œå‡å»å·²çŸ¥çš„æ ¡å‡†åç§»é‡ `d_offset`ã€‚\n\n### 2. â€œæ•°æ®é›†â€/å®éªŒåœºæ™¯ç»†èŠ‚\n\nè®ºæ–‡çš„éªŒè¯å®Œå…¨åœ¨**çœŸå®æœºå™¨äººï¼ˆFranka Emika Pandaï¼‰** ä¸Šè¿›è¡Œçš„**å®ç‰©åœºæ™¯å®éªŒ**ï¼Œæ²¡æœ‰ä½¿ç”¨é¢„æ”¶é›†çš„ç¦»çº¿æ•°æ®é›†ã€‚è¿™äº›åœºæ™¯æ˜¯ä¸“é—¨è®¾è®¡æ¥æµ‹è¯•æ¡†æ¶ä¸åŒèƒ½åŠ›çš„ï¼š\n\n| åœºæ™¯ç¼–å·ä¸åç§° | åœºæ™¯è®¾ç½®ä¸ç›®æ ‡ | æµ‹è¯•çš„æ ¸å¿ƒèƒ½åŠ› | å…³é”®ç»“æœï¼ˆæ¥è‡ªè®ºæ–‡ç¬¬VèŠ‚ï¼‰ |\n| :--- | :--- | :--- | :--- |\n| **åœºæ™¯ä¸€**<br>æ¡Œé¢é«˜åº¦ä¸ç‰©ä½“å±æ€§ | **è®¾ç½®**ï¼šæ¡Œé¢ä¸Šæ”¾ä¸€ä¸ªæ°´ç“¶ã€‚<br>**ç›®æ ‡**ï¼šä¼°è®¡**ç“¶å­è´¨é‡**å’Œ**æ¡Œå­é«˜åº¦**ã€‚<br>**å˜ä½“**ï¼š1) æä¾›/ä¸æä¾›ä»¿çœŸæè¿°(`ğ’Ÿ`); 2) ä½¿ç”¨**åˆæˆå›¾åƒ**æ›¿ä»£çœŸå®å›¾åƒï¼›3) å¤šç‰©ä½“ä¸­é€‰æ‹©æ€§æµ‹é‡ï¼›4) æµ‹è¯•å¹»è§‰ã€‚ | 1. **åŸºç¡€å‚æ•°è·å–èƒ½åŠ›**<br>2. **ä¸å…ˆéªŒä¿¡æ¯(`ğ’Ÿ`)çš„é›†æˆ**<br>3. **ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»æ½œåŠ›**<br>4. **ä»»åŠ¡ç›¸å…³æ¨ç†**<br>5. **æŠ—å¹»è§‰** | - è´¨é‡ä¼°è®¡ï¼š0.257Â±0.022 kg (çœŸå€¼0.254 kg)ã€‚<br>- é«˜åº¦ä¼°è®¡ï¼š0.764226Â±0.000039 m (çœŸå€¼0.765 m)ã€‚<br>- ç»™å®š`ğ’Ÿ`æ—¶ï¼Œä»…ç”Ÿæˆç¼ºå¤±å‚æ•°çš„BTã€‚<br>- åˆæˆå›¾åƒå¼•å¯¼ç”Ÿæˆçš„BTä¸çœŸå®å›¾åƒä¸€è‡´ã€‚ |\n| **åœºæ™¯äºŒ**<br>æ‘©æ“¦å‚æ•° | **è®¾ç½®**ï¼šæ¡Œé¢ä¸Šæ”¾ä¸€ä¸ªç“¶å­ã€‚<br>**ç›®æ ‡**ï¼šç”¨æˆ·è¯·æ±‚ä¼°è®¡**é™æ‘©æ“¦å’ŒåŠ¨æ‘©æ“¦ç³»æ•°**ã€‚ | **å¤„ç†æ— æ³•ç›´æ¥æµ‹é‡çš„å‚æ•°**ï¼ˆå› ä¸ºåŸå­åŠ¨ä½œé›†`ğ’œ`ä¸­æ²¡æœ‰ç›´æ¥â€œæµ‹æ‘©æ“¦â€çš„åŠ¨ä½œï¼‰ã€‚ | ç³»ç»Ÿç”ŸæˆBTæ¥æµ‹é‡**è´¨é‡**å’Œ**æ»‘åŠ¨è¿‡ç¨‹ä¸­çš„åŠ›**ï¼Œå¹¶è¾“å‡ºè§£é‡Šæ–‡æœ¬(`â„°`)ï¼Œå‘ŠçŸ¥ç”¨æˆ·å¯åŸºäºè¿™äº›æ•°æ®è‡ªè¡Œè®¡ç®—æ‘©æ“¦ç³»æ•°ã€‚å±•ç¤ºäº†åœ¨åŠ¨ä½œå—é™ä¸‹çš„**å¦¥åä¸è§£é‡Šèƒ½åŠ›**ã€‚ |\n| **åœºæ™¯ä¸‰**<br>ç‰©ä½“é®æŒ¡ | **è®¾ç½®**ï¼šä¸€ä¸ªå°è“ç›’è¢«ä¸€ä¸ªçº¢ç›’å®Œå…¨å‹åœ¨ä¸‹é¢ã€‚<br>**ç›®æ ‡**ï¼šä¼°è®¡**è¢«é®æŒ¡è“ç›’çš„è´¨é‡**ã€‚ | **è§£å†³å¤æ‚ç‰©ç†çº¦æŸçš„è§„åˆ’èƒ½åŠ›**ï¼ˆéœ€å…ˆç§»å¼€éšœç¢ç‰©ï¼‰ã€‚ | ç³»ç»ŸæˆåŠŸç”ŸæˆBTï¼Œé¡ºåºæ‰§è¡Œï¼š1) å°†çº¢ç›’ç§»è‡³ä¸´æ—¶ä½ç½®ï¼›2) æµ‹é‡è“ç›’è´¨é‡ï¼›3) æ¢å¤åŸåœºæ™¯ã€‚è¯æ˜äº†ç»„åˆåŸå­åŠ¨ä½œè§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚ |\n\n### æ€»ç»“\nè¯¥è®ºæ–‡çš„è¯„ä¼°èŒƒå¼æ˜¯ **â€œé¢å‘ä»»åŠ¡çš„ç³»ç»Ÿæ¼”ç¤ºä¸éªŒè¯â€** ï¼Œè€Œé **â€œåŸºäºåŸºå‡†çš„é‡åŒ–å¯¹æ¯”â€** ã€‚å…¶**è¯„ä»·æŒ‡æ ‡**ç´§å¯†å›´ç»•**ç³»ç»Ÿæ ¸å¿ƒåˆ›æ–°ç‚¹**ï¼š\n1.  **æ„å›¾é©±åŠ¨çš„è‡ªä¸»æ€§**ï¼ˆèƒ½å¦æ ¹æ®ç›®æ ‡ç”Ÿæˆæ­£ç¡®è®¡åˆ’ï¼‰ã€‚\n2.  **ç‰©ç†äº¤äº’çš„å‡†ç¡®æ€§**ï¼ˆæµ‹é‡å€¼æ˜¯å¦å¯é ï¼‰ã€‚\n3.  **ç³»ç»Ÿçš„é²æ£’æ€§ä¸é€‚åº”æ€§**ï¼ˆèƒ½å¦å¤„ç†é®æŒ¡ã€å…ˆéªŒä¿¡æ¯ä¸å…¨ç­‰å¤æ‚æƒ…å†µï¼‰ã€‚\n\nå…¶ **â€œæ•°æ®é›†â€** å®è´¨æ˜¯**ä¸€ç³»åˆ—ç²¾å¿ƒè®¾è®¡çš„çœŸå®ç‰©ç†å®éªŒåœºæ™¯**ï¼Œæ—¨åœ¨å…¨é¢å±•ç¤ºæ¡†æ¶åœ¨**è„±ç¦»å›ºå®šè„šæœ¬ã€ä¾èµ–é«˜å±‚æŒ‡ä»¤**çš„æƒ…å†µä¸‹ï¼Œå®Œæˆå¤šæ ·åŒ–Real2Simå‚æ•°è·å–ä»»åŠ¡çš„èƒ½åŠ›ã€‚è¿™ç§è¯„ä¼°æ–¹å¼å¼ºçƒˆçªå‡ºäº†å…¶**å®é™…åº”ç”¨ä»·å€¼**â€”â€”ä¸ºæ„å»ºé«˜ä¿çœŸæ•°å­—å­ªç”Ÿæä¾›äº†ä¸€ç§è‡ªåŠ¨åŒ–ã€å¯è§£é‡Šä¸”æ— éœ€æœºå™¨äººä¸“å®¶å…¨ç¨‹å¹²é¢„çš„æ–°è·¯å¾„ã€‚",
      "timestamp": "2026-01-20T17:55:04.086452",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
      "answer": "## è®ºæ–‡å®éªŒä¸è¯„ä¼°åˆ†æ\n\n### å®éªŒæ•°æ®é›†ä¸åœºæ™¯\næœ¬æ–‡**æœªä½¿ç”¨**ä¼ ç»Ÿçš„ç¦»çº¿æ•°æ®é›†è¿›è¡Œè®­ç»ƒæˆ–è¯„ä¼°ã€‚æ‰€æœ‰å®éªŒå‡åœ¨**çœŸå®ç‰©ç†æœºå™¨äººå¹³å°ï¼ˆFranka Emika Pandaï¼‰**ä¸Šï¼Œé€šè¿‡æ„å»º**ä¸‰ç§ä¸åŒçš„æ¡Œé¢åœºæ™¯**è¿›è¡ŒéªŒè¯ã€‚å®éªŒçš„æ ¸å¿ƒæ˜¯è¯„ä¼°æ¡†æ¶åœ¨**çœŸå®ä¸–ç•Œäº¤äº’**ä¸­è‡ªä¸»ç”Ÿæˆè¡Œä¸ºæ ‘å¹¶ä¼°è®¡ç‰©ç†å‚æ•°çš„èƒ½åŠ›ã€‚\n\nå®éªŒåœºæ™¯å…·ä½“åŒ…æ‹¬ï¼š\n1.  **ç¬¬ä¸€åœºæ™¯ï¼šæ¡Œé¢é«˜åº¦ä¸ç‰©ä½“è´¨é‡ä¼°è®¡**\n    *   **å¯¹è±¡**ï¼šè£…æœ‰ä¸åŒæ°´é‡ï¼ˆç”¨äºæ”¹å˜è´¨é‡ï¼‰çš„ç“¶å­ã€ä¸€å¼ æ¡Œå­ã€‚\n    *   **ä»»åŠ¡**ï¼šæ ¹æ®ç”¨æˆ·è¯·æ±‚ï¼Œä¼°è®¡ç“¶å­è´¨é‡å’Œæ¡Œé¢é«˜åº¦ã€‚\n2.  **ç¬¬äºŒåœºæ™¯ï¼šæ‘©æ“¦å‚æ•°ä¼°è®¡**\n    *   **å¯¹è±¡**ï¼šä¸€ä¸ªç“¶å­ã€‚\n    *   **ä»»åŠ¡**ï¼šä¼°è®¡ç“¶å­ä¸æ¡Œé¢é—´çš„é™æ‘©æ“¦å’ŒåŠ¨æ‘©æ“¦ç³»æ•°ã€‚\n3.  **ç¬¬ä¸‰åœºæ™¯ï¼šç‰©ä½“é®æŒ¡ä¸‹çš„è´¨é‡ä¼°è®¡**\n    *   **å¯¹è±¡**ï¼šä¸€ä¸ªè¢«çº¢è‰²æ–¹å—å®Œå…¨è¦†ç›–çš„è“è‰²å°æ–¹å—ã€‚\n    *   **ä»»åŠ¡**ï¼šåœ¨å­˜åœ¨é®æŒ¡çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡ä¸‹æ–¹è“è‰²æ–¹å—çš„è´¨é‡ã€‚\n\n### ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nç”±äºæœ¬æ–‡æå‡ºçš„æ˜¯ä¸€ä¸ªå…¨æ–°çš„ã€**æ„å›¾é©±åŠ¨**çš„Real2Simæµç¨‹ï¼Œå…¶è¯„ä¼°é‡ç‚¹ä¸åœ¨äºè¶…è¶ŠæŸä¸ªåŸºçº¿æ¨¡å‹çš„ç²¾åº¦ï¼Œè€Œåœ¨äº**éªŒè¯æ¡†æ¶çš„æ•´ä½“å¯è¡Œæ€§ã€é€‚åº”æ€§å’Œè‡ªä¸»æ€§**ã€‚ä¸»è¦è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼š\n\n1.  **å‚æ•°ä¼°è®¡å‡†ç¡®æ€§**ï¼šå°†æœºå™¨äººè‡ªåŠ¨æµ‹é‡çš„ç‰©ç†å‚æ•°ä¸**äººå·¥æ‰‹åŠ¨æµ‹é‡**çš„â€œçœŸå®å€¼â€è¿›è¡Œå¯¹æ¯”ã€‚\n    *   **è´¨é‡**ï¼šä¼°è®¡è´¨é‡ï¼ˆkgï¼‰ä¸æ‰‹å·¥æµ‹é‡è´¨é‡çš„å¯¹æ¯”ã€‚\n    *   **é«˜åº¦**ï¼šä¼°è®¡æ¡Œé¢é«˜åº¦ï¼ˆmï¼‰ä¸æ‰‹å·¥æµ‹é‡é«˜åº¦çš„å¯¹æ¯”ã€‚\n    *   **æ‘©æ“¦ç³»æ•°**ï¼šé€šè¿‡æµ‹é‡æ¨åŠ›å’Œè´¨é‡è®¡ç®—å¾—å‡ºçš„æ‘©æ“¦ç³»æ•°ï¼ˆæ— é‡çº²ï¼‰ã€‚\n\n2.  **ç³»ç»Ÿè‡ªä¸»æ€§ä¸é€‚åº”æ€§**ï¼šé€šè¿‡ä»¥ä¸‹æ–¹é¢å®šæ€§è¯„ä¼°ï¼š\n    *   **è¡Œä¸ºæ ‘ç”Ÿæˆèƒ½åŠ›**ï¼šVLMèƒ½å¦æ ¹æ®ä¸åŒçš„ç”¨æˆ·è¯·æ±‚ï¼ˆ`â„›`ï¼‰å’Œåœºæ™¯å›¾åƒï¼ˆ`â„`ï¼‰ç”Ÿæˆæ­£ç¡®ã€å¯æ‰§è¡Œçš„è¡Œä¸ºæ ‘ã€‚\n    *   **ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›**ï¼šå½“ä»¿çœŸæè¿°ï¼ˆ`ğ’Ÿ`ï¼‰ä¸­å·²åŒ…å«éƒ¨åˆ†å‚æ•°æ—¶ï¼Œç³»ç»Ÿæ˜¯å¦ä»…é’ˆå¯¹ç¼ºå¤±å‚æ•°ç”Ÿæˆæµ‹é‡è®¡åˆ’ã€‚\n    *   **å¤„ç†å¤æ‚æƒ…å†µçš„èƒ½åŠ›**ï¼šèƒ½å¦å¤„ç†ç‰©ä½“é®æŒ¡ã€ä½¿ç”¨åˆæˆå›¾åƒè¿›è¡Œè§„åˆ’ã€ä»¥åŠå¤„ç†å¤šç‰©ä½“åœºæ™¯ä¸­æŒ‡å®šç‰©ä½“çš„æµ‹é‡ã€‚\n    *   **è§£é‡Šæ€§è¾“å‡º**ï¼šå½“ä»»åŠ¡æ— æ³•å®Œå…¨è‡ªåŠ¨åŒ–å®Œæˆæ—¶ï¼ˆå¦‚æ‘©æ“¦ç³»æ•°ä¼°è®¡ï¼‰ï¼ŒVLMèƒ½å¦è¾“å‡ºè§£é‡Šæ–‡æœ¬ï¼ˆ`â„°`ï¼‰æŒ‡å¯¼ç”¨æˆ·ã€‚\n\n### æ€§èƒ½ç»“æœä¸æå‡\nè®ºæ–‡æœªæä¾›ä¸ç°æœ‰æ–¹æ³•çš„ç›´æ¥é‡åŒ–å¯¹æ¯”ï¼ˆå¦‚å‡†ç¡®ç‡æå‡ç™¾åˆ†æ¯”ï¼‰ï¼Œå› ä¸ºä¼ ç»ŸReal2Simæ–¹æ³•ä¾èµ–äºå›ºå®šçš„ã€é¢„ç¼–ç¨‹çš„æ¢ç´¢æµç¨‹ï¼Œä¸æœ¬æ–‡çš„**æ„å›¾é©±åŠ¨ã€æŒ‰éœ€æµ‹é‡**èŒƒå¼ä¸å…·ç›´æ¥å¯æ¯”æ€§ã€‚æ ¸å¿ƒæ€§èƒ½ä½“ç°åœ¨æ‰€æå‡ºæ¡†æ¶çš„**ç‹¬ç‰¹èƒ½åŠ›**ä¸Šï¼š\n\n1.  **å‡†ç¡®çš„ç‰©ç†å‚æ•°ä¼°è®¡**ï¼ˆå®šé‡ç»“æœï¼‰ï¼š\n    *   **è´¨é‡ä¼°è®¡**ï¼šå¯¹æ°´ç“¶çš„æµ‹é‡ç»“æœæ¥è¿‘æ‰‹å·¥æµ‹é‡å€¼ï¼ˆä¾‹å¦‚ï¼šä¼°è®¡å‡å€¼0.257 kg vs. æ‰‹å·¥0.254 kgï¼‰ã€‚\n    *   **é«˜åº¦ä¼°è®¡**ï¼šæ¡Œé¢é«˜åº¦ä¼°è®¡éå¸¸ç²¾ç¡®ä¸”æ–¹å·®æå°ï¼ˆå‡å€¼0.764226 mï¼Œæ ‡å‡†å·®0.000039 mï¼‰ã€‚\n    *   **æ‘©æ“¦ä¼°è®¡**ï¼šèƒ½å¤Ÿè·å¾—åˆç†çš„æ‘©æ“¦ç³»æ•°ä¼°è®¡å€¼ï¼ˆé™æ‘©æ“¦~0.41ï¼ŒåŠ¨æ‘©æ“¦~0.34ï¼‰ï¼Œä½†æ–¹å·®ç›¸å¯¹è¾ƒå¤§ï¼Œåæ˜ äº†åŠ¨æ€äº¤äº’æµ‹é‡çš„æŒ‘æˆ˜ã€‚\n\n2.  **å…³é”®æ€§èƒ½æå‡**ï¼ˆå®šæ€§/ç³»ç»Ÿæ€§è´¡çŒ®ï¼‰ï¼š\n    *   **ä»â€œå›ºå®šæµç¨‹â€åˆ°â€œæ„å›¾é©±åŠ¨â€**ï¼šè¿™æ˜¯æœ€ä¸»è¦çš„æå‡ã€‚ç³»ç»Ÿèƒ½ç†è§£â€œä¸ºä»€ä¹ˆâ€è¦æµ‹é‡ï¼ˆç”¨æˆ·ç›®æ ‡ï¼‰ï¼Œä»è€Œè‡ªåŠ¨å†³å®šâ€œæµ‹é‡ä»€ä¹ˆâ€å’Œâ€œå¦‚ä½•æµ‹é‡â€ï¼Œé¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ä¸å¿…è¦çš„ã€ exhaustiveçš„å‚æ•°æµ‹é‡ã€‚\n    *   **ä»â€œé«˜çº§æŠ€èƒ½â€åˆ°â€œåŸå­åŠ¨ä½œâ€**ï¼šä¸ä¹‹å‰ä½¿ç”¨VLMç”Ÿæˆè¡Œä¸ºæ ‘çš„å·¥ä½œä¸åŒï¼Œæœ¬æ–‡ç›´æ¥åŸºäºä½å±‚åŸå­åŠ¨ä½œï¼ˆå¦‚ç§»åŠ¨åˆ°æŸå§¿åŠ¿ã€å¼€åˆå¤¹çˆªã€æµ‹é‡åŠ›ï¼‰è¿›è¡Œè§„åˆ’ï¼Œå®ç°äº†æ›´ç²¾ç»†ã€æ›´ç‰©ç† grounded çš„äº¤äº’æ§åˆ¶ï¼Œç›´æ¥æœåŠ¡äºå‚æ•°è·å–ã€‚\n    *   **å¢å¼ºäº†å¯è§£é‡Šæ€§ä¸é²æ£’æ€§**ï¼šç”Ÿæˆçš„è¡Œä¸ºæ ‘ä¸ºæ•´ä¸ªæ¢ç´¢è¿‡ç¨‹æä¾›äº†æ¸…æ™°ã€æ¨¡å—åŒ–çš„è“å›¾ã€‚åŒæ—¶ï¼Œè¡Œä¸ºæ ‘å›ºæœ‰çš„ååº”å¼ç‰¹æ€§å…è®¸æœºå™¨äººåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ ¹æ®ä¼ æ„Ÿå™¨åé¦ˆï¼ˆå¦‚æ¥è§¦æ£€æµ‹ï¼‰è¿›è¡Œå®æ—¶è°ƒæ•´ã€‚\n    *   **å±•ç¤ºäº†å¤šåœºæ™¯é€‚åº”æ€§**ï¼šå®éªŒæˆåŠŸæ¶µç›–äº†**å¤šç‰©ä½“é€‰æ‹©æ€§æµ‹é‡**ã€**å­˜åœ¨é®æŒ¡**ã€**ç»“åˆå…ˆéªŒä»¿çœŸæè¿°**ã€**ä½¿ç”¨åˆæˆå›¾åƒè§„åˆ’**ç­‰å¤šç§å¤æ‚æƒ…å†µï¼Œè¯æ˜äº†æ¡†æ¶çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚\n\n**æ€»ç»“**ï¼šæœ¬æ–‡çš„å®éªŒä¸æ˜¯åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¯„ä¼°æŒ‡æ ‡ï¼Œè€Œæ˜¯åœ¨çœŸå®æœºå™¨äººä¸ŠéªŒè¯ä¸€ä¸ª**å…¨æ–°æ¡†æ¶çš„å¯è¡Œæ€§**ã€‚å…¶â€œæ€§èƒ½æå‡â€ä½“ç°åœ¨å¼€åˆ›äº†ä¸€ç§**è‡ªä¸»ã€è‡ªé€‚åº”ã€å¯è§£é‡Šçš„Real2Simæ–°èŒƒå¼**ï¼Œèƒ½å¤Ÿæ ¹æ®é«˜å±‚è¯­ä¹‰ç›®æ ‡ï¼ŒåŠ¨æ€ç”Ÿæˆç‰©ç†äº¤äº’ç­–ç•¥æ¥è·å–æœ€å°å¿…è¦å‚æ•°é›†ï¼Œä»è€Œåœ¨æ„å»ºæ•°å­—å­ªç”Ÿæ—¶æ˜¾è‘—æå‡æ•ˆç‡å’Œä»»åŠ¡ç›¸å…³æ€§ã€‚",
      "timestamp": "2026-01-20T17:55:39.534180",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    },
    {
      "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
      "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„æ·±å…¥åˆ†æï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶è™½ç„¶å…·æœ‰åˆ›æ–°æ€§ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹å‡ ä¸ªä¸»è¦å±€é™æ€§ï¼š\n\n- **å¯¹å…ˆéªŒä¿¡æ¯çš„ä¾èµ–**ï¼šç³»ç»Ÿå‡è®¾ç‰©ä½“çš„3Dä½å§¿ï¼ˆ`p_i`ï¼‰åœ¨è§„åˆ’æ—¶æ˜¯å·²çŸ¥çš„ï¼Œå¹¶ä½œä¸ºå…ƒæ•°æ®æä¾›ç»™VLMã€‚è¿™åœ¨å®é™…åº”ç”¨ä¸­æ˜¯ä¸€ä¸ªå¼ºå‡è®¾ï¼Œé™åˆ¶äº†ç³»ç»Ÿåœ¨å®Œå…¨æœªçŸ¥ã€éç»“æ„åŒ–ç¯å¢ƒä¸­çš„è‡ªä¸»æ€§ã€‚\n- **æ„ŸçŸ¥ä¸è§„åˆ’çš„åˆ†ç¦»**ï¼šè§†è§‰æ„ŸçŸ¥ï¼ˆRGBå›¾åƒï¼‰ä»…ç”¨äºåˆå§‹çš„è¡Œä¸ºæ ‘ï¼ˆBTï¼‰è§„åˆ’é˜¶æ®µï¼Œ**å¹¶æœªä¸æœºå™¨äººçš„å®æ—¶æ‰§è¡Œå½¢æˆé—­ç¯**ã€‚è¿™æ„å‘³ç€æ‰§è¡Œè¿‡ç¨‹ä¸­å¦‚æœåœºæ™¯å‘ç”Ÿæ„å¤–å˜åŒ–ï¼ˆå¦‚ç‰©ä½“è¢«æ„å¤–ç§»åŠ¨ï¼‰ï¼Œç³»ç»Ÿæ— æ³•åŠ¨æ€è°ƒæ•´è®¡åˆ’ã€‚\n- **åŸå­åŠ¨ä½œé›†çš„å·¥ç¨‹åŒ–çº¦æŸ**ï¼šæœºå™¨äººå¯æ‰§è¡Œçš„åŸå­åŠ¨ä½œé›†ï¼ˆ`ğ’œ`ï¼‰æ˜¯é¢„å…ˆå®šä¹‰ä¸”å›ºå®šçš„ã€‚è™½ç„¶ç»„åˆæ€§å¼ºï¼Œä½†**å…¶èƒ½åŠ›å’ŒèŒƒå›´é™åˆ¶äº†VLMæ‰€èƒ½è§„åˆ’çš„ä»»åŠ¡å¤æ‚æ€§å’Œç±»å‹**ã€‚ä¾‹å¦‚ï¼Œè®ºæ–‡ä¸­æåˆ°æ— æ³•ç›´æ¥æµ‹é‡æ‘©æ“¦ç³»æ•°ï¼Œåªèƒ½æä¾›åŸå§‹åŠ›æ•°æ®è®©ç”¨æˆ·è‡ªè¡Œè®¡ç®—ã€‚\n- **VLMçš„å¯é æ€§é—®é¢˜**ï¼šå°½ç®¡é€šè¿‡ç³»ç»Ÿæç¤ºè¯ï¼ˆ`ğ’®`ï¼‰å·¥ç¨‹æ¥ç¼“è§£ï¼Œä½†VLMä»å¯èƒ½å­˜åœ¨**å¹»è§‰æˆ–æ¨ç†é”™è¯¯**ï¼Œå¯¼è‡´ç”Ÿæˆä¸åˆç†æˆ–ä¸å®‰å…¨çš„è¡Œä¸ºæ ‘ã€‚ç³»ç»Ÿçš„ç¨³å¥æ€§ä¾èµ–äºåº•å±‚å¤§æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚\n- **ç¼ºä¹å…¨é¢çš„ä»¿çœŸä¿çœŸåº¦é‡åŒ–è¯„ä¼°**ï¼šè®ºæ–‡å±•ç¤ºäº†å‚æ•°ä¼°è®¡çš„å‡†ç¡®æ€§ï¼ˆå¦‚è´¨é‡ã€é«˜åº¦ï¼‰ï¼Œä½†**æœªå¯¹æ„å»ºçš„æ•°å­—å­ªç”Ÿï¼ˆDigital Twinï¼‰è¿›è¡Œç³»ç»Ÿæ€§çš„ä»¿çœŸ-ç°å®ä¸€è‡´æ€§éªŒè¯**ï¼Œä¾‹å¦‚åœ¨ä»¿çœŸä¸­æ‰§è¡Œæ§åˆ¶ç­–ç•¥å¹¶å¯¹æ¯”çœŸå®è½¨è¿¹çš„è¯¯å·®ã€‚\n- **åœºæ™¯é™æ€å‡è®¾**ï¼šç³»ç»Ÿå‡è®¾å·¥ä½œç©ºé—´åœ¨æœºå™¨äººæ‰§è¡ŒæœŸé—´æ˜¯é™æ€çš„ï¼Œä¸”æœºå™¨äººæ˜¯å”¯ä¸€èƒ½å¤Ÿæ”¹å˜åœºæ™¯çš„å®ä½“ã€‚è¿™**é™åˆ¶äº†å…¶åœ¨åŠ¨æ€æˆ–äººæœºåä½œåœºæ™¯ä¸­çš„åº”ç”¨**ã€‚\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\né’ˆå¯¹ä¸Šè¿°å±€é™æ€§ï¼Œè®ºæ–‡åœ¨ç»“è®ºéƒ¨åˆ†ä¹Ÿå±•æœ›äº†æœªæ¥çš„å·¥ä½œï¼Œä¸»è¦æ”¹è¿›æ–¹å‘åŒ…æ‹¬ï¼š\n\n- **é›†æˆè§†è§‰ä½å§¿ä¼°è®¡**ï¼š**æ”¾æ¾å¯¹å·²çŸ¥ä½å§¿çš„å‡è®¾**ï¼Œé€šè¿‡é›†æˆå¦‚FoundationPoseç­‰6Dä½å§¿ä¼°è®¡æ¨¡å‹ï¼Œç›´æ¥ä»RGB-Dç›¸æœºæ•°æ®ä¸­å®æ—¶è·å–ç‰©ä½“ä½å§¿ï¼Œå®ç°å®Œå…¨åŸºäºè§†è§‰çš„è‡ªä¸»æ„ŸçŸ¥-è§„åˆ’é—­ç¯ã€‚\n- **å®ç°å®æ—¶æ„ŸçŸ¥-æ‰§è¡Œé—­ç¯**ï¼šå°†è§†è§‰åé¦ˆå¼•å…¥è¡Œä¸ºæ ‘çš„æ‰§è¡Œå¾ªç¯ä¸­ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿ**æ ¹æ®å®æ—¶æ„ŸçŸ¥ä¿¡æ¯ï¼ˆå¦‚æ¥è§¦çŠ¶æ€ã€ç‰©ä½“ä½ç½®å˜åŒ–ï¼‰åŠ¨æ€è°ƒæ•´æˆ–é‡æ„è¡Œä¸ºæ ‘**ï¼Œæé«˜åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­çš„é²æ£’æ€§ã€‚\n- **æ‰©å±•åŸå­åŠ¨ä½œé›†ä¸æŠ€èƒ½åº“**ï¼šåœ¨ç°æœ‰ä½å±‚åŸå­åŠ¨ä½œåŸºç¡€ä¸Šï¼Œ**å¼•å…¥æˆ–å­¦ä¹ æ›´é«˜å±‚çš„æŠ€èƒ½åŸºå…ƒ**ï¼ˆå¦‚â€œä¼°è®¡æ‘©æ“¦ç³»æ•°â€ï¼‰ï¼Œå¹³è¡¡çµæ´»æ€§ä¸ä»»åŠ¡æ•ˆç‡ã€‚åŒæ—¶ï¼Œæ¢ç´¢ä»äº¤äº’æ•°æ®ä¸­è‡ªåŠ¨å‘ç°æˆ–å­¦ä¹ æ–°åŠ¨ä½œçš„å¯èƒ½æ€§ã€‚\n- **è¿›è¡Œç³»ç»Ÿçš„æ•°å­—å­ªç”Ÿä¿çœŸåº¦è¯„ä¼°**ï¼šæœªæ¥å·¥ä½œåº”åŒ…å«**ä¸¥æ ¼çš„ä»¿çœŸä¸ç°å®å¯¹æ¯”å®éªŒ**ï¼Œå®šä¹‰é‡åŒ–æŒ‡æ ‡ï¼ˆå¦‚è½¨è¿¹è·Ÿè¸ªè¯¯å·®ã€æ¥è§¦åŠ›é¢„æµ‹è¯¯å·®ã€ç­–ç•¥è¿ç§»æˆåŠŸç‡ï¼‰ï¼Œä»¥å…¨é¢è¯„ä¼°æ‰€æ„å»ºä»¿çœŸæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚\n- **èåˆç”Ÿæˆå¼AIè¿›è¡Œå®Œæ•´ä»¿çœŸæ„å»º**ï¼šä¸è®ºæ–‡ç¬¬II-BèŠ‚æåˆ°çš„ç”Ÿæˆå¼3Dåœºæ™¯æ„å»ºå·¥ä½œï¼ˆå¦‚SceneSynthesis, EmbodiedGenï¼‰ç›¸ç»“åˆï¼Œ**å®ç°ä»è§†è§‰å¤–è§‚åˆ°ç‰©ç†å±æ€§çš„å…¨è‡ªåŠ¨ã€é«˜ä¿çœŸä»¿çœŸç¯å¢ƒç”Ÿæˆ**ï¼Œè¶…è¶Šå½“å‰ä»…å…³æ³¨ç‰©ç†å‚æ•°ä¼°è®¡çš„èŒƒç•´ã€‚\n- **æ¢ç´¢åŠ¨æ€ä¸éç»“æ„åŒ–åœºæ™¯**ï¼šç ”ç©¶åœ¨**è½»å¾®åŠ¨æ€æˆ–å­˜åœ¨å¹²æ‰°çš„ç¯å¢ƒ**ä¸­å¦‚ä½•å®‰å…¨ã€æœ‰æ•ˆåœ°è¿›è¡Œå‚æ•°ä¼°è®¡ï¼Œè¿™å¯èƒ½æ¶‰åŠæ›´å¤æ‚çš„è¡Œä¸ºæ ‘é€»è¾‘ï¼ˆå¦‚æ›´é¢‘ç¹çš„æ¡ä»¶æ£€æŸ¥ã€ä¸­æ–­å¤„ç†ï¼‰ä»¥åŠå¯¹ä¸ç¡®å®šæ€§çš„æ˜¾å¼å»ºæ¨¡ã€‚\n\n**æ€»ç»“**ï¼šè¯¥è®ºæ–‡çš„æ ¸å¿ƒä»·å€¼åœ¨äºæå‡ºäº†ä¸€ä¸ª**æ„å›¾é©±åŠ¨ã€å¯è§£é‡Šä¸”è‡ªé€‚åº”çš„Real2Simæ–°èŒƒå¼**ï¼Œå…¶å±€é™æ€§å¤šæºäºå½“å‰é˜¶æ®µä¸ºä¿è¯ç³»ç»Ÿå¯è¡Œæ€§å’Œå®‰å…¨æ€§æ‰€åšçš„åˆç†ç®€åŒ–ã€‚æœªæ¥çš„æ”¹è¿›æ–¹å‘æ¸…æ™°åœ°æŒ‡å‘å¢å¼ºæ„ŸçŸ¥è‡ªä¸»æ€§ã€é—­ç¯é²æ£’æ€§ã€è¯„ä¼°ä¸¥è°¨æ€§ä»¥åŠä¸æ›´å‰æ²¿çš„ç”Ÿæˆå¼AIæŠ€æœ¯çš„èåˆï¼Œæ—¨åœ¨æœ€ç»ˆå®ç°**é«˜åº¦è‡ªåŠ¨åŒ–ã€é«˜ä¿çœŸä¸”å¯æ‰©å±•çš„ä»¿çœŸæ„å»ºç®¡é“**ã€‚",
      "timestamp": "2026-01-20T17:56:09.099600",
      "thinking": null,
      "is_reasoning": false,
      "parent_qa_id": null
    }
  ],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-13T11:28:46Z",
  "created_at": "2026-01-20T17:49:43.143401",
  "updated_at": "2026-01-20T17:49:43.143411"
}