{
    "id": "2512.01801v3",
    "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
    "authors": [
        "Yunfei Li",
        "Xiao Ma",
        "Jiafeng Xu",
        "Yu Cui",
        "Zhongren Cui",
        "Zhigang Han",
        "Liqun Huang",
        "Tao Kong",
        "Yuxiao Liu",
        "Hao Niu",
        "Wanli Peng",
        "Jingchao Qiao",
        "Zeyu Ren",
        "Haixin Shi",
        "Zhi Su",
        "Jiawen Tian",
        "Yuyang Xiao",
        "Shenyu Zhang",
        "Liwei Zheng",
        "Hang Li",
        "Yonghui Wu"
    ],
    "abstract": "æˆ‘ä»¬æå‡ºGR-RLè¿™ä¸€æœºå™¨äººå­¦ä¹ æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿå°†é€šç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥è½¬åŒ–ä¸ºæ“…é•¿é•¿æ—¶åºçµå·§æ“ä½œçš„ä¸“ä¸šåŒ–ç­–ç•¥ã€‚ç°æœ‰è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥çš„æ ¸å¿ƒå‡è®¾æ˜¯äººç±»æ¼”ç¤ºå…·æœ‰æœ€ä¼˜æ€§ï¼Œä½†æˆ‘ä»¬æŒ‡å‡ºåœ¨é«˜åº¦çµå·§ä¸”éœ€è¦ç²¾ç¡®æ§åˆ¶çš„æ“ä½œä»»åŠ¡ä¸­ï¼Œäººç±»æ¼”ç¤ºå­˜åœ¨å™ªå£°ä¸”å¹¶éæœ€ä¼˜ã€‚GR-RLè®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹æ¼”ç¤ºæ•°æ®è¿›è¡Œç­›é€‰ã€å¢å¼ºä¸ä¼˜åŒ–ã€‚é¦–å…ˆï¼ŒGR-RLå­¦ä¹ è§†è§‰-è¯­è¨€æ¡ä»¶ä¸‹çš„ä»»åŠ¡è¿›åº¦è¯„ä¼°å‡½æ•°ï¼Œå¯¹æ¼”ç¤ºè½¨è¿¹è¿›è¡Œç­›é€‰ï¼Œä»…ä¿ç•™å¯¹è¿›åº¦æœ‰ç§¯æè´¡çŒ®çš„çŠ¶æ€è½¬ç§»ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯æ˜ç›´æ¥åº”ç”¨ç¨€ç–å¥–åŠ±çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ‰€å¾—Qå€¼å¯ä½œä¸ºé²æ£’çš„è¿›åº¦å‡½æ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥å½¢æ€å¯¹ç§°å¢å¼ºæŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†GR-RLçš„æ³›åŒ–èƒ½åŠ›ä¸æ€§èƒ½ã€‚æœ€åï¼Œä¸ºä½¿è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥åœ¨é«˜ç²¾åº¦æ§åˆ¶ä¸­æ›´å¥½åœ°ä¸éƒ¨ç½²è¡Œä¸ºå¯¹é½ï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒæ½œåœ¨ç©ºé—´å™ªå£°é¢„æµ‹å™¨å®ç°åœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡è¿™ä¸€æµç¨‹ï¼ŒGR-RLæˆä¸ºé¦–ä¸ªèƒ½è‡ªä¸»å®Œæˆç©¿é‹å¸¦ä»»åŠ¡çš„å­¦ä¹ å‹ç­–ç•¥â€”â€”è¯¥ä»»åŠ¡éœ€å°†é‹å¸¦ä¾æ¬¡ç©¿è¿‡å¤šä¸ªé‹çœ¼ï¼Œè¦æ±‚é•¿æ—¶åºæ¨ç†èƒ½åŠ›ã€æ¯«ç±³çº§ç²¾åº¦åŠæŸ”æ€§ä½“äº¤äº’æ§åˆ¶ï¼Œå…¶æˆåŠŸç‡å¯è¾¾83.3%ã€‚æˆ‘ä»¬å¸Œæœ›GR-RLèƒ½ä¸ºé€šç”¨æœºå™¨äººåŸºç¡€æ¨¡å‹å‘å¯é ç°å®åœºæ™¯ä¸“å®¶åŒ–è½¬å‹æä¾›æ–°çš„æ€è·¯ã€‚",
    "url": "https://arxiv.org/abs/2512.01801v3",
    "html_url": "https://arxiv.org/html/2512.01801v3",
    "html_content": "\\correspondence\n\\contribution\n[]Full Author List in\nContributions and Acknowledgments\nGR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation\nByteDance Seed\nxiao.ma@bytedance.com\nAbstract\nWe present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation.\nAssuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal.\nGR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning.\nFirst, GR-RL learns a vision-language-conditioned\ntask progress\n, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting\nQ\nQ\n-values can be treated as a robust progress function.\nNext, we introduce\nmorphological symmetry augmentation\nthat greatly improves the generalization and performance of GR-RL.\nLastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor.\nWith this pipeline, GR-RL isâ€”to our knowledgeâ€”the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction.\nWe hope GR-RL provides a step toward enabling generalist robot foundation models to specialize into reliable real-world experts.\n\\checkdata\n[Project Page]\nhttps://seed.bytedance.com/gr_rl\n1\nIntroduction\nThe emergence of large-scale vision-language-action (VLA) models has rapidly advanced the ambition of building generalist robotic agents capable of performing a broad range of tasks given visual observations and free-form natural language instructions.\nRecent systems\n[\n9\n,\n10\n,\n8\n,\n56\n,\n7\n,\n11\n,\n4\n]\nhave demonstrated impressive generalization across objects, environments, and semantic concepts, suggesting that robotics can benefit from the scaling laws that propelled progress in vision-language models.\nHowever, being general is not equivalent to being reliable, and current VLA policies still fall short in two fundamental aspects for real-world deployment: (1)\nDexterity with precision\nâ€“ millimeter-level control over deformable objects remains unsolved.\n(2)\nLong-horizon robustness\nâ€“ errors accumulate over steps, and it gets worse when coupled with high-precision dexterous manipulation.\nConsider the task of threading the shoelaces: (1) the robot should be dexterous enough to handle the deformable objects, including both the shoelace and the shoe; (2) the robot needs millimeter-level control precision to thread the shoelace into the eyelets; (3) the robot also needs long-horizon manipulation capabilities to handle diverse and unexpected scenarios. Classic methods tackle shoelacing by motion planning with predefined action primitives and designed patterns\n[\n39\n,\n40\n,\n41\n]\n, and consequently, generalization to unseen configurations, recovering from failures, and other dexterous skills remain an open question. Naive extensions to behavior cloning will result in sub-optimal and limited skills in shoelacing\n[\n42\n]\n.\nOur starting point is GR-3\n[\n12\n]\n, a large-scale VLA policy trained from internet data, robot trajectories, and human demonstrations. Despite its strong generalization capabilities, GR-3 would fail when precision, dexterity, and long-horizon robustness matter. We observe that there are two key bottlenecks: (1)\nsuboptimal human demonstrations\nand (2) the\ndemonstration and inference mismatch\n.\nUnder extreme precise and dexterous manipulation scenarios, human demonstrators would slow down, hesitate, and introduce noisy suboptimal demonstrations to the policy. In addition, during standard offline training, a VLA policy learns to mimic human demonstrators by predicting action chunks of\nfixed lengths\nderived from a sliding window given human demonstrations\n[\n65\n,\n14\n]\n. However, to achieve smooth inference and control, post-smoothing to predicted trajectories (e.g., temporal ensembling\n[\n65\n]\n), asynchronous receding horizon control\n[\n14\n,\n8\n,\n25\n]\n, and other control-level optimizations are often applied. These system-level optimization methods are necessary for the smooth execution of a learning-based policy, while inevitably causing the mismatch between model training and inference.\nFigure 1\n:\nGR-RL performs long-horizon, dexterous, and high-precision manipulation, in the task of shoe lacing, by adopting a multi-stage training pipeline, consisting of 1) offline filtered behavior cloning with learned task progress, 2) simple yet effective action augmentation, 3) online reinforcement learning.\nWe present GR-RL for long-horizon dexterous and precise manipulation.\nGR-RL adopts a multi-stage reinforcement-augmented training pipeline that filters, augments, and reinforces the suboptimal and mismatched human demonstrations.\nFirst, instead of directly running behavior cloning on the entire human demonstration dataset, we initialize the base GR-RL VLA policy by cloning the\nfiltered\ntrajectories. Specifically, we train a critic model on both successful and failed trajectories with offline reinforcement learning (RL)\n[\n17\n]\n.\nGiven a sparse reward at the end of the episode, the predicted value naturally reflects the progress of the task, which we further use to filter only transitions that contribute positively to the progress and discard the rest.\nWe adopt distributional critics and observe that they give much more robust performance under offline sparse reward scenarios.\nNext, initialized from the offline pretrained checkpoints, we perform online reinforcement learning to further explore and fix the failure modes of the base policy. In particular, we achieve this by learning to steer the denoising process towards high-return regions\n[\n59\n]\n.\nLastly, we devise a simple yet effective method to augment the robot actions by mirroring the robot actions and observations, with a flipped text description. Such a scheme drastically improves the overall success rate and generalization capabilities of our policy.\nTo the best of our knowledge, GR-RL is the first learning-based model that can thread the shoelace through multiple eyelets, achieving an overall 83.3% success rate. We hope GR-RL provides a step towards enabling generalist robot foundation models to specialize and be applicable in challenging real-world scenarios.\nFigure 2\n:\nThe GR-RL Model.\nGR-RL adopts a Mixture-of-Transformer (MoT) architecture. It is co-trained on robot vision-language-action trajectories via a flow-matching objective, and Temporal-Difference (TD) errors via distributional reinforcement learning.\n2\nThe GR-RL Model\nGR-RL adopts a Mixture-of-Transformer architecture, consisting of a vision-language-action (VLA) model\nÏ€\nÎ¸\n\\pi_{\\theta}\nand a multi-task critic\nQ\nÏ•\nQ_{\\phi}\nwith a total of 5B parameters.\nPolicy:\nÏ€\nÎ¸\n\\pi_{\\theta}\ncontrols a bi-manual robot with a mobile base by generating a\nk\nk\n-length action chunk\nğš\nt\n=\na\nt\n:\nt\n+\nk\n\\mathbf{a}_{t}=a_{t:t+k}\nconditioned on the input language instruction\nl\nl\n, observation\nğ¨\nt\n\\mathbf{o}_{t}\n, and robot state\nğ¬\nt\n\\mathbf{s}_{t}\n,\ni.e.\n,\nğš\nt\n=\nÏ€\nÎ¸\nâ€‹\n(\nl\n,\nğ¨\nt\n,\nğ¬\nt\n)\n\\mathbf{a}_{t}=\\pi_{\\theta}(l,\\mathbf{o}_{t},\\mathbf{s}_{t})\n.\nFollowing the architecture design of GR-3\n[\n12\n]\n, GR-RL uses Qwen2.5-VL-3B-Instruct\n[\n3\n]\nas the Vision-Language-Model (VLM) backbone, and predicts the action chunks\nğš\nt\n\\mathbf{a}_{t}\nwith an action diffusion transformer (DiT) trained by flow matching objectives\n[\n8\n,\n36\n,\n34\n]\n. Specifically, we follow GR-3 and use only the KV cache from the latter half of the VLM layers for fast inference.\nCritic:\nSimilar to the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\n,\nQ\nÏ•\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nğš\nt\n)\nQ_{\\phi}(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{a}_{t})\nis a causal transformer that evaluates each action. Specifically, we follow\nQ\nQ\n-chunking\n[\n31\n,\n52\n,\n29\n]\nand predict a chunk of\nQ\nQ\n-values for each action chunk\nğš\nt\n\\mathbf{a}_{t}\nand adopt distributional reinforcement learning\n[\n5\n,\n23\n,\n52\n,\n16\n]\n.\nDifferent from unbounded regression-based policy evaluation, distributional critics treat values as a discrete distribution with upper and lower bounds.\nThis naturally captures uncertainty in real-world trajectories.\nUnder sparse reward settings, distributional critics give much stronger robustness than the non-distributional ones. By setting the upper bound to 1 and the lower bound to 0, our learned critic naturally reflects the\nprogress\nof the task, as shown in Fig.\n3\n.\n3\nTraining Recipe\nHuman demonstrations are suboptimal and noisy. In the context of long-horizon dexterous high-precision manipulation, human demonstrators tend to hesitate, make mistakes, and try to finish the task with inconsistent behaviors. In addition, inference-time optimizations, such as whole-body receding horizon control and temporal ensembling\n[\n65\n,\n14\n]\n, introduce further mismatch between training and deployment, which exacerbate the negative effect of suboptimal demonstrations.\nWe introduce a reinforcement-augmented training pipeline to achieve dexterous and precise robotic manipulation from human demonstrations. To prevent the policy from memorizing suboptimal behaviors during supervised learning, we learn a task progress model using offline RL and use it to filter out detrimental data (Sec.\n3.1\n). We then augment the demonstrations to improve the robustness of the offline policy based on the symmetry in bimanual manipulation (Sec.\n3.2\n). Finally, we perform online reinforcement learning, allowing the model to learn from trial-and-error in a closed loop, which mitigates the mismatch between training and deployment and boosts its overall performance (Sec.\n3.3\n).\n3.1\nData Filtering with a Learned Task Progress Evaluator\nFor high-precision manipulation with deformable objects, such as shoe lacing, collecting perfect demonstrations is extremely difficult. Even the trajectories collected by experienced teleoperators contain suboptimal fragments: erroneous attempts, hesitations, etc. Directly imitating all the data would unnecessarily introduce noisy multi-modal actions to the training and lead to a policy with suboptimal performance. However, labeling the suboptimal fragments is non-trivial and might introduce even more subjective and noisy human priors.\nTo identify and filter out suboptimal actions, we propose to learn a task progress model using offline RL. Specifically, we train the critic using TD3+BC\n[\n17\n]\n.\nWe adopt a sparse reward defined as\nr\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nğš\nt\n)\n=\n{\nÎ³\nT\nâˆ’\nt\nâ€‹\nğ•€\nâ€‹\n(\nÏ„\n)\n,\nt\n>\nT\nâˆ’\nk\n,\n0\n,\nt\nâ‰¤\nT\nâˆ’\nk\n,\nr(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{a}_{t})=\\begin{cases}\\gamma^{T-t}\\mathbb{I}(\\tau),&t>T-k,\\\\\n0,&t\\leq T-k,\\end{cases}\n(1)\nwhere\nğ•€\nâ€‹\n(\nâ‹…\n)\n\\mathbb{I}(\\cdot)\nis an indicator function to evaluate whether a trajectory\nÏ„\n\\tau\nis successful or not,\nT\nT\nis the length of the trajectory, and\nÎ³\n\\gamma\nmeans the discount factor.\nSince most of the collected trajectories end in success, we annotate the retry keyframes in each demonstration and create more failed trajectories in hindsight\n[\n1\n]\n.\nSuppose frames\nm\ni\n,\n0\nâ‰¤\ni\n<\nM\nm_{i},0\\leq i<M\nare marked as retry keyframes in a successful trajectory\nÏ„\n0\n:\nT\n\\tau_{0:T}\n, we can augment\nM\nM\nfailed trajectories\nÏ„\n0\n:\nm\ni\n,\n0\nâ‰¤\ni\n<\nM\n\\tau_{0:m_{i}},0\\leq i<M\nin addition to the original successful one. With temporal difference learning over both successful and failed data, the critic\nQ\nÏ•\nQ_{\\phi}\ncould function as a robust task progress evaluator.\nFigure 3\n:\nExamples of learned task progress.\nAfter obtaining a task progress model, we evaluate\nQ\nÏ•\nQ_{\\phi}\nand compute its mean value of the categorical distribution as progress\nÏ\n\\rho\nfor all the transitions in the dataset,\nÏ\nt\nâ‰”\nğš–ğšğšŠğš—\nâ€‹\n(\nQ\nÏ•\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nğš\nt\n)\n)\n.\n\\rho_{t}\\coloneq\\mathtt{mean}(Q_{\\phi}(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{a}_{t})).\n(2)\nAn example of the predicted progress is shown in Fig.\n3\n. We can observe a sudden drop in the progress when the teleoperator makes a mistake.\nWe define a sample\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nğš\nt\n)\n(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{a}_{t})\nat timestep\nt\nt\nas suboptimal if there is a value drop greater than a certain threshold\nÎ´\n\\delta\nin the sequence\nÏ\nt\n:\nt\n+\nk\n\\rho_{t:t+k}\n, and exclude all the suboptimal ones from the dataset for policy learning. We can then train\nÏ€\nÎ¸\n\\pi_{\\theta}\nsimply with behavior cloning using the filtered dataset of higher quality.\n3.2\nImitation Learning with Data Augmentation\nDuring the offline training stage, we apply a simple yet effective\nmorphological symmetry augmentation\nparadigm, which further boosts the policy performance.\nThe augmentation paradigm leverages the morphological symmetry in our bimanual task settings.\nFor image observations\nğ¨\nt\n\\mathbf{o}_{t}\n, we flip all the images horizontally, then swap the images from the left wrist with those from the right wrist. All transformations in proprioception states\nğ¬\nt\n\\mathbf{s}_{t}\nand actions\nğš\nt\n\\mathbf{a}_{t}\nare converted via mirror symmetry in the world frame, and then transformed back to local wrist frames. We also flip the spatial description in the language instructions accordingly, e.g., changing â€œthe hole on the leftâ€ to â€œthe hole on the rightâ€. Empirically, the symmetry data augmentation can effectively enhance the performance of the policy.\n3.3\nOnline Steering for Policy Deployment Alignment\nSystem-level postprocessing is commonly applied to ensure smooth robot motions when deploying chunking policies, such as temporal ensembling and receding horizon control\n[\n14\n,\n65\n]\n.\nHowever, these optimization tricks cause a mismatch between training and deployment: what the policy has seen during training (raw actions) is different from the ones actually being executed during deployment (optimized actions).\nIn the context of dexterous and precise manipulation, the mismatch becomes non-negligible.\nTo adapt to the discrepancy, we find it crucial for the model to explore and improve itself via closed-loop online interactions with\naligned\nactions.\nPerforming online RL in long-horizon, precise manipulation tasks remains non-trivial, especially in exploration. Since the task requires millimeter precision to complete, adding noise to wrist poses or joint positions hardly leads to success.\nWe instead perform structured exploration in a latent space and steer the trained flow policy\n[\n59\n]\n. Specifically, we add a\nnoise predictor\nÏ€\nÎ¸\nâ€²\n\\pi_{\\theta^{\\prime}}\nafter the shared VLM backbone to predict the initial noise\nÏµ\nt\n\\mathbf{\\epsilon}_{t}\nfor the action DiT.\nThe number of trainable parameters in\nÏ€\nÎ¸\nâ€²\n\\pi_{\\theta^{\\prime}}\nis 51.5M.\nTo avoid generating arbitrary actions from noise out of the offline training distribution, we penalize the noise predictor when its output diverges from the original normal distribution beyond a certain threshold\nÎ²\n\\beta\n. Following\n[\n59\n]\n, we also distill a\nQ\nQ\nfunction over noise space\nQ\nÏ•\nâ€²\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nÏµ\nt\n)\nQ_{\\phi^{\\prime}}(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{\\epsilon}_{t})\nto avoid back-propagating through the flow model during policy optimization. The critic in the original action space\nQ\nÏ•\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nğš\nt\n)\nQ_{\\phi}(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{a}_{t})\nis trained via standard TD3. The online training objectives for the noise transformer and the critic in noise space are as follows:\nâ„’\nâ€‹\n(\nÏ€\nÎ¸\nâ€²\n)\n=\nğ”¼\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n)\nâˆ¼\nğ’Ÿ\nâ€‹\n[\nâˆ’\nQ\nÏ•\nâ€²\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nÏµ\nt\n)\n+\nc\nâ€‹\nmax\nâ¡\n(\n1\n2\nâ€‹\nâ€–\nÏµ\nt\nâ€–\n2\nâˆ’\nÎ²\n,\n0\n)\n]\n,\nÏµ\nt\nâˆ¼\nÏ€\nÎ¸\nâ€²\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n)\n,\n\\mathcal{L}(\\pi_{\\theta^{\\prime}})=\\mathbb{E}_{(\\mathbf{o}_{t},l,\\mathbf{s}_{t})\\sim\\mathcal{D}}\\left[-Q_{\\phi^{\\prime}}(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{\\epsilon}_{t})+c\\max(\\frac{1}{2}\\|\\mathbf{\\epsilon}_{t}\\|^{2}-\\beta,0)\\right],\\mathbf{\\epsilon}_{t}\\sim\\pi_{\\theta^{\\prime}}(\\mathbf{o}_{t},l,\\mathbf{s}_{t}),\n(3)\nâ„’\nâ€‹\n(\nQ\nÏ•\nâ€²\n)\n=\nğšŒğš›ğš˜ğšœğšœ\nâ€‹\n_\nâ€‹\nğšğš—ğšğš›ğš˜ğš™ğš¢\nâ€‹\n(\nQ\nÏ•\nâ€²\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nÏµ\nt\n)\n,\nQ\nÏ•\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n,\nÏ€\nÎ¸\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n|\nÏµ\nt\n)\n)\n)\n,\nÏµ\nt\nâˆ¼\n{\nğ’©\nâ€‹\n(\nğŸ\n,\nğŸ\n)\nw.p.\nâ€‹\n0.5\n,\nÏ€\nÎ¸\nâ€²\nâ€‹\n(\nğ¨\nt\n,\nl\n,\nğ¬\nt\n)\notherwise\n.\n\\mathcal{L}(Q_{\\phi^{\\prime}})=\\mathtt{cross\\_entropy}\\left(Q_{\\phi^{\\prime}}(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\mathbf{\\epsilon}_{t}),Q_{\\phi}(\\mathbf{o}_{t},l,\\mathbf{s}_{t},\\pi_{\\theta}(\\mathbf{o}_{t},l,\\mathbf{s}_{t}|\\mathbf{\\epsilon}_{t}))\\right),\\mathbf{\\epsilon}_{t}\\sim\\begin{cases}\\mathcal{N}(\\mathbf{0},\\mathbf{1})&\\text{{w.p.} }0.5,\\\\\n\\pi_{\\theta^{\\prime}}(\\mathbf{o}_{t},l,\\mathbf{s}_{t})&\\textrm{otherwise}.\\end{cases}\n(4)\nDifferent from the original implementation\n[\n59\n]\n, to ensure a good coverage on the noise space when distilling\nQ\nÏ•\nâ€²\nQ_{\\phi^{\\prime}}\n, we sample the input noise from the original normal distribution with 0.5 probability (\nw.p.\n0.5), and from the noise predictor otherwise.\nFor sample-efficient offline to online adaptation, we maintain an off-policy buffer and an on-policy buffer, and sample batches from them evenly. Before training starts, we warm up the off-policy buffer with online rollouts of offline-trained checkpoints, similar to Warm-start RL\n[\n66\n]\n. We intentionally choose not to mix teleoperated trajectories into the buffer to prevent the policy from training on mismatched dynamics. The on-policy buffer only stores trajectories generated from the two most recent checkpoints, and the stale data is pushed into the off-policy buffer.\n4\nRobot & System\nThe robot we utilize to verify GR-RL is\nByteMini-v2\n, as illustrated in Fig.\n4\n. ByteMini-v2 is a wheeled mobile manipulation robot equipped with 7-DoF dual robotic arms, featuring a unique wrist spherical joint design\n[\n58\n]\n. The operational dexterity, stability, consistency, and usability of ByteMini-v1 robots have been fully validated in the work of GR-3\n[\n12\n]\n. Meanwhile, drawing on the limitations identified during ByteMini-v1 usage in GR-3, we have implemented the following three key design optimizations on ByteMini-v2 robots.\nFigure 4\n:\nThe ByteMini-v2 Robot.\nWe show the robot specifications in terms of sensors, DoFs and electronic devices.\nHigher Load for Manipulation\nThe maximum load ability for the 7-DoF arm is limited by the maximum output torque of the elbow actuator. We update the elbow actuator from peak output torque 17â€‰Nm to 35â€‰Nm. As a result, the peak load of 7-DoF arms on ByteMini-v2 robots is increased from 1.4â€‰kg to 3.15â€‰kg, which will significantly expand the robotâ€™s operational capabilities.\nEnhanced Mobility in Confined Spaces\nThe projected area of the ByteMini-v1 chassis is 500â€‰mm x 720â€‰mm. And now for improving the mobility in confined spaces, the projected area of ByteMini-v2 robot is reduced to 450â€‰mm x 650â€‰mm respectively. Meanwhile, the design of the servo steering wheels on the mobile platform has also been optimized accordingly, enabling the synchronous adjustment of the steering wheelsâ€™ motion in both yaw and pitch directions\n[\n61\n]\nand thereby enhancing the robotâ€™s rapid direction-changing capability in confined spaces.\nHigher Polished Robot Design and Enhanced Usability\nAs shown in Fig.\n4\n, the ByteMini-v2 robot has been designed with enhanced refinement, featuring an ID and proper encapsulation of exposed electrical wiring harnesses. The position of the portable monitor has been adjusted from the robotsâ€™ chassis to the shoulder, in order to achieve a better user experience.\n5\nExperiments\nTask Description\nWe present GR-RL in a shoe lacing task, a challenging scenario featuring long-horizon, dexterous and precise manipulation. The observations are composed of three views of RGB images, proprioception states, and language instructions.\nIn model inference, we incorporate a trajectory optimization module that imposes constraints on jerk and temporal continuity to refine the predicted action chunks.\nDuring RL training, we adopt a binary sparse reward setting, meaning that a positive reward of 1 is obtained only when the shoelace is threaded through the correct eyelet and put down on the table completely.\nMain Results\nFigure 5\n:\nLeft\n: the success rate of our multi-stage training recipe. Data filtering, mirror augmentation, and online tuning all contribute to the final performance.\nRight\n: the binary success signal per episode (dots) and the moving average of success rate (curve) during online finetuning. The performance increases rapidly after an offline-to-online adaptation phase.\nWe report the effect of our multi-stage training recipe in Fig.\n5\n. Our base model GR-3, trained with behavior cloning over all the human teleoperated data, achieves a success rate of 45.7%. After filtering the data with our learned task progress model, the success rate is boosted to 61.6%. It highlights the importance of the data cleaning mechanism when learning precise and long-horizon manipulation. With symmetry data augmentation, the success rate of a filtered behavior cloning policy is further increased to 72.7%.\nFigure 6\n:\nDetailed success rates of different models for completing intermediate stages. The height of each hatched area denotes the decrease in success rate from the previous stage to the current stage.\nThe â€œFiltered BC + Aug.â€ model serves as the starting point of online steering RL. To better align the offline-trained critic to the distribution of model rollout, we collect 673 trajectories generated by the offline model, then freeze the VLM backbone and continue to train the critic heads\nQ\nÏ•\nQ_{\\phi}\n,\nQ\nÏ•\nâ€²\nQ_{\\phi^{\\prime}}\nusing these data. These rollout trajectories are also populated into the off-policy replay buffer in online RL to stabilize training. We then tune both the noise predictor\nÏ€\nÎ¸\nâ€²\n\\pi_{\\theta^{\\prime}}\nand the two critic heads for 50 optimization steps once we gather 12 new episodes online. The moving average of the success rate over a window size of 24 is shown as the curve on the right side of Fig.\n5\n, and the binary success signals for all online episodes are represented as dots. In the first few iterations, the success rate decreases due to the distribution shift from offline to online RL. Later, the success rate rapidly recovers and grows beyond the offline performance to over 90%. The checkpoint at 500 online training steps is used for evaluation, and achieves a success rate of 83.3%.\nTo better understand the failure modes for different models in this long-horizon task, we evaluate whether the models succeed at several critical stages, including picking up the correct shoelace, threading the shoelace into the correct eyelet, handing it over to another gripper, and pulling the shoelace tight. The detailed success rate is illustrated in Fig.\n6\n. The colored area denotes the success rate for completing each stage. The hatched area represents the decrease in the success rate compared to the previous stage. Data filtering and online RL can largely reduce the failure during threading. Data augmentation improves the model performance in all stages, although with a smaller magnitude.\nAblation on the Progress Evaluator\nWe validate the effectiveness of the RL-based progress evaluator by comparing it with a regression variant. The regression baseline is trained by directly regressing the temporal progress\nt\nT\n\\frac{t}{T}\nof each state\nğ¬\nt\n\\mathbf{s}_{t}\nin successful trajectories.\nAs shown in Fig.\n3\n, the regression-based predictor tends to overly smooth the progress prediction. It makes reasonable predictions in normal cases but is less sensitive to subtle failure (oftentimes only millimeters from success), such as failing to pull out the shoelace or imprecise insertion. Also, the regression-based predictor is not good at capturing transitions with long-term effects. There is a significant value jump in the predictions of GR-RL when the robot intentionally puts down the shoelace to adjust the grasping pose, but the regression-based prediction is almost flat during the adjustment.\nWe also train a non-distributional critic with the same TD3+BC algorithm to compare with our distributional critic. The models are evaluated over a high-quality successful trajectory, as illustrated in Fig.\n7\n. Due to the long horizon and binary sparse reward in our setting, the non-distributional critic suffers from severe over-estimation, especially in earlier parts of trajectories where the reward supervision signal is weak. The value prediction of our distributional critic falls in a predefined range, thus converging to a reasonable scale more robustly and demonstrating better alignment with the true temporal order.\nFigure 7\n:\nComparison of progress prediction by distributional vs. non-distributional critics. Non-distributional critics are unbounded in the output range, and fail to reflect the positive progress in a successful trajectory.\nVisualization of the Learned Behaviors\nGR-RL demonstrates robust behaviors in various cases. It can handle shoes with different colors and sizes, as shown in Fig.\n8\n. The model automatically retries when the shoelace accidentally drops down (Fig.\n8(b)\n) or when the shoelace misses the eyelet (Fig.\n8(c)\n). The model can actively manipulate the scene to make the task easier to solve. In Fig.\n8(d)\n, the initial grasping point is far from the tip of the shoelace. The model decides to drop the shoelace on top of the deformable shoe and regrasp closer to the tip before threading. In Fig.\n8(e)\n, the model first reorients the shoe from the left side to straighten it, then starts threading. Similarly, for a shoe initially positioned on the far side of the table (Fig.\n8(f)\n), the robot can pull it near, adjust the position of the shoelace, and then complete the task. In cases where the two ends of the shoelace are crossed, and the end that should be grasped is underneath (Fig.\n8(g)\n), the model can identify the correct one and pull it out.\n(a)\nThread shoelace for a shoe with a different color.\n(b)\nRegrasp the shoelace when it drops.\n(c)\nRe-attempt when the shoelace is not threaded precisely through the eyelet.\n(d)\nAdjust the grasp pose intentionally on the surface of the shoe.\n(e)\nReorient the shoe before threading.\n(f)\nAdjust the positions of the shoe and the shoelace before threading.\n(g)\nPull out the shoelace underneath another one from the crossed laces.\nFigure 8\n:\nRobust behavior of GR-RL in various cases.\n6\nRelated Work\nGeneralist Robotic Foundation Policy\nBuilding generalist robotic foundation manipulation policies is a long-standing challenge for robotics research and applications\n[\n9\n,\n10\n,\n27\n,\n8\n,\n25\n,\n7\n,\n57\n,\n32\n,\n33\n,\n50\n,\n6\n,\n55\n,\n4\n]\n.\nRecent advances in building vision-language-action (VLA) models advocate adapting the vision-language models (VLMs) pretrained with web-scale data to robotic actions by adding the action modality\n[\n47\n,\n57\n,\n27\n,\n8\n,\n48\n,\n25\n,\n15\n,\n60\n,\n37\n,\n49\n,\n30\n]\n.\nThe core idea is to leverage large-scale real-world robot trajectories collected by human teleoperators and generalize to potentially novel scenes and tasks.\nGR-RL sits upon the prior success of GR-3\n[\n12\n]\n, a generalist policy co-trained with web-scale data and human demonstrations. GR-RL further improves GR-3 by filtering high-quality data, augmenting actions, and online real-world RL, enabling it to perform long-horizon dexterous and precise manipulation.\nReal-World Reinforcement Learning\nA central limitation of pure imitation learning is its susceptibility to compounding errors and its inability to exceed the performance of demonstrations. To address these issues, a significant body of work explores online data collection and real-world reinforcement learning (RL) to improve manipulation robustness beyond supervised training\n[\n31\n,\n28\n,\n45\n,\n43\n,\n2\n,\n54\n,\n26\n,\n53\n,\n44\n]\n.\nIn the context of VLAs, some recent works aim to perform policy improvement via on-policy RL\n[\n51\n,\n38\n,\n35\n,\n13\n,\n63\n]\nin simulation. However, transferring their success to real-world scenarios remains difficult because real-world interactions are sample inefficient and noisy.\nAnother line of work learns world models and performs on-policy RL interacting with the learned world model\n[\n18\n,\n19\n,\n20\n,\n62\n,\n21\n,\n22\n,\n67\n,\n46\n]\n. World models alleviate the issue for real-robot interaction, but introduce further issues given inaccurate visual predictions. GR-RL follows\n[\n43\n,\n53\n,\n44\n,\n64\n]\nand focuses on real-world off-policy RL. This allows us to efficiently utilize past trajectories and improve the sample efficiency.\nTreating the noisy real-world reward as a distribution, we show that distributional critics significantly improve the robustness compared with standard regression-based critic models. Concurrent to our work,\nÏ€\n0.6\nâˆ—\n\\pi^{*}_{0.6}\npresents a real-world RL pipeline for high-precision manipulation\n[\n24\n]\n. Similar to\nÏ€\n0.6\nâˆ—\n\\pi^{*}_{0.6}\n, we both adopt distributional critics that learn the progress of the task. However, instead of performing advantage-conditioned denoising, we directly perform filtered behavior cloning and also observe a strong performance boost.\nGiven the stronger base offline policy, it helps reduce the search space during online exploration.\nWe hope GR-RL reveals certain insights to the community on building capable specialist agents from generalist policies and helps to push the boundaries of deployable robotics research.\n7\nLimitations & Conclusions\nLimitations\nDespite the strong performance of GR-RL in long-horizon high-precision dexterous tasks, it still has clear limitations.\nOne of the major issues of our current pipeline is the behavior-drifting problem. Given a sparse and noisy reward, our policy behavior could be unstable during online RL. This is possibly due to the limited capacity of the lightweight noise predictor, or the challenging credit assignment issue in the large latent action space.\nFurthermore, distilling the improved policy into the base VLA could be a potential direction for obtaining both capable and general manipulation policies.\nWe leave these issues for future study.\nConclusion\nWe introduce GR-RL, a robotic learning framework for building specialist VLA policies for long-horizon dexterous and precise manipulation. The key insight of GR-RL is that the mismatch between data collection and policy inference needs online alignment.\nGR-RL learns an RL-based evaluator by treating the critic value learned from sparse reward as task progress prediction, and uses it to filter high-quality transitions to train a robust base policy. During this process, we also introduce a simple yet effective morphological symmetry augmentation method to improve the overall performance. Last, we perform online RL that aligns the rollout behavior with our training signals.\nTo the best of our knowledge, GR-RL is the first learning-based policy capable of lacing up shoes. We hope GR-RL can be a small step towards capable real-world specialist robot policies.\nReferences\nAndrychowicz et al. [2017]\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba.\nHindsight experience replay.\nIn I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n, volume 30, 2017.\nAnkile et al. [2025]\nLars Ankile, Zhenyu Jiang, Rocky Duan, Guanya Shi, Pieter Abbeel, and Anusha Nagabandi.\nResidual off-policy rl for finetuning behavior cloning policies.\narXiv preprint arXiv:2509.19301\n, 2025.\nBai et al. [2025]\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al.\nQwen2.5-vl technical report.\narXiv preprint arXiv:2502.13923\n, 2025.\nBarreiros et al. [2025]\nJose Barreiros, Andrew Beaulieu, Aditya Bhat, Rick Cory, Eric Cousineau, Hongkai Dai, Ching-Hsin Fang, Kunimatsu Hashimoto, Muhammad Zubair Irshad, Masha Itkina, et al.\nA careful examination of large behavior models for multitask dexterous manipulation.\narXiv preprint arXiv:2507.05331\n, 2025.\nBellemare et al. [2017]\nMarc G Bellemare, Will Dabney, and RÃ©mi Munos.\nA distributional perspective on reinforcement learning.\nIn\nInternational conference on machine learning\n, pages 449â€“458. PMLR, 2017.\nBharadhwaj et al. [2024]\nHomanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, and Vikash Kumar.\nRoboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 4788â€“4795. IEEE, 2024.\nBjorck et al. [2025]\nJohan Bjorck, Fernando CastaÃ±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al.\nGr00t n1: An open foundation model for generalist humanoid robots.\narXiv preprint arXiv:2503.14734\n, 2025.\nBlack et al. [2024]\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al.\nÏ€\n\\pi\n0: A vision-language-action flow model for general robot control.\narXiv preprint arXiv:2410.24164\n, 2024.\nBrohan et al. [2022]\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al.\nRt-1: Robotics transformer for real-world control at scale.\narXiv preprint arXiv:2212.06817\n, 2022.\nBrohan et al. [2023]\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al.\nRt-2: Vision-language-action models transfer web knowledge to robotic control.\narXiv preprint arXiv:2307.15818\n, 2023.\nCheang et al. [2024]\nChi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al.\nGr-2: A generative video-language-action model with web-scale knowledge for robot manipulation.\narXiv preprint arXiv:2410.06158\n, 2024.\nCheang et al. [2025]\nChilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al.\nGr-3 technical report.\narXiv preprint arXiv:2507.15493\n, 2025.\nChen et al. [2025]\nKang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, et al.\nOnline rl fine-tuning for flow-based vision-language-action models.\narXiv preprint arXiv:2510.25889\n, 2025.\nChi et al. [2024]\nCheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song.\nDiffusion policy: Visuomotor policy learning via action diffusion.\nThe International Journal of Robotics Research\n, 2024.\nDoshi et al. [2024]\nRia Doshi, Homer Walke, Oier Mees, Sudeep Dasari, and Sergey Levine.\nScaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation.\narXiv preprint arXiv:2408.11812\n, 2024.\nFarebrother et al. [2024]\nJesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali TaÃ¯ga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, et al.\nStop regressing: Training value functions via classification for scalable deep rl.\narXiv preprint arXiv:2403.03950\n, 2024.\nFujimoto and Gu [2021]\nScott Fujimoto and Shixiang Shane Gu.\nA minimalist approach to offline reinforcement learning.\nAdvances in neural information processing systems\n, 34:20132â€“20145, 2021.\nHafner et al. [2019]\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.\nDream to control: Learning behaviors by latent imagination.\narXiv preprint arXiv:1912.01603\n, 2019.\nHafner et al. [2020]\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba.\nMastering atari with discrete world models.\narXiv preprint arXiv:2010.02193\n, 2020.\nHafner et al. [2023]\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.\nMastering diverse domains through world models.\narXiv preprint arXiv:2301.04104\n, 2023.\nHansen et al. [2022]\nNicklas Hansen, Xiaolong Wang, and Hao Su.\nTemporal difference learning for model predictive control.\narXiv preprint arXiv:2203.04955\n, 2022.\nHansen et al. [2023]\nNicklas Hansen, Hao Su, and Xiaolong Wang.\nTd-mpc2: Scalable, robust world models for continuous control.\narXiv preprint arXiv:2310.16828\n, 2023.\nHessel et al. [2018]\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.\nRainbow: Combining improvements in deep reinforcement learning.\nIn\nProceedings of the AAAI conference on artificial intelligence\n, volume 32, 2018.\nIntelligence et al. [2025a]\nPhysical Intelligence, Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, and Zhiyuan Zhou.\nÏ€\nâˆ—\n\\pi^{*}\n0.6: a vla that learns from experience.\narXiv preprint arXiv:2511.14759\n, 2025a.\nIntelligence et al. [2025b]\nPhysical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al.\nÏ€\n\\pi\n0.5: a vision-language-action model with open-world generalization.\narXiv preprint arXiv:2504.16054\n, 2025b.\nKalashnikov et al. [2018]\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al.\nScalable deep reinforcement learning for vision-based robotic manipulation.\nIn\nConference on robot learning\n, pages 651â€“673. PMLR, 2018.\nKim et al. [2024]\nMoo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al.\nOpenvla: An open-source vision-language-action model.\narXiv preprint arXiv:2406.09246\n, 2024.\nLevine et al. [2016]\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.\nEnd-to-end training of deep visuomotor policies.\nJournal of Machine Learning Research\n, 17(39):1â€“40, 2016.\nLi et al. [2025a]\nGe Li, Dong Tian, Hongyi Zhou, Xinkai Jiang, Rudolf Lioutikov, and Gerhard Neumann.\nTOP-ERL: Transformer-based off-policy episodic reinforcement learning.\nIn\nThe Thirteenth International Conference on Learning Representations\n, 2025a.\nURL\nhttps://openreview.net/forum?id=N4NhVN30ph\n.\nLi et al. [2024a]\nQixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al.\nCogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation.\narXiv preprint arXiv:2411.19650\n, 2024a.\nLi et al. [2025b]\nQiyang Li, Zhiyuan Zhou, and Sergey Levine.\nReinforcement learning with action chunking.\narXiv preprint arXiv:2507.07969\n, 2025b.\nLi et al. [2023]\nXinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al.\nVision-language foundation models as effective robot imitators.\narXiv preprint arXiv:2311.01378\n, 2023.\nLi et al. [2024b]\nXinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu.\nTowards generalist robot policies: What matters in building vision-language-action models.\narXiv preprint arXiv:2412.14058\n, 2024b.\nLipman et al. [2022]\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.\nFlow matching for generative modeling.\narXiv preprint arXiv:2210.02747\n, 2022.\nLiu et al. [2025]\nJijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang.\nWhat can rl bring to vla generalization? an empirical study.\narXiv preprint arXiv:2505.19789\n, 2025.\nLiu [2022]\nQiang Liu.\nRectified flow: A marginal preserving approach to optimal transport.\narXiv preprint arXiv:2209.14577\n, 2022.\nLiu et al. [2024]\nSongming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu.\nRdt-1b: a diffusion foundation model for bimanual manipulation.\narXiv preprint arXiv:2410.07864\n, 2024.\nLu et al. [2025]\nGuanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang.\nVla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning.\narXiv preprint arXiv:2505.18719\n, 2025.\nLuo and Demiris [2023]\nHaining Luo and Yiannis Demiris.\nBi-manual robot shoe lacing.\nIn\n2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n, pages 8772â€“8775, 2023.\n10.1109/IROS55552.2023.10341934\n.\nLuo and Demiris [2024]\nHaining Luo and Yiannis Demiris.\nBenchmarking and simulating bimanual robot shoe lacing.\nIEEE Robotics and Automation Letters\n, 2024.\nLuo and Demiris [2025]\nHaining Luo and Yiannis Demiris.\nTsl: Tracking deformable linear objects for bimanual shoe lacing.\nIEEE Robotics and Automation Letters\n, 2025.\nLuo et al. [2025]\nHaining Luo, Rodrigo ChacÃ³n Quesada, Fernando EstÃ©vez Casado, Nico Lingg, and Yiannis Demiris.\nInterface matters: Comparing first and third-person perspective interfaces for bi-manual robot behavioural cloning.\nIn\n2025 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 7385â€“7391. IEEE, 2025.\nLuo et al. [2024a]\nJianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, and Sergey Levine.\nSerl: A software suite for sample-efficient robotic reinforcement learning.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 16961â€“16969. IEEE, 2024a.\nLuo et al. [2024b]\nJianlan Luo, Charles Xu, Jeffrey Wu, and Sergey Levine.\nPrecise and dexterous robotic manipulation via human-in-the-loop reinforcement learning.\narXiv preprint arXiv:2410.21845\n, 2024b.\nLv et al. [2025]\nLei Lv, Yunfei Li, Yu Luo, Fuchun Sun, Tao Kong, Jiafeng Xu, and Xiao Ma.\nFlow-based policy for online reinforcement learning.\narXiv preprint arXiv:2506.12811\n, 2025.\nMa et al. [2021]\nXiao Ma, Siwei Chen, David Hsu, and Wee Sun Lee.\nContrastive variational reinforcement learning for complex observations.\nIn\nConference on robot learning\n, pages 959â€“972. PMLR, 2021.\nOâ€™Neill et al. [2024]\nAbby Oâ€™Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al.\nOpen x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 6892â€“6903. IEEE, 2024.\nPertsch et al. [2025]\nKarl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine.\nFast: Efficient action tokenization for vision-language-action models.\narXiv preprint arXiv:2501.09747\n, 2025.\nQu et al. [2025]\nDelin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al.\nSpatialvla: Exploring spatial representations for visual-language-action model.\narXiv preprint arXiv:2501.15830\n, 2025.\nReuss et al. [2024]\nMoritz Reuss, Ã–mer ErdinÃ§ YaÄŸmurlu, Fabian Wenzel, and Rudolf Lioutikov.\nMultimodal diffusion transformer: Learning versatile behavior from multimodal goals.\narXiv preprint arXiv:2407.05996\n, 2024.\nSchulman et al. [2017]\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\nProximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347\n, 2017.\nSeo and Abbeel [2024]\nYounggyo Seo and Pieter Abbeel.\nCoarse-to-fine q-network with action sequence for data-efficient robot learning.\narXiv preprint arXiv:2411.12155\n, 2024.\nSeo et al. [2024]\nYounggyo Seo, Jafar UruÃ§, and Stephen James.\nContinuous control with coarse-to-fine reinforcement learning.\narXiv preprint arXiv:2407.07787\n, 2024.\nSharma et al. [2023]\nArchit Sharma, Ahmed M Ahmed, Rehaan Ahmad, and Chelsea Finn.\nSelf-improving robots: End-to-end autonomous visuomotor reinforcement learning.\narXiv preprint arXiv:2303.01488\n, 2023.\nShukor et al. [2025]\nMustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al.\nSmolvla: A vision-language-action model for affordable and efficient robotics.\narXiv preprint arXiv:2506.01844\n, 2025.\nTeam et al. [2025]\nGemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al.\nGemini robotics: Bringing ai into the physical world.\narXiv preprint arXiv:2503.20020\n, 2025.\nTeam et al. [2024]\nOcto Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al.\nOcto: An open-source generalist robot policy.\narXiv preprint arXiv:2405.12213\n, 2024.\nTian et al. [2025]\nJiawen Tian, Liqun Huang, Zhongren Cui, Jingchao Qiao, Jiafeng Xu, Xiao Ma, and Zeyu Ren.\nBytewrist: A parallel robotic wrist enabling flexible and anthropomorphic motion for confined spaces.\narXiv preprint arXiv:2509.18084\n, 2025.\nWagenmaker et al. [2025]\nAndrew Wagenmaker, Mitsuhiko Nakamoto, Yunchu Zhang, Seohong Park, Waleed Yagoub, Anusha Nagabandi, Abhishek Gupta, and Sergey Levine.\nSteering your diffusion policy with latent space reinforcement learning.\narXiv preprint arXiv:2506.15799\n, 2025.\nWang et al. [2024]\nLirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He.\nScaling proprioceptive-visual learning with heterogeneous pre-trained transformers.\nAdvances in neural information processing systems\n, 37:124420â€“124450, 2024.\nWu et al. [2023a]\nJimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser.\nTidybot: Personalized robot assistance with large language models.\nAutonomous Robots\n, 47(8):1087â€“1102, 2023a.\nWu et al. [2023b]\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg.\nDaydreamer: World models for physical robot learning.\nIn\nConference on robot learning\n, pages 2226â€“2240. PMLR, 2023b.\nXia et al. [2025]\nWenke Xia, Yichu Yang, Hongtao Wu, Xiao Ma, Tao Kong, and Di Hu.\nRobotic policy learning via human-assisted action preference optimization.\narXiv preprint arXiv:2506.07127\n, 2025.\nXiao et al. [2025]\nWenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi Fan, et al.\nSelf-improving vision-language-action models with data generation via residual rl.\narXiv preprint arXiv:2511.00091\n, 2025.\nZhao et al. [2023]\nTony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn.\nLearning fine-grained bimanual manipulation with low-cost hardware.\narXiv preprint arXiv:2304.13705\n, 2023.\nZhou et al. [2024]\nZhiyuan Zhou, Andy Peng, Qiyang Li, Sergey Levine, and Aviral Kumar.\nEfficient online reinforcement learning fine-tuning need not retain offline data.\narXiv preprint arXiv:2412.07762\n, 2024.\nZhu et al. [2025]\nFangqi Zhu, Zhengyang Yan, Zicong Hong, Quanxin Shou, Xiao Ma, and Song Guo.\nWmpo: World model-based policy optimization for vision-language-action models.\narXiv preprint arXiv:2511.09515\n, 2025.\nContributions and Acknowledgments\nAuthors are listed in alphabetical order.\nCore Contributors\nYunfei Li, Xiao Ma, Jiafeng Xu\nContributors\nYu Cui, Zhongren Cui, Zhigang Han, Liqun Huang, Tao Kong, Yuxiao Liu, Hao Niu, Wanli Peng, Jingchao Qiao, Zeyu Ren, Haixin Shi, Zhi Su, Jiawen Tian, Yuyang Xiao, Shenyu Zhang, Liwei Zheng\nSupervisors\nHang Li, Yonghui Wu\nAcknowledgments\nWe would like to thank Yichu Yang for the insightful discussion on morphological mirror augmentation. We thank Chilam Cheang, Jinming Guo, Zetian Li, Xin Zhao, Mingyang Wang, Zhiguo Hao, Tianxiang Gong, Yang Zhao, Shuzhai Guo, Ziye Liu and all the data annotators for their help on data curation. We thank Degong Yang and Yang Liu for their help on hardware system development and maintenance.",
    "preview_text": "We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundation models to specialize into reliable real-world experts.\n\n\\correspondence\n\\contribution\n[]Full Author List in\nContributions and Acknowledgments\nGR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation\nByteDance Seed\nxiao.ma@bytedance.com\nAbstract\nWe present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation.\nAssuming the optimality of human demonstrations is cor",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "robotic manipulation",
        "vision-language-action policy",
        "reinforcement learning",
        "long-horizon tasks",
        "dexterous manipulation"
    ],
    "one_line_summary": "GR-RLæ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ å°†é€šç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥è½¬åŒ–ä¸ºä¸“é—¨ç”¨äºé•¿æ—¶ç¨‹çµå·§æœºå™¨äººæ“ä½œçš„æ¡†æ¶ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T15:33:59Z",
    "created_at": "2026-01-10T10:43:56.137173",
    "updated_at": "2026-01-10T10:43:56.137180",
    "flag": true
}