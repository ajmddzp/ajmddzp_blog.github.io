{
  "id": "2601.09988v1",
  "title": "In-the-Wild Compliant Manipulation with UMI-FT",
  "authors": [
    "Hojung Choi",
    "Yifan Hou",
    "Chuer Pan",
    "Seongheon Hong",
    "Austin Patel",
    "Xiaomeng Xu",
    "Mark R. Cutkosky",
    "Shuran Song"
  ],
  "abstract": "Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors on each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. In evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:https://umi-ft.github.io/.",
  "url": "https://arxiv.org/abs/2601.09988v1",
  "html_url": "https://arxiv.org/html/2601.09988v1",
  "html_content": "In-the-Wild Compliant Manipulation with UMI-FT\nHojung Choi*\n1\n, Yifan Hou*\n1\n, Chuer Pan\n1\n, Seongheon Hong\n2\n, Austin Patel\n1\n, Xiaomeng Xu\n1\n,\nMark R. Cutkosky\n2\n, and Shuran Song\n1\n*Equal contribution.\n1\nDepartment of Electrical Engineering, Stanford University, CA, USA.\n2\nDepartment of Mechanical Engineering, Stanford University, CA, USA.\nAbstract\nMany manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors\non each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy\nthat predicts position targets, grasp force, and stiffness for execution on standard compliance controllers.\nIn evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:\nhttps://umi-ft.github.io/\n.\nI\nIntroduction\nHandheld manipulation devices, such as UMI\n[\n1\n]\n, are becoming increasingly popular for manipulation data collection\n[\n2\n,\n3\n,\n4\n]\ndue to their flexibility and natural haptic feedback. However, most existing handheld data collection systems focus only on position and visual information, without considering force sensing. Yet force sensing is a key element of human manipulation, enabling careful contact modulation that is especially critical when handling soft or fragile objects.\nIn this paper, we introduce UMI-FT, a novel force-sensing handheld data collection device. Our key improvement over UMI is the integration of CoinFT\n[\n5\n]\n, a custom, compact force–torque sensor mounted at each finger. This design captures the complete force information present during a demonstration (both the external forces transmitted through the device structure and the internal grasp forces exerted by the user), thereby providing an authentic representation of what the human demonstrator experiences.\nPrior approaches to collecting force data can be broadly divided into two categories. The first employs a wrist-mounted commercial six-axis F/T sensor (e.g.,\n[\n6\n,\n7\n]\n). In contrast, UMI-FT offers two key advantages:\n•\nFinger-level force\n. The compact design of CoinFT enables a sensor to be mounted at each finger. This placement bypasses the gripper mass, providing more accurate and low-latency contact force measurements, and allows the robot to regulate internal grasp forces.\n•\nScalability\n. CoinFT is low-cost ($10 BOM per sensor) and resilient to impact, making it more scalable than expensive, fragile commercial F/T sensors.\nThe second category of prior sensing solutions is to install\ntactile sensors\non fingers\n[\n3\n,\n4\n,\n2\n]\n. Here again, UMI-FT provides the following key advantages:\n•\nStandard and consistent measurements\n. The measurements from UMI-FT can be directly calibrated using a commercial F/T sensor, providing consistent data across different units. Consistency is maintained regardless of installation location, since measured wrenches can be transformed into any desired coordinate frame (typically the robot tool frame).\n•\nCompatibility with standard compliance control\n. The wrench measurements from UMI-FT can be used with standard compliance controllers (e.g., admittance or impedance control) to implement a stable, accurate, and user-specified compliance profile.\n•\nFull finger sensing\n. Unlike tactile sensors which must physically cover all potential contact regions, UMI-FT can measure contact forces anywhere along the finger body, as long as the load path passes through the sensor.\nTo showcase these advantages, we train an Adaptive Compliance Policy (ACP)\n[\n8\n]\nusing data collected through UMI-FT, and demonstrate safe, yet forceful interactions. Ablation studies highlight the benefits of explicit finger-level force data, particularly for tasks requiring forceful manipulation, such as wiping a whiteboard, skewering a zucchini and lightbulb insertion.\nIn summary, our main contributions are as follows.\n•\nUMI with per-finger force/torque sensing.\nA portable and scalable handheld device equipped with per-finger F/T sensors, capturing both external contact forces and internal grasp force that human demonstrators naturally exploit through haptic feedback.\n•\nA control architecture for compliant manipulation\n. We augment ACP with the ability to regulate both external contact forces and internal grasp force, enabling robust and safe manipulation without requiring bulky, expensive F/T sensors.\n•\nExperimental validation.\nWe demonstrate significant improvements from both finger-level force sensing and robot compliance enabled by UMI-FT, across tasks that require safe yet forceful manipulation.\n•\nOpen-source hardware and software\n, to facilitate large-scale adoption and encourage broader research on multimodal robot data collection.\nII\nRelated Work\nII-A\nForce/Tactile Sensing for Manipulation Data Collection and Policy Learning\nPrior imitation learning systems have attempted to obtain direct force/torque measurements through motor joint current\n[\n9\n]\nor wrist-mounted commercial force/torque (F/T) sensors\n[\n7\n,\n8\n]\n. As explained in the Introduction, these systems lack finger-level sensing and are hard to scale due to their size, fragility, and cost.\nTactile sensors have compact sizes and provide rich contact information directly from the fingertip.\nA wide range of tactile sensors have been developed to capture different tactile features, including contact pressure and location, multi-axial force, slippage, temperature, and proximity\n[\n10\n,\n11\n]\n.\nVision-based tactile sensors have also grown in popularity due to their high resolution and sensitivity\n[\n12\n,\n13\n]\n. Tactile sensors have been integrated into a variety of data collection interfaces, including teleoperation\n[\n14\n,\n15\n]\n, handheld devices\n[\n4\n,\n3\n]\n, wearable systems\n[\n16\n]\n, and kinesthetic teaching\n[\n8\n,\n17\n]\n.\nThese modalities have been shown to benefit policy learning at multiple levels. Contact microphones can capture dynamic events and material properties\n[\n3\n]\n. High-resolution tactile arrays and vision-based tactile sensors can infer object pose and contact intensity\n[\n14\n,\n4\n,\n15\n,\n18\n,\n19\n]\n. Wearable systems provide demonstrators with direct haptic feedback, embedding fine-grained strategies that can transfer to robot policies\n[\n16\n]\n. Direct F/T measurements through kinesthetic interfaces have likewise proven effective for learning compliant behaviors, both at the wrist\n[\n8\n]\nand fingertip level\n[\n17\n]\n.\nOur objective is to approximate what a human user experiences while operating a hand-held gripper device. Primarily, this includes dynamic forces, transmitted through the structure of the device and a sense of the applied grasp force. For this reason, miniature force/torque sensors at the fingertips\n(e.g. as in\n[\n20\n]\n) are particularly of interest. In particular, we choose CoinFT\n[\n5\n]\nfor its small size (20 mm⌀\n×\n\\times\n3 mm thick) and low weight, robustness, and low cost—allowing replacement should damage occur.\nII-B\nCompliance and Compliance Control\nCompliance refers to the elastic behavior of a physical body under external force, commonly described by a combination of stiffness, inertia and damping\n[\n21\n]\n. At low speed, only stiffness remains significant. A suitable stiffness profile can improve the disturbance rejection capability of manipulation systems\n[\n22\n,\n23\n]\n. Compliance is often necessary for stability in contact-rich manipulations\n[\n24\n,\n18\n,\n25\n,\n26\n]\n.\nWhen force feedback is available, specific compliance profiles can be achieved with standard impedance\n[\n27\n]\ncontrol on backdrivable robots, or admittance control\n[\n28\n]\non stiff, high accuracy robots.\nIII\nMultimodal Sensing using UMI-FT\nIn this section, we introduce our hardware suite for collecting multimodal data from human demonstrations.\nIII-A\nUMI-FT Design\nUMI-FT is a modified version of the original UMI device, designed to collect multimodal sensory data including vision, pose, and six-axis F/T signals at each finger, during human demonstrations (Fig.\nLABEL:fig:teaser\n). The system integrates CoinFT sensors mounted at both fingers and an iPhone 15 Pro. The iPhone is rigidly attached to the UMI-FT structure with a 15°  tilt to better capture the finger workspace and ArUco markers (for measuring gripper width). It provides synchronized main RGB (approximately 80° diagonal FoV), ultrawide RGB (120° FoV), depth, and pose data via ARKit. The main RGB and pose stream are recorded at 60 Hz, ultrawide RGB at 10 Hz, and depth at 30 Hz.\nEach finger is equipped with a CoinFT sensor, which measures wrenches at 360 Hz. Its unit weight, 2 g, has minimal impact on overall weight of the UMI-FT device. These sensors are connected to a USB-serial bus via I\n2\nC, and the bus is connected to a laptop with a USB cable for data logging. CoinFT data is timestamped using internet time and synchronized with the iPhone data streams during post-processing.\nSignificant modifications were made to the original fin-ray finger design of the UMI to accommodate the constraints of CoinFT (Fig.\nLABEL:fig:teaser\n). While CoinFT is mechanically robust under compressive loading, its tensile tolerance is lower. During manipulation, excessive moments caused by grasp forces and long moment arms can lead to delamination of the sensor. To mitigate this, each CoinFT is mounted closer to the fingertip, reducing the moment arm and ensuring that grasping primarily results in compressive forces. This placement allows the UMI-FT fingers to perform a stronger grasp while preserving mechanical integrity.\nThe CoinFT itself was also redesigned to enhance robustness and increase sensing range, with a tradeoff in sensitivity. The dielectric layer consists of pillars with oval cross section, where in the outermost layer the major and minor axes are approximately 4 mm and 0.5 mm, respectively, in contrast to circular pillars with a 0.2 mm diameter from the original design\n[\n5\n]\n.\nIII-B\nUMI-FT Finger Calibration\nUnlike common commercial F/T sensors such as the Gamma (ATI), CoinFT exhibits a nonlinear signal profile due to both its mechanical properties and the physics of capacitive sensing\n[\n5\n]\n. The signal behavior cannot be easily modeled using traditional linear techniques under large loads that may occur during manipulation.\nTo achieve accurate sensing, we calibrate each CoinFT sensor in situ, with the UMI-FT fingertip mounted, ensuring that the calibration model captures the effects of the full finger structure (Fig.\n1\n(a)). The Gamma (ATI) is used as ground truth reference. During data collection, a human operator applies randomized combinations of forces and torques along the UMI-FT finger, covering a target range of 25 N in the normal direction, 20 N in shear, and up to 500 mNm in moment.\nThe mapping from capacitance to force/torque is learned using a multi-layer perceptron (MLP) with five hidden fully connected layers, with layer widths of 128, 64, 36, 24, and 12, respectively (Fig.\n1\n(b)). Each hidden layer employs ReLU activation, and the final layer performs a six-dimensional regression to estimate the full six-axis F/T. This model achieves a mean squared error of 0.18 N, 0.15 N, 0.58 N, 159 mNm, 231 mNm, 17 mNm for\nF\n​\nx\nFx\n,\nF\n​\ny\nFy\n,\nF\n​\nz\nFz\n,\nT\n​\nx\nTx\n,\nT\n​\ny\nTy\n,\nT\n​\nz\nTz\n, respectively (Fig.\n1\n(c)).\nFigure 1:\nUMI-FT finger calibration. (a) Raw capacitance data from CoinFT and the corresponding F/T from a reference sensor (Gamma, ATI) are collected with random F/T inputs. (b) The raw capacitance is mapped to F/T through an MLP layer. (c) Calibration results on unseen input. Only one of the shear axes (x, y) were plotted due to similarity.\nIII-C\nDeployment on robot\nFollowing\n[\n1\n]\n, the robot end-effector retains the same design as the handheld UMI-FT, including the fingers, CoinFTs, and the iPhone. The only modification is that all data modalities are streamed directly to the control desktop for real-time policy inference.\nIV\nCompliant Manipulation using UMI-FT\nIn this section, we introduce our software suite for compliant manipulation utilizing force/torque sensing from both fingers of UMI-FT.\nIV-A\nController architecture:\nThe force measurements from the fingers are used in three parallel loops, as illustrated in Fig.\n2\n.\nThe slowest loop is a learned visuomotor policy that performs manipulation reasoning. The policy takes as input the most recent 32 frames of both force sensor readings. The policy is trained per task and is detailed in Sec.\nIV-B\n. The policy outputs reference position, grasp force and stiffness, which are taken as input by the two model-based compliance controllers.\nWrist compliance control:\nThe wrist compliance controller implements 6D task space admittance control to move the robot arm like a virtual spring-mass-damper system\n[\n21\n]\n. The 6D wrench measurements from both FT sensors,\nW\nS\n​\n1\n,\nW\nS\n​\n2\nW_{S1},W_{S2}\nare converted to the robot tool frame and combined as the wrench feedback for the admittance control, so the robot can respond to external forces on either finger or both:\nW\nw\n​\nr\n​\ni\n​\ns\n​\nt\n=\nAd\nS\n1\n​\nT\nT\n​\nW\nS\n​\n1\n+\nAd\nS\n2\n​\nT\nT\n​\nW\nS\n​\n2\nW_{wrist}={\\rm Ad}^{T}_{S_{1}T}W_{S1}+{\\rm Ad}^{T}_{S_{2}T}W_{S2}\n(1)\nwhere\nAd\nS\n1\n​\nT\n,\nAd\nS\n2\n​\nT\n{\\rm Ad}_{S_{1}T},{\\rm Ad}_{S_{2}T}\ndenotes the adjoint transformation matrix from one of the sensor frames to the robot tool frame.\nUnlike in\n[\n8\n]\nwhere the robot tool frame (TCP) was set close to the robot flange for comfortable kinesthetic teaching, with UMI-FT we found it helpful to set TCP to the center of the two fingertips. This allows useful and robust compliant behaviors in our manipulation tasks, such as better alignment with a surface while wiping.\nFigure 2:\nController structure of UMI-FT and the flow of information. Sensor modalities are listed on the left, with proprioception omitted for clarity. They flow to three controller loops in the middle, where the learned policy runs the slowest and generates reference targets to the other two model-based controllers.\nGrasp force control:\nGrasp force\nf\nG\nf_{G}\nis measured as the average between the CoinFT readings of the two fingers in the grasp axis direction. Then we can regulate grasp force to a target\nf\nG\n∗\nf_{G}^{*}\nwith a velocity-resolved admittance controller:\nv\nG\n=\nK\np\n​\n(\nx\nG\n∗\n−\nx\nG\n)\n+\nK\nf\n​\n(\nf\nG\n∗\n−\nf\nG\n)\nv_{G}=K_{p}(x_{G}^{*}-x_{G})+K_{f}(f_{G}^{*}-f_{G})\n(2)\nwhere the position gain\nK\np\nK_{p}\nand force gain\nK\nf\nK_{f}\ncan be tuned to adjust how sensitive the grasping motion is to position and force tracking error. Grasping velocity\nv\nG\nv_{G}\nis sent to the gripper controller.\nBoth the wrist and gripper compliance controller runs at the rate limit for their corresponding hardware (500Hz for the robot, 30Hz for the gripper).\nIV-B\nImitation Learning with Compliance\nFigure 3:\nStructure of the adaptive compliance policy with modified inputs and outputs.\nWe employ Adaptive Compliance Policy (ACP)\n[\n8\n]\nto learn compliant manipulation behaviors. On the high level, ACP predicts not only target positions but also target forces and desired compliance parameters, which can be executed on a robot by a standard compliance controller. With UMI-FT, we augment both inputs and outputs of ACP to accommodate hardware improvements.\nObservation Encoding:\nOur policy architecture processes multimodal observations through specialized encoders for each modality.\nWe sample iPhone RGB images from the two previous timesteps, with each image processed by a CLIP-pretrained ViT-B/32 encoder\n[\n29\n]\n. Images are resized to 224×224 pixels and augmented using random cropping and color jittering.\nDepth input from the iPhone is processed using the same vision encoder architecture as RGB. To ensure compatibility with the RGB-based ViT model, depth images are copied across three channels, emulating a grayscale RGB image. Depth values are clipped at 0.5 m to emphasize near-surface geometry and suppress distant background information.\nForce/torque measurements from each CoinFT are encoded using a causal convolutional network\n[\n24\n]\n. The encoder takes in the previous 32 timesteps of wrench data and outputs a single feature vector per sensor.\nThe resulting tokens from the RGB, depth, and F/T encoders are passed to a transformer encoder layer with self-attention to obtain a combined visual-force representation.\nFinally, the output of this fusion layer is concatenated with low-dimensional proprioceptive data, including end-effector pose from the previous two timesteps. The combined feature vector conditions the downstream diffusion policy\n[\n30\n]\n.\nOutput Decoding:\nThe policy outputs the position target for the robot, the stiffness matrix, the reference grasp force, and the gripper action in a 21-dimensional vector:\n•\nReference pose: 9D pose vector following convention in\n[\n30\n]\n, where the last six elements are the top two rows of a rotation matrix.\n•\nVirtual target pose: another 9D pose vector representing the actual set target of the compliance controller.\n•\nStiffness value: following\n[\n8\n]\n, this scalar value encodes the stiffness matrix of the robot arm when used together with the reference/virtual target poses.\n•\nGripper width and grasp force: two scalar values representing the desired gripper width and grasp force.\nBefore training, we first follow ACP and post-process the force-motion data to obtain stiffness and virtual target labels following a time-varying 3D mechanical spring. We also extract grasp forces from the CoinFT sensors.\nAt inference time, we first reconstruct the full stiffness matrix following ACP, then send both the stiffness matrix and the virtual target to the low level compliance controller, send gripper width and grasp force to the gripper force controller.\nV\nEvaluations\nFigure 4:\nWhiteboard Wiping\npolicy rollout, test scenarios, and representative baseline failures.\nWe evaluate our approach on three tasks that highlight the benefits of compliance control with grasp force modulation. Experiments are conducted on a UR5e robot equipped with a WSG50 gripper and UMI-FT fingers. We compare our method against three baseline policies:\n•\nDiffusion Policy with Force (DP w/ F)\n: This baseline takes wrench information from the CoinFT sensors as input but does not perform compliance control. The wrench signals are also used for grasp force modulation.\n•\nDiffusion Policy (DP)\n: The original diffusion policy\n[\n30\n]\nwithout force observation. The gripper is commanded purely through position control.\n•\nDiffusion Policy with Contact Microphone (DP w/ CM)\n: This baseline combines diffusion policy with dynamic contact events detected via a contact microphone, following\n[\n3\n]\n. The contact microphone is connected to the iPhone via a USB-C mic adapter and captures audio at 44.1 kHz. The mic is embedded on the right finger of the UMI-FT (Fig.\n4\n)\nFor all baselines, both RGB and depth images are used as visual input. Consistent with\n[\n8\n]\n, compliance is applied in the 3-D translational space, though the framework naturally extends to full 6-D compliance when necessary.\nV-A\nWhiteboard Wiping\nIn this task, the robot must approach and grasp the eraser, retract, move to the drawing on the whiteboard, and wipe until the surface is clean. During grasping, the robot must apply sufficient force to firmly secure the eraser, beyond mere contact. During wiping, it must apply enough normal force to ensure effective wipe, while avoiding unnecessarily high forces that could damage the environment or the robot.\nWe collected 275 demonstrations for this task, incorporating variations in the initial pose, table height, and drawing patterns. Regardless of whether wrench information was included in the policy observation, we enforced safety thresholds of 25 N in the grasp direction (normal to the CoinFT sensor) and 20 N in all other directions. Task success is defined as less than a total of 1 cm\n×\n\\times\n1 cm drawing remaining in 5 wipes.\nTest Scenarios:\nWe applied variations not present in the training set during policy rollout. Each of the following test case was executed five times, yielding 25 rollouts per policy.\n•\nNormal\n: Variations in the initial positions of the whiteboard, eraser, and location of drawing, similar to training data.\n•\nDifferent table height\n: includes values outside those seen in the training data.\n•\nUnseen drawings in a new color\n: Drawings were presented in an unseen color, with novel patterns.\n•\nElevated whiteboard\n: The whiteboard was mounted at a higher elevation relative to the table, a configuration not seen during training.\n•\nNarrower eraser\n: The eraser width was reduced by 7 mm, creating a shape not observed in training.\nTABLE I:\nSuccess Rates for Whiteboard Wiping Task.\nNormal\nBoard height\nTable height\nNew color\nNarrow eraser\nOverall\nOurs (ACP)\n5/5\n4/5\n4/5\n5/5\n5/5\n92 %\nDP w/ F\n2/5\n1/5\n1/5\n1/5\n2/5\n28 %\nDP\n2/5\n0/5\n1/5\n1/5\n0/5\n16 %\nDP w/ CM\n0/5\n0/5\n0/5\n0/5\n0/5\n0 %\nFindings:\nThe results of the whiteboard wiping task are summarized in Table\nI\n. Overall, high-frequency contact modulation through compliance control and grasp force regulation enables our method to reliably execute forceful tasks such as wiping, while generalizing to diverse environmental disturbances.\nA.1) Grasp force modulation improves robustness to unseen object shapes.\nOur method performed reliably in the\nnarrower eraser\nscenario. The [Diffusion Policy with Force] succeeded in grasping the eraser, but failed due to incomplete wipes or excessive force for this scenario. In contrast, all baselines without force information never succeeded grasping in this scenario, and also failed intermittently in other scenarios.\nA.2) Dynamic contact information alone is insufficient for continuous contact modulation.\nThe [Diffusion Policy with Contact Mic] baseline never succeeded, with the dominant failure mode being excessive applied force. Additional failures included grasping errors, such as failing to establish contact with the narrow eraser or barely touching the eraser without lifting it. While ManiWAV\n[\n3\n]\nshowed that audio from a contact microphone can improve contact detection, it remains inadequate for regulating continuous force.\nV-B\nSkewering Zucchini\nV-B\n1\nIn-Lab Experiments\nIn this task, the robot must skewer a pre-grasped zucchini slice onto a spike. The robot should approach the spike at an appropriate relative pose while maintaining a firm grasp, ensuring that the zucchini is properly punctured without falling off or excessively rotating due to slippage. Task success is defined as achieving a complete puncture with less than a 45°of rotation.\nWe collected 200 demonstrations for this task, with variations in initial relative pose of the spike, location along the finger where the zucchini is pre-grasped. The zucchini diameter had natural variations, while the slice thickness was controlled to 1 cm. As in the\nwhiteboard wiping\ntask, safety thresholds of 25 N in the grasp direction and 20 N in all other directions were enforced, measured by the CoinFTs.\nTest Scenarios:\nEach policy was evaluated over 20 rollouts, with 5 trials per test case, to assess generalization and robustness. During rollout, the zucchini was always aligned with the center of the CoinFTs. The [Diffusion policy with contact mic] baseline was removed for this task as the gripper kept dropping the zucchini slice while approaching and never succeeded.\n•\nNormal\n: Variations in the initial pose of the spike, using a 1 cm green zucchini slice, similar to training.\n•\nYellow Zucchini\n: A 1 cm slice of yellow zucchini, unseen in training.\n•\nThicker Zucchini\n: A 2 cm slice of green zucchini, unseen in training, requiring a larger puncture force.\n•\nFork\n: A fork was used instead of a spike, introducing both visual variation and higher puncture force. This case was not present in the training set.\nFigure 5:\nSkewering Zucchini\npolicy rollout, test scenarios, representative baseline failures.\nTABLE II:\nSuccess Rates for Zucchini Skewering Task.\nNormal\nYellow\nThicker\nFork\nOverall\nOurs (ACP)\n5/5\n5/5\n3/5\n3/5\n80 %\nDP w/ F\n5/5\n4/5\n3/5\n2/5\n70 %\nDP\n2/5\n2/5\n2/5\n0/5\n30 %\nFindings:\nThe evaluation results are summarized in Table\nII\n. We find that\nB.1) grasp force regulation is critical for maintaining a secure hold on the zucchini during skewering\n. As shown in the [Diffusion Policy] baseline, the success rate of the task drops significantly without force information. This occurs because the gripper is controlled solely through predicted width and is unaware of contact forces, causing the zucchini to frequently slip while skewering. As shown in Fig.\n5\n(c), the failures are due to linear or rotational slip. In contrast, once the grasp force regulation is included, these failures are largely eliminated. Moreover, the difference between having or not having compliance control becomes small. We hypothesize that this is because the zucchini itself is compliant, reducing the additional benefit of compliance at the manipulator level.\nV-B\n2\nIn-the-Wild Experiments\nTo investigate in-the-wild generalization, we collected 630 demonstrations across 15 different scenes. Other than the scene variance, the data collection protocol remained consistent with the in-lab case.\nTest Scenarios:\nWe compared our modified ACP policy trained on either in-lab data or in-the-wild data. These policies were evaluated over 20 rollouts, in a scene not included in either dataset with added clutter using unseen objects (Fig.\n6\n). Each rollout varied in initial pose of the spike and different sets of unseen objects were used every five rollouts.\nFindings:\nWe find that\nB.2) in-the-wild data enables generalization to unseen environments\n(Fig.\n6\n). As shown in Table\nIII\n, the policy trained on diverse data achieves a notably higher success rate (20/20), successfully navigating to the skewer while maintaining grasp force despite unseen surroundings and clutter. In contrast, the policy trained on data with limited scene-diversity rarely succeeds (4/20), where the dominant failure mode was missing the skewer due to poor navigation.\nFigure 6:\nSkewering Zucchini\nin-the-wild.\nTABLE III:\nResults for In-the-Wild Zucchini Skewering.\nMethod\nSuccess Rate\nACP w/ in-the-wild data\n20/20\nACP w/ in-lab data\n4/20\nFigure 7:\nLightbulb Insertion\npolicy rollout, test scenarios, and representative baseline failures.\nV-C\nLightbulb Insertion\nIn this task, the robot begins with the bulb pre-grasped and must navigate toward the socket, align the bayonet pins with the slot on the socket, insert the bulb, apply sufficient force (at least 15 N, measured using a Mark-10 digital force gauge) to overcome the spring-loaded electrode, and then rotate to turn on the light (Fig.\n7\n(a), (b)). For successful alignment and insertion, it is desirable that the bayonet pin remains in contact with the socket rim during rotation, ensuring that it drops into the slot once properly aligned. The gripper must hold the bulb firmly and apply enough force to compress the spring-loaded electrode, while avoiding excessive force that could lead to slippage. We manually shortened the spring to reduce the socket’s reaction force from 30 N to 15 N, as 30 N was excessive for our setup. Task success is defined as the bulb illuminating.\nWe collected 200 demonstrations for this task, varying the initial socket pose, table height, and the bulb’s pre-grasp location relative to the fingers. Similar to the previous tasks, safety thresholds of 50 N in the grasp direction and 20 N in all other directions were enforced, measured using the CoinFT sensors.\nTest Scenarios:\nEach policy was evaluated over 20 rollouts, with 10 trials for the\nnormal\ncase, and 5 trials for the others, to investigate generalization and robustness.\n•\nNormal:\nVariations in the initial pose of the socket, similar to training.\n•\nTable Height:\nVariations in table height, including values not present in the training set.\n•\nStiffer Socket:\nThe socket spring was tuned to exert a higher reaction force during insertion (20 N instead of 15 N). This condition was not included in training.\nTABLE IV:\nSuccess Rates for Lightbulb Insertion Task.\nNormal\nTable height\nStiffer socket\nOverall\nOurs (ACP)\n10/10\n5/5\n4/5\n95 %\nDP w/ F\n4/10\n5/5\n3/5\n60 %\nDP\n0/10\n0/5\n0/5\n0 %\nDP w/ CM\n3/10\n1/5\n0/5\n20 %\nFindings:\nThe evaluation results are summarized in Table\nIV\n.\nC.1) Compliance control is critical for haptic search.\nA crucial step in this task is to locate the slot, align the bayonet pin, and insert it. Due to occlusion from the bulb and the small size of the slot, this is difficult to achieve relying only on vision. Our method consistently succeeded by maintaining controlled contact force with the socket while rotating the bulb, allowing the pin to slide into the slot once aligned. In contrast, the baselines often failed to maintain contact during rotation, overshooting and missing the slot (Fig.\n7\n(c)). Even when insertion succeeded, excessive contact forces frequently caused slippage during the haptic search, sometimes resulting in rotation before full insertion. Moreover, the [Diffusion Policy] baseline showed larger navigation errors when approaching the socket compared to multimodal baselines, consistent with observations in\n[\n3\n]\n.\nVI\nFuture Work and Conclusion\nThe current UMI-FT data collection setup is tethered, as the USB–serial bridge connecting the two CoinFTs to the laptop relies on a USB cable. UMI-FT could be made wireless by using a Bluetooth-capable microcontroller, allowing the iPhone to directly connect to the CoinFTs. We leave this extension as future work.\nAlthough the iPhone provides an ultrawide RGB (120° FoV) stream, in this project we only used the main RGB camera (approx. 80° diagonal FoV). Our chosen tasks did not require a wider field of view, but the same architecture presented here can be applied in the future when larger spatial coverage is needed.\nThe CoinFT itself also has room for improvement. As discussed Section\nIII\n, CoinFT can delaminate under sufficiently large tensile forces. Further design improvements such as modifying the pillar geometry or improving the bonding method could enhance mechanical reliability. In addition, the current microcontroller (PSoC 4100S) could be upgraded to a newer generation chip to improve overall performance.\nIn this paper, we presented UMI-FT, a scalable multimodal data collection platform equipped with custom F/T sensors on each finger. Through real-world robot experiments, we demonstrated that policies learned with UMI-FT enable compliant behaviors, allowing robots to reliably perform tasks that require careful modulation of both external contact forces and internal grasp forces.\nVII\nAcknowledgments\nThis work was supported in part by the NSF Award #2143601, #2037101, and #2132519, Toyota Research Institute, Samsung and Amazon. We thank Google and TRI for the UR5 robot hardware. We thank Huy Ha, Zeyi Liu, and other members of the Robotics and Embodied Artificial Intelligence Lab for the fruitful discussions. We also thank Alice Wu, Eric Cousineau, Rick Cory, Jeannette Bohg for their valuable advice and insights. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsors.\nReferences\n[1]\nC. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song, “Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots,” in\nProceedings of Robotics: Science and Systems\n, 2024.\n[2]\nR. Bhirangi, V. Pattabiraman, E. Erciyes, Y. Cao, T. Hellebrekers, and L. Pinto, “Anyskin: Plug-and-play skin sensing for robotic touch,”\narXiv preprint arXiv:2409.08276\n, 2024.\n[3]\nZ. Liu, C. Chi, E. Cousineau, N. Kuppuswamy, B. Burchfiel, and S. Song, “Maniwav: Learning robot manipulation from in-the-wild audio-visual data,”\narXiv preprint arXiv:2406.19464\n, 2024.\n[4]\nX. Zhu, B. Huang, and Y. Li, “Touch in the wild: Learning fine-grained manipulation with a portable visuo-tactile gripper,”\narXiv preprint arXiv:2507.15062\n, 2025.\n[5]\nH. Choi, J. E. Low, T. M. Huh, G. A. Uribe, S. Hong, K. A. W. Hoffman, J. Di, T. G. Chen, A. A. Stanley, and M. R. Cutkosky, “Coinft: A coin-sized, capacitive 6-axis force torque sensor for robotic applications,”\narXiv preprint arXiv:2503.19225\n, 2025.\n[6]\nE. Galbally, A. Piedra, C. Brosque, and O. Khatib, “Elly: A real-time failure recovery and data collection system for robotic manipulation,”\narXiv preprint arXiv:2208.11845\n, 2022.\n[7]\nW. Liu, J. Wang, Y. Wang, W. Wang, and C. Lu, “Forcemimic: Force-centric imitation learning with force-motion capture system for contact-rich manipulation,” in\nIEEE International Conference on Robotics and Automation\n, 2025.\n[8]\nY. Hou, Z. Liu, C. Chi, E. Cousineau, N. Kuppuswamy, S. Feng, B. Burchfiel, and S. Song, “Adaptive compliance policy: Learning approximate compliance for diffusion guided control,”\narXiv preprint arXiv:2410.09309\n, 2025.\n[9]\nP. Wu, Y. Shentu, Q. Liao, D. Jin, M. Guo, K. Sreenath, X. Lin, and P. Abbeel, “Robocopilot: Human-in-the-loop interactive imitation learning for robot manipulation,”\narXiv preprint arXiv:2503.07771\n, 2025.\n[10]\nQ. Li, O. Kroemer, Z. Su, F. F. Veiga, M. Kaboli, and H. J. Ritter, “A review of tactile information: Perception and action through touch,”\nIEEE Transactions on Robotics\n, vol. 36, no. 6, pp. 1619–1634, 2020.\n[11]\nS. Luo, N. F. Lepora, W. Yuan, K. Althoefer, G. Cheng, and R. Dahiya, “Tactile robotics: An outlook,”\narXiv preprint arXiv:2508.11261\n, 2025.\n[12]\nS. Li, Z. Wang, C. Wu, X. Li, S. Luo, B. Fang, F. Sun, X.-P. Zhang, and W. Ding, “When vision meets touch: A contemporary review for visuotactile sensors from the signal processing perspective,”\nIEEE Journal of Selected Topics in Signal Processing\n, vol. 18, no. 3, pp. 267–287, 2024.\n[13]\nW. Yuan, S. Dong, and E. H. Adelson, “Gelsight: High-resolution robot tactile sensors for estimating geometry and force,”\nSensors\n, vol. 17, no. 12, p. 2762, 2017.\n[14]\nB. Huang, Y. Wang, X. Yang, Y. Luo, and Y. Li, “3d-vitac: Learning fine-grained manipulation with visuo-tactile sensing,”\narXiv preprint arXiv:2410.24091\n, 2024.\n[15]\nJ. Zhao, N. Kuppuswamy, S. Feng, B. Burchfiel, and E. Adelson, “Polytouch: A robust multi-modal tactile sensor for contact-rich manipulation using tactile-diffusion policies,”\narXiv preprint arXiv:2504.19341\n, 2025.\n[16]\nM. Xu, H. Zhang, Y. Hou, Z. Xu, L. Fan, M. Veloso, and S. Song, “Dexumi: Using human hand as the universal manipulation interface for dexterous manipulation,”\narXiv preprint arXiv:2505.21864\n, 2025.\n[17]\nC. Chen, Z. Yu, H. Choi, M. Cutkosky, and J. Bohg, “Dexforce: Extracting force-informed actions from kinesthetic demonstrations for dexterous manipulation,”\nIEEE Robotics and Automation Letters\n, vol. 10, no. 6, pp. 6416–6423, 2025.\n[18]\nH. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. Adelson, L. Fei-Fei, R. Gao, and J. Wu, “See, hear, and feel: Smart sensory fusion for robotic manipulation,”\narXiv preprint arXiv:2212.03858\n, 2022.\n[19]\nY. Liu, X. Xu, W. Chen, H. Yuan, H. Wang, J. Xu, R. Chen, and L. Yi, “Enhancing generalizable 6d pose tracking of an in-hand object with tactile sensing,”\nIEEE Robotics and Automation Letters\n, vol. 9, no. 2, pp. 1106–1113, 2023.\n[20]\nA. El-Azizi, S. Islam, P. Piacenza, K. Jiang, I. Kymissis, and M. Ciocarlie, “Compact led-based displacement sensing for robot fingers,”\narXiv preprint arXiv:2410.03481\n, 2024.\n[21]\nL. Villani and J. De Schutter, “Force control,” in\nSpringer handbook of robotics\n. Springer, 2016, pp. 195–220.\n[22]\nY. Hou and M. T. Mason, “Robust execution of contact-rich motion plans by hybrid force-velocity control,” in\nInternational Conference on Robotics and Automation\n, 2019, pp. 1933–1939.\n[23]\nY. Hou, Z. Jia, and M. T. Mason, “Manipulation with shared grasping,” in\nRobotics: Science and Systems\n, 2020.\n[24]\nM. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg, “Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks,” in\nInternational conference on robotics and automation\n. IEEE, 2019, pp. 8943–8950.\n[25]\nJ. Luo, C. Xu, J. Wu, and S. Levine, “Precise and dexterous robotic manipulation via human-in-the-loop reinforcement learning,”\nScience Robotics\n, vol. 10, no. 105, 2025.\n[26]\nX. Xu, Y. Hou, Z. Liu, and S. Song, “Compliant residual dagger: Improving real-world contact-rich manipulation with human corrections,”\narXiv preprint arXiv:2506.16685\n, 2025.\n[27]\nN. Hogan, “Impedance control: An approach to manipulation: Part ii—implementation,”\nJournal of dynamic systems, measurement, and control\n, vol. 107, no. 1, pp. 8–16, 1985.\n[28]\nJ. Maples and J. Becker, “Experiments in force control of robotic manipulators,” in\nIEEE International Conference on Robotics and Automation\n, vol. 3, 1986, pp. 695–702.\n[29]\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\net al.\n, “An image is worth 16x16 words: Transformers for image recognition at scale,”\narXiv preprint arXiv:2010.11929\n, 2020.\n[30]\nC. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, “Diffusion policy: Visuomotor policy learning via action diffusion,”\nThe International Journal of Robotics Research\n, 2024.",
  "preview_text": "Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors on each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. In evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:https://umi-ft.github.io/.\n\nIn-the-Wild Compliant Manipulation with UMI-FT\nHojung Choi*\n1\n, Yifan Hou*\n1\n, Chuer Pan\n1\n, Seongheon Hong\n2\n, Austin Patel\n1\n, Xiaomeng Xu\n1\n,\nMark R. Cutkosky\n2\n, and Shuran Song\n1\n*Equal contribution.\n1\nDepartment of Electrical Engineering, Stanford University, CA, USA.\n2\nDepartment of Mechanical Engineering, Stanford University, CA, USA.\nAbstract\nMany manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors\non each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Us",
  "is_relevant": true,
  "relevance_score": 3.0,
  "extracted_keywords": [
    "locomotion",
    "whole body control"
  ],
  "one_line_summary": "这篇论文介绍了一种手持式数据收集平台UMI-FT，用于学习顺应性操作策略，但未直接涉及强化学习、VLA、扩散模型、Flow Matching、VLM等关键词。",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-15T02:00:03Z",
  "created_at": "2026-01-20T17:49:52.936745",
  "updated_at": "2026-01-20T17:49:52.936755"
}