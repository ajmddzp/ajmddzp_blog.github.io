{
    "id": "2601.11076v1",
    "title": "A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation",
    "authors": [
        "Jiaqi Liang",
        "Yue Chen",
        "Qize Yu",
        "Yan Shen",
        "Haipeng Zhang",
        "Hao Dong",
        "Ruihai Wu"
    ],
    "abstract": "å®¶å…·ç»„è£…å¯¹æœºå™¨äººè€Œè¨€æ˜¯ä¸€é¡¹å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç²¾ç¡®çš„åŒè‡‚åè°ƒï¼šä¸€åªæ‰‹è‡‚æ“ä½œéƒ¨ä»¶ï¼Œå¦ä¸€åªæ‰‹è‡‚æä¾›åä½œæ”¯æ’‘ä¸ç¨³å®šã€‚ä¸ºæ›´é«˜æ•ˆåœ°å®Œæˆè¯¥ä»»åŠ¡ï¼Œæœºå™¨äººéœ€åœ¨é•¿æ—¶åºçš„ç»„è£…è¿‡ç¨‹ä¸­ä¸»åŠ¨è°ƒæ•´æ”¯æ’‘ç­–ç•¥ï¼ŒåŒæ—¶é€‚åº”ä¸åŒéƒ¨ä»¶çš„å‡ ä½•å½¢æ€ã€‚æˆ‘ä»¬æå‡ºA3Dæ¡†æ¶ï¼Œé€šè¿‡å­¦ä¹ è‡ªé€‚åº”åŠŸèƒ½æ„ŸçŸ¥æ¥ç¡®å®šå®¶å…·éƒ¨ä»¶ä¸Šçš„æœ€ä¼˜æ”¯æ’‘ä¸ç¨³å®šä½ç½®ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¯†é›†ç‚¹çº§å‡ ä½•è¡¨å¾æ¥å»ºæ¨¡éƒ¨ä»¶äº¤äº’æ¨¡å¼ï¼Œä»è€Œå®ç°å¯¹å¤šæ ·åŒ–å‡ ä½•å½¢æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºåº”å¯¹åŠ¨æ€æ¼”å˜çš„ç»„è£…çŠ¶æ€ï¼Œæˆ‘ä»¬å¼•å…¥è‡ªé€‚åº”æ¨¡å—ï¼Œåˆ©ç”¨äº¤äº’åé¦ˆæœºåˆ¶åŸºäºå†å²äº¤äº’åŠ¨æ€è°ƒæ•´ç»„è£…è¿‡ç¨‹ä¸­çš„æ”¯æ’‘ç­–ç•¥ã€‚æˆ‘ä»¬æ„å»ºäº†åŒ…å«8ç±»å®¶å…·å…±50ç§å·®å¼‚åŒ–éƒ¨ä»¶çš„ä»¿çœŸç¯å¢ƒï¼Œä¸“é—¨ç”¨äºåŒè‡‚åä½œæ€§èƒ½è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä»¿çœŸä¸çœŸå®åœºæ™¯ä¸­å‡èƒ½æœ‰æ•ˆæ³›åŒ–è‡³ä¸åŒå‡ ä½•å½¢æ€çš„éƒ¨ä»¶åŠå®¶å…·ç±»åˆ«ã€‚",
    "url": "https://arxiv.org/abs/2601.11076v1",
    "html_url": "https://arxiv.org/html/2601.11076v1",
    "html_content": "A3D:\nA\ndaptive\nA\nffordance\nA\nssembly with\nD\nual-Arm Manipulation\nJiaqi Liang\n1\n\\equalcontrib\n,\nYue Chen\n1\n\\equalcontrib\n,\nQize Yu\n1\n\\equalcontrib\n,\nYan Shen\n1\n,\nHaipeng Zhang\n1\n,\nHao Dong\n1\n,\nRuihai Wu\n1\nCorresponding Authors.\nAbstract\nFurniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization.\nTo accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries.\nWe propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions.\nWe establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.\nCode\nâ€”\nhttps://github.com/a3d-11011/Adaptive-Affordance-Assembly-with-Dual-Arm-Manipulation\nFigure 1:\nProcedure of assembling a furniture (Row 1).\nSingle Arm\nmay not stably assemble parts and a second robot is then introduced.\nBefore Interaction\n, part kinematics and dynamics, indicated by affordance, are ambiguous, and the interaction may fail.\nAfter Interaction\n, the adapted affordance proposes actions for stable support during assembly.\nIntroduction\nRobotic furniture assembly\n(Funkhouser\net al.\n2011\n; Jones\net al.\n2021\n; Lee\net al.\n2021\n; Tian\net al.\n2022\n,\n2025\n)\n, the task of combining functional components such as chair base, legs, and arms into a fully constructed shape, with a focus on both the overall structure and functions of each part, is a critical capability for home-assistive robots.\nRecent studies have addressed various aspects of robotic assembly, including motion planning\n(SuÃ¡rez-Ruiz\net al.\n2018\n; Sundaram\net al.\n2001\n; Le\net al.\n2009\n; Zhang\net al.\n2020b\n)\n, assembly pose estimation\n(Yu\net al.\n2021\n; Huang\net al.\n2020\n; Tie\net al.\n2025b\n; Jones\net al.\n2021\n; Shen\net al.\n2025\n)\n, and RL-based combinatorial sequence search\n(Xu\net al.\n2023\n; Zhang\net al.\n2024\n; Funk\net al.\n2022\n; Ghasemipour\net al.\n2022\n)\n. However, current robotic systems remain limited in their ability to assemble objects across diverse categories. Prior research has primarily focused on specific object types using a single robot arm\n(Heo\net al.\n2023\n; Lee\net al.\n2021\n)\n. In contrast, generalizable furniture assembly requires vision understanding and bi-manual operation that frequently changes which part to hold to counter-balance the insertion force from the other hand. This presents new challenges to vision perception and precise manipulation. First, assembling unseen furniture demands understanding functional affordances across various part geometries, requiring robots to identify viable support locations. Second, long-horizon assembly induces sequential state transitions where support strategies must dynamically refine based on part relations. Third, fine-grained assembly requires robust control skills to achieve precisly during contact-rich interactions.\nTo bridge these gaps, we introduce\nA3D\n: a framework that learns\nA\ndaptive\nA\nffordance\nA\nssembly for collaborative\nD\nual-arm manipulation. To enable geometric awareness, A3D leverages affordance as a representation of per-point actionability on objects for furniture assembly tasks. These per-point features are extracted hierarchically from local to global, effectively capturing detailed local geometry information for support and stabilization, as well as the contextual part relations that indicate whether the action would disturb other parts. This hierarchical structure enables A3D to localize stable support regions through fine-grained geometric cues while modeling higher-level part contexts to anticipate potential disturbances during manipulation.\nHowever, static affordance derived solely from passive observations fails to account for critical kinematic (e.g., joint locations and limits) and dynamic uncertainties (e.g., contact direction and force), which might misdirect manipulation (Fig.\n1\n). So we actively incorporate interaction feedback into affordance predictions, enabling dynamic adjustment of support strategies throughout assembly.\nAlthough existing simulation environments have facilitated progress in robotic manipulation, they remain limited in supporting the study of dual-arm furniture assembly.\nPrevious works predominantly focus on single-arm manipulation or utilize limited furniture assets\n(Niekum\net al.\n2013\n; SuÃ¡rez-Ruiz\net al.\n2018\n; Kimble\net al.\n2020\n; Heo\net al.\n2023\n; Zhang\net al.\n2024\n; Jones\net al.\n2021\n; Li\net al.\n2020\n)\n, failing to capture the unique physical and coordination challenges inherent in dual-arm assembly scenarios. To bridge this critical gap, we introduce a new evaluation environment extending FurnitureBench\n(Heo\net al.\n2023\n)\n, featuring 4 assembly task categories with 50 geometrically diverse parts across 8 furniture types. Both qualitative and quantitative results from simulations and real-world experiments demonstrate the effectiveness of our framework.\nWe also note that while our adaptation allows iterative refinement (max 3 rounds), most test cases succeeded after a single interaction (effective\nk\n=\n1\nk=1\n), demonstrating robustness and efficiency.\nIn conclusion, our contributions mainly include:\nâ€¢\nWe propose affordance learning framework for generalizable support and stabilization prediction in furniture assembly, enabling generalization across diverse parts.\nâ€¢\nWe further develop an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions.\nâ€¢\nWe build a simulation environment for dual-arm collaborative assembly featuring 50+ geometrically diverse parts across 8 furniture types and 4 task categories.\nâ€¢\nExtensive experiments in both simulation and real world demonstrate the effectiveness of our framework.\nRelated Work\nFurniture Assembly\nFurniture assembly is a prominent application in shape assembly, where individual components, each serving a distinct functional role (\ne.g.\n, chair arm, table leg), must be assembled following both geometric constraints and common-sense spatial and functional relations (e.g., a chair leg must be attached to the seat base with proper orientation and stability). The complexity arises from the need to reason about part functionality, structural dependencies, and physical constraints simultaneously. Previous research has mostly focused on assembly pose estimation\n(Li\net al.\n2020\n; Yu\net al.\n2021\n; Li\net al.\n2024\n; Huang\net al.\n2020\n)\n. For instance,\nLi\net al.\n(\n2020\n)\nlearns to assemble 3D shapes from 2D images, while\nHuang\net al.\n(\n2020\n)\nproposes image-free generative models for pose generation.\nHowever, these methods might neglect the challenges in dynamic robotic execution, particularly the need for precise dual-arm coordination, where one arm manipulates one part while the other actively provides collaborative support and stabilization throughout assembly. Addressing this challenge,\nespecially adaptive support strategies and generalization across diverse geometries, is a core objective of our work.\nVisual Affordance for Robotic Manipulation\nVisual affordance\n(Gibson\n1977\n)\nsuggests possible ways for\nagents to interact with objects for various manipulation tasks. This approach has been widely used in grasping\n(Corona\net al.\n2020\n; Kokic\net al.\n2020\n; Zeng\net al.\n2018\n)\n, articulated manipulation\n(Yuan\net al.\n2024\n; Tie\net al.\n2025a\n)\n, and scene interaction\n(Nagarajan and Grauman\n2020\n; Nagarajan\net al.\n2020\n)\n. Point-level affordance, in particular, assigns an actionability score to each point, and thus enables fine-grained geometry understanding and improved cross-shape generalization in diverse tasks, such as articulated\n(Mo\net al.\n2021\n; Wang\net al.\n2022\n; Chen\net al.\n2024\n)\n, and deformable\n(Wu\net al.\n2024\n,\n2023\n,\n2025\n; Wang\net al.\n2025\n)\nmanipulation. For furniture assembly scenarios, where parts vary significantly in geometry and require precise dual-arm collaboration, we empower point-level affordance with the awareness of part geometry, and further leverage active interactions to efficiently query uncertain kinematic or dynamic factors for learning more accurate instance-adaptive visual affordance.\nFigure 2:\nFramework Overview.\nAt each operation stage, the policy takes the point cloud and the selected action point as inputs to predict the support action. The robot moves the gripper to the recommended pose to support the assembly. If part displacement occursâ€”indicating insufficient supportâ€”the system logs the pre-support point cloud, executed action, and displacement as interaction context, then re-predicts the support action using the updated point cloud and accumulated context.\nFigure 3:\nPoint-Level Adaptation Support Affordance Framework.\nThe model completes support decisions by extracting visual features, computing Top-K point-level affordances and generating candidate directions, scoring and selecting pointâ€“direction pairs, and extracting interaction context features.\nMethod\nOur goal is to enable effective dual-arm coordination for furniture assembly, where a tool arm executes assembly operations while a support arm provides adaptive stabilization to prevent part displacement and ensure task success. As shown in Fig.\n2\n, our framework integrates two key components: (1)\nSupport Affordance Module\npredicts initial affordance heatmaps and corresponding action directions from visual observations and operation points; (2)\nInteraction Context Adaptation Module\nleverages physical feedback from interaction history to adjust affordance predictions.\nProblem Formulation\nWe formulate this as learning a closed-loop adaptive policy\nÏ€\nâ€‹\n(\nu\nt\n|\nS\nt\n,\nI\nt\n)\n\\pi(u_{t}|S_{t},I_{t})\n. At each timestep\nt\nt\n, the policy predicts the stabilization action\nu\nt\nu_{t}\nfor the support arm, conditioned on the observed state\nS\nt\nS_{t}\n, and the interaction context\nI\nt\nI_{t}\nwhich records the history of previous assembly trials.\nState\n:\nS\nt\n=\n(\nO\nt\n,\np\nt\no\nâ€‹\np\n)\nS_{t}=(O_{t},p^{op}_{t})\n, where\nO\nt\nâˆˆ\nâ„\nN\nÃ—\n6\nO_{t}\\in\\mathbb{R}^{N\\times 6}\nrepresents a 3D partial point cloud of the furniture parts with surface normals, and\np\nt\no\nâ€‹\np\np^{op}_{t}\ndenotes the operation point where the tool gripper contacts the target part.\nAction\n:\nu\nt\n=\n(\np\nt\ns\nâ€‹\np\n,\nğ\nğ­\n)\nu_{t}=(p^{sp}_{t},\\mathbf{d_{t}})\n, where\np\nt\ns\nâ€‹\np\nâˆˆ\nO\nt\np^{sp}_{t}\\in O_{t}\nis the support point and\nğ\nğ­\nâˆˆ\nS\nâ€‹\nO\nâ€‹\n(\n3\n)\n\\mathbf{d_{t}}\\in SO(3)\nis the support gripper orientation.\nInteraction Context\n:\nI\nt\n=\n{\n(\nO\ni\n,\nu\ni\n,\nm\ni\n)\n}\ni\n=\nt\nâˆ’\nk\nt\nâˆ’\n1\nI_{t}=\\{(O_{i},u_{i},m_{i})\\}_{i=t-k}^{t-1}\nstores information from previous\nk\nk\ninteraction steps, where\nm\ni\nm_{i}\ndenotes the base part displacement after step\ni\nâ€‹\ns\nis\n.\nTask Success\n:\nAn episode succeeds if the primary operation reaches its geometric goal while maintaining base part displacement\nm\ni\n<\nÏµ\nm_{i}<\\epsilon\nduring execution.\nSupport Affordance Module\nThe Support Affordance Module employs an affordanceâ€“proposalâ€“scoring architecture: the Affordance submodule predicts affordance maps and selects top-K candidate points; the Proposal submodule generates multiple candidate directions for each point; the Scoring submodule scores all pointâ€“direction pairs and selects the optimal support action (Steps 1â€“3, Fig.\n3\n).\nVisual Feature Extractor.\nPointNet++\n(Qi\net al.\n2017\n)\ngenerates point-wise features\nf\np\ni\nâˆˆ\nâ„\n128\nf_{p_{i}}\\in\\mathbb{R}^{128}\nfrom the point cloud\nO\nO\n. Operation and support points are encoded via shared MLPs into\nf\no\nâ€‹\np\n,\nf\ns\nâ€‹\np\nâˆˆ\nâ„\n32\nf_{op},f_{sp}\\in\\mathbb{R}^{32}\n, while gripper direction\nğ\n\\mathbf{d}\nand displacement\nm\nm\nare encoded into\nf\nğ\n,\nf\nm\nâˆˆ\nâ„\n32\nf_{\\mathbf{d}},f_{m}\\in\\mathbb{R}^{32}\n.\nAffordance Module.\nModule\nğ’œ\n\\mathcal{A}\npredict an affordance score\na\np\nâˆˆ\n[\n0\n,\n1\n]\na_{p}\\in[0,1]\nfor each point\np\np\n. It concatenates the operation-point feature\nf\np\no\nâ€‹\np\nf_{p^{op}}\n, the point feature\nf\np\ni\nf_{p_{i}}\nthe operation-point encoding\nf\no\nâ€‹\np\nf_{op}\n, and interaction context\nf\nI\nf_{I}\n, feeds this into the MLP, and outputs\na\np\ni\na_{p_{i}}\n. The top-K points by score are then selected as support candidates.\nAction Proposal Module.\nAction Proposal Module\nğ’«\n\\mathscr{P}\nimplements a Conditional Variational Autoencoder (cVAE). The encoder processes the operation point feature\nf\np\no\nâ€‹\np\nf_{p^{op}}\n, candidate support point feature\nf\np\ns\nâ€‹\np\nf_{p^{sp}}\nin point cloud features, operation point embedding\nf\no\nâ€‹\np\nf_{op}\n, support point embedding\nf\ns\nâ€‹\np\nf_{sp}\n, and interaction context feature\nf\nI\nf_{I}\n, to output latent vector\nz\nâˆˆ\nâ„\n128\nz\\in\\mathbb{R}^{128}\n.\nThe decoder then generates direction vector\nğ\n\\mathbf{d}\nfrom\nz\nz\n.\nAction Scoring Module.\nAction Scoring Module\nğ’®\n\\mathscr{S}\npredicts success scores\nc\nâˆˆ\n[\n0\n,\n1\n]\nc\\in[0,1]\nfor each action. An MLP takes concatenated features\nf\np\no\nâ€‹\np\nf_{p^{op}}\n,\nf\np\ns\nâ€‹\np\nf_{p^{sp}}\n,\nf\no\nâ€‹\np\nf_{op}\n,\nf\ns\nâ€‹\np\nf_{sp}\n, and\nf\nğ\nf_{\\mathbf{d}}\nand outputs the success likelihood. A higher\nc\nc\nsuggests a greater chance for the support hand to collaborate effectively and complete the task.\nInteraction Context Adaptation Module\nIf visual priors are insufficient (Fig.\n3\n, step 4), the system records the support action and its feedback. The Context Extractor derives features from these records, concatenates them with current visual features, and feeds them back to the Affordance, Proposal, and Scoring submodules to refine predictions that adhere to physical dynamics.\nInteraction Context Extractor Module.\nFor interaction context\nI\nt\n=\n(\nO\ni\n,\nu\ni\n,\nm\ni\n)\ni\n=\nt\nâˆ’\nk\nt\nâˆ’\n1\nI_{t}={(O_{i},u_{i},m_{i})}_{i=t-k}^{t-1}\n, we extract features for each historical step using the same encoders as above. Features are combined via\nf\nI\ni\n=\nM\nâ€‹\nL\nâ€‹\nP\nâ€‹\n(\nc\nâ€‹\no\nâ€‹\nn\nâ€‹\nc\nâ€‹\na\nâ€‹\nt\nâ€‹\n(\nf\nO\ni\n,\nf\nu\ni\n,\nf\nm\ni\n)\n)\n,\ni\nâˆˆ\n[\nt\nâˆ’\nk\n,\nt\nâˆ’\n1\n]\n.\nf_{I_{i}}=MLP(concat(f_{O_{i}},f_{u_{i}},f_{m_{i}})),i\\in[t-k,t-1].\n(1)\nTo aggregate information from all previous interactions, we adopt a lightweight attention mechanism.\nEach previous interaction feature\nf\nI\ni\nf_{I_{i}}\nis passed through an MLP to compute an attention weight\nw\ni\nw_{i}\n, and the final interaction context feature is obtained as a weighted average:\nf\nI\n=\nâˆ‘\ni\n=\nt\nâˆ’\nk\nt\nâˆ’\n1\nf\nI\ni\nÃ—\nw\ni\nâˆ‘\ni\n=\nt\nâˆ’\nk\nt\nâˆ’\n1\nw\ni\n.\nf_{I}=\\frac{\\sum_{i=t-k}^{t-1}f_{I_{i}}\\times w_{i}}{\\sum_{i=t-k}^{t-1}w_{i}}.\n(2)\nFigure 4:\nAffordance Map.\nThe figure displays affordance heatmaps generated for various objects in simulation before and after interaction. Red arrows indicate the direction of part movement, and circled regions denote the highest-scoring areas. In the subplots labeled â€œConcentrated,â€ it is evident that after interaction, high-scoring points converge more tightly at the correct locations; in the other subplots, the high-scoring points have shifted in accordance with the observed movement trends.\nTrain and Loss\nAction Scoring Loss.\nThe Action Scoring Module predicts a success score\nr\n^\n\\hat{r}\nand is trained with an MSE loss against a â€œrealâ€ score\nr\nr\n. This real score combines the objectâ€™s SE(3) movement distance\ng\nd\ng_{d}\nand a taskâ€completion term\ng\nc\ng_{c}\nâ€”the latter decreasing as completion improvesâ€”via weighted sum, then clamps the result to [0,1]:\nr\n=\nclamp\nâ€‹\n(\n(\n1\nâˆ’\n(\nÎ±\nÃ—\ng\nd\n+\nÎ²\nÃ—\ng\nc\n)\n)\n,\n0\n,\n1\n)\nr=\\text{clamp}((1-(\\alpha\\times g_{d}+\\beta\\times g_{c})),0,1)\n(3)\nwhere\nÎ±\n\\alpha\nand\nÎ²\n\\beta\nbalance the distance and completion.\nAction Proposal Loss.\nWe evaluate the loss using cosine similarity and Kullback-Leibler (KL) divergence.\n1.\nCosine Similarity Loss\n: We use a cosine-similarity loss\nâ„’\ncosine\n=\n1\nâˆ’\n(\nğ\n^\nâ‹…\nğ\n)\n/\n(\nâ€–\nğ\n^\nâ€–\nâ€‹\nâ€–\nğ\nâ€–\n)\n\\mathcal{L}_{\\text{cosine}}=1-(\\hat{\\mathbf{d}}\\cdot\\mathbf{d})/(\\|\\hat{\\mathbf{d}}\\|\\|\\mathbf{d}\\|)\nto align the predicted direction\nğ\n^\n\\hat{\\mathbf{d}}\nwith the ground-truth\nğ\n\\mathbf{d}\n.\n2.\nKL Divergence Loss\n: We add a KLâ€divergence term to regularize the latent variable\nz\nz\ninferred from\nğ\n^\n\\hat{\\mathbf{d}}\nand\nf\ni\nâ€‹\nn\nf^{in}\ntowards a standard normal:\nâ„’\nKL\n=\nD\nKL\n(\nq\n(\nz\n|\nğ\n^\n,\nf\ni\nâ€‹\nn\n)\n|\n|\nğ’©\n(\n0\n,\n1\n)\n\\mathcal{L}_{\\text{KL}}=D_{\\text{KL}}(q(z|\\hat{\\mathbf{d}},f^{in})||\\mathcal{N}(0,1)\n(4)\nThe overall Action Proposal loss then balances direction alignment and latent regularization:\nâ„’\np\nâ€‹\nr\nâ€‹\no\nâ€‹\np\nâ€‹\no\nâ€‹\ns\nâ€‹\na\nâ€‹\nl\n=\nÎ»\nd\nâ€‹\ni\nâ€‹\nr\nâ€‹\nâ„’\nc\nâ€‹\no\nâ€‹\ns\nâ€‹\ni\nâ€‹\nn\nâ€‹\ne\n+\nÎ»\nK\nâ€‹\nL\nâ€‹\nâ„’\nK\nâ€‹\nL\n\\mathcal{L}_{proposal}=\\lambda_{dir}\\mathcal{L}_{cosine}+\\lambda_{KL}\\mathcal{L}_{KL}\n(5)\nwhere\nÎ»\nd\nâ€‹\ni\nâ€‹\nr\n\\lambda_{dir}\nand\nÎ»\nK\nâ€‹\nL\n\\lambda_{KL}\nweight the cosine similarity and KL terms, respectively.\nAffordance Prediction Loss.\nSimilar to Where2Act and DualAfford, we define each pointâ€™s affordance score\na\na\nas the predicted success probability of actions proposed by the Action Proposal Module, and evaluated by the Action Scoring Module. Concretely, for each point\np\ni\np_{i}\nwe sample\nN\nN\nsupport directions, score them via Action Scoring Network to obtain\nN\nN\naction scores and average the top.\na\np\ni\n=\n1\nK\nâ€‹\nâˆ‘\nj\n=\n1\nK\nğ’®\nâ€‹\n(\nf\np\nâ€‹\ni\ni\nâ€‹\nn\n,\nğ’«\nâ€‹\n(\nf\np\nâ€‹\ni\ni\nâ€‹\nn\n,\nz\nj\n)\n)\na_{p_{i}}=\\frac{1}{K}\\sum_{j=1}^{K}\\mathscr{S}(f^{in}_{pi},\\mathscr{P}(f^{in}_{pi},z_{j}))\n(6)\nWe then apply L1 loss to measure the difference between the predicted affordance score\na\np\ni\n^\n\\hat{a_{p_{i}}}\nand the ground-truth\na\np\ni\na_{p_{i}}\n:\nâ„’\naffordance\n=\n|\na\np\ni\n^\nâˆ’\na\np\ni\n|\n\\mathcal{L}_{\\text{affordance}}=|\\hat{a_{p_{i}}}-a_{p_{i}}|\n(7)\nExperiment\nSetup\nEnvironment.\nWe build upon FurnitureBench in IsaacGym by extending it to support dual-arm coordination and modifying camera configurations, allowing us to study the collaborative support and stabilization using a second arm.\nTo boost and evaluate policy generalization, we extend the assets by increasing object geometric diversity.\nFor training, we collect Â 10k samples per task focusing on specific furniture types (e.g., desk, drawer, basket), each with multiple variants. Testing utilizes entirely unseen furniture types to validate cross-category generalization.\nTasks.\nWe evaluate on four fundamental assembly operations: (1)\nScrewing\n: rotating components while the support arm provides counteracting force; (2)\nInsertion\n: pushing components along rails with support arm guidance; (3)\nExtraction\n: pulling components while the support arm stabilizes the base; (4)\nPicking\n: lifting and placing with dual-arm coordination.\nMetrics.\nWe use success rate as the evaluation metric. Success requires: target component reaching desired pose within tolerance, base structure remaining stable (displacement/rotation below thresholds), and secure grasping above specified height for picking tasks.\nTrain Categories\nTest Categories\nMethod\nScrew\nPush\nPull\nPick Up\nScrew\nPush\nPull\nPick Up\nRandom\n10.7%\n11.1%\n5.0%\n13.9%\n9.0%\n7.2%\n4.0%\n6.5%\nHeuristic\n54.5%\n70.9%\n43.1%\n37.5%\n46.7%\n52.8%\n31.9%\n31.4%\nDP3\n23.2%\n41.5%\n19.4%\n22.9%\n17.4%\n22.1%\n10.1%\n11.5%\nLLM-Guided\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\n0.0%\nw/o Topâ€‘K\n66.7%\n73.5%\n67.7%\n66.1%\n54.1%\n53.7%\n52.4%\n41.1%\nw/o Adaptation\n54.9%\n74.3%\n76.3%\n52.9%\n34.2%\n63.9%\n55.6%\n42.2%\nOurs\n70.7%\n80.6%\n80.0%\n62.0%\n56.3%\n67.9%\n61.7%\n47.1\n%\nTable 1:\nComparison of baseline and ablation variants on the success rate metric.\nFigure 5:\nQualitative Analysis of Ablations.\n(Left) Without Top-K sampling, the robot fails to find robust manipulation points. (Right) Without interaction context, the robot lacks physical awareness to adjust its actions.\nBaselines and Ablations\nOur work targets adaptive support in dual-arm assembly, a novel setting not directly addressed by prior work. Thus, no existing method serves as a direct SOTA baseline. We compare against the following baselines and ablations:\nâ€¢\nRandom\n: Random selection of support points and directions.\nâ€¢\nHeuristic\n: Support point selection by geometric rules.\nâ€¢\n3D Diffusion Policy (DP3)\n(Ze\net al.\n2024\n)\n: Imitation learning for support prediction with point cloud input.\nâ€¢\nLLM-Guided\n(Comanici\net al.\n2025\n)\n: Inferring support point and action using Gemini 2.5 Pro.\nTo demonstrate the necessity of the proposed module, we compare with the following ablated versions:\nâ€¢\nw/o Top-K\n: Selecting only the single highest-scoring point instead of Top-K candidates.\nâ€¢\nw/o Adaptation\n: Removing adaptation with interaction.\nResults and Analysis\nFig.\n4\ndemonstrates the predicted affordance before and after the interactions, for different tasks, over training and novel object categories.\nBefore the interaction,\nthe learned affordance might be ambiguous (indicating a larger number of points that are plausible for manipulation) due to the uncertainty of object kinematics and dynamics.\nAfter a support action executed by the second robot, on the point selected by the proposed affordance,\nthe affordance will be adapted by the interaction feedback.\nEventually,\nthe manipulation regions indicated by the adapted affordance will be more concentrated on plausible support points.\nMoreover, the learned and adapted affordance, and the corresponding policy can generalize to novel geometries and categories, as point-level affordance aggregates both the low geometry (indicating where can be manipulated) and overall structure (indicating where to support).\nTab.\n1\nshows the quantitative results, and our proposed framework outperforms all baseline and ablation methods.\nHeuristic\nmethod, though more effective than\nRandom\nactions, requires manual rule design for each task and even object.\nDP3\nlacks the understanding of diverse shapes and categories.\nLLM-Guided\napproaches lack essential 3D geometry and a low-level fine-grained action understanding for precise manipulation.\nFor the analysis of\nablations\n, Tab.\n1\nand Fig.\n5\ntogether showcase the effectiveness of the proposed components.\nFigure 6:\nReal-World Experiments.\nWe validate our framework in real-world conditions. The experiments include three scenarios:screw a desk leg, screw chair leg, and push a cabinet door. The left path (â€Goodâ€) shows our policy directly finding a stable support. The right path (â€Badâ€ to â€Adaptationâ€) show the ability to adapt its support strategy to ultimately succeed.\nFor\nTop-K Sampling\n, generates a wider set of high-quality candidate actions for the support goal, for the following Action Scoring Module to further select the best actions.\nOn the contrary,\nif the framework only selects the best point indicated by the learned affordance,\nchances are that on this selected point the best action direction is worse than the action directions sampled on other points with high affordance scores.\nThe left side of Fig.\n5\nillustrates two failure cases when Top-K sampling is omitted. In the top-left case, although the affordance module provides the highest-scoring contact point, its combination with the direction proposed by the action module fails to provide optimal support due to the collision problem. In the bottom-left case, the highest-scoring contact point is located on a corner of the box that yields an unstable grasp, making successful execution highly improbable. In our scenarios with complex geometries and diverse tasks,\nTop-K sampling\neffectively expands the high-quality search space, markedly improving the probability of selecting the best action.\nThe\nAdaptation Mechanism\nbased on the interaction context enhances the perception of real-time physical properties, endowing the model with â€™physical awarenessâ€™. The right side of Fig.\n5\npresents failure examples when this module is removed. In the top-right of the â€Screwâ€ task, although providing a seemingly correct support action, without interaction contexts, the model is unaware of the complex thread direction. This leads to an inverse action that cannot support the tightening operation well.\nIn the bottom-right of the â€Pick-upâ€ task, the initial grasp causes the bucket to incline; by incorporating this tilt as interaction feedback, the model can automatically adjust the contact point and successfully lift the target object.\nAs shown in Fig.\n4\n, after incorporating interaction context, the model not only highlights contactable regions more accurately, but also reveals differences in physical interaction properties such as force direction and stability, significantly enhancing its perception and understanding of interaction states.\nReal-Word Experiments\nMethod\nScrew\nPush\nPick Up\nRandom\n0 / 15\n0 / 15\n0 / 15\nHeuristic\n8 / 15\n10 / 15\n5 / 15\nDP3\n4 / 15\n6 / 15\n5 / 15\nOurs\n11 / 15\n12 / 15\n9 / 15\nTable 2:\nReal world experimental results.\nWe set up two Franka Panda with the furniture positioned between them. Three RealSense cameras capturing 3D point cloud are mounted around the scene. Robot control is managed through ROS\n(Quigley\net al.\n2009\n)\nand the frankapy library\n(Zhang\net al.\n2020a\n)\n. Fig.\n6\ndemonstrates the complete pipeline from scene perception and adaptive affordance prediction based on interaction feedback.\nWe evaluate each task over 15 trials with varying furniture configurations on 3 tasks. As shown in Tab.\n2\n, our method significantly outperforms baselines and achieves high success rates in real-world assembly tasks.\nFig.\n6\nshows real-world observations, affordance and adaptation. Additional videos are provided in the supplementary material.\nThe primary failure mode in real-world stems from motion planning limitations.The RRTConnect algorithm cannot find feasible trajectories due to robotic arm or environmental constraints. In the future work, we plan to develop a policy for motion refinement to improve real-world robustness.\nConclusion\nWe propose A3D, a framework that learns adaptive affordances for dual-arm furniture assembly by identifying optimal support and stabilization locations. Our approach combines dense geometric representations for cross-geometry generalization with an adaptive module that leverages interaction feedback to dynamically adjust strategies. Experiments demonstrate superior performance in both simulation and real-world settings.\nAcknowledgments\nThe authors gratefully acknowledge the hard work and close collaboration of the team members. In particular, Jiaqi contributed to the code implementation, experimental work, and real-robot experiments; Yue provided mentoring and contributed to experimental design, real-robot experiments, and manuscript preparation; Qize contributed to portions of the experiments, including real-robot experiments, as well as figure preparation and website development. Yan assisted with manuscript polishing, and Ruihai contributed to idea generation and overall research advising.Haipeng initialized the project code and simulation.\nThe funding author for this paper unexpectedly withdrew the funding after camera-ready submission, and Ruihai covered all the associated costs.\nReferences\nY. Chen, C. Tie, R. Wu, and H. Dong (2024)\nEqvAfford: se(3) equivariance for point-level affordance learning\n.\nExternal Links:\n2408.01953\n,\nLink\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nG. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen,\net al.\n(2025)\nGemini 2.5: pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities\n.\narXiv preprint arXiv:2507.06261\n.\nCited by:\n4th item\n.\nE. Corona, A. Pumarola, G. Alenya, F. Moreno-Noguer, and G. Rogez (2020)\nGanhand: predicting human grasp affordances in multi-object scenes\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 5031â€“5041\n.\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nN. Funk, G. Chalvatzaki, B. Belousov, and J. Peters (2022)\nLearn2assemble with structured representations and search for robotic architectural construction\n.\nIn\nConference on Robot Learning\n,\npp.Â 1401â€“1411\n.\nCited by:\nIntroduction\n.\nT. Funkhouser, H. Shin, C. Toler-Franklin, A. G. CastaÃ±eda, B. Brown, D. Dobkin, S. Rusinkiewicz, and T. Weyrich (2011)\nLearning how to match fresco fragments\n.\nJournal on Computing and Cultural Heritage (JOCCH)\n4\n(\n2\n),\npp.Â 1â€“13\n.\nCited by:\nIntroduction\n.\nS. K. S. Ghasemipour, S. Kataoka, B. David, D. Freeman, S. S. Gu, and I. Mordatch (2022)\nBlocks assemble! learning to assemble with large-scale structured reinforcement learning\n.\nIn\nInternational Conference on Machine Learning\n,\npp.Â 7435â€“7469\n.\nCited by:\nIntroduction\n.\nJ. J. Gibson (1977)\nThe theory of affordances\n.\nHilldale, USA\n1\n(\n2\n),\npp.Â 67â€“82\n.\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nM. Heo, Y. Lee, D. Lee, and J. J. Lim (2023)\nFurnitureBench: reproducible real-world benchmark for long-horizon complex manipulation\n.\nIn\nRobotics: Science and Systems\n,\nCited by:\nIntroduction\n,\nIntroduction\n.\nJ. Huang, G. Zhan, Q. Fan, K. Mo, L. Shao, B. Chen, L. Guibas, and H. Dong (2020)\nGenerative 3d part assembly via dynamic graph learning\n.\nExternal Links:\n2006.07793\n,\nLink\nCited by:\nIntroduction\n,\nFurniture Assembly\n.\nB. Jones, D. Hildreth, D. Chen, I. Baran, V. G. Kim, and A. Schulz (2021)\nAutomate: a dataset and learning approach for automatic mating of cad assemblies\n.\nACM Transactions on Graphics (TOG)\n40\n(\n6\n),\npp.Â 1â€“18\n.\nCited by:\nIntroduction\n,\nIntroduction\n,\nIntroduction\n.\nK. Kimble, K. Van Wyk, J. Falco, E. Messina, Y. Sun, M. Shibata, W. Uemura, and Y. Yokokohji (2020)\nBenchmarking protocols for evaluating small parts robotic assembly systems\n.\nIEEE robotics and automation letters\n5\n(\n2\n),\npp.Â 883â€“889\n.\nCited by:\nIntroduction\n.\nM. Kokic, D. Kragic, and J. Bohg (2020)\nLearning task-oriented grasping from human activity datasets\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp.Â 3352â€“3359\n.\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nD. T. Le, J. CortÃ©s, and T. SimÃ©on (2009)\nA path planning approach to (dis) assembly sequencing\n.\nIn\n2009 IEEE International Conference on Automation Science and Engineering\n,\npp.Â 286â€“291\n.\nCited by:\nIntroduction\n.\nY. Lee, E. S. Hu, and J. J. Lim (2021)\nIKEA furniture assembly environment for long-horizon complex manipulation tasks\n.\nIn\n2021 ieee international conference on robotics and automation (icra)\n,\npp.Â 6343â€“6349\n.\nCited by:\nIntroduction\n,\nIntroduction\n.\nY. Li, K. Mo, Y. Duan, H. Wang, J. Zhang, and L. Shao (2024)\nCategory-level multi-part multi-joint 3d shape assembly\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 3281â€“3291\n.\nCited by:\nFurniture Assembly\n.\nY. Li, K. Mo, L. Shao, M. Sung, and L. Guibas (2020)\nLearning 3d part assembly from a single image\n.\nIn\nComputer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part VI 16\n,\npp.Â 664â€“682\n.\nCited by:\nIntroduction\n,\nFurniture Assembly\n.\nK. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani (2021)\nWhere2act: from pixels to actions for articulated 3d objects\n.\nIn\nCVPR\n,\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nT. Nagarajan and K. Grauman (2020)\nLearning affordance landscapes for interaction exploration in 3d environments\n.\nIn\nNeurIPS\n,\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nT. Nagarajan, Y. Li, C. Feichtenhofer, and K. Grauman (2020)\nEGO-topo: environment affordances from egocentric video\n.\nIn\nCVPR\n,\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nS. Niekum, S. Chitta, A. G. Barto, B. Marthi, and S. Osentoski (2013)\nIncremental semantically grounded learning from demonstration.\n.\nIn\nRobotics: Science and Systems\n,\nVol.\n9\n,\npp.Â 10â€“15607\n.\nCited by:\nIntroduction\n.\nC. R. Qi, L. Yi, H. Su, and L. J. Guibas (2017)\nPointnet++: deep hierarchical feature learning on point sets in a metric space\n.\nAdvances in neural information processing systems\n30\n.\nCited by:\nVisual Feature Extractor.\n.\nM. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng (2009)\nROS: an open-source Robot Operating System\n.\nIn\nICRA Workshop on Open Source Software\n,\nKobe, Japan\n,\npp.Â 5\n.\nCited by:\nReal-Word Experiments\n.\nY. Shen, R. Wu, Y. Ke, X. Song, Z. Li, X. Li, H. Fan, H. Lu, and H. dong (2025)\nBiAssemble: learning collaborative affordance for bimanual geometric assembly\n.\nExternal Links:\n2506.06221\n,\nLink\nCited by:\nIntroduction\n.\nF. SuÃ¡rez-Ruiz, X. Zhou, and Q. Pham (2018)\nCan robots assemble an ikea chair?\n.\nScience Robotics\n3\n(\n17\n),\npp.Â eaat6385\n.\nCited by:\nIntroduction\n,\nIntroduction\n.\nS. Sundaram, I. Remmler, and N. M. Amato (2001)\nDisassembly sequencing using a motion planning approach\n.\nIn\nProceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164)\n,\nVol.\n2\n,\npp.Â 1475â€“1480\n.\nCited by:\nIntroduction\n.\nY. Tian, J. Jacob, Y. Huang, J. Zhao, E. Gu, P. Ma, A. Zhang, F. Javid, B. Romero, S. Chitta, S. Sueda, H. Li, and W. Matusik (2025)\nFabrica: dual-arm assembly of general multi-part objects via integrated planning and learning\n.\nExternal Links:\n2506.05168\n,\nLink\nCited by:\nIntroduction\n.\nY. Tian, J. Xu, Y. Li, J. Luo, S. Sueda, H. Li, K. D. Willis, and W. Matusik (2022)\nAssemble them all: physics-based planning for generalizable assembly by disassembly\n.\nACM Transactions on Graphics (TOG)\n41\n(\n6\n),\npp.Â 1â€“11\n.\nCited by:\nIntroduction\n.\nC. Tie, Y. Chen, R. Wu, B. Dong, Z. Li, C. Gao, and H. Dong (2025a)\nET-seed: efficient trajectory-level se(3) equivariant diffusion policy\n.\nExternal Links:\n2411.03990\n,\nLink\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nC. Tie, S. Sun, J. Zhu, Y. Liu, J. Guo, Y. Hu, H. Chen, J. Chen, R. Wu, and L. Shao (2025b)\nManual2Skill: learning to read manuals and acquire robotic skills for furniture assembly using vision-language models\n.\nExternal Links:\n2502.10090\n,\nLink\nCited by:\nIntroduction\n.\nY. Wang, R. Wu, K. Mo, J. Ke, Q. Fan, L. Guibas, and H. Dong (2022)\nAdaAfford: learning to adapt manipulation affordance for 3d articulated objects via few-shot interactions\n.\nEuropean conference on computer vision (ECCV 2022)\n.\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nY. Wang, R. Wu, Y. Chen, J. Wang, J. Liang, Z. Zhu, H. Geng, J. Malik, P. Abbeel, and H. Dong (2025)\nDexGarmentLab: dexterous garment manipulation environment with generalizable policy\n.\narXiv preprint arXiv:2505.11032\n.\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nR. Wu, H. Lu, Y. Wang, Y. Wang, and H. Dong (2024)\nUniGarmentManip: a unified framework for category-level garment manipulation via dense visual correspondence\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nR. Wu, C. Ning, and H. Dong (2023)\nLearning foresightful dense visual affordance for deformable object manipulation\n.\nIn\nIEEE International Conference on Computer Vision (ICCV)\n,\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nR. Wu, Z. Zhu, Y. Wang, Y. Chen, J. Wang, and H. Dong (2025)\nGarmentPile: point-level visual affordance guided retrieval and adaptation for cluttered garments manipulation\n.\nExternal Links:\n2503.09243\n,\nLink\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nJ. Xu, S. Kim, T. Chen, A. R. Garcia, P. Agrawal, W. Matusik, and S. Sueda (2023)\nEfficient tactile simulation with differentiability for robotic manipulation\n.\nIn\nConference on Robot Learning\n,\npp.Â 1488â€“1498\n.\nCited by:\nIntroduction\n.\nM. Yu, L. Shao, Z. Chen, T. Wu, Q. Fan, K. Mo, and H. Dong (2021)\nRoboassembly: learning generalizable furniture assembly policy in a novel multi-robot contact-rich simulation environment\n.\narXiv preprint arXiv:2112.10143\n.\nCited by:\nIntroduction\n,\nFurniture Assembly\n.\nC. Yuan, C. Wen, T. Zhang, and Y. Gao (2024)\nGeneral flow as foundation affordance for scalable robot learning\n.\narXiv preprint arXiv:2401.11439\n.\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nY. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu (2024)\n3D diffusion policy: generalizable visuomotor policy learning via simple 3d representations\n.\nExternal Links:\n2403.03954\n,\nLink\nCited by:\n3rd item\n.\nA. Zeng, S. Song, K. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma, O. Taylor, M. Liu, E. Romo,\net al.\n(2018)\nRobotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching\n.\nIn\nICRA\n,\nCited by:\nVisual Affordance for Robotic Manipulation\n.\nK. Zhang, M. Sharma, J. Liang, and O. Kroemer (2020a)\nA modular robotic arm control stack for research: franka-interface and frankapy\n.\narXiv preprint arXiv:2011.02398\n.\nCited by:\nReal-Word Experiments\n.\nX. Zhang, M. Tomizuka, and H. Li (2024)\nBridging the sim-to-real gap with dynamic compliance tuning for industrial insertion\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 4356â€“4363\n.\nCited by:\nIntroduction\n,\nIntroduction\n.\nX. Zhang, R. Belfer, P. G. Kry, and E. Vouga (2020b)\nC-space tunnel discovery for puzzle path planning\n.\nACM Transactions on Graphics (TOG)\n39\n(\n4\n),\npp.Â 104â€“1\n.\nCited by:\nIntroduction\n.\nAppendix A\nSimulation Environment\nFurnitureBench\nFurnitureBench is a benchmark designed for real-world furniture assembly tasks. Built on Isaac Gym, it provides a single-arm Franka robot and a variety of furniture assets. We extended the original simulation environment with several modifications: we introduced an auxiliary gripper, designating the existing Franka arm as the primary manipulator and the added gripper as an assistant to execute our support policy.\nConcurrently, we adjusted the number and placement of cameras in the environment by deploying four camerasâ€”positioned at the front, rear, left, and rightâ€”to capture depth images, which were subsequently processed into point clouds. The camera resolution is set to\n1280\nÃ—\n720\n1280\\times 720\nwith a frame rate of 30 fps.\nFigure 7:\nPartial Assets utilized in the simulation.\nAdditionally, to validate our methodâ€™s generalization across different geometries, we manually adjusted the meshes of selected furniture assets, creating a broader set of parts with diverse shapes.\nTask Details\nWe selected four common operations in assembly scenarios as our primary task categories:\nâ€¢\nScrewing\n: This category includes specific tasks such as screwing legs onto tables of various shapes, tightening chair legs, and screwing bulbs into lamps.\nâ€¢\nPushing\n: This category includes tasks such as inserting drawer boxes into drawer bodies at various handle positions and installing cabinet doors into wardrobe frames along guide rails.\nâ€¢\nPulling\n: This category includes pulling out a drawer box and extracting target components from a stacked assembly.\nâ€¢\nPicking Up\n: This category involves picking up objects of various shapes, such as drawer boxes, buckets, baskets, and boxes.\nAll tasks inherently require dual-arm coordination: the primary arm executes the operation (screwing, pushing, pulling, or picking), while the support arm applies stabilizing forces to ensure physical feasibility. Without this support policy, these tasks cannot be completed successfully in either simulation or real-world settings.\nAppendix B\nReal World Experiments\nIn the real-world experiments, we evaluated the\nscrew\n,\npush\n, and\npick up\ntasks. Due to the size constraints of the 3D-printed parts and the physical workspace limitations of the Franka arm, we did not perform experiments for the\npull\ntask. The task configurations remained consistent with those in the simulation.\nNotably, since the LLM-guided approach performed poorly in the simulation environment, it was not included in the real-world experimental comparisons.\nAppendix C\nSimulation Experiments\nFor each method, we conducted three test rounds consisting of 100 trials each and reported the average success rate.\nLLM-guided Baseline\nImplementation Details\nThe LLM baseline receives four RGB views and task-specific textual prompts. Although the model allows for coarse-level object localization, its predictions frequently fall off-object or violate physical constraints (e.g., selecting non-contactable surfaces). Consequently, the success rate was 0% across all tasks. We retain this baseline primarily for reference to highlight the gap between geometric/physical reasoning and current visionâ€“language capabilities.\nPrompt Design\nThe box below illustrates the prompt structure used. For different tasks,\n{object_name}\nand\n{task_specific_description}\nare replaced with their respective task-specific content.\nYou will receive four RGB images showing a robotic workspace from the front, back, left, and right perspectives, in that order. The scene depicts a human hand (simulated by another robot) and a robot arm working together to assemble a {object_name}. The current task is to {task_specific_description}.\nFirst, you must identify the precise location and boundaries of the target object in each imageâ€”remember it typically has a color that contrasts strongly with the background and is usually near the center of the frame. Once you have localized the object, choose the single best interaction point on its surface.\nYour goal is to identify the single best point for the robot arm to interact with to successfully assist the human hand. Analyze all four images for full context, but your final output must be a three-element tuple indicating:\n1. Which image (use 0 for front, 1 for back, 2 for left, 3 for right)\n2. The row index (pixel row)\n3. The column index (pixel column)\nThe chosen point must lie on the target object and represent a stable, effective location for the robotâ€™s gripper to apply force or grasp. Double-check that your selected point truly fulfills the taskâ€™s requirements before answering.\nYou must return **only** the 3-tuple. No additional text or explanation.\nFor example: (0, 412, 351)\nBelow is an example of a task-specific description used in the prompt:\nScrew Task Description:\nAssist with a screwing task. The human hand is actively screwing a part (e.g., a leg) into the main body of the {object_name}. To prevent the main body from rotating, the robot arm needs to apply counter-pressure by firmly holding or bracing the main body. Identify the best point on the main body for the robot to press against to provide this stability.\nResults\nFig.\n8\nillustrates a failure case where the selected point does not lie on the object surface.\nFigure 8:\nAn example of an LLM failure case.\nAppendix D\nImplementation Details\nData Collection\nDataset Overview\nFor each task category (screwing, pushing, pulling, picking), we collected approximately 10,000 samples on a single furniture type (desk for screwing, drawer for push/pull, basket for pick), each containing several geometric variants. All evaluation tasks utilized unseen furniture types to ensure category-level generalization.\nOffline Data Collection\nDuring the data collection process, we observed that traditional random sampling was inefficient; some tasks yielded very low success rates with random sampling, hindering the effective training of the Action Proposal Module. To address this, we adopted a hybrid sampling approach combining random sampling and heuristic sampling.\nIn\nrandom sampling\n, we select a point randomly and sample a support direction near the pointâ€™s normal. In\nheuristic sampling\n, we manually designate regions on the objectâ€™s point cloud where interaction is more likely to succeed for specific tasks. We then sample points and support directions within these regions. This hybrid approach ensures both diversity and a sufficient proportion of successful trials for effective training. Crucially, the collected offline data includes both pre-interaction successes and failures, enabling the model to learn a robust prior affordance distribution before online adaptation.\nSince our model requires\ninteraction context\ndata, we employed a multi-step interaction sampling strategy. We define a maximum interaction context horizon and a movement threshold\nÏµ\n\\epsilon\n. If the manipulated objectâ€™s movement exceeds\nÏµ\n\\epsilon\n, we infer that the current support action\nu\nu\nis insufficient. We then record the action\nu\nu\n, the pre-operation point cloud\nO\nO\n, and the movement\nm\nm\nas one interaction context frame. During multi-step sampling, we randomly switch between random and heuristic methods at each step until task completion or the horizon limit is reached. We collected 10,000 samples for each task to train our policy.\nOnline Adaptive Data Collection\nAlthough the hybrid offline collection improves efficiency, it may not cover all scenarios. To enhance robustness, we propose online adaptive data collection to sample from a broader range of potential success sub-regions.\nSpecifically, the model selects the most suitable manipulation point and generates action proposals. The best proposal is selected via a scoring mechanism, and action\nu\nu\nis executed. During execution, multiple actions may be chosen (similar to offline collection) to generate interaction context data. Upon task completion, the data is saved.\nWhen the collected online data reaches a specified volume, it is combined with an equal number of offline samples. This mixed dataset is used to fine-tune the model.\nFor adaptation, we use 64 samples (32 from the current test interaction, 32 from prior offline data) per update, repeating the update 3 times. This is an offline adaptation process rather than continuous online fine-tuning. Inference speed remains at the millisecond level, causing no noticeable delay.\nPolicy Network\nThe network architecture is detailed in the main text. Specifically, we employ a segmentation version of PointNet++ as the visual feature extractor, obtaining 128-dimensional features for each point. For both action-point and support-point features, we use the same single-layer MLP to extract 32-dimensional representations; support-direction features are similarly extracted via a single-layer MLP with 32-dimensional outputs. The Affordance and Critic modules are implemented as multi-layer MLPs (hidden dim=128) mapping input features to scores. The Actor module utilizes a Conditional Variational Autoencoder (CVAE) with hidden layers and latent space dimension set to 128. Each moduleâ€™s feature extractor is independent and does not share parameters.\nTraining Details\nFor the Critic module, we set the weights\nÎ±\n\\alpha\nand\nÎ²\n\\beta\nof the Action Score Loss to 1:1. For the Actor module, the weights balancing KL divergence (\nÎ»\nK\nâ€‹\nL\n\\lambda_{KL}\n) and cosine similarity (\nÎ»\nd\nâ€‹\ni\nâ€‹\nr\n\\lambda_{dir}\n) in the Action Proposal Loss are also set to 1:1.\nWe employ the\nAdam\noptimizer with an\ninitial learning rate\nof\n0.001\nand a\nweight decay\nof\n1e-5\n. A\nStepLR scheduler\nmultiplies the learning rate by\n0.9\nevery\n500\nsteps. The\nbatch size\nis\n64\n. We use early stopping if the learning rate falls below 5e-7.\nSoftware and Hardware\nExperiments were conducted on Ubuntu 20.04.6 LTS, equipped with an Intel(R) Xeon(R) Gold 5220 CPU @ 2.20 GHz and 125 GiB RAM. The GPU is an NVIDIA GeForce RTX 3090 with CUDA 12.4.",
    "preview_text": "Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries. We propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions. We establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.\n\nA3D:\nA\ndaptive\nA\nffordance\nA\nssembly with\nD\nual-Arm Manipulation\nJiaqi Liang\n1\n\\equalcontrib\n,\nYue Chen\n1\n\\equalcontrib\n,\nQize Yu\n1\n\\equalcontrib\n,\nYan Shen\n1\n,\nHaipeng Zhang\n1\n,\nHao Dong\n1\n,\nRuihai Wu\n1\nCorresponding Authors.\nAbstract\nFurniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization.\nTo accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries.\nWe propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric represent",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "dual-arm manipulation",
        "adaptive affordances",
        "generalization",
        "furniture assembly",
        "point-level geometric representations"
    ],
    "one_line_summary": "A3Dæ¡†æ¶é€šè¿‡è‡ªé€‚åº”æ„ŸçŸ¥å­¦ä¹ ï¼Œåœ¨åŒè‡‚æœºå™¨äººè£…é…å®¶å…·ä»»åŠ¡ä¸­ä¼˜åŒ–æ”¯æ’‘ç­–ç•¥ï¼Œå®ç°è·¨å‡ ä½•æ³›åŒ–ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-16T08:21:42Z",
    "created_at": "2026-01-20T17:49:59.142978",
    "updated_at": "2026-01-20T17:49:59.142988"
}