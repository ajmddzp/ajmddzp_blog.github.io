{
    "id": "2512.01809v2",
    "title": "Much Ado About Noising: Dispelling the Myths of Generative Robotic Control",
    "authors": [
        "Chaoyi Pan",
        "Giri Anantharaman",
        "Nai-Chieh Huang",
        "Claire Jin",
        "Daniel Pfrommer",
        "Chenyang Yuan",
        "Frank Permenter",
        "Guannan Qu",
        "Nicholas Boffi",
        "Guanya Shi",
        "Max Simchowitz"
    ],
    "abstract": "ÁîüÊàêÂºèÊ®°ÂûãÔºåÂ¶ÇÊµÅÊ®°ÂûãÂíåÊâ©Êï£Ê®°ÂûãÔºåÊúÄËøëÂú®Êú∫Âô®‰∫∫Â≠¶‰∏≠Â∑≤Êàê‰∏∫ÊµÅË°å‰∏îÈ´òÊïàÁöÑÊîøÁ≠ñÂèÇÊï∞ÂåñÊñπÊ≥ï„ÄÇÂÖ≥‰∫éÂÖ∂ÊàêÂäüËÉåÂêéÁöÑÂõ†Á¥†ÔºåÂ≠¶ÁïåÂ≠òÂú®ËØ∏Â§öÊé®ÊµãÔºå‰ªéÊçïÊçâÂ§öÊ®°ÊÄÅÂä®‰ΩúÂàÜÂ∏ÉÂà∞Ë°®ËææÊõ¥Â§çÊùÇÁöÑË°å‰∏∫Á≠â„ÄÇÊú¨Á†îÁ©∂ÂØπÂ∏∏ËßÅÁöÑË°å‰∏∫ÂÖãÈöÜÂü∫ÂáÜÊµãËØï‰∏≠ÊµÅË°åÁöÑÁîüÊàêÂºèÊéßÂà∂Á≠ñÁï•ËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÁîüÊàêÂºèÊéßÂà∂Á≠ñÁï•ÁöÑÊàêÂäüÂπ∂ÈùûÊ∫ê‰∫éÂÖ∂ÊçïÊçâÂ§öÊ®°ÊÄÅÊàñË°®ËææÊõ¥Â§çÊùÇËßÇÂØü-Âä®‰ΩúÊò†Â∞ÑÁöÑËÉΩÂäõ„ÄÇÁõ∏ÂèçÔºåÂÖ∂‰ºòÂäøÊ∫ê‰∫éËø≠‰ª£ËÆ°ÁÆóÔºåÂâçÊèêÊòØËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂØπ‰∏≠Èó¥Ê≠•È™§ËøõË°åÁõëÁù£Ôºå‰∏îËøôÁßçÁõëÁù£‰∏éÈÄÇÂΩìÁöÑÈöèÊú∫ÊÄßÊ∞¥Âπ≥Áõ∏ÁªìÂêà„ÄÇ‰∏∫È™åËØÅËøô‰∏ÄÂèëÁé∞ÔºåÊàë‰ª¨ËØÅÊòé‰∫Ü‰∏ÄÁßçÊúÄÂ∞èËø≠‰ª£Á≠ñÁï•‚Äî‚Äî‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑ‰∏§Ê≠•ÂõûÂΩíÁ≠ñÁï•‚Äî‚ÄîÂü∫Êú¨‰∏éÊµÅÁîüÊàêÂºèÊéßÂà∂Á≠ñÁï•ÁöÑÊÄßËÉΩÁõ∏ÂΩìÔºå‰∏îÂæÄÂæÄ‰ºò‰∫éËí∏È¶èÂêéÁöÑÁÆÄÂåñÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÁîüÊàêÂºèÊéßÂà∂Á≠ñÁï•‰∏≠ÁöÑÂàÜÂ∏ÉÊãüÂêàÁªÑ‰ª∂Âπ∂‰∏çÂÉèÈÄöÂ∏∏ËÆ§‰∏∫ÁöÑÈÇ£Ê†∑ÈáçË¶ÅÔºåËøô‰∏∫‰∏ìÊ≥®‰∫éÊéßÂà∂ÊÄßËÉΩÁöÑÊñ∞ËÆæËÆ°Á©∫Èó¥ÊåáÊòé‰∫ÜÊñπÂêë„ÄÇÈ°πÁõÆÈ°µÈù¢Ôºöhttps://simchowitzlabpublic.github.io/much-ado-about-noising-project/",
    "url": "https://arxiv.org/abs/2512.01809v2",
    "html_url": "https://arxiv.org/html/2512.01809v2",
    "html_content": "Much Ado About Noising: Dispelling the Myths of Generative Robotic Control\nChaoyi Pan\n1\n1\n1\n{chaoyip, giria, naichieh, clairej, gqu, nboffi, gshi, msimchow}@andrew.cmu.edu\n,\n$\n\\mathdollar\nGiri Anantharaman\n1\nNai-Chieh Huang\n1\nClaire Jin\n1\nDaniel Pfrommer\n2\n2\n2\ndpfrom@mit.edu\nChenyang Yuan\n3\n3\n3\n{chenyang.yuan,frank.permenter}tri.global\nFrank Permenter\n3\nGuannan Qu\n1\n,\n‚Ä†\n\\dagger\nNicholas Boffi\n1\n,\n‚Ä†\n\\dagger\nGuanya Shi\n1\n,\n‚Ä†\n\\dagger\nMax Simchowitz\n1\n,\n‚Ä†\n\\dagger\n$\nProject lead.\n‚Ä†\nEqual advising.\na\nCarnegie Mellon University\nb\nMassachusetts Institute of Technology\nc\nToyota Research Institute\nAbstract\nGenerative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors.\nIn this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks.\nWe find that GCPs\ndo not\nowe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings.\nInstead, we find that their advantage stems from\niterative computation\n, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of\nstochasticity\n.\nAs a validation of our findings, we show that a minimal iterative policy (\nMIP\n), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models.\nOur results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance.\nFigure 1:\nLeft:\nAfter careful ablation on each component over 28 common behavior cloning benchmarks with diverse input modalities (state, pixel, point cloud and language), architectures (with both raw and pre-trained models, like\n0\n) and tasks (standard single-task benchmarks and multi-task benchmark like\nLIBERO\n), we refute a number of popularly held misconceptions about why\ngenerative control policies\n(GCPs) outperform regression policies (RCP) on these tasks.\nCenter:\nWe identify that the most important factor contributing to GCP success is a combination of\nstochastic injection\n(\nC\n2\n) and\nsupervised iterative computation\n(\nC\n3\n).\nSurprisingly, distribution learning (\nC\n1\n) is the least important factor, due to the absence of learned multi-modality (\nSection\nÀú\n3.2\n).\nRight:\nThe average relative success rate to flow of 7 most challenging tasks. We propose a simple two-step\nminimal iterative policy\n(\nMIP\n) whose performance matches that of flow-based GCPs.\nWebsite:\nhttps://simchowitzlabpublic.github.io/much-ado-about-noising-project/\nCode:\nhttps://github.com/simchowitzlabpublic/much-ado-about-noising\nDocumentation:\nhttps://simchowitzlabpublic.github.io/much-ado-about-noising/\n1\nIntroduction\nLong-horizon, dexterous manipulation tasks such as furniture assembly, food preparation, and manufacturing have been a holy grail in robotics.\nRecent large robot action models\n(\nteamCarefulExaminationLarge2025\n;\nblack2024pi_0\n;\nkim2024openvla\n)\nhave made substantial breakthroughs towards these goals by imitating expert demonstrations of diverse qualities.\nWe provide a more comprehensive review of related work in\nSection\nÀú\n6\n, but highlight here a key trend:\nwhile supervised learning from demonstration, also known as\nbehavior cloning\n(BC), has been applied across domains for decades\n(\npomerleau1988alvinn\n)\n, its recent success in robotic manipulation has coincided with the adoption of what we term\ngenerative control policies\n(GCPs): robotic control policies that use generative modeling architectures, such as diffusion models, flow models, and autoregressive transformers, as parameterizations of the mapping from observation to action.\nGiven the seemingly transformative nature of GCPs for robot learning, there has been much speculation about the origin of their superior performance relative to policies trained with a regression loss, henceforth\nregression control policies\n(RCPs).\nGCPs, by modeling conditional distributions over actions, are uniquely suited to the multi-task pretraining paradigm popular in today‚Äôs large robotic models.\nHowever, a number of hypotheses regarding the superiority of GCPs pertain even in the\nsingle task\nsetting\n(\nchi2023diffusion\n;\nreuss2023goal\n)\n:\nH1\n.\nBetter performance on pixel-based control\nH2\n.\nCapturing multi-modality in the training data\nH3\n.\nGreater expressivity due to iterative computation of the observation-to-action mapping\nH4\n.\nRepresentation learning due to stochastic data augmentation\nH5\n.\nImproved training stability and scalability\nIn this work, we systematically investigate these hypotheses to understand the mechanism by which GCPs have attained superior performance over RCPs. We aim to answer:\nIs there\nreally\na benefit to using GCPs for behavior cloning, or are their claimed successes ‚Ä¶\nmuch ado about noising?\nThe gap between generative modeling and generative control.\nThe objective for generative modeling in text and image domains is fundamentally different from the goal in a control task. In the former, one aims to generate high-quality and\ndiverse\nsamples from the original data distribution. In the latter, it suffices to select\nany\naction that leads to better downstream performance. Whereas much of the generative modeling literature has focused on the distribution of the\ngenerated variable\n(\nlee2023convergence\n)\n,\nwe aim to understand if it is necessary to reproduce the expert data distribution‚Äîfor example by capturing any multi-modality‚Äîto attain strong control performance. If not, is most salient to capture about the\nconditioning relationship\nmapping\no\n‚Üí\na\no\\to a\n?\n1.1\nContributions.\nThis paper adopts careful experimental methodology to rigorously test the key design components (\nSection\nÀú\n4\n) that contribute to the observed success of GCPs, and to account for the key mechanisms by which they contribute to improved performance in behavior cloning (\nSection\nÀú\n5\n).\nWe restrict our study to flow-based GCPs, given their popularity and adoption in industry\n(\nblack2024pi_0\n;\nintelligence$p_05$VisionLanguageActionModel2025\n;\nnvidia2025gr00t\n)\n.\nWe begin by first identifying which factors\ndo not\ncontribute to the advantage of GCPs over RCPs.\nContribution 1\n(\nNeither multi-modality nor policy expressivity account for GCPs‚Äô success\n,\nSection\nÀú\n3\n)\n.\nThrough careful benchmarking, we show that RCPs with appropriate architectures are highly competitive on both state- and image-based (\nH1\n) robot learning benchmarks as well as vision-language-action (VLA) model finetuning (\nSection\nÀú\n3.1\n). Performance gaps only arise on certain tasks requiring high precision. However, we show that neither multi-modality (\nH2\n,\nSection\nÀú\n3.2\n) nor the ability to express more complex functions via multiple integration steps (\nH3\n,\nSection\nÀú\n3.3\n) satisfactorily accounts for this phenomenon. In fact, GCPs do not even provide greater trajectory diversity compared to RCPs (\nAppendix\nÀú\nG\n).\nEssential to this finding is controlling for architecture: to our knowledge, we are the first work to carefully benchmark expressive architectures popularized for Diffusion\n(\nchi2023diffusion\n;\ndasariIngredientsRoboticDiffusion2024\n)\nas regression policies. To determine what contributes to GCPs performance on these high-precision tasks (beyond architectural optimization), we parse the design space of generative control policies into three components, depicted in\nFigure\nÀú\n1\n(left).\nContribution 2\n(\nExposing the design space of GCPs\n,\nSection\nÀú\n4\n)\n.\nWe introduce a novel taxonomy that parses the three essential design components of GCPs:\nC\n1\n.\nDistributional Learning\n: matching a conditional distribution of actions given observations.\nC\n2\n.\nStochasticity Injection\n: injecting noise during training to improve the learning dynamics.\nC\n3\n.\nSupervised Iterative Computation\n: generating output with multiple steps, each of which receives supervision during training.\nWith this taxonomy in hand,\nSection\nÀú\n4.1\nintroduces a family of algorithms, each of which lies along a spectrum between GCPs and RCPs by exhibiting different combinations of the above components. While we find that neither\nC\n2\nnor\nC\n3\nin isolation improve over regression, we find their combination yields a policy whose performance is competitive with flow, leading to our next contribution.\nContribution 3\n(\nMIP\n: the power of\nC\n2\n+\nC\n3\n,\nSections\nÀú\n4.2\nand\n4.1\n)\n.\nAs an algorithmic ablation that only combines\nC\n2\n+\nC\n3\n, we devise a\nminimal iterative policy\n(\nMIP\n), which invokes only two iterations, one-step of stochasticity during training, and deterministic inference. Despite its simplicity,\nMIP\nessentially matches the performance of flow-based GCPs across state-, pixel- and 3D point-cloud-based BC tasks, exposing that the combination of\nC\n2\n+\nC\n3\nis responsible for the observed success of GCPs. In addition, we find that\nMIP\noften outperforms shortcut/few-step policies\n(\nSection\nÀú\n4.3\n). This confirms our findings that distributional learning (which few-step policies, but not\nMIP\n, achieve) is not needed in robotic control.\nAs described in\nSection\nÀú\n4.3\n,\nMIP\nis substantively distinct from flow-map-based models\n(\nboffiFlowMapMatching2025\n;\nboffiHowBuildConsistency2025\n)\n, including consistency models\n(\nsongConsistencyModels2023\n;\nkim2023consistency\n)\nand their extensions\n(\ngengMeanFlowsOnestep2025\n;\nfransOneStepDiffusion2024\n)\n, in that the latter do satisfy\nC\n1\n, and require training over a continuum of noise levels.\nContribution 4\n(\nAttributing the benefits of\nC\n2\n+\nC\n3\n,\nSection\nÀú\n5\n)\n.\nWe identify that a property we term\nmanifold adherence\ncaptures the inductive bias of GCPs and\nMIP\nrelative to RCPs, even in the absence of lower validation loss. We explain how this property is a useful proxy for closed-loop performance in control tasks.\nFinally, we expose how\nC\n3\n, through iterative computation, encourages manifold adherence, but only if stochasticity during training (\nC\n2\n) is present to mitigate compounding errors across iteration steps (as described in\nSection\nÀú\n5.2\n).\nManifold adherence in\nSection\nÀú\n5.1\nmeasures the generated action‚Äôs plausibility given out of distribution observations, where only off-manifold component is evaluated rather than the distance to the neighbors\n(\npari2021surprising\n)\n. Note that manifold adherence reflects a favorable inductive bias during learning, rather than brute expressivity of more complex behavior (\nH3\n). Moreover,\nC\n2\nprovides more of a supporting role to\nC\n3\n, rather than enhancing data-augmentation in its own right (\nH4\n).\nIn addition, we find that\nC\n2\n+\nC\n3\nalso enhance scaling behavior (\nH5\n), likely due to better model utilization through decoupling across iterations. Finally, we identify that the subtle interplay between architecture choice, policy parameterization and task can affect performance by an even greater magnitude than the choice of policy parametrization (\nSection\nÀú\n5.3\n).\nTakeaway.\nIn robotic applications, our findings suggest that the distributional formulation of GCPs ‚Äî sampling from a\ndistribution\nof actions given observations ‚Äî is the least important facet that contributes to their success.\nRather, our work highlights that\nC\n2\n+\nC\n3\noffer an exciting and under-explored sandbox for future algorithm design in continuous control and beyond.\n2\nPreliminaries\nWe consider a continuous control setting with observations\no\n‚àà\nO\no\\in O\nand actions\na\n‚àà\nA\na\\in A\nwhere\nO\nO\nis the observation space and\nA\nA\nis the action space.\nWe learn a policy\n:\nO\n‚Üí\n(\nA\n)\n\\pi:O\\to\\Delta(A)\nfrom observations to (distributions over) actions to maximize the probability of success\nJ\n‚Äã\n(\n)\nJ(\\pi)\non a given task, which we refer to as ‚Äúperformance.‚Äù\nThis can be formulated as maximizing reward in an Markov Decision Process, which for completeness we formalize in\nSection\nÀú\nK.1\n.\nWe consider the performance of policies learned via BC‚Äîthat is, supervised learning from a distribution of (observation, actions pairs) drawn from a training distribution\np\ntrain\np_{\\mathrm{train}}\n.\nWe now describe two popular classes of control policies, and their respective training objectives.\nIn applications, the actions\na\na\nare often a short-open loop sequence of actions, or\naction-chunks\n, which have been shown to work more effectively for complex tasks with end-effector position commands\n(\nzhaoLearningFineGrainedBimanual2023\n)\n. See\nSection\nÀú\n6\nfor an unabridged related work.\nRegression Control Policies (RCPs).\nA historically common policy choice for BC is regression control policies (RCPs)\n(\npomerleau1988alvinn\n;\nbain1995framework\n;\nross2011reduction\n;\nosa2018algorithmic\n)\n, given by a deterministic map\n:\nO\n‚Üí\nA\n\\pi:O\\to A\n. In applications, it is parameterized by a neural network  and trained so as to minimize the\nL\n2\nL_{2}\n-loss on training data:\n‚âà\narg\n‚Äã\nmin\n‚Å°\nE\n‚Äã\n‚Äñ\n(\no\n)\n‚àí\na\n‚Äñ\n2\n,\n(\no\n,\na\n)\n‚àº\np\ntrain\n.\n\\displaystyle\\approx\\operatorname*{arg\\,min}\\mdmathbb{E}\\|(o)-a\\|^{2},\\quad(o,a)\\sim p_{\\mathrm{train}}.\n(2.1)\nGenerative Control Policies (GCPs).\nGenerative control policies (GCPs) parameterize a\ndistribution\nof actions\na\na\ngiven an observation\no\no\n.\nThis is often accomplished in practice by representing the policy  with a generative model such as a diffusion\n(\nchi2023diffusion\n)\n, flow\n(\nzhang2024flowpolicy\n)\n, or tokenized autoregressive transformer\n(\nshafiullah2022behavior\n)\n.\nGiven their popularity, we focus on flow-based GCPs (flow-GCPs).\nA flow-GCP learns a conditional flow field\n(\nlipman2023flow\n;\nchisari2024learning\n;\nnguyen2025flowmp\n;\nalbergo2022building\n;\nheitz2023iterative\n;\nliu2022flow\n)\nb\n:\n[\n0\n,\n1\n]\n√ó\nA\n√ó\nO\n‚Üí\nA\nb:[0,1]\\times A\\times O\\to A\nby minimizing the objective\nb\n‚âà\narg\n‚Äã\nmin\nE\n‚à•\nb\nt\n(\nI\nt\n‚à£\no\n)\n‚àí\nI\nÀô\nt\n‚à•\n2\n,\nt\n‚àº\nUnif\n(\n[\n0\n,\n1\n]\n)\n,\nz\n‚àº\nN\n(\n0\n,\nùêà\n)\n,\n\\displaystyle b\\approx\\operatorname*{arg\\,min}\\mdmathbb{E}\\|b_{t}(I_{t}\\mid o)-\\dot{I}_{t}\\|^{2},\\quad t\\sim\\mathrm{Unif}([0,1]),\\>\\>z\\ \\sim\\mathrm{N}(0,\\mathbf{I}),\n(2.2)\nwhere again\n(\no\n,\na\n)\n‚àº\np\ntrain\n(o,a)\\sim p_{\\mathrm{train}}\n,\nI\nt\n=\nt\n‚Äã\na\n+\n(\n1\n‚àí\nt\n)\n‚Äã\nz\nI_{t}=ta+(1-t)z\nis the stochastic interpolant between the training action\na\na\nand noise variable\nz\nz\n, and where\nI\nÀô\nt\n=\na\n‚àí\nz\n\\dot{I}_{t}=a-z\nis the time derivative of\nI\nt\nI_{t}\n. We note that this is a special case of the stochastic interpolant framework\n(\nalbergo2022building\n;\nalbergo2023stochastic\n;\nalbergo2024stochastic\n)\n, which permits a larger menu of design decisions.\nA flow model then predicts an action by integrating a flow. In the limit of infinite discretization steps, this amounts to sampling\na\n‚àº\n(\n‚ãÖ\n‚à£\no\n)\na\\sim(\\cdot\\mid o)\nby sampling\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\nz\\sim\\mathrm{N}(0,\\mathbf{I})\n, and then setting\na\n=\na\n1\na=a_{1}\n, where\n{\na\nt\n}\nt\n‚àà\n[\n0\n,\n1\n]\n\\{a_{t}\\}_{t\\in[0,1]}\nsolves the ODE:\nd\nd\n‚Äã\nt\n‚Äã\na\nt\n=\nb\nt\n‚Äã\n(\na\nt\n‚à£\no\n)\nwith initial condition\na\n0\n=\nz\n.\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}a_{t}=b_{t}(a_{t}\\mid o)\\qquad\\text{with initial condition}\\qquad a_{0}=z.\n(2.3)\nIn practical implementation, sampling is conducted via discretized Euler integration (see\nSection\nÀú\nK.2\nfor details).\nThis yields a policy\na\n=\n(\nz\n,\no\n)\na=(z,o)\nwhich is a deterministic function of the initial noise\nz\nz\nand the observation\no\no\n.\nAll experiments, unless otherwise stated, perform\n9\n9\nintegration steps. We reiterate that other GCPs, e.g. based on diffusion models and autoregressive transformers, have been studied elsewhere. We choose to focus on flow models due to their state-of-the-art performance\n(\nchi2023diffusion\n;\nchisari2024learning\n;\nzhang2024flowpolicy\n)\nand deployment in industry\n(\nblack2024pi_0\n;\nintelligence$p_05$VisionLanguageActionModel2025\n;\nnvidia2025gr00t\n)\n.\nMulti-Modality in Robot Learning.\nPast work has conjectured that for salient robotic control tasks,\np\ntrain\n‚Äã\n(\na\n‚à£\no\n)\np_{\\mathrm{train}}(a\\mid o)\nexhibit\nmulti-modality\n, i.e. the conditional distribution of\na\na\ngiven\no\no\nhas multiple modes\n(\nshafiullah2022behavior\n;\nzhaoLearningFineGrainedBimanual2023\n;\nflorence2022implicit\n)\n. This motivated the earliest use of GCPs\n(\nchi2023diffusion\n)\n(\nH2\n).\nSection\nÀú\n3.2\ncalls into question the extent to which GCPs do in fact learn multi-modal distributions of\na\n‚à£\no\na\\mid o\non popular benchmarks, including those claimed to highlight multimodality as a core challenge.\n3\nMulti-modality and expressivity do not explain GCPs‚Äô performance\nThis section demonstrates that neither advantages on pixel-based control (\nH1\n), nor multi-modality (\nH2\n), nor improved expressivity (\nH3\n) fully account for the GCPs performance relative to RCPs.\n\nInstead, our analysis indicates that the advantage of GCPs is\nlargely due to architectural innovations\nfound in GCPs‚Äîspecifically, the adoption of powerful models like Transformers and UNets, along with the use of action chunking techniques.\nAppendices\nÀú\nF\nand\nG\naddresses other hypotheses, such as\nk\nk\n-nearest neighbor approximation and the behavior diversity.\nFigure 2:\nRelative performance of RCPs compared to GCPs across common benchmarks.\nFor single-task benchmarks, we implement\nChi-Transformer\n,\nSudeep-DiT\nand\nChi-UNet\n. For each architecture, we average performance of the best training checkpoint across three seeds.\nFor multi-task benchmarks, we use\n0\nas base policy and finetune it on full\nLIBERO\nbenchmark (130 tasks).\nWe then report the performance of the best-performing architecture, chosen individually for both RCPs and GCPs.\nFor Flow, we always do 9 step Euler integrations, where its performance plateaued.\nFor readability, RCPs success rates are plotted relative to flow, with flow normalized to performance of\n1\n1\nper task. Tasks are grouped by observation modality, and ordered by relative RCPs performance. Red dashed line indicates threshold at which RCP attains\n<\n95\n%\n<95\\%\nsuccess of GCPs.\nNote that RP and\nFlow\nperform comparably on most Image, 3D-based and VLA-based multi-task benchmarks.\n3.1\nWhen controlled for architecture, GCPs only outperform on few tasks\nWe first isolate the tasks in which GCPs exhibit stronger performance by comparing across 28 popular BC benchmarks including multi-task benchmarks like\nLIBERO\n(detailed in\nSection\nÀú\nB.1\n), encompassing diverse data quality, modalities (\nstate\n,\npoint clouds\n,\nimage\nand\nlanguage\n), and domains (e.g.,\nMetaWorld\n,\nRobomimic\n,\nAdroit\n,\nD4RL\n,\nMeta-World\n,\nLIBERO\n).\nCrucially, we implement RCPs using the\nexact same architectures\nas their corresponding flow models by simply setting the noise level and initial noise to zero:\nz\n=\n0\nz=0\n,\nt\n=\n0\nt=0\n, and study three widely-used architectures (\nChi-Transformer\n,\nSudeep-DiT\n,\nChi-UNet\nas well as pre-trained VLA models like\n0\n(\nblack2024pi_0\n)\n; detailed in\nSection\nÀú\nB.2\n).\nThis architectural alignment enables RCPs to benefit from the sophisticated network designs typically reserved for GCPs, ensuring a truly fair comparison.\nUnder controlled comparison, we find that GCPs and RCPs achieve parity across the vast majority of state-based, image-based, and VLA-based BC benchmarks.\nPerformance gaps emerge only on a small subset of tasks requiring\nhigh precision\n(e.g. precise insertion tasks). We report best-case results in\nFig.\nÀú\n2\nand comprehensive ablations (including worst-case architectures and loss variants) in\nSection\nÀú\nB.4\n.\nOur evaluation yields three key insights:\n‚Ä¢\nRare Benefit of GCPs:\nGCPs outperform RCPs by\n>\n5\n%\n>5\\%\non only a handful of tasks.\n‚Ä¢\nModality Independence:\nContrary to popular belief, observation modality does\nnot\ncorrelate with GCP advantage.\n‚Ä¢\nArchitectural Dominance:\nArchitecture choice dictates performance far more than the generative vs. regression distinction.\nWe posit that the perceived superiority of GCPs in prior work was confounded by architectural asymmetry.\nTo our knowledge, this is the first study to benchmark\nSudeep-DiT\n,\nChi-UNet\n, and\n0\nbackbones as regression policies.\nIn\nSection\nÀú\n5.3\n, we demonstrate that when equipped with these modern backbones‚Äîor even tuned\nMLP\nbaselines‚ÄîRCPs are highly competitive.\nFurthermore, we find that hyperparameters such as\naction-chunking\nhorizon\n(\nzhao2023learning\n;\nchi2023diffusion\n;\nzhang2025actionchunkingexploratorydata\n)\nexert a greater influence on success rate than the choice of objective function (\nSection\nÀú\nF.1\n).\nDesign decisions like architecture and action-chunking have a significant and consistent impact on control performance. In contrast, the choice between GCPs and RCPs is largely negligible outside of high-precision regimes.\n3.2\nGCPs‚Äô performance does not arise from multi-modality\nEarlier literature suggested that capturing multi-modality, as defined in\nSection\nÀú\n2\n, was precisely the root of the observed performance benefits of GCPs\n(\nchi2023diffusion\n;\nreuss2023goal\n)\n.\nHowever, examining\nFig.\nÀú\n2\n, we see that many tasks which have been understood to be multimodal (e.g.,\nPush\n-\nT\n) do not show substantial performance gaps between RCPs and GCPs.\nOn the other hand, RCPs and GCPs differ only on tasks that demand high precision (e.g.\nTool\n-\nHang\n,\nTransport\n).\nIn this section, we provide additional evidence that\nmultimodality is not the main factor responsible for witnessed performance advantages of GCPs\n.\nFigure 3:\nA. Visualized action distribution with Q values.\nDistinct modes are\nnot\nobserved in planned actions even at symmetric and ambiguous states. (\nKitchen\nand\nTool\n-\nHang\n, t-SNE visualization.) In\nPush\n-\nT\n, we all trajectories goes to one side. For the rest, there is no clear clustering of actions or Q.\nTask\nz\n‚â°\n0\nz\\equiv 0\nN\n‚Äã\n(\n0\n,\nùêà\n)\n\\mathrm{N}(0,\\mathbf{I})\nMean\nz\nz\nPush\n-\nT\n0.97\n0.97\n0.95\nKitchen\n0.99\n0.99\n0.97\nTool\n-\nHang\n0.78\n0.80\n0.76\nTable 1:\nB. Performance comparison of different sampling strategies\n. We compare sampling\nz\n=\n0\nz=0\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\nz\\sim\\mathrm{N}(0,\\mathbf{I})\n, and mean over 64\nz\n(\ni\n)\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\nz^{(i)}\\sim\\mathrm{N}(0,\\mathbf{I})\n. Different sampling strategies show minor performance difference, indicating absence of distinct action modes.\nDataset\nFlow\nReg.\nOriginal\n0.78\n0.58\nDeterministic\n0.72\n0.64\nTable 2:\nC. GCPs outperforms RCPs with deterministic experts.\nPolicy average success rate over 3 architectures, 3 seeds and 3 architectures given different dataset: one from original human demonstration and another collected by rolling out a flow policy in deterministic mode starting from zero noise.\nEvidence A: GCPs exhibit unstructured action distributions.\nFor fixed observations, we draw multiple action samples by denoising from different initial latents and visualize the resulting action set with their Q values\nQ\n‚Äã\n(\na\n,\no\n)\nQ(a,o)\n.\nWe deliberately choose\nsymmetry-critical\nor\nhigh-ambiguity\nstates to\nmaximize\npotential multi-modality:\n(a)\nPush\n-\nT\nat the symmetry axis of the T-shape, where taking the left or right path is equivalent,\n(b)\nKitchen\nfrom an initial state with multiple first-subtask choices, and\n(c)\nTool\n-\nHang\nat the insertion pre-contact pose where human demonstrators pause for varying durations.\nIn (a-c) we observe\nsingle\nclusters rather than distinct modes (high-dimensional actions visualized with t-SNE); see\nFig.\nÀú\n3\n.\nMoreover, adherence to action cluster means do not correlate with performance: We color-code actions by Q-value, i.e. Monte-Carlo-estimated rewards-to-go (\nSection\nÀú\nD.1\n). Highest returns are distributed evenly across samples.\nEvidence B: Taking mean actions does not meaningfully degrade GCPs‚Äô performance.\nWe evaluate flow policy‚Äôs performance with three sampling strategies: zero noise\na\n=\n(\nz\n=\n0\n,\no\n)\na=\\pi(z=0,o)\n, stochastic sampling\na\n=\n(\nz\n,\no\n)\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nI\n)\na=\\pi(z,o),z\\sim\\mathrm{N}(0,I)\n, and\nmean action\na\n=\nE\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nI\n)\n‚Äã\n[\n(\nz\n,\no\n)\n]\na=\\mdmathbb{E}_{z\\sim\\mathrm{N}(0,I)}[\\pi(z,o)]\n(via Monte Carlo approximation).\nIf the learned distribution were strongly multi-modal, or if their distributions lied on a manifold whose\ncurvature\nwas crucial to task success, the conditional mean would\ncollapse\nmodes and severely degrade performance.\nHowever,\nTable\nÀú\n1\nshows that replacing stochastic sampling with the mean action only slightly affects performance, indicating absence of distinct action modes.\nEvidence C: GCPs outperform RCPs on certain tasks even with deterministic experts.\nTo fully remove any residual multi-modality, we recollect the dataset with trained flow policy evaluated in deterministic mode (\nz\n=\n0\nz=0\n) detailed in\nSection\nÀú\nD.2\n.\nThe new dataset is fully deterministic because action labels are provided by a deterministic policy evaluated in a deterministic environment. While the gap in performance between GCPs and RCPs shrinks somewhat, we still find that GCPs still outperforms RCPs, as in\nTable\nÀú\n2\n, suggesting that capturing some ‚Äúhidden‚Äù stochasticity or multimodality in the data does not suffice to explain the gap between the two.\nCollectively, (A)‚Äì(C) indicate that the commonly cited explanation‚Äî‚ÄúGCPs win because demonstrations are multi-modal‚Äù‚Äîdoes not hold for most studied behavior cloning benchmarks.\nMulti-modality and data coverage.\nThe absence of observed multimodality is likely attributable to the large observation dimension of tasks relative to total number of demonstrations. That is, we rarely see two ‚Äúconflicting‚Äù actions for nearby observation vectors (note: to grid a space of dimension\nd\nd\nrequires\n2\nd\n2^{d}\npoints). Some degree of ‚Äúhidden‚Äù multi-modality may still be present, as indicated by the slight narrowing of the performance gap in\nTable\nÀú\n2\n. Still, our central claim is that multi-modality is not\nsufficient\nto explain the full difference in performance. Understanding to what extent multimodality appears in the multi-task setting is an exciting direction for future research.\n3.3\nLimitations of the expressivity of GCPs in the absence of multimodality\nFigure 4:\nGCP behavior given different types of data.\n(a) Given true multi-modal data (a.1), where expert have two behaviors at the same state, GCPs can learn both modes while RCP collapse to the middle (a.2).\n(b) In reality, the data is often sparse given high-dimensional space. Given the sparse data (b.1), GCPs have two possible behaviors: (b.2) still learn both modes given close-by states, (b.3) learn a high-Lipschitz policy to quickly switch between modes. In our experiments, we find that both\nGCPs and RCPs\nlearn (b.3) in high-dimensional tasks (\nSection\nÀú\n3.2\n). In this regime,\nTheorem\nÀú\n1\nthen suggests that, from a pure expressivity perspective, GCPs have a limited advantage over RCPs.\nAn alternative to learning explicit multimodality is to represent rapid transition between actions as the observation changes. This is depicted in\nFigure\nÀú\n4\n, where data that appears multi-modal can be fit with a policy that has a high Lipschitz constant, i.e. in which\n‚àá\no\n(\na\n‚à£\no\n)\n\\nabla_{o}\\pi(a\\mid o)\nis large. This reflects a broader principle in control that we need only capture the mapping from observation to a single effective action, rather than reproduce the distribution over all possible actions.\nOne may still conjecture that GCPs more easily higher-Lipschitz policies by leveraging iterative computation, as compared to RCPs. This is because deeper networks can express larger-Lipschitz functions more easily\n(\ntelgarsky2016benefits\n)\n, and many have equated the multi-step computation in flow-based generative models to depth\n(\nchen2018neural\n)\n. Step-by-step generation is known to drastically increase expressivity in other domains as well, such as autoregressive language models\n(\nli2024chainthoughtempowerstransformers\n)\n,\nHowever, flow-based generative models use their multi-step computation to express complex distributions over the\ngenerated variable\n(\nho2020denoising\n;\nsong2021denoising\n;\nzhang2022fast\n;\nnichol2021improved\n)\n.\nIt is less clear if the iteration computation assists with represent complex\nobservation-to-action\nmappings. Thus, we ask:\nDoes the iterative computation in GCPs aid in learning more complex observation-to-action mappings, even if the learned action distributions for a fixed observation are themselves are relatively simple (i.e. unimodal)?\nFigure 5:\nThe Myth of Superior Expressivity: Fitting High-Frequency Functions.\nWe evaluate GCPs, RCPs, and minimal iterative policy (\nMIP\n,\nSection\nÀú\n4\n) on fitting the high-Lipschitz function\ny\n=\nsin\n‚Å°\n(\n1\n/\nx\n)\ny=\\sin(1/x)\n(\nN\n=\n1024\nN=1024\n, 4-layer MLP).\nContrary to the belief that iterative evaluation yields sharper function approximation, both GCPs and RCPs fail to capture the high-frequency structure given limited network capacity.\nWhile RCPs succumb to spectral bias by averaging the oscillations, GCPs merely trade this averaging for stochastic variance.\nCrucially, when initial noise is fixed (\nx\n0\n=\n0\nx_{0}=0\n), the Flow policy collapses to the exact same mean-seeking behavior as regression. Only when averaged over initial noise variance to we start to see a tradeoff from epistemic uncertainty to aleatoric variance.\nThis demonstrates that GCPs do not inherently bypass the spectral limitations of the underlying backbone to achieve greater Lipschitz expressivity.\nWe now provide evidence that suggests ‚Äú\nno\n.‚Äù We show that in the absence of multi-modality (as shown in\nSection\nÀú\n3.2\n), GCPs cannot express more complex mappings from the conditioning variable\no\no\nto the generated variable\na\na\nthan RCPs can.\nWe begin by considering a ground-truth conditional flow field\nb\nt\n‚ãÜ\n‚Äã\n(\no\n‚à£\na\n)\nb_{t}^{\\star}(o\\mid a)\n.\nLet\n(\nz\n,\no\n)\n‚ãÜ\n{}^{\\star}(z,o)\nrepresent the exactly integrated\nb\n‚ãÜ\nb^{\\star}\nfrom initial noise\nz\nz\nto\na\na\n.\nGiven the absence of multi-modality (\nSection\nÀú\n3.2\n), we assume that the distribution of\na\n‚à£\no\na\\mid o\nis -log-concave (\nAppendix\nÀú\nH\n), satisfied by many classical unimodal distributions.\nWe prove that the Lipschitz constant of\n(\nz\n,\no\n)\n‚ãÜ\n{}^{\\star}(z,o)\nwith respect to\no\no\n, a measure of the expressivity of the\no\n‚Üí\na\no\\to a\nmapping, is bounded by that of\nb\nt\n‚ãÜ\nb^{\\star}_{t}\n:\nTheorem 1\n(Informal)\n.\nLet\n‚à•\n‚ãÖ\n‚à•\n\\|\\cdot\\|\ndenote either the matrix operator or Frobenius norm, and suppose that the distribution of\na\n‚à£\no\na\\mid o\nis -log-concave. Moreover, suppose that the flow field\nb\nt\n‚ãÜ\n‚Äã\n(\na\n‚à£\no\n)\nb^{\\star}_{t}(a\\mid o)\nis\nL\nL\n-Lipschitz:\n‚à•\n‚àá\no\nb\nt\n‚ãÜ\n(\na\n‚à£\no\n)\n‚à•\n‚â§\nL\n\\|\\nabla_{o}b^{\\star}_{t}(a\\mid o)\\|\\leq L\n. Then, with infinite integration steps, we have the bound\n‚à•\n‚àá\no\n(\nz\n,\no\n)\n‚ãÜ\n‚à•\n‚â§\nL\n‚ãÖ\n1\n+\n‚àí\n1\n.\n\\displaystyle\\|\\nabla_{o}{}^{\\star}(z,o)\\|\\leq L\\cdot\\sqrt{1+{}^{-1}}.\n(3.1)\nSee\nAppendix\nÀú\nH\nfor a formal statement and proof, adopting a careful argument from\ndaniels2025contractivity\n.\nA classical example of a log concave distribution is\na\n‚à£\no\n‚àº\nN\n‚Äã\n(\n(\no\n)\n,\n1\n)\na\\mid o\\sim\\mathrm{N}(\\mu(o),\\frac{1}{\\kappa})\n; as long as the variance\n1\n/\n1/\\kappa\nis bounded\nabove\n(even in the limit of a Dirac measure), there is at most a constant-multiplicative factor increase in the Lipschitz constant.\nWhen training a flow,\nb\nt\n‚ãÜ\n‚Äã\n(\na\n‚à£\no\n)\nb^{\\star}_{t}(a\\mid o)\nis approximated by the neural network.\nThus, in the prototypical unimodal example of -log-concave distributions, GCPs are not arbitrarily more expressive than RCPs.\nIn fewer words:\nmore integration steps (i.e. more iterative computation), even infinitely many, need not enable greater expressivity of high Lipschitz\no\n‚Üí\na\no\\to a\nmappings\n.\nMethod\nPush\n-\nT\nKitchen\nTool\n-\nHang\nState\nImage\nState\nState\nImage\nRegression\n0.90\n0.90\n0.55\n0.55\n14.07\n14.07\n1.71\n1.71\n1.65\n1.65\nFlow\n0.45\n0.45\n0.20\n0.20\n12.43\n12.43\n1.41\n1.41\n1.37\n1.37\nTable 3:\nPolicy Lipschitz constant comparison.\nLipschitz constant is averaged over 100 states.\nTo verify our theoretical prediction, we quantify learned policies‚Äô Lipchitz constants with a zeroth-order proxy:\nstarting from dataset states\ns\nt\ns_{t}\nwith observation\no\nt\no_{t}\n, we inject small Gaussian perturbations in the executed action to reach a\nfeasible\nnearby state\ns\nt\n+\n1\n(\ni\n)\ns_{t+1}^{(i)}\nwith observation\no\nt\n+\n1\n(\ni\n)\no_{t+1}^{(i)}\n, then measure input‚Äìoutput sensitivity via finite differences of the policy around the perturbed states (full algorithm and per-architecture results in\nAppendix\nÀú\nC\n).\nThis construction (i) avoids reliance on noisy higher-order gradients in complex architectures, and (ii) keeps evaluations on feasible observation to prevent conflating expressivity with model error on dynamically infeasible states.\nAs predicted by our theory, GCPs are not strictly more expressive than RCPs as shown in\nTable\nÀú\n3\n.\nOn the contrary, RCPs show increased Lipschitz constants off the manifold of training data, ruling out the assumption that GCPs win due to expressing policies with greater sensitivity to the input variable. We note that our methodology, which perturbs actions rather than states, is compatible pixel observations. To summarize:\nIn the absence of multimodality, GCPs do not enjoy an advantage over RCPs in expressing high Lipschitz behavior, such as rapid transitions between action modes.\n3.4\nGCPs and RCPs Exhibit Comparable Behavior Diversity\nFigure 6:\nTask completion order in Kitchen environment with different methods.\nWe plot the count of different task completion orders for different methods to evaluate the diversity of the policies.\nThe x-axis shows the task completion order, where each sub-task is represented by its initials.\nFor each run, we collect 1000 trajectories with the same seed shared by all methods.\nFor flow, we evaluate both stochastic and deterministic modes.\nWe conclude by rebutting a commonly believed hypothesis is that GCPs can express more diverse behaviors than RCPs by capturing the full distribution of expert actions\n(\nshafiullah2022behavior\n)\n.\n4\n4\n4\nNote that the expert might be stochastic but unimodal, so the findings in this section do not directly follow form those in\nSection\n3.2\n.\nWe evalute different variants of GCPs and RCPs on\nFranka\n-\nKitchen\n, where the expert shows multiple task completion orders.\nAs demonstrated in\nFig.\nÀú\n6\n, GCPs with both stochastic and deterministic sampling show similar task completion order diversity.\nDeterministic policies like regression and\nMIP\n(to be introduced in\nSection\nÀú\n4\n) also demonstrate similar task completion order diversity.\nThis indicates that, given sparse expert demonstrations, both GCPs and RCP learns high-Lipschitz policies to switch between different modes given different observations (corresponding to (b.2) case in\nFigure\nÀú\n4\n). RCPs and GCPs are equally good at learning such behaviors (\nFigure\nÀú\n6\n), which explain why we see similar performance for both policy parametrizations, even on seemingly multi-modal tasks like\nFranka\n-\nKitchen\n.\n4\nMinimal Iterative Policy (\nMIP\n): Isolating the Source of GCPs‚Äô Success\nIn this section, we introduce a number of intermediates between RCPs and GCPs that isolate which design decisions contribute to the latter‚Äôs superior performance. This leads to a Minimal Iterative Policy (\nMIP\n), which matches GCPs performance, thereby identifying the source of GCPs‚Äô success.\nWe begin with a taxonomy of the three key algorithmic components (\nFigure\nÀú\n7\n) present in GCPs.\nComponent 1\n.\nDistributional learning\ndenotes training a model to fit a conditional distribution\na\n‚àº\n(\no\n)\na\\sim(o)\nof actions given observations, as opposed to deterministic predictions (i.e.,\na\n=\n(\no\n)\na=(o)\n).\n5\n5\n5\nNote that\nComponent\nÀú\n1\nrefers to\ntraining\na model to fit a conditional distribution, not necessarily to the sampling. For example, training\nb\nb\nvia flow model but conducting deterministic inference with\n(\nz\n=\n0\n‚à£\no\n)\n,\neul\n{}_{\\theta,\\mathrm{eul}}(z=0\\mid o)\nis still considered distributional learning.\nComponent 2\n.\nStochasticity injection\ndenotes the injection of additional stochastic inputs into the neural network during training time (e.g., the variable\nz\nz\nin\nEq.\nÀú\n2.2\n).\nComponent 3\n.\nSupervised Iterative Computation (SIC)\ndenotes the iterative refinement of predictions by feeding the previous outputs into the same network again during inference, and providing\nsupervision signals\nat every step of the generation procedure at training time. For example, in flow GCPs, we integrate a supervised flow field\nb\nt\n‚Äã\n(\na\nt\n‚à£\no\n)\nb_{t}(a_{t}\\mid o)\nover time to get the final action\na\na\n, and that\nb\nt\nb_{t}\nreceives an independent supervisory signal for each\nt\nt\nat training time (\nEq.\nÀú\n2.2\n).\nFigure 7:\nTaxonomy of GCPs.\nWe elucidate 3 key design components of GCPs: distributional learning (fitting a distribution), stochasticity injection (injecting noise during training), and supervised iterative computation (multiple generation steps, each with its own supervised learning loss).\nTo ablate different design components, we introduce Straight Flow (\nSF\n,\nEq.\nÀú\n4.2\n), Residual Regression (\nRR\n,\nEq.\nÀú\nA.2\n) and Minimal Iterative Policy (\nMIP\n,\nEq.\nÀú\n4.4\n), which are variants of flow that only exhibit\nComponent\nÀú\n2\nand\nComponent\nÀú\n3\n, respectively.\nFrom here,\nSection\nÀú\n4.1\nproposes algorithmic variants which ablate these components: two novel variants we call minimal iterative policy (\nMIP\n,\nComponents\nÀú\n3\nand\n2\n) and straight-flow (\nSF\n,\nComponent\nÀú\n2\nonly), as well as a residual regression baseline (\nRR\n,\nComponent\nÀú\n3\nonly). We evaluate the performance of different variants on challenging tasks in\nSection\nÀú\n4.2\n, finding that\nMIP\nexhibits virtually the\nsame\nperformance as\nFlow\nacross tasks, whereas\nSF\nmatches the performance of\nRegression\nand\nRR\nexhibits even worse performance.\nThis establishes that\nComponents\nÀú\n3\nand\n2\n: SIC, when combined with stochasticity injection, drive performance. Finally, we contrast\nMIP\nwith other popular step policies (\nSection\nÀú\n4.3\n).\n4.1\nMIP\n: a minimal intermediate between RCPs and GCPs\nWe introduce a range of policies which lie along the spectrum between RCP and flow-based GCPs via varying combinations of\nComponents\nÀú\n3\nand\n2\n, culminating in the Minimal Iterative Policy (\nMIP\n). These policies do not satisfy\nComponent\nÀú\n1\n, because\nSections\nÀú\n3.2\nand\nG\nsuggests that this is not needed.\nIn particular, we consider networks\n(\no\n,\nI\nt\n,\nt\n)\n(o,I_{t},t)\nthat predict\nactions\n, not velocities, and given observations\no\no\n, time indices\nt\nt\n, and interpolants\nI\nt\nI_{t}\ncorresponding to noising actions. We state all networks below of\nL\n2\nL_{2}\nminimization, but our findings remain consistent when minimizing\nL\n1\nL_{1}\nerror instead (\nSection\nÀú\nF.2\n).\nRegression as Single-Step Denoising.\nWe begin by expression a regression policies (RCPs) as solving a single-step denoising problem, obtained by minimizing the\nL\n2\nL_{2}\nprediction error of the action given observation and null action interpolant:\n‚âà\nRCP\narg\n‚Äã\nmin\nE\n[\n‚à•\n(\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n‚àí\na\n‚à•\n2\n]\n.\n\\displaystyle{}^{\\textbf{{RCP}}}\\approx\\operatorname*{arg\\,min}\\underset{}{\\mdmathbb{E}}\\left[\\|((o,I_{0}=0,t=0)-a\\|^{2}\\right].\n(4.1)\nwhere\n(\no\n,\na\n)\n‚àº\np\ntrain\n(o,a)\\sim p_{\\mathrm{train}}\n.In the limit of infinity data, RCPs predict the conditional mean of\na\n‚à£\no\na\\mid o\nby mapping any noise\nz\nz\nto the same action\na\na\ngiven\no\no\n.\n6\n6\n6\nNote that in our comparisons between RCP and GCP (\nSection\n3\n) in, we use the\nEq.\n4.1\nto implement RCPs on GCP architectures.\nStraight Flow\n(\nSF\n, ours). Next we introduce Straightflow (\nSF\n), which adds only stochasticity injection\nComponent\nÀú\n2\nto RCPs. This is achieved by setting the interpolant\nI\n0\nI_{0}\nto be Gaussian:\n‚âà\nSF\narg\n‚Äã\nmin\nE\n‚à•\n(\no\n,\nI\n0\n=\nz\n,\nt\n=\n0\n)\n‚àí\na\n‚à•\n2\n,\n\\displaystyle{}^{\\textbf{{SF}}}\\approx\\operatorname*{arg\\,min}\\mdmathbb{E}\\|(o,I_{0}=z,t=0)-a\\|^{2},\n(4.2)\nwhere\n(\no\n,\na\n)\n‚àº\np\ntrain\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\n(o,a)\\sim p_{\\mathrm{train}},z\\sim\\mathrm{N}(0,\\mathbf{I})\n.\nInference is performed in a single step, by setting\na\n=\n(\no\n,\nz\n,\nt\n=\n0\n)\nSF\na={}^{\\textbf{{SF}}}(o,z,t=0)\n. Equivalently,\nSF\ncan be viewed as a flow model in which the flow field is constrained to be straight.\nLike RCPs, the optimal\nSF\npolicy is the conditional mean of\na\n‚à£\no\na\\mid o\n. The only difference between the two is injection of stochastic input\nz\nz\nduring training.\nOur experiments with\nSF\nprecisely isolate this effect‚Äîfor example, determining if the additional stochasticity during training improves learning dynamics, or behaves like data augmentation.\nLike\nMIP\nbelow, we set\nI\n0\n=\n0\nI_{0}=0\nat inference time, as stochasticity at inference time has little effect on policy performance.\nTwo-Step Denoising.\nAs a next step towards GCPs, we now consider a\ntwo-step denoising\n(\nTSD\n) policy. \nAs discussed in\nSection\nÀú\n4.3\n, this parametrization is superficially similar to, but substantively different than, popular flow-map/consistency/shortcut models\n(\nboffiHowBuildConsistency2025\n)\n.\nTSD\nperforms two steps of denoising, one from zero, and a second from a fixed index\nt\n‚ãÜ\n=\n.9\nt_{\\star}=.9\n:\n‚âà\nTSD\narg\n‚Äã\nmin\nE\n[\n‚à•\n(\n(\no\n,\nI\n0\n=\nz\n,\nt\n=\n0\n)\n‚àí\n(\nt\n‚ãÜ\n)\n‚àí\n1\nI\nt\n‚ãÜ\n)\n‚à•\n2\n+\n‚à•\n(\n(\no\n,\nI\nt\n‚ãÜ\n,\nt\n‚ãÜ\n)\n‚àí\na\n)\n‚à•\n2\n]\n.\n\\displaystyle{}^{\\textbf{{TSD}}}\\approx\\operatorname*{arg\\,min}\\underset{}{\\mdmathbb{E}}\\left[\\|((o,I_{0}=z,t=0)-(t_{\\star})^{-1}I_{t_{\\star}})\\|^{2}+\\|((o,I_{t_{\\star}},t_{\\star})-a)\\|^{2}\\right].\n(4.3)\nwhere\n(\no\n,\na\n)\n‚àº\np\ntrain\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\n(o,a)\\sim p_{\\mathrm{train}},z\\sim\\mathrm{N}(0,\\mathbf{I})\n, and\nI\nt\n=\nt\n‚Äã\na\n+\n(\n1\n‚àí\nt\n)\n‚Äã\nz\nI_{t}=ta+(1-t)z\nis the same interpolant used in flow models, and where\nt\n‚ãÜ\n=\n.9\nt_{\\star}=.9\nis fixed. The normalization by\nt\n‚ãÜ\nt_{\\star}\nin\nEq.\nÀú\n4.3\ncomes from the identity\nt\n‚ãÜ\n‚Äã\na\n=\nE\nz\n‚Äã\n[\nI\nt\n‚ãÜ\n]\nt_{\\star}a=\\mdmathbb{E}_{z}[I_{t_{\\star}}]\n.\nWe then sample\na\n^\n0\nTSD\n‚Üê\n(\no\n,\nz\n,\n0\n)\n\\hat{a}^{\\textbf{{TSD}}}_{0}\\leftarrow(o,z,0)\nand\na\n^\nTSD\n‚Üê\n(\no\n,\nt\n‚ãÜ\n‚Äã\na\n^\n0\nTSD\n+\n(\n1\n‚àí\nt\n‚ãÜ\n)\n‚Äã\nz\n,\nt\n‚ãÜ\n)\n\\hat{a}^{\\textbf{{TSD}}}\\leftarrow(o,t_{\\star}\\hat{a}^{\\textbf{{TSD}}}_{0}+(1-t_{\\star})z,t_{\\star})\n.\nMinimal Iterative Policy.\nWe find that\nTSD\n{}^{\\textbf{{TSD}}}\nperforms equivalently to a minimal policy which only adds training noise in the second step and has no stochasticity at inference time, which we call the minimal iterative policy.\nMinimal Iterative Policy (\nMIP\n; ours)\nMinimal Iterative Policy (\nMIP\n), representing\nComponents\nÀú\n3\nand\n2\n, is trained via\n‚âà\nMIP\n\\displaystyle{}^{\\textbf{{MIP}}}\\approx\narg\n‚Äã\nmin\nE\n(\n‚à•\n(\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n‚àí\na\n)\n‚à•\n2\n+\n‚à•\n(\n(\no\n,\nI\nt\n‚ãÜ\n,\nt\n‚ãÜ\n)\n‚àí\na\n)\n‚à•\n2\n)\n,\n\\displaystyle\\operatorname*{arg\\,min}\\underset{}{\\mdmathbb{E}}(\\|((o,I_{0}=0,t=0)-a)\\|^{2}+\\|((o,I_{t_{\\star}},t_{\\star})-a)\\|^{2}),\n(4.4)\nwhere\n(\no\n,\na\n)\n‚àº\np\ntrain\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\n,\nt\n‚ãÜ\n:=\n.9\n(o,a)\\sim p_{\\mathrm{train}},z\\sim\\mathrm{N}(0,\\mathbf{I}),t_{\\star}:=.9\n. At inference time, we compute:\na\n^\n0\nMIP\n‚Üê\n(\no\n,\n0\n,\nt\n=\n0\n)\nMIP\n,\na\n^\nMIP\n‚Üê\n(\no\n,\nt\n‚ãÜ\na\n^\n0\nMIP\n,\nt\n‚ãÜ\n)\nMIP\n.\n\\displaystyle\\hat{a}^{\\textbf{{MIP}}}_{0}\\leftarrow{}^{\\textbf{{MIP}}}(o,0,t=0),\\quad\\hat{a}^{\\textbf{{MIP}}}\\leftarrow{}^{\\textbf{{MIP}}}(o,t_{\\star}\\hat{a}^{\\textbf{{MIP}}}_{0},t_{\\star}).\n(4.5)\nMinimal iterative policy provides a\nminimal\nimplementation that still exhibits competitive performance with flow. Starting, with\nTSD\nand replace\n(\nt\n‚ãÜ\n)\n‚àí\n1\n‚Äã\nI\nt\n‚ãÜ\n(t_{\\star})^{-1}I_{t_{\\star}}\nin the first term of the loss in\nEq.\nÀú\n4.3\nwith its expectation\na\n=\n(\nt\n‚ãÜ\n)\n‚àí\n1\n‚Äã\nE\n‚Äã\n[\nI\nt\n‚ãÜ\n]\na=(t_{\\star})^{-1}\\mdmathbb{E}[I_{t_{\\star}}]\n. We set the initial noise\nI\n0\n=\n0\nI_{0}=0\nto be zero, so that\nz\nz\nonly contributes to the second training loss. Finally, we sample with\nz\n=\n0\nz=0\nto isolate the effect of adding stochasticity at training time, without stochasticity at inference time (as suggested by\nTable\nÀú\n1\n).\nSince we provide supervision for both first step\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\nMIP\n{}^{\\textbf{{MIP}}}(o,I_{0}=0,t=0)\nand second step\n(\no\n,\nI\n0\n=\nI\nt\n‚ãÜ\n,\nt\n=\nt\n‚ãÜ\n)\nMIP\n{}^{\\textbf{{MIP}}}(o,I_{0}=I_{t_{\\star}},t=t_{\\star})\nwith ground truth action\na\na\n,\nMIP\nalso exemplifies SIC in its simplest form. We compare\nMIP\nto Shorctu Models in\nSection\nÀú\n4.3\n.\nAdditional methods.\nFinally, we study residual regression (\nRR\n), which replaces\nI\nt\n‚ãÜ\nI_{t_{\\star}}\nin\nEq.\nÀú\n4.4\nwith its expectation over\nz\nz\n:\nE\n‚Äã\n[\nI\nt\n‚ãÜ\n]\n=\nt\n‚ãÜ\n‚Äã\na\n\\mdmathbb{E}[I_{t_{\\star}}]=t_{\\star}a\n. This preserves SIC (\nComponent\nÀú\n3\n) yet removes stochasticity injection.\nFull details are provided in\nAppendix\nÀú\nA\n.\nTo summarize, minimal iterative policy (\nMIP\n), straight-flow (\nSF\n) and residual regression (\nRR\n) represent all combinations of\nComponents\nÀú\n3\nand\n2\nwithout exhibiting\nComponent\nÀú\n1\n.\n4.2\nComponents\nÀú\n3\nand\n2\ndrive performance:\nMIP\nmatches\nFlow\nFigure 8:\nPerformance comparison between\nMIP\nand its variants on single-task benchmarks.\nAverage relative success rate on worst architecture and the best relative success rate on optimal architecture are reported. ‚Äú\nS\nS\n‚Äù: state; ‚Äú\nI\nI\n‚Äù: image.\nMethod\nLIBERO Object\nLIBERO Goal\nLIBERO Spatial\nLIBERO 10\nRCP (\n‚Ñì\n2\n\\ell_{2}\nloss)\n92.6\n94.6\n97.2\n78.0\nRCP (\n‚Ñì\n1\n\\ell_{1}\nloss)\n95.2\n88.0\n95.8\n62.4\nFlow\n97.4\n95.0\n95.8\n81.6\nMIP\n95.8\n95.2\n97.6\n82.2\nTable 4:\nPerformance comparison on multi-task\nLIBERO\nbenchmark.\nWe report the success rate of the checkpoint trained with 50k gradient steps of finetuning\n0\non the full\nLIBERO\ndataset. We implement\nMIP\nwith\nt\n‚àó\n=\n0.9\nt^{*}=0.9\nand integrate flow with 10 steps.\nFor regression, we train with both\n‚Ñì\n2\n\\ell_{2}\nand\n‚Ñì\n1\n\\ell_{1}\nloss as suggested in\n(\nkim2024openvla\n)\n.\nBased on the design space parsing in\nSection\nÀú\n4\n, we are able to systematically ablate different design components‚Äô contribution to the final performance in\nFigs.\nÀú\n8\nand\n4\n.\nOur evaluation shows that either stochasticity injection (\nComponent\nÀú\n2\n, exhibit by\nSF\n) or supervised iterative computation (\nComponent\nÀú\n3\n, exhibited by\nRR\n) in isolation do not match the success of GCPs.\nMIP\n, being the only method which combines\nsupervised\niterative computation and stochasticity injection, achieves success on par with flow.\nThus we conclude: the performance of GCPs comes from combining stochastic injection and iterative computation. Distributional training appears to be the least important factor.\nRemark 4.1\n.\nSection\nÀú\nA.3\nexhibits two further variants which preserve\nComponents\nÀú\n2\nand\n3\n: one that does not supervise intermediate steps, and a second which does not condition a time step\nt\n‚ãÜ\nt_{\\star}\n. The latter does not enable network to learn separate functions across time steps. Both perform even worse than regression, confirming the importance of supervision of intermediate steps and decoupling network behavior across time steps.\n4.3\nMIP\ncompares favorably to shortcut policies\nMIP\nis superficially similar to Shortcut Models\n(\nboffiFlowMapMatching2025\n;\nboffiHowBuildConsistency2025\n;\nsongConsistencyModels2023\n;\ngengMeanFlowsOnestep2025\n)\n, as both perform inferences in few-steps. Shortcut models correctly learn target distributions (i.e. satisfy\nComponent\nÀú\n1\n) by integrating a flow field. On the other hand,\nMIP\nare trained to predict the conditional mean of the interpolant, which is not a valid objective for distribution fitting. The performance of\nMIP\nsupports our overall theme that, in robotic control applications, faithfully capturing the full conditional distribution over actions is not needed for control performance.\nMethod\nTransport\nTool\n-\nHang\nmh\nph\nFlow\n0.52/0.40\n0.80/\n0.73\n0.84/0.70\nMIP\n0.62\n/\n0.46\n0.80/0.69\n0.92\n/\n0.88\nCTM\n0.57/0.32\n0.90\n/0.58\n0.56/0.26\nTable 5:\nPerformance comparison between\nMIP\nand shortcut policies.\nReport best/average performance across 5 checkpoints with 3 random seeds. Task is state-based. For\nCTM\n, we report the performance with 2 integration steps, which is the same as\nMIP\n. Note that\nMIP\nis always best or near-best on average-over-seed performance, whereas CTM‚Äôs average performance struggles.\nWhile being competitive with flow models performance-wise,\nMIP\ntakes less integration steps (number of function evaluations (NFEs) = 2) compared to flow models (NFEs = 9).\nTo further validate the computation efficiency of\nMIP\n, we compare it with consistency models which accelerate the sampling process of flow by distilling the learned flow into a shortcut model\n(\nsongConsistencyModels2023\n;\nboffiFlowMapMatching2025\n;\nfransOneStepDiffusion2024\n;\ngengMeanFlowsOnestep2025\n)\n.\nWe benchmark\nMIP\nagainst consistency trajectory model (CTM)\n(\nkim2023consistency\n)\n, where latter is trained in two-stage manner.\nThus, CTM requires twice as many training time compared to\nMIP\n.\nAs shown in\nTable\nÀú\n5\n,\nMIP\nmatches, and often outperforms CTM on most challenging tasks since CTM exhibits certain level of performance degradation compared to the teacher flow models.\nThis again highlights that the fact that distributional learning is not necessary condition for GCPs performance and bypassing it offers computation efficiency at training and inference time.\nWe further compare\nMIP\nwith other few-step methods like Lagrangian map distillation (LMD)\n(\nboffiFlowMapMatching2025\n)\nand present full results in\nSection\nÀú\nB.8\n.\n5\nInductive Bias, not Expressivity, Explains\nMIP\n‚Äôs Performance\n5.1\nManifold adherence, not reconstruction, drives performance\nDataset\nMetric\nRegression\nSF\nRR\nMIP\nFlow\nOriginal\nOff-manifold\nL\n2\nL_{2}\n0.058\n0.058\n0.061\n0.061\n0.057\n0.057\n0.043\n0.043\n0.032\n0.032\nOff-manifold\nL\n1\nL_{1}\n0.072\n0.072\n0.073\n0.073\n0.071\n0.071\n0.057\n0.057\n0.046\n0.046\nValidation\nL\n2\nL_{2}\n0.073\n0.073\n0.071\n0.071\n0.062\n0.062\n0.069\n0.069\n0.074\n0.074\nValidation\nL\n1\nL_{1}\n0.110\n0.110\n0.106\n0.106\n0.124\n0.124\n0.104\n0.104\n0.116\n0.116\nDeterministic\nOff-manifold\nL\n2\nL_{2}\n0.067\n0.067\n0.063\n0.063\n0.062\n0.062\n0.054\n0.054\n0.042\n0.042\nOff-manifold\nL\n1\nL_{1}\n0.082\n0.082\n0.078\n0.078\n0.077\n0.077\n0.063\n0.063\n0.051\n0.051\nValidation\nL\n2\nL_{2}\n0.290\n0.290\n0.234\n0.234\n0.224\n0.224\n0.195\n0.195\n0.217\n0.217\nValidation\nL\n1\nL_{1}\n0.336\n0.336\n0.374\n0.374\n0.386\n0.386\n0.331\n0.331\n0.356\n0.356\nTable 6:\nComparison of different methods on manifold adherence and reconstruction error.\nResults are averaged across 3 different architectures and 32 states on state-based\nTool\n-\nHang\n.\nValidation\nL\n2\nL_{2}\n/\nL\n1\nL_{1}\nnorm is evaluated on validation set from expert trajectories.\nOff-manifold\nL\n2\nL_{2}\n/\nL\n1\nL_{1}\nnorm is evaluated on out-of-distribution states.\nMIP\n, and the absence of multimodality, suggest a better ability to approximate the expert more accurately on training data.\nWe test this by evaluating the\nL\n2\nL_{2}\n-error, i.e., reconstruction error, on validation set.\nSurprisingly, we find that\nMIP\n,\nFlow\n, and RCP exhibit the\nsame\nvalidation loss; hence validation loss does predict their relative performance.\nSection\nÀú\nE.1\nreveals that validation loss doesn‚Äôt correlate with performance across other axes of variation. Indeed, policy performance requires taking good actions on\no.o.d. states\nunder compounding error at deployment time\n(\nsimchowitz2025pitfalls\n)\n.\nFigure 9:\nManifold adherence illustration.\nSampled trajectories in\nPush\n-\nT\ntasks from flow model with different NFEs.\nThus, we study a proxy which reflects performance in o.o.d. situations. We perturb expert trajectories in dataset as described in\nSection\nÀú\nC.1\n, and evalute a novel metric that we call the\noff-manifold norm\n. Informally, this measures the projection error of a predicted action\na\na\nonto the space spanned by expert actions at neighboring states; see\nSection\nÀú\nE.2\na for formal definition. Our metric assesses the quality of actions under simulated compounding error.\nTable\nÀú\n6\nreports both\nL\n2\nL_{2}\nvalidation loss and off-manifold\nL\n2\nL_{2}\nnorm for different methods: while all methods achieve low validation loss, only\nMIP\nand\nFlow\nare able to achieve low off-manifold\nL\n2\nL_{2}\nnorm, indicating their better manifold adherence. As\nSF\ndoes not exhibit the same benefit,\nwe conclude that supervised iterative computation facilitates projection onto the manifold of expert actions by refining the prediction across sequential steps.\nFigure\nÀú\n9\nprovides additional illustraion of manifold hypothesis: with more iterations, flow model samples more plausible trajectories, which goes to the side of T-shape object rather than colliding right into it.\nAppendix\nÀú\nJ\nprovides additional confirmation of this hypothesis on comprehensive toy experiments: GCPs are no better than RCP at fitting high frequency functions, but exhibit lower on-manifold error, suitably defined.\nWhy manifold adherence matters for control.\nWe conjecture that, for high-precision tasks, the sensitivity to errors is not homogeneous across error directions in action space. Our findings present preliminary evidence that some form an ‚Äúon-manifold inductive bias‚Äù directly aligns with minimizing error along relevant directions, yet is permissive to error in directions of lesser consequence. We think that rigorously establishing this hypothesis is an exicting direction for future work.\nNo known mechanism accounts for greater manifold adherence in GCPs vs. RCPs.\nThere is a growing body of literature that shows that, if training data are supported on a given low dimensional manifold\n‚Ñ≥\n\\mathcal{M}\n, then generative models learn to project onto\n‚Ñ≥\n\\mathcal{M}\n(\nboffi2024shallowdiffusionnetworksprovably\n;\npermenter2024interpretingimprovingdiffusionmodels\n)\n. However, to our knowledge, there is no work that explains why this inductive bias would be\nstronger\nthan what would be achieved with a well-trained regression model. Specifically, if\no\n‚à£\na\no\\mid a\nlies in some (local) manifold, regression too should learn to project onto it.\nOne might conjecture that the iterative computation provides many changes to predict an action that ‚Äústick‚Äù to the action manifold. However, such a mechanism would require that once an on-manifold action is predicted, subsequent predictions do not nudge the prediction off-manifold.\nIn\nAppendix\nÀú\nI\n, we show that\nsimple arguments based on implicit regularization in linear models do not suffice to explain this hypothesis, at least for\nMIP\n. Much like the usefulness of manifold adherence for control described above, the mechanism behind manifold adherence remains a mystery for future study.\n5.2\nStochasticity stabilizes iterative computation\nFigure 10:\nStochasticity Stabilizes Iteration.\nNoise injection broadens the generation path into a ‚Äútube.‚Äù\nThis creates provides supervision when the imperfectly trained GCP goes off-distribution , ensuring robust iterative computation.\nWe recall from\nFigure\nÀú\n8\nthat\nSF\nmatches regression, whilst\nRR\nunder-performs regression. This suggests that sequential action generation is highly brittle in the absence of stochasticity\n(\npermenter2024interpretingimprovingdiffusionmodels\n)\n.\nOur findings support the hypothesis that stochasticity injection serves to provide ‚Äúcoverage‚Äù of the generative process as illustrated in\nFigure\nÀú\n10\n.\nNote that this is different from task MDP-level augmentation like image augmentation or exploratory data collection since the augmentation happens in iterative generative process.\nSpecifically, we can think of learning to perform two-stage action generation as an ‚Äúinternal‚Äù behavior cloning problem\n(\nren2024diffusion\n)\nunder the dynamics induced by the generative process.\nInjecting stochasticity amounts to enhancing coverage of the action\na\n^\n0\n\\hat{a}_{0}\nin the first step of\nMIP\n, thus enable iterative improvement with more NFEs (\nSection\nÀú\nB.7\n).\nIts benefits are analogous to trajectory noising effective in other behavior cloning applications\n(\nlaskey2017dart\n;\nblock2023butterfly\n;\nblock2024provable\n;\nsimchowitz2025pitfalls\n;\nzhang2025actionchunkingexploratorydata\n)\n.\nSimilar benefits are found in the improved sensitivity analysis of diffusion relative to flows\n(\nalbergo2024stochastic\n)\n.\n5.3\nArchitecture remains essential for scaling\nWhile all methods do scale, regression,\nenjoys stronger relative performance at the smallest model sizes but scales more poorly than flow and\nMIP\nwith increased model capacity (\nFig.\nÀú\n11\n).\nWe conjecture that supervised iterative computation can better utilize larger models, both by introducing more supervision steps at training, and by providing more parameters to represent different computations at successive generation steps. Nevertheless,\narchitecture design\nplays an incredibly significant role.\nTo showcase its importance, we ablate the performance of different method‚Äôs average performance across both the 3 architectures above, and the more traditional\nMLP\nand\nRNN\narchitectures, implemented with modern best practices including FiLM conditioning\n(\nperez2018film\n)\n, and skip-connections\n(\nhe2016deep\n)\n/LayerNorm\n(\nba2016layer\n)\nwhere appropriate (details in\nSection\nÀú\nB.2\n).\nAs demonstrated in\nFig.\nÀú\n11\n, the combination of training method and architecture design has a strong yet somewhat erratic effect on both GCPs and RCP performance.\nIn\nTool\n-\nHang\n, RCP achieves the best performance with an\nMLP\narchitecture. In\nTransport\n,\nMLP\nwith flow can even outperform more expressive architectures like\nChi-Transformer\n.\nThe coupling between training and architecture choice highlights the importance of controlling architecture design when comparing across methods.\nFigure 11:\nArchitecture and model size ablation.\nSuccess rate are averaged across 3 seeds and 5 checkpoints on\nTool\n-\nHang\nand\nTransport\ntasks. Left 2 plots: architecture ablation. Right 2 plots: Model size ablation. While all methods performance scales with model size, regression can outperform flow and\nMIP\nwith smaller capacity., highlighting the importance of aligning the model capacity when comparing different methods.\n6\nRelated Work\nRobotic Behavior Cloning.\nBehavior cloning (BC), also known as learning from demonstrations (LfD), has become a popular paradigm to enable robots to conduct complex, diverse and long-horizon manipulation tasks by learning from expert demonstrations\n(\nargallSurveyRobotLearning2009\n;\nzhuRobotLearningDemonstration2018\n;\nzhaoLearningFineGrainedBimanual2023\n;\nchiUniversalManipulationInterface2024\n;\nlin2024data\n)\n.\nIn parallel, ‚Äúrobot foundation models‚Äù scale BC with internet-pretrained vision-language transformer-based backbones\n(\nbrohan2022rt\n;\nzitkovich2023rt\n;\no2024open\n)\nand large-scale teleoperation datasets\n(\nkim2024openvla\n;\nteamOctoOpenSourceGeneralist2024\n)\n.\nMore recently, to better model continuous actions, generative models like diffusion and flow have been adopted to replace the tokenization method in transformers to achieve more expressive policies\n(\nnvidia2025gr00t\n;\nblack2024pi_0\n;\nintelligence$p_05$VisionLanguageActionModel2025\n;\nliu2024rdt\n)\n.\nThis work focuses on the generative modeling part of the behavior cloning pipeline, ablating the key design choices that lead to the success of generative control policies.\nGenerative Modeling.\nThe recent success of behavior cloning policies is built upon a rapid evolution of generative modeling techniques, starting from tokenization methods\n(\nbrownLanguageModelsAre2020\n;\nchenDecisionTransformerReinforcement2021\n;\npertschFASTEfficientAction2025\n)\nand adversarial methods\n(\nbrockLargeScaleGAN2019\n;\ngoodfellow2020generative\n;\nho2016generative\n)\n.\nLater, probabilistic generative models with iterative computation like diffusion models\n(\nho2020denoising\n;\nsongScoreBasedGenerativeModeling2021\n;\nlu2025dpm\n;\nsongDenoisingDiffusionImplicit2022\n;\nnichol2021improved\n;\nkarras2022elucidating\n)\nbecame a popular choice for generative modeling thanks to their better training stability and sampling quality.\nFlow models\n(\nlipman2023flow\n;\nalbergo2022building\n;\nliu2022flow\n)\nand consistency/shortcut models\n(\nsongConsistencyModels2023\n;\nsongImprovedTechniquesTraining2023\n;\nmeng2023distillation\n;\nboffiFlowMapMatching2025\n;\ngengMeanFlowsOnestep2025\n)\nwere later developed to achieve faster sampling while maintaining the expressivity of diffusion models.\nThough there have been extensive studies on probabilistic generative modeling‚Äôs effectiveness in image and text generation\n(\nlee2023convergence\n;\nchenSamplingEasyLearning2023\n)\n, its mechanism in control, especially the key design choices, are still opaque in decision making.\nGenerative Control Policies.\nTo model diverse and complex behaviors, GCPs parameterize the relationship between observations and actions as a distribution rather than a deterministic function.\nEarly works use transformers with tokenizers\n(\nchenDecisionTransformerReinforcement2021\n;\nshafiullah2022behavior\n)\n, energy functions\n(\nflorence2022implicit\n;\ndasariIngredientsRoboticDiffusion2024\n)\nand VAEs\n(\nzhaoLearningFineGrainedBimanual2023\n)\nto parameterize the distribution.\nDiffusion models\n(\nreuss2023goal\n;\nchi2023diffusion\n;\nke20243d\n;\ndongCleanDiffuserEasytouseModularized2024\n;\njanner2022planning\n;\nyangEquiBotSIM3EquivariantDiffusion2024\n)\nwere introduced for their better expressivity of complex and multi-modal behaviors, followed by flow-based\n(\nzhang2024flowpolicy\n;\nblack2024pi_0\n;\nintelligence$p_05$VisionLanguageActionModel2025\n)\nand flow-map/consistency-model/shortcut-model-based acceleration methods\n(\nhu2024adaflow\n;\nprasadConsistencyPolicyAccelerated2024\n;\nshengMP1MeanFlowTames2025\n)\n.\nTheoretical Literature on GCPs.\nblock2024provable\nestablished that GCPs can imitate arbitrary expert distributions. Given our findings on the absence of multi-modality, a more closely related theoretical findings is that of\nsimchowitz2025pitfalls\n, which elucidates how GCPs can circumvent certain worst-case compounding error phenomena in continuous-control imitation learning.\nThough the proposed mechanism is different, that finding is conceptually similar to our own: GCPs benefits arise from their favorable out-of-distribution properties, rather than raw expressivity of fitting in-distribution expert behavior.\n6.1\nPrevious Works‚Äô Connection with GCP‚Äôs Taxonomy.\nWe classify GCPs into three components: distributional learning, stochasticity injection, and supervised iterative computation.\nStarting from regression, it has none of the three components.\nTo model a more complex distribution, Gaussian Mixture Model (GMM)\n(\nzhuRobotLearningDemonstration2018\n)\nwas used to parameterize the distribution, trained with cross entropy loss.\nTo make the network be able to represent more complex distirbutions, prior to diffusion, non-parametric method like VAEs\n(\nzhaoLearningFineGrainedBimanual2023\n)\nwas used to parameterize the distribution, trained with reconstruction loss.\nDuring the training, a latent variables is predicted to predict the style the motion by mapping it from a noise\nz\nz\n.\nAnother line of work try to improve the policy expressivity by introducing iterative compute, like implicit behavior cloning\n(\nflorence2022implicit\n;\ndasariIngredientsRoboticDiffusion2024\n)\nand behavior transformer\n(\nshafiullah2022behavior\n)\n.\nIn IBC, the idea is to allow the network predict the energy function of the action rather the action itself.\nCompared to diffusion, the major difference is that they do not explicitly injecting noise during training and no intermediate supervision is provided for the intermediate results.\nSimilarly, in behavior transformer, a two step policy is introduced to first predict the policy class and then refine it with another network to achieve higher precision control.\nLastly, flow-based GCPs\n(\nzhang2024flowpolicy\n;\nblack2024pi_0\n;\nintelligence$p_05$VisionLanguageActionModel2025\n)\n, which holds all the three components and demonstrate state-of-the-art performance on popular benchmarks.\nIn this paper, we look into a new combination that haven‚Äôt been explored before, which is the combination of stochasticity injection and supervised iterative computation.\n7\nDiscussion\nOur comprehensive evaluation reveals a fundamental divergence between the objectives of generative modeling in vision or text and those in robotic control. We demonstrate that for control, fitting the exact data distribution (\nC\n1\n) is secondary; rather, the inductive bias of manifold adherence‚Äîfacilitated by stochastic iterative computation (\nC\n2\n+\nC\n3\n)‚Äîis paramount. This insight not only demystifies the success of GCPs but also enables the design of streamlined architectures like\nMIP\n.\nTheoretical Gaps.\nWhile we empirically identify manifold adherence as a proxy for closed-loop performance, a theoretical framework explaining why stochastic supervision with MSE loss induces this behavior remains elusive. Developing this theoretical grounding is a critical next step to replace exhaustive empirical benchmarking with principled policy design.\nBroader Applications.\nFinally, our analysis focuses on behavior cloning. It remains an open question whether the benefits of the\nC\n2\n+\nC\n3\nparadigm persist in other settings, such as RL-finetuning, large-scale pretraining, or long-horizon planning. Future work should explore whether the \"myths\" of generative control hold true in these broader domains.\nAcknowledgements\nMS and GA acknowledge a TRI University 2.0 Fellow and Google Robotics Research Award. MS and CP thank Nur Muhummad (Mahi) Shuffiulah for his insightful feedback, and thank MS also thanks Aviral Kumar, Sarvesh Patil, and Andrej Risteski for their thoughtful suggestions. GS holds concurrent appointments as an Assistant Professor at Carnegie Mellon University and as an Amazon Scholar. This paper describes work performed at Carnegie Mellon University and is not associated with Amazon.\nContents\n1\nIntroduction\n1.1\nContributions.\n2\nPreliminaries\n3\nMulti-modality and expressivity do not explain GCPs‚Äô performance\n3.1\nWhen controlled for architecture, GCPs only outperform on few tasks\n3.2\nGCPs‚Äô performance does not arise from multi-modality\n3.3\nLimitations of the expressivity of GCPs in the absence of multimodality\n3.4\nGCPs and RCPs Exhibit Comparable Behavior Diversity\n4\nMinimal Iterative Policy (\nMIP\n): Isolating the Source of GCPs‚Äô Success\n4.1\nMIP\n: a minimal intermediate between RCPs and GCPs\n4.2\nComponents\nÀú\n3\nand\n2\ndrive performance:\nMIP\nmatches\nFlow\n4.3\nMIP\ncompares favorably to shortcut policies\n5\nInductive Bias, not Expressivity, Explains\nMIP\n‚Äôs Performance\n5.1\nManifold adherence, not reconstruction, drives performance\n5.2\nStochasticity stabilizes iterative computation\n5.3\nArchitecture remains essential for scaling\n6\nRelated Work\n6.1\nPrevious Works‚Äô Connection with GCP‚Äôs Taxonomy.\n7\nDiscussion\nA\nAdditional Policy Parametrizations\nA.1\nFull Abalation of\nMIP\nVariants\nA.2\nAdditional Noise Injection Methods\nA.3\nExperiment Results\nB\nControl Experiments\nB.1\nTask Settings\nB.2\nArchitecture Design\nB.3\nFinetuning\n0\non\nLIBERO\nB.4\nFull Results for Flow and Regression Comparison\nB.5\nDataset Quality Ablation\nB.6\nFull Results for\nMIP\nand its variants\nB.7\nDifferent Method‚Äôs Performance with Different Number of Function Evaluations\nB.8\nComparing\nMIP\nwith Consistency Models\nC\nLipschitz Constant Study Details\nC.1\nLipschitz Evluation Method\nC.2\nFull Lipschitz Evaluation Results\nD\nMulti-Modality Study Details\nD.1\nQ Function Estimation\nD.2\nDeterministic Dataset Generation\nE\nManifold Adherence Study Details\nE.1\nValidation Loss Is Not a Good Proxy for Policy Performance\nE.2\nManifold Adherence Evaluation Method\nF\nNearest Neighbor Hypothesis Study\nF.1\nAction Chunk Size Study\nF.2\nLoss Norm Type Ablation Study\nG\nDiversity of GCPs and RCPs\nH\nTheoretical analysis of GCP‚Äôs expressivity\nH.1\nFormal statement of Theorem\n1\nH.2\nSupporting lemmas\nH.3\nProof of Theorem\n2\nI\nRegularization does not account for manifold adherence\nI.1\nRidge regression for observation-to-action mapping (penalty on\nB\nB\n)\nI.2\nRidge regression for action-to-action mapping (penalty on\nC\nC\n)\nI.3\nTwo-pass linear surrogate of\nMIP\nJ\nToy experiments: Testing the function approximation capabilities of regression and flow models\nJ.1\nOverview\nJ.2\nEvaluation Metrics\nJ.3\nKey Findings\nJ.4\nTraining Loss Considerations\nJ.5\nArchitectural Observations\nJ.6\nImplications for Method Selection\nJ.7\nExperimental Details\nK\nAppendix for Section\n2\nK.1\nMarkov Decision Processes Configuration\nK.2\nIntegrated Flow Prediction\nAppendix A\nAdditional Policy Parametrizations\nThis section further elaborates the design space of\nMIP\nin stochasticity injection, iterative computation and intermediate supervision.\nA.1\nFull Abalation of\nMIP\nVariants\nThis section formally describes the training process of all\nMIP\nwith different stochasticity injection and supervised iterative computation design.\nResidual Regression (\nRR\n)\nremoves all stochasticity in training and the training objective is:\n‚âà\nRR\narg\n‚Äã\nmin\nE\n(\no\n,\na\n)\n‚àº\np\ntrain\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\n\\displaystyle{}^{\\textbf{{RR}}}\\approx\\operatorname*{arg\\,min}\\mdmathbb{E}_{(o,a)\\sim p_{\\mathrm{train}},z\\sim\\mathrm{N}(0,\\mathbf{I})}\n(A.1)\n(\n‚à•\n(\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n‚àí\nt\n‚ãÜ\na\n)\n‚à•\n2\n+\n‚à•\n(\n(\no\n,\nsg\n(\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n)\n,\nt\n‚ãÜ\n)\n‚àí\na\n)\n‚à•\n2\n)\n.\n\\displaystyle\\left(\\|((o,I_{0}=0,t=0)-t_{\\star}a)\\|^{2}+\\|((o,\\mathrm{sg}((o,I_{0}=0,t=0)),t_{\\star})-a)\\|^{2}\\right).\n(A.2)\nTwo-Step Denoising (\nTSD\n)\nThe training objective is:\n‚âà\nTSD\narg\n‚Äã\nmin\nE\n(\no\n,\na\n)\n‚àº\np\ntrain\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\n\\displaystyle{}^{\\textbf{{TSD}}}\\approx\\operatorname*{arg\\,min}\\mdmathbb{E}_{(o,a)\\sim p_{\\mathrm{train}},z\\sim\\mathrm{N}(0,\\mathbf{I})}\n(\n‚Äñ\n(\n(\no\n,\nI\n0\n,\nt\n=\n0\n)\n‚àí\nt\n‚ãÜ\n‚Äã\na\n)\n‚Äñ\n2\n+\n‚Äñ\n(\n(\no\n,\nsg\n‚Äã\n(\n(\no\n,\nI\n0\n,\nt\n=\n0\n)\n)\n+\n(\n1\n‚àí\nt\n‚ãÜ\n)\n‚Äã\nz\n,\nt\n‚ãÜ\n)\n‚àí\na\n)\n‚Äñ\n2\n)\n.\n\\displaystyle\\left(\\|((o,I_{0},t=0)-t_{\\star}a)\\|^{2}+\\|((o,\\mathrm{sg}((o,I_{0},t=0))+(1-t_{\\star})z,t_{\\star})-a)\\|^{2}\\right).\nwhere\nI\n0\n=\nz\nI_{0}=z\n. Compared to\nMIP\n,\nTSD\nadds stochasticity to both first step training.\nMIP\nwith Data Augmentation (\nMIP-Dagger\n)\nTo understand the importance of decoupling for enabling iterative computation, we propose an additional variant of\nMIP\nthat lies between\nMIP\nand\nRR\n, where the two steps are partially coupled. Since the training method of second iteration is similar to data augmentation, we call this variant\nMIP-Dagger\n:\n‚âà\nMIP-Dagger\narg\n‚Äã\nmin\nE\n(\no\n,\na\n)\n‚àº\np\ntrain\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\n\\displaystyle{}^{\\textbf{{MIP-Dagger}}}\\approx\\operatorname*{arg\\,min}\\underset{(o,a)\\sim p_{\\mathrm{train}},z\\sim\\mathrm{N}(0,\\mathbf{I})}{\\mdmathbb{E}}\n(\n‚à•\n(\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n‚àí\nt\n‚ãÜ\na\n)\n‚à•\n2\n+\n‚à•\n(\n(\no\n,\nt\n‚ãÜ\nsg\n(\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n)\n+\n(\n1\n‚àí\nt\n‚ãÜ\n)\nz\n,\nt\n‚ãÜ\n)\n‚àí\na\n)\n‚à•\n2\n)\n,\n\\displaystyle(\\|((o,I_{0}=0,t=0)-t_{\\star}a)\\|^{2}+\\|((o,t_{\\star}\\mathrm{sg}((o,I_{0}=0,t=0))+(1-t_{\\star})z,t_{\\star})-a)\\|^{2}),\nwhere the major difference compared to\nMIP\nis the second step takes in the interpolant between first step output and noise rather than the action and noise.\nMIP\nwithout intermediate supervision (\nMIP-NoSupervision\n)\nTo understand the effect of intermediate supervision on iterative computation, we propose one variant of\nMIP\nthat removes the supervision of intermediate computation steps while preserving stochasticity injection at training time, named\nMIP-NoSupervision\n:\n‚âà\nMIP-NoSupervision\narg\n‚Äã\nmin\nE\n(\no\n,\na\n)\n‚àº\np\ntrain\n,\nz\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\n\\displaystyle{}^{\\textbf{{MIP-NoSupervision}}}\\approx\\operatorname*{arg\\,min}\\underset{(o,a)\\sim p_{\\mathrm{train}},z\\sim\\mathrm{N}(0,\\mathbf{I})}{\\mdmathbb{E}}\n(\n‚à•\n(\n(\no\n,\nt\n‚ãÜ\nsg\n(\n(\no\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n)\n+\n(\n1\n‚àí\nt\n‚ãÜ\n)\nz\n,\nt\n‚ãÜ\n)\n‚àí\na\n)\n‚à•\n2\n)\n,\n\\displaystyle(\\|((o,t_{\\star}\\mathrm{sg}((o,I_{0}=0,t=0))+(1-t_{\\star})z,t_{\\star})-a)\\|^{2}),\nwhere the first step‚Äôs output is unsupervised.\nMIP\nwithout\nt\nt\nconditioning\nBy removing\nt\nt\nconditioning in\nMIP\n, it degenerates to\nSF\n. Here we present the multi-step integration process for straight flow when action distribution is Dirac delta. The integrator from\ns\ns\nto\nt\nt\nis:\na\nt\n=\nt\n‚àí\ns\n1\n‚àí\ns\n‚Äã\n(\no\n,\ns\n‚ãÖ\na\ns\n)\n+\n1\n‚àí\nt\n1\n‚àí\ns\n‚Äã\na\ns\n\\displaystyle a_{t}=\\frac{t-s}{1-s}(o,s\\cdot a_{s})+\\frac{1-t}{1-s}a_{s}\nA.2\nAdditional Noise Injection Methods\nWhile\nMIP\nonly injects noise to action, we also explore the possibility of injecting noise to observation.\nWe propose two variants of\nMIP\n:\nMIP-Obs\nand\nMIP-Dagger-Obs\n.\nIn\nMIP-Obs\n, we perturb the first step‚Äôs observation with noise\nz\nz\n, while the second step‚Äôs training is the same as the original\nMIP\nwith decoupled training.\nIn\nMIP-Dagger-Obs\n, we perturb the first step‚Äôs observation with noise\nz\nz\n, and the second step‚Äôs training is conditioned on the first step‚Äôs output, making it similar to Dagger.\nMajor differnce compared to the original MIP: perturb the first step‚Äôs observation.\nIn both variants, we fixed\nt\n‚àó\n=\n0.9\nt_{*}=0.9\nand all observation perturbation happens at observation embedding space with normalized features.\nMIP-Obs\n\\displaystyle\\tiny{}^{\\textsc{MIP-Obs}}\n‚âà\narg\n‚Äã\nmin\nE\n(\no\n+\n(\n1\n‚àí\nt\n‚àó\n)\n‚Äã\nz\n,\na\n)\n‚àº\np\ntrain\nz\n‚àº\nùí©\n‚Äã\n(\n0\n,\nI\n)\n[\n‚à•\n(\n(\no\n+\n(\n1\n‚àí\nt\n‚àó\n)\nz\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n‚àí\nt\n‚àó\na\n)\n‚à•\n2\n\\displaystyle\\approx\\operatorname*{arg\\,min}\\underset{\\begin{subarray}{c}(o+(1-t_{*})z,a)\\sim p_{\\text{train}}\\\\\nz\\sim\\mathcal{N}(0,I)\\end{subarray}}{\\mdmathbb{E}}\\Bigg[\\|((o+{\\color[rgb]{0.65234375,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.65234375,0,0}(1-t_{*})z},I_{0}=0,t=0)-t_{*}a)\\|^{2}\n+\n‚à•\n(\n(\no\n,\nI\nt\n‚àó\n,\nt\n‚àó\n)\n‚àí\na\n)\n‚à•\n2\n]\n\\displaystyle\\quad+\\|((o,I_{t_{*}},t_{*})-a)\\|^{2}\\Bigg]\nMIP-Dagger-Obs\n\\displaystyle{}^{\\textsc{MIP-Dagger-Obs}}\n‚âà\narg\n‚Äã\nmin\nE\n(\no\n,\na\n)\n‚àº\np\ntrain\nz\n‚àº\nùí©\n‚Äã\n(\n0\n,\nI\n)\n[\n‚à•\n(\n(\no\n+\n(\n1\n‚àí\nt\n‚àó\n)\nz\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n‚àí\nt\n‚àó\na\n)\n‚à•\n2\n\\displaystyle\\approx\\operatorname*{arg\\,min}\\underset{\\begin{subarray}{c}(o,a)\\sim p_{\\text{train}}\\\\\nz\\sim\\mathcal{N}(0,I)\\end{subarray}}{\\mdmathbb{E}}\\Bigg[\\|((o+{\\color[rgb]{0.65234375,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.65234375,0,0}{(1-t_{*})z}},I_{0}=0,t=0)-t_{*}a)\\|^{2}\n+\n‚à•\n(\n(\no\n,\nt\n‚àó\nstopgrad\n(\n(\no\n+\n(\n1\n‚àí\nt\n‚àó\n)\nz\n,\nI\n0\n=\n0\n,\nt\n=\n0\n)\n)\n+\n1\n‚àí\nt\n‚àó\n)\nz\n,\nt\n‚àó\n)\n‚àí\na\n)\n‚à•\n2\n]\n\\displaystyle\\quad+\\|((o,t_{*}\\text{stopgrad}((o+{\\color[rgb]{0.65234375,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.65234375,0,0}{(1-t_{*})z}},I_{0}=0,t=0))+1-t_{*})z,t_{*})-a)\\|^{2}\\Bigg]\nWe find that perturbing observations introduces data conflicts and degrades performance (\nTable\nÀú\n7\n). In a two-step model, selecting noise levels that prevent observation overlap becomes challenging and brittle, leading to training instability across architectures.\nArchitecture\nMethod (L2)\nTransport\n(ph)\nTool\n-\nHang\n(ph)\nChi-Transformer\nRegression\n0.50/0.45\n0.50/0.37\nChi-Transformer\nMIP\n0.79/0.69\n0.92/0.85\nChi-Transformer\nMIP-Dagger-Obs\n0.00/0.00\n0.00/0.00\nChi-Transformer\nMIP-Obs\n0.61/0.46\n0.13/0.08\nChi-Transformer\nFlow\n0.81/0.71\n0.89/0.75\nSudeep-DiT\nRegression\n0.65/0.54\n0.31/0.25\nSudeep-DiT\nMIP\n0.80/0.69\n0.80/0.72\nSudeep-DiT\nMIP-Dagger-Obs\n0.00/0.00\n0.00/0.00\nSudeep-DiT\nMIP-Obs\n0.00/0.00\n0.00/0.00\nSudeep-DiT\nFlow\n0.79/0.65\n0.73/0.61\nChi-UNet\nRegression\n0.66/0.59\n0.73/0.59\nChi-UNet\nMIP\n0.81/0.72\n0.82/0.71\nChi-UNet\nMIP-Dagger-Obs\n0.00/0.00\n0.00/0.00\nChi-UNet\nMIP-Obs\n0.00/0.00\n0.00/0.00\nChi-UNet\nFlow\n0.83/0.75\n0.87/0.73\nTable 7:\nPerformance comparison of different methods with observation perturbation on state-based tasks. For each methods and architecture, we report the average and best performance across 5 checkpoints with 3 random seeds.\nA.3\nExperiment Results\nWe benchmark all methods on the\nTool\n-\nHang\ntask, given it is the one with the largest gap between RCP and GCPs.\nFrom\nTable\nÀú\n8\n, we can see that the important part is to add stochasticity injection between two iterations, and intermediate supervision is also important to realize the potential of iterative computation.\nMethod\nNFEs\nSuccess Rate\nTSD\n2\n0.80\nMIP\n2\n0.80\nMIP-NoSupervision\n2\n0.42\nMIP-Dagger\n2\n0.64\nRR\n2\n0.54\nSF\n1\n0.54\nSF\n3\n0.55\nSF\n9\n0.52\nTable 8:\nSuccess rates across different\nMIP\nvariants and\nRR\non\nTool\n-\nHang\ntask over 5 checkpoints across 3 architectures.\nAppendix B\nControl Experiments\nB.1\nTask Settings\nThis section introduces all the tasks presented in the main paper.\nTo reach a sound conclusion, use common benchmarks appears in previous works:\nRobomimic\nRobomimic\n(\nrobomimic2021\n)\nis a large-scale robotic manipulation benchmark designed to study imitation learning and offline reinforcement learning.\nIt contains five manipulation tasks (\nLift\n,\nCan\n,\nSquare\n,\nTransport\n,\nTool\n-\nHang\n) with\nproficient human (PH)\nteleoperated demonstrations, and for four of them, additional\nmixed proficient/non-proficient human (MH)\ndemonstration datasets are provided (9 variants in total).\nWe report results on both\nstate-based\nand\nimage-based\nobservations, since these two modalities pose distinct challenges.\nAmong the tasks,\nTool\n-\nHang\nrequires extremely precise end-effector positioning and fine-grained contact control, while\nTransport\ndemands high-dimensional control and coordination over extended horizons.\nPush\n-\nT\nPush\n-\nT\n(\nflorence2022implicit\n)\nis adapted from the Implicit Behavior Cloning (IBC). The task involves pushing a T-shaped block to a fixed target location using a circular end-effector. Randomized initializations of both the block and the end-effector introduce significant variability. The task is contact-rich and requires modeling complex object dynamics for precise block placement. Two observation variants are considered: (\ni\n) raw RGB image observations and (\nii\n) state-based observations containing object pose and end-effector position.\nKitchen\nThe Franka\nKitchen\nenvironment is designed to test the ability of IL and offline RL methods to perform long-horizon, multi-task manipulation.\nIt includes 7 interactive objects, with human demonstration data consisting of 566 sequences, each completing 4 sub-tasks in arbitrary order (e.g., opening a cabinet, turning a knob).\nSuccess is measured by completing as many of the demonstrated sub-tasks as possible, regardless of order. This setup explicitly introduces both short-horizon and long-horizon multimodality, requiring policies to generalize across compositional tasks.\nMetaWorld\nMetaWorld\nis a large-scale suite of diverse manipulation tasks built in MuJoCo, where agents must perform challenging object interactions using a robotic gripper. We adopt the 3D observation setting using point cloud representations, ported from the DP3 framework\n(\nze3DDiffusionPolicy2024\n)\n, to better evaluate geometric reasoning and spatial generalization. Tasks in MetaWorld are categorized into different difficulty levels, with benchmarks testing few-shot adaptation and multi-task transfer learning.\nAdroit\nAdroit\nis a suite of dexterous manipulation tasks featuring a 24-DoF anthropomorphic robotic hand.\nTasks include pen rotation, door opening, and object relocation, all of which demand precise, coordinated multi-finger control. Following DP3\n(\nze3DDiffusionPolicy2024\n)\n, we use point cloud observations to capture fine-grained 3D object-hand interactions. Policies are trained using VRL3, highlighting the challenges of high-dimensional control and sim-to-real transfer in dexterous manipulation.\nLIBERO\nLIBERO\nis a common multi-task benchmark to evaluate VLA‚Äôs generalization ability. It is composed of 130 tasks and can be categorized into multiple categories, including object, goal, spatial, and 10-task. The 10-task is long horizon and considered the most challenging to solve.\nB.2\nArchitecture Design\nWe study four policy backbones‚Äî\nChi-Transformer\n,\nSudeep-DiT\n,\nChi-UNet\n,\nRNN\n, and\nMLP\n‚Äîunder a common training recipe and data interface. Unless otherwise specified,\nall models are capacity-matched to\n‚àº\n\\sim\n20M parameters\nto enable fair comparison.\nChi-UNet\nis adopted from Diffusion Policy\n(\nchi2023diffusion\n)\nwhich built on top of 1D temporal U-Net\n(\njanner2022planning\n)\nwith FiLM conditioning\n(\nperez2018film\n)\non observation\no\no\nand flow time\nt\nt\n.\nChi-UNet\nhas a strong inductive bias for the temporal structure of the action and tends to smooth out the action.\nChi-Transformer\nfollows the time‚Äìseries diffusion transformer from Diffusion Policy\n(\nchi2023diffusion\n)\n, where the noisy action tokens\na\nt\na_{t}\nform the input sequence and a\npositional embedding\nof the flow time\nt\nt\nis prepended as the first token; observations\no\no\nare mapped by a shared MLP into an observation-embedding sequence that conditions the decoder stack.\nCompared to\nChi-UNet\n,\nChi-Transformer\nuses token-wise self-attention over the whole action sequence, thus can model less-smooth and more complex actions.\nSudeep-DiT\nis a DiT-style (Diffusion Transformer) conditional noise network specialized for policies adopted from DiT-Policy\n(\ndasariIngredientsRoboticDiffusion2024\n)\n: observation\no\no\nare first encoded into observation vectors;\nthe flow time\nt\nt\nis embedded via\npositional embedding\n; an encoder‚Äìdecoder transformer then fuses these with initial noise\nz\nz\nto predict next action.\nThe key ingredient of\nSudeep-DiT\nis replacing standard cross-attention with\nadaLN-Zero\nblocks‚Äîadaptive LayerNorm modulation using the mean encoder embedding and the time embedding, with zero-initialized output-scale projections‚Äîstabilizing diffusion training at scale.\nCompared to\nChi-Transformer\n,\nSudeep-DiT\nhas adaLN-based conditioning (instead of vanilla cross-attention) and an explicit encoder-decoder split, yielding better training stability.\nRNN\nThe\nRNN\nbackbone processes sequences with a stacked LSTM/GRU.\nFor each action time step in the chunk, the input vector concatenates: the current noised action\na\nt\na_{t}\n, a time embedding for\nt\nt\n, and a observation embedding for\no\no\n. The RNN outputs are fed to a MLP head with LayerNorm\n+\n+\nApproxGELU\n+\n+\nDropout blocks before output the action with final linear head.\nAll linear and recurrent weights use\northogonal initialization\n(biases zero), and RNN layer dropout is applied when depth\n>\n>\n1.\nMLP\nThe\nMLP\nbackbone flattens the action and observation, appending the time embedding.\nEach mlp block has LayerNorm, ApproxGELU and Dropout blocks with residual connection and\northogonal\nweight initialization throughout. Each block output is then modulated with FiLM conditioning.\nDP3\nbuilt on top of\nChi-UNet\nwith extra 3d perception encoder. We use the exact same architecture as 3D diffusion policy\n(\nze3DDiffusionPolicy2024\n)\n.\nModel hyperparameters\nIn the main experiments, we align the model capacity to 20M parameters for default if not specified, with detailed hyperparameters report in\nTable\nÀú\n9\n.\nBackbone\nHeads\nLayers\nEmbedding dim\nDropout\nSudeep-DiT\n8\n8\n256\n0.1\nChi-UNet\n‚Äì\n‚Äì\n256\n‚Äì\nChi-Transformer\n4\n8\n‚Äì\n0.1\nRNN\n‚Äì\n8\n512\n0.1\nMLP\n‚Äì\n8\n512\n‚Äì\nTable 9:\nModel hyperparameters.\nB.3\nFinetuning\n0\non\nLIBERO\nFor\n0\nfinetuning experiments, we use\nlerobot\nframework\n(\ncadene2024lerobot\n)\nto finetune\n0\non\nLIBERO\n.\nOur flow-based finetuning experiments match their reported results.\nTo finetune\n0\nto regression policy, we use the same architecture but set the initial noise always to zero and let the model directly predict the action.\nTo finetune\n0\nto\nMIP\n, we use the same practice where we modify the time step\nt\nt\nto be uniformly sample from\n{\n0\n,\nt\n‚àó\n}\n\\{0,t^{*}\\}\nuniformly and set the initial noise to zero.\nWe train all policies until convergence with 50k gradient steps on 1 node with 8 H100 GPUs.\nB.4\nFull Results for Flow and Regression Comparison\nIn the paper, we only present the aggregated results across 3 architectures.\nFigure\nÀú\n12\npresent the full results across all architectures with different training methods.\nFigure 12:\nRelative performance of RCP compared to GCP across common benchmarks (worst-case architecture).\nFor each task, we implement\nChi-Transformer\n,\nSudeep-DiT\nand\nChi-UNet\n. For each architecture, we average performance of the last 5 training checkpoints across three seeds. We then report the performance of the worst-performing architecture, chosen individually for both RCP and GCP, to demonstrate method robustness. For\nFlow\n, we always do 9 step Euler integrations, where its performance plateaued. For readability, RCP success rates are plotted relative to flow, with flow normalized to performance of\n1\n1\nper task. Tasks are grouped by observation modality, and ordered by relative RCP performance. Red dashed line indicates threshold at which RCP attains\n<\n95\n%\n<95\\%\nsuccess of GCP.\nTo further rule out the effect of training method, we also compare different methods‚Äô performance with\n‚Ñì\n1\n\\ell_{1}\n, which is observed to be superior for regression policy\n(\nkim2024openvla\n)\n.\nWe also benchmark the performance of flow model and\nMIP\nwith\n‚Ñì\n1\n\\ell_{1}\nloss to understand the effect of loss function on the performance of GCPs.\nTable\nÀú\n10\nshows the performance comparison of different methods with\n‚Ñì\n1\n\\ell_{1}\nand\n‚Ñì\n2\n\\ell_{2}\nloss, where we find that\n‚Ñì\n1\n\\ell_{1}\nloss generally outperforms\n‚Ñì\n2\n\\ell_{2}\nloss, especially for regression policy.\nWe attribute the superior performance of\n‚Ñì\n1\n\\ell_{1}\nloss to the fact that it can capture the expert behavior better by learning the medium instead of the mean of the action.\nHowever, even with\n‚Ñì\n1\n\\ell_{1}\nloss, we still observe that Regression\n<\n<\nMIP\n‚âà\n\\approx\nFlow, highlighting the importance of the stochasticity injection and iterative computation is independent of the loss function.\nArchitecture\nMethod\nTransport (ph)\nTool Hang (ph)\nSudeep-DiT\nRegression\n‚Ñì\n1\n\\ell_{1}\n0.72/0.64\n0.76/0.65\nSudeep-DiT\nRegression\n‚Ñì\n2\n\\ell_{2}\n0.50/0.45\n0.50/0.37\nSudeep-DiT\nMIP\n‚Ñì\n1\n\\ell_{1}\n0.81/0.73\n0.91/0.84\nSudeep-DiT\nMIP\n‚Ñì\n2\n\\ell_{2}\n0.79/0.69\n0.92/0.85\nSudeep-DiT\nFlow\n‚Ñì\n1\n\\ell_{1}\n0.83/0.76\n0.93/0.84\nSudeep-DiT\nFlow\n‚Ñì\n2\n\\ell_{2}\n0.81/0.71\n0.89/0.75\nChi-Transformer\nRegression\n‚Ñì\n1\n\\ell_{1}\n0.67/0.57\n0.44/0.33\nChi-Transformer\nRegression\n‚Ñì\n2\n\\ell_{2}\n0.65/0.54\n0.31/0.25\nChi-Transformer\nMIP\n‚Ñì\n1\n\\ell_{1}\n0.80/0.68\n0.85/0.77\nChi-Transformer\nMIP\n‚Ñì\n2\n\\ell_{2}\n0.80/0.69\n0.80/0.72\nChi-Transformer\nFlow\n‚Ñì\n1\n\\ell_{1}\n0.77/0.69\n0.81/0.71\nChi-Transformer\nFlow\n‚Ñì\n2\n\\ell_{2}\n0.79/0.65\n0.73/0.61\nChi-UNet\nRegression\n‚Ñì\n1\n\\ell_{1}\n0.84/0.71\n0.71/0.55\nChi-UNet\nRegression\n‚Ñì\n2\n\\ell_{2}\n0.66/0.59\n0.73/0.59\nChi-UNet\nMIP\n‚Ñì\n1\n\\ell_{1}\n0.85/0.68\n0.76/0.67\nChi-UNet\nMIP\n‚Ñì\n2\n\\ell_{2}\n0.81/0.72\n0.82/0.71\nChi-UNet\nFlow\n‚Ñì\n1\n\\ell_{1}\n0.85/0.69\n0.87/0.71\nChi-UNet\nFlow\n‚Ñì\n2\n\\ell_{2}\n0.83/0.75\n0.87/0.73\nTable 10:\nComparison of\n‚Ñì\n1\n\\ell_{1}\nvs\n‚Ñì\n2\n\\ell_{2}\nnorm across different methods and architectures. Report average/best performance across 5 checkpoints with 3 random seeds.\nB.5\nDataset Quality Ablation\nGCPs are believed to handle data with diverse quality better.\nTo test that assumption, we manually corrupt the expert dataset and inject stochactity and multi-modality in to the dataset.\nIn\nTable\nÀú\n11\n, we compare 4 different datasets (3 of them collected by ourselves).\nIn the collected dataset, we manually inject noise to the policy and add delay the policy from time to time to introduce multi-modality that is common in the real world.\nArchitecture\nMethod\nNFEs\nDelayed & Noisy Policy\nDelayed Policy\nZero-Flow\nProficient Human\n(Worst Quality)\n(Mixed Quality)\n(Better Quality)\n(Good Quality)\nChi-UNet\nRegression\n1\n0.70/0.63\n0.80/0.72\n0.76/0.65\n0.76/0.62\nChi-UNet\nSF\n1\n0.70/0.62\n0.82/0.76\n0.84/0.77\n0.62/0.38\nChi-UNet\nMIP\n2\n0.80/0.72\n0.82/0.61\n0.74/0.64\n0.80/0.68\nChi-UNet\nFlow\n9\n0.76/0.68\n0.74/0.50\n0.76/0.54\n0.84/0.70\nChi-Transformer\nRegression\n1\n0.38/0.22\n0.40/0.31\n0.42/0.26\n0.50/0.24\nChi-Transformer\nSF\n1\n0.46/0.35\n0.68/0.50\n0.56/0.41\n0.62/0.48\nChi-Transformer\nMIP\n2\n0.56/0.49\n0.70/0.54\n0.64/0.56\n0.72/0.68\nChi-Transformer\nFlow\n9\n0.56/0.34\n0.54/0.48\n0.62/0.49\n0.68/0.54\nSudeep-DiT\nRegression\n1\n0.42/0.29\n0.36/0.28\n0.42/0.32\n0.30/0.19\nSudeep-DiT\nSF\n1\n0.66/0.41\n0.60/0.54\n0.72/0.57\n0.68/0.50\nSudeep-DiT\nMIP\n2\n0.66/0.56\n0.74/0.58\n0.70/0.61\n0.86/0.78\nSudeep-DiT\nFlow\n9\n0.56/0.45\n0.66/0.58\n0.72/0.65\n0.78/0.68\nTable 11:\nPerformance comparison across different methods and data quality levels. We evaluate on the task\nTool\n-\nHang\nwith state observations using 10M parameter networks. Success rates are reported as averages over 5 checkpoints across 3 seeds.\nB.6\nFull Results for\nMIP\nand its variants\nArchitecture\nMethod\nLift\nCan\nSquare\nTransport\nTool\n-\nHang\nPush\n-\nT\nKitchen\nmh\nph\nmh\nph\nmh\nph\nmh\nph\nSudeep-DiT\nFlow\n1.00\n/0.99\n1.00\n/\n1.00\n1.00\n/0.94\n1.00\n/1.00\n0.88/0.75\n1.00\n/0.94\n0.40/0.27\n0.80/0.70\n0.86/0.75\n1.00\n/\n1.00\n0.98/0.96\nSudeep-DiT\nRegression\n1.00\n/0.99\n1.00\n/\n1.00\n0.92/0.90\n1.00\n/0.98\n0.72/0.53\n0.94/0.86\n0.12/0.06\n0.50/0.44\n0.52/0.39\n1.00\n/\n1.00\n0.98/0.92\nSudeep-DiT\nStraight Flow\n1.00\n/0.98\n1.00\n/\n1.00\n0.96/0.90\n1.00\n/0.99\n0.72/0.66\n0.96/0.93\n0.20/0.14\n0.56/0.48\n0.70/0.59\n1.00\n/\n1.00\n0.96/0.91\nSudeep-DiT\nMIP\n1.00\n/0.99\n1.00\n/\n1.00\n0.98/0.95\n1.00\n/\n1.00\n0.90/0.81\n0.98/\n0.94\n0.44/0.38\n0.76/0.68\n0.92\n/\n0.88\n1.00\n/\n1.00\n1.00\n/\n0.97\nChi-Transformer\nFlow\n1.00\n/\n1.00\n1.00\n/\n1.00\n1.00\n/0.93\n1.00\n/0.98\n0.78/0.74\n0.96/0.89\n0.44/0.34\n0.88\n/0.64\n0.68/0.54\n1.00\n/\n1.00\n1.00\n/0.96\nChi-Transformer\nRegression\n1.00\n/0.99\n1.00\n/0.99\n0.98/0.92\n1.00\n/0.96\n0.74/0.61\n0.92/0.85\n0.28/0.20\n0.68/0.51\n0.40/0.36\n1.00\n/\n1.00\n0.98/0.91\nChi-Transformer\nStraight Flow\n1.00\n/0.99\n1.00\n/\n1.00\n0.98/0.92\n1.00\n/0.99\n0.68/0.58\n0.96/0.89\n0.24/0.16\n0.62/0.54\n0.60/0.55\n1.00\n/\n1.00\n0.96/0.92\nChi-Transformer\nMIP\n1.00\n/\n1.00\n1.00\n/\n1.00\n0.96/0.95\n1.00\n/1.00\n0.86/0.73\n0.96/0.89\n0.42/0.37\n0.80/0.68\n0.76/0.69\n1.00\n/\n1.00\n0.98/0.96\nChi-UNet\nFlow\n1.00\n/\n1.00\n1.00\n/\n1.00\n1.00\n/0.98\n1.00\n/\n1.00\n0.90/0.78\n0.98/0.94\n0.52/0.40\n0.80/\n0.73\n0.84/0.70\n1.00\n/\n1.00\n1.00\n/0.97\nChi-UNet\nRegression\n1.00\n/\n1.00\n1.00\n/\n1.00\n1.00\n/0.96\n1.00\n/0.99\n0.94\n/\n0.82\n1.00\n/0.91\n0.22/0.16\n0.64/0.55\n0.68/0.64\n1.00\n/\n1.00\n0.92/0.88\nChi-UNet\nStraight Flow\n1.00\n/1.00\n1.00\n/\n1.00\n1.00\n/0.92\n1.00\n/0.99\n0.94\n/0.79\n0.98/0.90\n0.22/0.15\n0.64/0.52\n0.50/0.00\n1.00\n/\n1.00\n0.86/0.79\nChi-UNet\nMIP\n1.00\n/\n1.00\n1.00\n/\n1.00\n1.00\n/\n0.98\n1.00\n/0.99\n0.92/0.81\n1.00\n/\n0.94\n0.62\n/\n0.46\n0.80/0.69\n0.80/0.64\n1.00\n/\n1.00\n1.00\n/0.96\nTable 12:\nPerformance comparison of Flow and Regression methods across different\nstate-based\nrobotic manipulation tasks. For each task, we report the best checkpoint performance / averaged performance over last 5 checkpoints. Each experiment is run with 3 seeds and we report the average performance across all seeds.\nArchitecture\nMethod\nLift\nCan\nSquare\nTransport\nTool Hang\nPushT\nmh\nph\nmh\nph\nmh\nph\nmh\nph\nDiT\nFlow\n1.00\n/\n1.00\n1.00\n/1.00\n0.96/0.94\n1.00\n/0.99\n0.82/0.76\n0.96/0.94\n0.32/0.20\n0.84/0.83\n0.78\n/0.57\n1.00\n/\n1.00\nDiT\nRegression\n1.00\n/0.99\n1.00\n/\n1.00\n0.92/0.81\n1.00\n/\n1.00\n0.74/0.67\n0.94/0.84\n0.14/0.08\n0.74/0.56\n0.28/0.18\n1.00\n/\n1.00\nDiT\nStraight Flow\n1.00\n/0.99\n1.00\n/0.99\n0.98/0.95\n1.00\n/0.98\n0.82/0.72\n1.00\n/0.93\n0.26/0.19\n0.86/0.83\n0.46/0.40\n1.00\n/\n1.00\nDiT\nMIP\n1.00\n/0.99\n1.00\n/\n1.00\n1.00\n/0.96\n1.00\n/0.98\n0.90/0.83\n1.00\n/0.92\n0.50/0.31\n0.90/0.84\n0.76/\n0.66\n1.00\n/\n1.00\nTransformer\nFlow\n1.00\n/0.99\n1.00\n/1.00\n0.98/0.92\n1.00\n/0.96\n0.70/0.66\n0.98/0.93\n0.24/0.22\n0.80/0.77\n0.54/0.40\n1.00\n/\n1.00\nTransformer\nRegression\n1.00\n/0.98\n1.00\n/0.98\n1.00\n/0.94\n1.00\n/0.96\n0.76/0.70\n0.98/0.90\n0.40/0.27\n0.94/0.87\n0.44/0.36\n1.00\n/\n1.00\nTransformer\nStraight Flow\n1.00\n/\n1.00\n1.00\n/\n1.00\n1.00\n/0.95\n1.00\n/0.98\n0.90/0.78\n0.98/\n0.94\n0.32/0.25\n0.86/0.70\n0.36/0.28\n1.00\n/\n1.00\nTransformer\nMIP\n1.00\n/0.98\n1.00\n/1.00\n0.96/0.91\n1.00\n/0.98\n0.72/0.21\n0.90/0.04\n0.18/0.06\n0.86/0.69\n0.60/0.48\n1.00\n/\n1.00\nUNet\nFlow\n1.00\n/\n1.00\n1.00\n/\n1.00\n1.00\n/\n0.97\n1.00\n/0.98\n0.90/0.79\n0.96/0.90\n0.24/0.16\n0.78/0.61\n0.48/0.37\n1.00\n/\n1.00\nUNet\nRegression\n1.00\n/0.96\n1.00\n/0.99\n0.84/0.70\n0.98/0.87\n0.74/0.66\n0.94/0.86\n0.18/0.10\n0.66/0.64\n0.30/0.23\n1.00\n/\n1.00\nUNet\nStraight Flow\n1.00\n/0.94\n1.00\n/0.99\n0.98/0.93\n1.00\n/0.96\n0.72/0.68\n0.92/0.62\n0.00/0.00\n0.50/0.22\n0.06/0.02\n1.00\n/\n1.00\nUNet\nMIP\n1.00\n/\n1.00\n1.00\n/\n1.00\n1.00\n/0.95\n1.00\n/0.98\n0.92\n/\n0.84\n0.96/0.91\n0.52\n/\n0.37\n0.96\n/\n0.91\n0.56/0.50\n1.00\n/\n1.00\nTable 13:\nPerformance comparison of Flow and Regression methods across different\nimage-based\nrobotic manipulation tasks. For each task, we report the best checkpoint performance / averaged performance over last 5 checkpoints. Each experiment is run with 3 seeds and we report the average performance across all seeds.\nArchitecture\nMethod\nAdroit\nMetaWorld\nHammer\nDoor\nPen\nStick-Push\nAssembly\nDisassemble\nDP3\nFlow\n0.96\n¬±\n0.02\n0.96\\pm 0.02\n0.60\n¬±\n0.06\n\\mathbf{0.60\\pm 0.06}\n0.54\n¬±\n0.11\n\\mathbf{0.54\\pm 0.11}\n0.92\n¬±\n0.04\n0.92\\pm 0.04\n0.98\n¬±\n0.03\n\\mathbf{0.98\\pm 0.03}\n0.72\n¬±\n0.14\n0.72\\pm 0.14\nRegression\n0.97\n¬±\n0.04\n\\mathbf{0.97\\pm 0.04}\n0.52\n¬±\n0.16\n0.52\\pm 0.16\n0.47\n¬±\n0.08\n0.47\\pm 0.08\n0.95\n¬±\n0.06\n\\mathbf{0.95\\pm 0.06}\n0.98\n¬±\n0.03\n\\mathbf{0.98\\pm 0.03}\n0.78\n¬±\n0.08\n\\mathbf{0.78\\pm 0.08}\nTable 14:\nPerformance comparison of Flow and Regression methods using DP3 architecture across different\npoint-cloud-based\nrobotic manipulation tasks. For each task, we report the best checkpoint performance / averaged performance over last 5 checkpoints. Each experiment is run with 3 seeds and we report the average performance across all seeds.\nFor\nKitchen\n, the task has multiple stages. In the main results, we only report the performance of the last stage since it is the most challenging one.\nTable\nÀú\n15\nshows the performance comparison across different design choices on\nKitchen\ntask.\nArchitecture\nMethod\nP1\nP2\nP3\nP4\nChi-UNet\nFlow\n1.0\n1.0\n1.0\n0.98\nMIP\n1.0\n1.0\n1.0\n0.94\nRegression\n0.98\n0.94\n0.94\n0.86\nChi-Transformer\nFlow\n1.0\n1.0\n1.0\n1.0\nMIP\n1.00\n0.98\n0.98\n0.96\nRegression\n1.0\n1.0\n0.98\n0.94\nSudeep-DiT\nFlow\n1.0\n1.0\n1.0\n0.98\nMIP\n1.00\n1.00\n1.00\n0.98\nRegression\n1.0\n0.98\n0.96\n0.88\nTable 15:\nPerformance comparison across different design choices on kitchen task.\nKitchen task has multiple stages and we report the success rate of finishing\nn\nn\ntasks in the table. For the performance reported in the main paper and previous tables, we report the success rate of finishing 4 tasks.\nB.7\nDifferent Method‚Äôs Performance with Different Number of Function Evaluations\nWe also provide detailed evaluation on different method‚Äôs scaling behavior given different amount of online computation budgets.\nTable\nÀú\n16\nhighlights that only\nMIP\nand\nFlow\nbenefit from iterative computate.\nMethod\nReg.\nSF\nRR\nMIP\nFlow\nNFEs\n1\n1\n3\n9\n1\n2\n1\n2\n1\n3\n9\nS.R.\n0.46\n0.54\n0.55\n0.52\n0.31\n0.33\n0.50\n0.74\n0.32\n0.55\n0.66\nTable 16:\nComparison of methods and their corresponding number of function evaluations (NFEs).\nEvaluated on state-based\nTool\n-\nHang\ntask over\nChi-UNet\n. Average success rate is reported across 3 seeds and 5 checkpoints.\nB.8\nComparing\nMIP\nwith Consistency Models\nGiven\nMIP\ntakes less integration steps compared to flow model, we compare it with consistency models which accelerate the sampling process of flow by distilling the learned flow into a shortcut model.\nThe major difference between\nMIP\nand consistency models is that the latter do satisfy\nC\n1\n, and require training over a continuum of noise levels.\nOn the other hand,\nMIP\nis trained to predict the conditional mean of the interpolant, and thus, doesn‚Äôt need extra distillation stage.\nAs shown in\nTable\nÀú\n17\n, We benchmarks\nMIP\nto common consistency model training methods including consistency trajectory model (CTM)\n(\nkim2023consistency\n)\nand Lagrangian map distillation (LMD)\n(\nboffiFlowMapMatching2025\n)\n, where LMD only works for\nChi-UNet\ndue to its dependency on jacobian matrix computation.\nThe benchmarking results indicates that, given best architecture,\nMIP\noutperforms consistency models.\nIn terms of training time,\nMIP\nonly takes half of the time compared to CTM, where LMD training takes even longer due to jacobian matrix computation.\nArchitecture\nMethod\nTransport\nTool Hang\nmh\nph\nSudeep-DiT\nFlow\n0.40/0.27\n0.80/0.70\n0.86/0.75\nSudeep-DiT\nMIP\n0.44/0.38\n0.76/0.68\n0.92\n/\n0.88\nChi-Transformer\nCTM\n0.57/0.32\n0.90\n/0.58\n0.56/0.26\nChi-Transformer\nFlow\n0.44/0.34\n0.88/0.64\n0.68/0.54\nChi-Transformer\nMIP\n0.42/0.37\n0.80/0.68\n0.76/0.69\nChi-UNet\nCTM\n0.40/0.32\n0.72/0.63\n0.46/0.37\nChi-UNet\nFlow\n0.52/0.40\n0.80/\n0.73\n0.84/0.70\nChi-UNet\nLMD\n0.44/0.32\n0.76/0.68\n0.74/0.52\nChi-UNet\nMIP\n0.62\n/\n0.46\n0.80/0.69\n0.80/0.64\nTable 17:\nBenchmark results across different architectures and methods on state-based tasks on consistency models and\nMIP\n. Report average/best performance across 5 checkpoints with 3 random seeds. Both LMD and CTM integrate 2 steps, which is the same as\nMIP\n.\nAppendix C\nLipschitz Constant Study Details\nC.1\nLipschitz Evluation Method\nWe note that not all inputs\no\no\nare dynamically feasible, and our dataset lies only on a narrow manifold of the observation space.\nTherefore, we must carefully evaluate the Lipschitz constant on the feasible observation space to avoid conflating model expressivity with errors arising from infeasible states.\nTo ensure feasibility, instead of directly perturbing the state, we perturb the action and then roll it out in the environment.\nThis guarantees that both the perturbed state and the resulting observation remain feasible.\nIn practice, we identify states that exhibit the highest ambiguity of actions in the dataset, referred to as\ncritical states\n.\nFor each critical state, we inject Gaussian noise\n‚àº\nùí©\n‚Äã\n(\n0\n,\nI\n2\n)\n\\eta\\sim\\mathcal{N}(0,{}^{2}I)\ninto the normalized action, unnormalize it, and then roll it out.\nWe select\n100\n100\ncritical states from the dataset. For each state, we perturb the corresponding expert action\na\na\nwith\n64\n64\nindependent Gaussian samples.\nLet\no\no\ndenote the next nominal observation after applying the nominal action\na\na\n.\nAfter rolling out the perturbed actions, we obtain perturbed observations\no\n(\n1\n)\n,\n‚Ä¶\n,\no\n(\nN\nperturb\n)\no^{(1)},\\dots,o^{(N_{\\text{perturb}})}\n.\nThe policy then predicts the perturbed actions\na\n(\ni\n)\n=\n(\no\n(\ni\n)\n)\na^{(i)}=\\pi(o^{(i)})\n.\nTo ensure comparability across different states and tasks, we evaluate the Lipschitz constant with respect to normalized observations\no\n¬Ø\n=\no\n‚àí\no\no\n\\bar{o}=\\frac{o-{}_{o}}{{}_{o}}\nand normalized actions\na\n¬Ø\n=\na\n‚àí\na\na\n\\bar{a}=\\frac{a-{}_{a}}{{}_{a}}\n.\nFinally, the Lipschitz constant is estimated using a zeroth-order approximation:\nL\n‚âà\nmax\ni\n‚Å°\n‚Äñ\na\n¬Ø\n(\ni\n)\n‚àí\na\n¬Ø\n‚Äñ\n2\n‚à•\n‚à•\n2\n.\n\\displaystyle L\\approx\\max_{i}\\frac{\\|\\bar{a}^{(i)}-\\bar{a}\\|_{2}}{\\|\\eta\\|_{2}}.\n(C.1)\nFull version of above process is stated in\nAlgorithm\nÀú\n1\n.\nAlgorithm 1\nLipschitz Constant Estimation via Action Perturbation\n1:\nDataset\nùíü\n\\mathcal{D}\n, policy , noise scale , number of critical states\nN\ns\n=\n100\nN_{s}{=}100\n, number of perturbations\nN\np\n=\n64\nN_{p}{=}64\n2:\nEstimated Lipschitz constant\nL\nL\n3:\nS\n‚Üê\nS\\leftarrow\nidentify\nN\ns\nN_{s}\ncritical states from\nùíü\n\\mathcal{D}\n‚ä≥\n\\triangleright\nSelect states with highest action ambiguity\n4:\nfor all\ncritical state\ns\n‚àà\nS\ns\\in S\ndo\n5:\n(\na\n,\no\n)\n‚Üê\n(a,o)\\leftarrow\nexpert action and nominal next observation for\ns\ns\n‚ä≥\n\\triangleright\nGet ground truth action-observation pair\n6:\n(\na\n¬Ø\n,\no\n¬Ø\n)\n‚Üê\n(\\bar{a},\\bar{o})\\leftarrow\nnormalize\n(\na\n,\no\n)\n(a,o)\nusing dataset statistics\n‚ä≥\n\\triangleright\nEnsure comparability across states/tasks\n7:\nfor\ni\n=\n1\ni=1\nto\nN\np\nN_{p}\ndo\n8:\n‚àº\nùí©\n‚Äã\n(\n0\n,\nI\n2\n)\n\\eta\\sim\\mathcal{N}(0,{}^{2}I)\n‚ä≥\n\\triangleright\nSample Gaussian perturbation\n9:\na\npert\n‚Üê\na_{\\text{pert}}\\leftarrow\nunnormalize\n(\na\n¬Ø\n+\n)\n(\\bar{a}+\\eta)\n‚ä≥\n\\triangleright\nCreate perturbed action in original scale\n10:\no\n(\ni\n)\n‚Üê\no^{(i)}\\leftarrow\nrollout\n(\na\npert\n)\n(a_{\\text{pert}})\nin environment\n‚ä≥\n\\triangleright\nExecute perturbed action to get feasible state\n11:\no\n¬Ø\n(\ni\n)\n‚Üê\n\\bar{o}^{(i)}\\leftarrow\nnormalize\n(\no\n(\ni\n)\n)\n(o^{(i)})\n‚ä≥\n\\triangleright\nNormalize perturbed observation\n12:\na\n(\ni\n)\n‚Üê\n(\no\n(\ni\n)\n)\na^{(i)}\\leftarrow\\pi(o^{(i)})\n‚ä≥\n\\triangleright\nGet policy prediction on perturbed state\n13:\na\n¬Ø\n(\ni\n)\n‚Üê\n\\bar{a}^{(i)}\\leftarrow\nnormalize\n(\na\n(\ni\n)\n)\n(a^{(i)})\n‚ä≥\n\\triangleright\nNormalize predicted action\n14:\nr\ni\n‚Üê\n‚Äñ\na\n¬Ø\n(\ni\n)\n‚àí\na\n¬Ø\n‚Äñ\n2\n‚à•\n‚à•\n2\nr_{i}\\leftarrow\\frac{\\|\\bar{a}^{(i)}-\\bar{a}\\|_{2}}{\\|\\eta\\|_{2}}\n‚ä≥\n\\triangleright\nCompute finite difference approximation\n15:\nL\ns\n‚Üê\nmax\ni\n‚Å°\nr\ni\nL_{s}\\leftarrow\\max_{i}r_{i}\n‚ä≥\n\\triangleright\nLocal Lipschitz constant for state\ns\ns\n16:\nL\n‚Üê\n1\nN\ns\n‚Äã\n‚àë\ns\n=\n1\nN\ns\nL\ns\nL\\leftarrow\\frac{1}{N_{s}}\\sum_{s=1}^{N_{s}}L_{s}\n‚ä≥\n\\triangleright\nAverage across all critical states\n17:\nreturn\nL\nL\nC.2\nFull Lipschitz Evaluation Results\nIn the main text, we only report the average Lipschitz constant on critical states across 3 architectures.\nHere, we report the full Lipschitz constant evaluation reuslt in\nTable\nÀú\n18\nwith different architectures and tasks.\nTask\nArchitecture\nMethod\nLipschitz Constant (Policy)\nPush\n-\nT\n(State)\nChi-UNet\nRegression\n0.85\n¬±\n0.58\n0.85\\pm 0.58\nFlow\n0.31\n¬±\n0.01\n0.31\\pm 0.01\nSudeep-DiT\nRegression\n0.52\n¬±\n0.11\n0.52\\pm 0.11\nFlow\n0.22\n¬±\n0.02\n0.22\\pm 0.02\nChi-Transformer\nRegression\n1.33\n¬±\n1.14\n1.33\\pm 1.14\nFlow\n0.82\n¬±\n0.26\n0.82\\pm 0.26\nKitchen\n(State)\nChi-UNet\nRegression\n13.47\n¬±\n2.80\n13.47\\pm 2.80\nFlow\n13.31\n¬±\n4.13\n13.31\\pm 4.13\nSudeep-DiT\nRegression\n15.37\n¬±\n3.69\n15.37\\pm 3.69\nFlow\n12.54\n¬±\n5.09\n12.54\\pm 5.09\nChi-Transformer\nRegression\n13.37\n¬±\n4.00\n13.37\\pm 4.00\nFlow\n11.44\n¬±\n4.10\n11.44\\pm 4.10\nTool\n-\nHang\n(PH, State)\nChi-UNet\nRegression\n1.63\n¬±\n0.79\n1.63\\pm 0.79\nFlow\n1.53\n¬±\n1.01\n1.53\\pm 1.01\nSudeep-DiT\nRegression\n1.86\n¬±\n0.81\n1.86\\pm 0.81\nFlow\n1.34\n¬±\n0.97\n1.34\\pm 0.97\nChi-Transformer\nRegression\n1.76\n¬±\n1.02\n1.76\\pm 1.02\nFlow\n1.40\n¬±\n0.99\n1.40\\pm 0.99\nTable 18:\nDetailed: Per-architecture policy Lipschitz.\nAppendix D\nMulti-Modality Study Details\nD.1\nQ Function Estimation\nTo rule out the possibility of hidden multi-modality, we also plot Q functions for each action to see if there is any clear clustering pattern of\nQ\nQ\nw.r.t. different actions in t-SNE visualization.\nSince we only have access to expert actions rather than their policy, we estimate the Q function by Monte Carlo sampling with the learned flow policy.\nThe detailed procedure is as follows:\nStarting from one ‚Äúcritical state‚Äù, we first sample\nN\nN\nactions\na\n(\ni\n)\n=\n(\no\n,\nz\n(\ni\n)\n,\ns\n=\n0\n,\nt\n=\n1\n)\n,\ni\n=\n1\n,\n‚Ä¶\n,\nN\n,\nz\n(\ni\n)\n‚àº\nN\n(\n0\n,\nùêà\n)\n.\na^{(i)}=\\Phi(o,z^{(i)},s=0,t=1),\\quad i=1,\\dots,N,\\quad z^{(i)}\\sim\\mathrm{N}(0,\\mathbf{I}).\nFor each sampled action\na\n(\ni\n)\na^{(i)}\n, we execute one environment step to obtain the next observation\no\n‚Ä≤\n‚Å£\n(\ni\n)\no^{\\prime(i)}\nand immediate reward\nr\n‚Äã\n(\no\n,\na\n(\ni\n)\n)\nr(o,a^{(i)})\n.\nThen, starting from\no\n‚Ä≤\n‚Å£\n(\ni\n)\no^{\\prime(i)}\n, we rollout the learned policy for\nN\nMC\nN_{\\text{MC}}\nepisodes until termination (horizon\nH\nH\n), and average the cumulative returns to obtain an estimate of the continuation value.\nThus, the Q-value for action\na\n(\ni\n)\na^{(i)}\nis approximated as:\nQ\n‚Äã\n(\na\n(\ni\n)\n,\no\n)\n\\displaystyle Q(a^{(i)},o)\n=\nr\n‚Äã\n(\no\n,\na\n(\ni\n)\n)\n+\n1\nN\nMC\n‚Äã\n‚àë\nj\n=\n1\nN\nMC\n‚àë\nt\n=\n1\nH\nr\n‚Äã\n(\no\nt\n(\nj\n)\n,\na\nt\n(\nj\n)\n)\n.\n\\displaystyle=r(o,a^{(i)})+\\frac{1}{N_{\\text{MC}}}\\sum_{j=1}^{N_{\\text{MC}}}\\sum_{t=1}^{H}r\\big(o_{t}^{(j)},a_{t}^{(j)}\\big).\n(D.1)\nWe set the discount factor\n=\n1.0\n\\gamma=1.0\nsince rewards are sparse and triggered only at task completion.\nThe reward for\nTool\n-\nHang\nand\nKitchen\nis defined by the\nfinal\nsuccess signal (with\nKitchen\n‚Äôs success requiring all 4 subtasks to be completed).\nThe reward for\nPush\n-\nT\nis defined by\nfinal\ncoverage.\nAlgorithm 2\nQ Function Estimation via Monte Carlo Sampling\n1:\nDataset\nùíü\n\\mathcal{D}\n, flow policy , reward function\nr\nr\n, number of critical states\nN\ns\n=\n100\nN_{s}{=}100\n, number of action samples\nN\nN\n, Monte Carlo samples\nN\nMC\nN_{\\text{MC}}\n2:\nFor each state\no\no\n, pairs\n{\n(\na\n(\ni\n)\n,\nQ\n‚Äã\n(\na\n(\ni\n)\n,\no\n)\n)\n}\ni\n=\n1\nN\n\\{(a^{(i)},Q(a^{(i)},o))\\}_{i=1}^{N}\n3:\nS\n‚Üê\nS\\leftarrow\nidentify\nN\ns\nN_{s}\ncritical states from\nùíü\n\\mathcal{D}\n‚ä≥\n\\triangleright\nSelect states with highest action ambiguity\n4:\nfor all\ncritical state\ns\n‚àà\nS\ns\\in S\ndo\n5:\no\n‚Üê\no\\leftarrow\nobservation for state\ns\ns\n6:\nfor\ni\n=\n1\ni=1\nto\nN\nN\ndo\n‚ä≥\n\\triangleright\nSample actions and compute Q estimates\n7:\nz\n(\ni\n)\n‚àº\nN\n‚Äã\n(\n0\n,\nùêà\n)\nz^{(i)}\\sim\\mathrm{N}(0,\\mathbf{I})\n8:\na\n(\ni\n)\n‚Üê\n(\no\n,\nz\n(\ni\n)\n,\ns\n=\n0\n,\nt\n=\n1\n)\na^{(i)}\\leftarrow\\Phi(o,z^{(i)},s{=}0,t{=}1)\n9:\nExecute\n(\no\n,\na\n(\ni\n)\n)\n(o,a^{(i)})\nin env\n‚Üí\n\\to\nobtain\no\n‚Ä≤\n‚Å£\n(\ni\n)\no^{\\prime(i)}\n,\nr\n(\ni\n)\n=\nr\n‚Äã\n(\no\n,\na\n(\ni\n)\n)\nr^{(i)}=r(o,a^{(i)})\n10:\nfor\nj\n=\n1\nj=1\nto\nN\nMC\nN_{\\text{MC}}\ndo\n‚ä≥\n\\triangleright\nMonte Carlo rollouts from\no\n‚Ä≤\n‚Å£\n(\ni\n)\no^{\\prime(i)}\n11:\nRollout  from\no\n‚Ä≤\n‚Å£\n(\ni\n)\no^{\\prime(i)}\nuntil horizon\nH\nH\nto get cumulative return\nR\nj\n(\ni\n)\nR_{j}^{(i)}\n12:\nQ\n‚Äã\n(\na\n(\ni\n)\n,\no\n)\n‚Üê\nr\n(\ni\n)\n+\n1\nN\nMC\n‚Äã\n‚àë\nj\n=\n1\nN\nMC\nR\nj\n(\ni\n)\nQ(a^{(i)},o)\\leftarrow r^{(i)}+\\frac{1}{N_{\\text{MC}}}\\sum_{j=1}^{N_{\\text{MC}}}R_{j}^{(i)}\n13:\nStore\n{\n(\na\n(\ni\n)\n,\nQ\n‚Äã\n(\na\n(\ni\n)\n,\no\n)\n)\n}\ni\n=\n1\nN\n\\{(a^{(i)},Q(a^{(i)},o))\\}_{i=1}^{N}\nfor state\ns\ns\nThe procedure above explicitly computes Q-values by rolling out trajectories separately for each sampled action.\nD.2\nDeterministic Dataset Generation\nTo generate a deterministic dataset that completely eliminates any potential multi-modality, we follow a systematic process:\nFirst, we train a flow expert policy  on the original dataset. Then, we collect a new dataset by rolling out this expert policy from different initial states (using different random seeds than those used during testing). Crucially, during rollout, we always evaluate the flow policy deterministically by setting the initial noise to zero:\nz\n=\n0\nz=0\n. This ensures that the policy produces deterministic actions given any observation, completely removing any stochasticity from the action generation process.\nDuring data collection, we discard all failed trajectories to maintain the same success rate as the original dataset. We continue collecting until we reach the target number of trajectories\nN\ntraj\nN_{\\text{traj}}\n.\nAlgorithm 3\nDeterministic Dataset Generation\n1:\nTrained flow policy , target number of trajectories\nN\ntraj\nN_{\\text{traj}}\n, maximum episode steps\nT\nmax\nT_{\\max}\n2:\nDeterministic dataset\nùíü\ndet\n\\mathcal{D}_{\\text{det}}\n3:\nùíü\ndet\n‚Üê\n‚àÖ\n\\mathcal{D}_{\\text{det}}\\leftarrow\\emptyset\n4:\nn\ncollected\n‚Üê\n0\nn_{\\text{collected}}\\leftarrow 0\n5:\nwhile\nn\ncollected\n<\nN\ntraj\nn_{\\text{collected}}<N_{\\text{traj}}\ndo\n6:\nReset environment with new random seed\n7:\no\n0\n‚Üê\no_{0}\\leftarrow\ninitial observation\n8:\n‚Üê\n[\n(\no\n0\n,\n‚ãÖ\n)\n]\n\\tau\\leftarrow[(o_{0},\\cdot)]\n‚ä≥\n\\triangleright\nInitialize trajectory\n9:\nfor\nt\n=\n0\nt=0\nto\nT\nmax\n‚àí\n1\nT_{\\max}-1\ndo\n10:\na\nt\n‚Üê\n(\nz\n=\n0\n,\no\nt\n,\ns\n=\n0\n,\nt\n=\n1\n)\na_{t}\\leftarrow\\Phi(z=0,o_{t},s=0,t=1)\n‚ä≥\n\\triangleright\nDeterministic action\n11:\no\nt\n+\n1\n,\nr\nt\n,\ndone\n‚Üê\nenv.step\n‚Äã\n(\na\nt\n)\no_{t+1},r_{t},\\text{done}\\leftarrow\\text{env.step}(a_{t})\n12:\n‚Üê\n‚à™\n[\n(\no\nt\n,\na\nt\n)\n]\n\\tau\\leftarrow\\tau\\cup[(o_{t},a_{t})]\n13:\nif\ndone\nthen\n14:\nbreak\n15:\nif\ntrajectory  is successful\nthen\n16:\nùíü\ndet\n‚Üê\nùíü\ndet\n‚à™\n{\n}\n\\mathcal{D}_{\\text{det}}\\leftarrow\\mathcal{D}_{\\text{det}}\\cup\\{\\tau\\}\n17:\nn\ncollected\n‚Üê\nn\ncollected\n+\n1\nn_{\\text{collected}}\\leftarrow n_{\\text{collected}}+1\n18:\nreturn\nùíü\ndet\n\\mathcal{D}_{\\text{det}}\nAppendix E\nManifold Adherence Study Details\nE.1\nValidation Loss Is Not a Good Proxy for Policy Performance\nTo investigate whether validation loss serves as a reliable proxy for policy performance, we examine its relationship with success rates on\nTool\n-\nHang\nacross different architectures given different training methods.\nEvidence that validation loss is poorly correlated with success rate can be seen by comparing flow policies with varying numbers of function evaluations (NFEs) and their corresponding validation losses.\nTable\nÀú\n19\ndemonstrates that increasing NFEs does not reduce validation loss, yet policy performance consistently improves.\nWe hypothesize that higher NFEs introduce stronger inductive bias and regularization, which projects actions back onto the data manifold, thereby enhancing generalization.\nArchitecture\nMethod\nNFEs\nAverage Success Rate\nL\n2\nL_{2}\nValidation Loss\nChi-UNet\nRegression\n1\n0.54\n0.063\nFlow\n1\n0.36\n0.053\nFlow\n3\n0.44\n0.052\nFlow\n9\n0.58\n0.053\nChi-Transformer\nRegression\n1\n0.18\n0.084\nFlow\n1\n0.06\n0.093\nFlow\n3\n0.72\n0.092\nFlow\n9\n0.68\n0.089\nSudeep-DiT\nRegression\n1\n0.20\n0.063\nFlow\n1\n0.62\n0.082\nFlow\n3\n0.76\n0.080\nFlow\n9\n0.76\n0.080\nTable 19:\nComparison of validation loss and success rate across different architectures and methods on state-based\nTool\n-\nHang\n. The results show that validation loss is not a reliable proxy for policy performance.\nE.2\nManifold Adherence Evaluation Method\nTo evaluate the manifold adherence, we compute the projection error of a predicted action\na\na\nonto the space spanned by expert actions at neighboring states.\nConcretely, given a state, we compute its\n‚Ñì\n2\n\\ell_{2}\ndistance to all states in the training set.\nThen, we pick\nk\nk\nnearest neighbor states and gather their corresponding actions\nA\n=\n[\na\n(\n0\n)\n,\na\n(\n1\n)\n,\n‚Ä¶\n,\na\n(\nk\n)\n]\nA=[a^{(0)},a^{(1)},\\dots,a^{(k)}]\n.\nLastly, we compute projection error by projecting\na\na\nto the column space of\nA\nA\n:\n‚Äñ\na\n‚àí\nP\nA\n‚Äã\n(\na\n)\n‚Äñ\n2\n=\nmin\nc\n‚Å°\n‚Äñ\na\n‚àí\nA\n‚Äã\nc\n‚Äñ\n2\n\\|a-P_{A}(a)\\|_{2}=\\min_{c}\\|a-Ac\\|_{2}\n.\nAppendix F\nNearest Neighbor Hypothesis Study\nAnother popular hypothesis is that GCPs are learning a lookup table of observation-to-action mappings\n(\npari2021surprising\n;\nhe2025demystifyingdiffusionpoliciesaction\n)\n.\nThis might be true for relatively simple tasks that do not require high precision and complex generalization, such as\nCan\n.\nHowever, for tasks that require higher precision and more contact, such as\nTool\n-\nHang\n, the nearest-neighbor/lookup-table assumption is insufficient to explain the success of GCPs.\nWe evaluate the performance of a nearest-neighbor policy (VINN\n(\npari2021surprising\n)\n) on state-based\nTool\n-\nHang\nand find that it achieves a success rate of only\n12\n%\n12\\%\nas shown in\nTable\nÀú\n20\n.\nThis is significantly lower than both flow and regression methods, indicating that the action manifold is not linearly spanned by the expert actions.\nNevertheless, nearest-neighbor can still serve as a proxy for the expert action manifold, as it captures the general trend of actions‚Äîeven though linear combinations of actions in the dataset cannot directly produce the correct action, the expert action manifold should not be too distant.\nTherefore, in this paper, we use nearest-neighbor as a proxy for the linearized expert action manifold rather than directly computing the distance between expert actions in the validation set and predicted actions.\nAction Chunk Size\nSuccess Rate (%)\n1\n0\n8\n4\n4\n16\n12\n12\n32\n2\n2\nTable 20:\nPerformance of k-nearest neighbor policy on state-based\nTool\n-\nHang\ntask. Using the same method as VINN with softmax over k=5 nearest neighbors.\nF.1\nAction Chunk Size Study\nAnother equivalent important factor is the action chunk size.\nFig.\nÀú\n13\nhighlights the importance of action chunk size, where regression with larger action chunk can outperform flow with smaller action chunk size.\nOur ablation also indicates that\nMIP\noutperforms flow with smaller action chunk size and matches the performance of flow with larger action chunk size.\nFigure 13:\nAction chunk size ablation.\nSuccess rate are averaged across 3 seeds, 3 architectures and 5 checkpoints on\nTool\n-\nHang\nand\nTransport\ntasks. Prediction horizon is set to powers of 2 to make sure it is compatible with\nChi-UNet\nArchitecture.\nF.2\nLoss Norm Type Ablation Study\nPrevious work\n(\nkim2024openvla\n)\nshows that\n‚Ñì\n1\n\\ell_{1}\nloss is superior to\n‚Ñì\n2\n\\ell_{2}\nloss for regression policy.\nTo test this hypothesis, we ablate the loss norm type and compare the performance of different methods with\n‚Ñì\n1\n\\ell_{1}\nand\n‚Ñì\n2\n\\ell_{2}\nloss.\nTable\nÀú\n21\nshows the performance comparison of different methods with\n‚Ñì\n1\n\\ell_{1}\nand\n‚Ñì\n2\n\\ell_{2}\nloss, where we find that\n‚Ñì\n1\n\\ell_{1}\nloss generally outperforms\n‚Ñì\n2\n\\ell_{2}\nloss, especially for regression policy.\nHowever, even with\n‚Ñì\n1\n\\ell_{1}\nloss, we still observe that Regression\n<\n<\nMIP\n‚âà\n\\approx\nFlow, highlighting the importance of the stochasticity injection and iterative computation is independent of the loss function.\nArchitecture\nMethod\nTransport (ph)\nTool Hang (ph)\nDiT\nRegression L1\n0.72/0.64\n0.76/0.65\nDiT\nRegression L2\n0.50/0.45\n0.50/0.37\nDiT\nMIP L1\n0.81/0.73\n0.91/0.84\nDiT\nMIP L2\n0.79/0.69\n0.92/0.85\nDiT\nFlow L1\n0.83/0.76\n0.93/0.84\nDiT\nFlow L2\n0.81/0.71\n0.89/0.75\nTransformer\nRegression L1\n0.67/0.57\n0.44/0.33\nTransformer\nRegression L2\n0.65/0.54\n0.31/0.25\nTransformer\nMIP L1\n0.80/0.68\n0.85/0.77\nTransformer\nMIP L2\n0.80/0.69\n0.80/0.72\nTransformer\nFlow L1\n0.77/0.69\n0.81/0.71\nTransformer\nFlow L2\n0.79/0.65\n0.73/0.61\nUNet\nRegression L1\n0.84/0.71\n0.71/0.55\nUNet\nRegression L2\n0.66/0.59\n0.73/0.59\nUNet\nMIP L1\n0.85/0.68\n0.76/0.67\nUNet\nMIP L2\n0.81/0.72\n0.82/0.71\nUNet\nFlow L1\n0.85/0.69\n0.87/0.71\nUNet\nFlow L2\n0.83/0.75\n0.87/0.73\nTable 21:\nComparison of L1 vs L2 norm across different methods and architectures. Report average/best performance across 5 checkpoints with 3 random seeds.\nAppendix G\nDiversity of GCPs and RCPs\nA commonly believed hypothesis is that GCPs can express more diverse behaviors than RCPs by capturing the full distribution of expert actions\n(\nshafiullah2022behavior\n)\n.\nWe evalute different variants of GCPs and RCPs on\nFranka\n-\nKitchen\n, where the expert shows multiple task completion orders.\nAs demonstrated in\nFig.\nÀú\n6\n, GCPs with both stochastic and deterministic sampling show similar task completion order diversity.\nDeterministic policies like regression and\nMIP\n(to be introduced in\nSection\nÀú\n4\n) also demonstrate similar task completion order diversity.\nThis indicates that, given sparse expert demonstrations, both GCPs and RCP learns high-Lipschitz policies to switch between different modes given different observations (corresponding to (b.2) case in\nFigure\nÀú\n4\n). RCPs and GCPs are equally good at learning such behaviors (\nFigure\nÀú\n14\n) which explain why we see similar performance for both policy parametrizations, even on seemingly multi-modal tasks like\nFranka\n-\nKitchen\n.\nFigure 14:\nTask completion order in Kitchen environment with different methods.\nWe plot the count of different task completion orders for different methods to evaluate the diversity of the policies.\nThe x-axis shows the task completion order, where each sub-task is represented by its initials.\nFor each run, we collect 1000 trajectories with the same seed shared by all methods.\nFor flow, we evaluate both stochastic and deterministic modes.\nAppendix H\nTheoretical analysis of GCP‚Äôs expressivity\nH.1\nFormal statement of Theorem\n1\nIn this section, we introduce the notation and definition required for the subsequent proofs and provide the formal statement of\nTheorem\nÀú\n1\nfrom the main text. Throughout, let\n‚à•\n‚ãÖ\n‚à•\n‚àò\n\\|\\cdot\\|_{\\circ}\ndenote any matrix norm satisfying the property\n‚Äñ\nX\n1\n‚Äã\nX\n2\n‚Äñ\n‚àò\n‚â§\n‚Äñ\nX\n1\n‚Äñ\nop\n‚Äã\n‚Äñ\nX\n2\n‚Äñ\n‚àò\n\\|X_{1}X_{2}\\|_{\\circ}\\leq\\|X_{1}\\|_{\\mathrm{op}}\\|X_{2}\\|_{\\circ}\n. In contrast to the notation used in the main text, we define\n(\na\n,\no\n)\ns\n,\nt\n{}_{s,t}(a,o)\nas the solution at time\nt\nt\nof the ODE:\nd\nd\n‚Äã\nt\n‚Äã\na\nt\n=\nb\nt\n‚ãÜ\n‚Äã\n(\na\nt\n‚à£\no\n)\n,\nwith initial condition\n‚Äã\na\ns\n=\na\n.\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}a_{t}=b^{\\star}_{t}(a_{t}\\mid o),\\quad\\text{with initial condition }a_{s}=a.\n(H.1)\nNote that\n(\na\n0\n=\nz\n,\no\n)\n0\n,\n1\n{}_{0,1}(a_{0}=z,o)\ncoincides with the definition of\n(\nz\n,\no\n)\n‚ãÜ\n{}^{\\star}(z,o)\nin the main text. Next, we define the notion of\n-log-concavity\n.\nDefinition H.1\n(-log-concavity)\n.\nA distribution with density\n=\ne\n‚àí\nV\n‚Äã\n(\nx\n)\n\\rho=e^{-V(x)}\nis said to be\n-log-concave\nif\nV\n‚àà\nC\n2\n‚Äã\n(\nR\nd\n)\nV\\in C^{2}(\\mdmathbb{R}^{d})\nand its Hessian satisfies\n‚àá\n2\nV\n‚Äã\n(\nx\n)\n‚Äã\nùêà\n\\nabla^{2}V(x)\\succcurlyeq\\kappa\\mathbf{I}\nfor all\nx\n‚àà\nR\nd\nx\\in\\mdmathbb{R}^{d}\nand some\n>\n0\n\\kappa>0\n.\nWith this notation in place, we now state the formal version of\nTheorem\nÀú\n1\n.\nTheorem 2\n.\nSuppose that\nb\nt\n‚ãÜ\n=\nE\n[\nI\nÀô\nt\n‚à£\nI\nt\n,\no\n]\n,\nwhere\nI\nt\n=\n(\n1\n‚àí\nt\n)\na\n0\n+\nt\na\n1\n,\na\n0\n‚àº\nN\n(\n0\n,\nùêà\n)\n,\na\n1\n‚àº\n,\n1\n\\displaystyle b^{\\star}_{t}=\\mdmathbb{E}[\\dot{I}_{t}\\mid I_{t},o],\\qquad\\text{where }I_{t}=(1-t)a_{0}+ta_{1},\\quad a_{0}\\sim\\mathrm{N}(0,\\mathbf{I}),\\quad a_{1}\\sim{}_{1},\n(H.2)\nwhere\n1\nis -log-concave. Then, we have\n‚à•\n‚àá\no\n(\na\n0\n,\no\n)\n0\n,\nt\n‚à•\n‚àò\n‚â§\n‚à´\n0\nt\n(\n1\n‚àí\nt\n)\n2\n+\nt\n2\n(\n1\n‚àí\ns\n)\n2\n+\ns\n2\n‚ãÖ\n‚à•\n‚àá\no\nb\ns\n‚ãÜ\n(\na\ns\n‚à£\no\n)\n‚à•\n‚àò\nd\ns\n.\n\\displaystyle\\|\\nabla_{o}{}_{0,t}(a_{0},o)\\|_{\\circ}\\leq\\int_{0}^{t}\\sqrt{\\frac{\\kappa(1-t)^{2}+t^{2}}{\\kappa(1-s)^{2}+s^{2}}}\\cdot\\|\\nabla_{o}b^{\\star}_{s}(a_{s}\\mid o)\\|_{\\circ}\\mathrm{d}s.\n(H.3)\nIn particular, for\nt\n=\n1\nt=1\nwe obtain\n‚à•\n‚àá\no\n(\na\n0\n,\no\n)\n0\n,\n1\n‚à•\n‚àò\n‚â§\n1\n+\n‚àí\n1\n‚à´\n0\n1\n‚à•\n‚àá\no\nb\ns\n‚ãÜ\n(\na\ns\n‚à£\no\n)\n‚à•\n‚àò\nd\ns\n.\n\\displaystyle\\|\\nabla_{o}{}_{0,1}(a_{0},o)\\|_{\\circ}\\leq\\sqrt{1+{}^{-1}}\\int_{0}^{1}\\|\\nabla_{o}b^{\\star}_{s}(a_{s}\\mid o)\\|_{\\circ}\\mathrm{d}s.\n(H.4)\nRemark H.1\n.\nTheorem\nÀú\n1\nfollows immediately from the fact that both the operator and the Frobenius norms satisfy\n‚Äñ\nX\n1\n‚Äã\nX\n2\n‚Äñ\n‚àò\n‚â§\n‚Äñ\nX\n1\n‚Äñ\nop\n‚Äã\n‚Äñ\nX\n2\n‚Äñ\n‚àò\n\\|X_{1}X_{2}\\|_{\\circ}\\leq\\|X_{1}\\|_{\\mathrm{op}}\\|X_{2}\\|_{\\circ}\ntogether with the inequality\nEq.\nÀú\nH.4\n.\nH.2\nSupporting lemmas\nWe state the supporting lemmas for proving\nTheorem\nÀú\n2\nbelow and provide their proofs immediately for completeness. As a first step, we analyze the dynamical system satisfied by\n‚àá\no\n(\na\n,\no\n)\ns\n,\nt\n\\nabla_{o}{}_{s,t}(a,o)\n.\nLemma H.1\n.\nDefine\na\nt\n:=\n(\na\n0\n,\no\n)\n0\n,\nt\na_{t}:={}_{0,t}(a_{0},o)\nwhere\na\n0\na_{0}\nis the initial condition, and define the matrices\nM\nt\n:=\n‚àá\no\n(\na\n0\n,\no\n)\n0\n,\nt\n,\nA\nt\n:=\n(\n‚àá\na\nb\nt\n‚ãÜ\n)\n(\na\nt\n‚à£\no\n)\n,\nE\nt\n:=\n(\n‚àá\no\nb\nt\n‚ãÜ\n)\n(\na\nt\n‚à£\no\n)\n\\displaystyle M_{t}:=\\nabla_{o}{}_{0,t}(a_{0},o),\\quad A_{t}:=(\\nabla_{a}b^{\\star}_{t})(a_{t}\\mid o),\\quad E_{t}:=(\\nabla_{o}b^{\\star}_{t})(a_{t}\\mid o)\n(H.5)\nThen,\nd\nd\n‚Äã\nt\n‚Äã\nM\nt\n=\nA\nt\n‚Äã\nM\nt\n+\nE\nt\n,\nM\n0\n=\n0\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}M_{t}=A_{t}M_{t}+E_{t},\\quad M_{0}=0\n(H.6)\nProof.\nSince\n(\na\n0\n,\no\n)\n0\n,\n0\n=\na\n0\n{}_{0,0}(a_{0},o)=a_{0}\n,\nM\n0\n=\n0\nM_{0}=0\n. Moreover,\nd\nd\n‚Äã\nt\n‚àá\no\n(\na\n0\n,\no\n)\n0\n,\nt\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\nabla_{o}{}_{0,t}(a_{0},o)\n=\n‚àá\no\nd\nd\n‚Äã\nt\n(\na\n0\n,\no\n)\n0\n,\nt\n=\n‚àá\no\n(\nb\nt\n‚ãÜ\n(\n(\na\n0\n,\no\n)\n0\n,\nt\n‚à£\no\n)\n)\n\\displaystyle=\\nabla_{o}\\frac{\\mathrm{d}}{\\mathrm{d}t}{}_{0,t}(a_{0},o)=\\nabla_{o}(b^{\\star}_{t}({}_{0,t}(a_{0},o)\\mid o))\n(H.7)\n=\n(\n‚àá\na\nb\nt\n‚ãÜ\n)\n(\na\nt\n‚à£\no\n)\n‚ãÖ\n‚àá\no\n(\na\n0\n,\no\n)\n0\n,\nt\n+\n(\n‚àá\no\nb\nt\n‚ãÜ\n)\n(\na\nt\n‚à£\no\n)\n\\displaystyle=(\\nabla_{a}b^{\\star}_{t})(a_{t}\\mid o)\\cdot\\nabla_{o}{}_{0,t}(a_{0},o)+(\\nabla_{o}b^{\\star}_{t})(a_{t}\\mid o)\n(H.8)\n‚àé\nNote that, from the previous lemma, we may introduce\ns,t\nas the solution to the matrix ODE\nd\nd\n‚Äã\nt\n=\ns\n,\nt\nA\nt\n,\ns\n,\nt\n=\ns\n,\ns\nùêà\n.\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}{}_{s,t}=A_{t}{}_{s,t},\\quad{}_{s,s}=\\mathbf{I}.\n(H.9)\nMoreover, it follows that\n=\ns\n,\nt\n‚àá\na\n(\na\ns\n,\no\n)\ns\n,\nt\n.\n\\displaystyle{}_{s,t}=\\nabla_{a}{}_{s,t}(a_{s},o).\n(H.10)\nWe are now ready to state the relation between\nM\nt\nM_{t}\nand\ns,t\n.\nLemma H.2\n.\nM\nt\n=\n‚à´\n0\nt\nE\ns\ns\n,\nt\n‚Äã\nd\ns\n.\n\\displaystyle M_{t}=\\int_{0}^{t}{}_{s,t}E_{s}\\mathrm{d}s.\n(H.11)\nProof.\nUsing\nd\nd\n‚Äã\nt\n=\n0\n,\nt\n‚àí\n1\n‚àí\nA\nt\n0\n,\nt\n‚àí\n1\n\\frac{\\mathrm{d}}{\\mathrm{d}t}{}_{0,t}^{-1}=-{}_{0,t}^{-1}A_{t}\nand we consider the time derivative of\nM\nt\n0\n,\nt\n‚àí\n1\n{}_{0,t}^{-1}M_{t}\n:\nd\nd\n‚Äã\nt\n‚Äã\n(\nM\nt\n0\n,\nt\n‚àí\n1\n)\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}({}_{0,t}^{-1}M_{t})\n=\n(\nd\nd\n‚Äã\nt\n)\n0\n,\nt\n‚àí\n1\nM\nt\n+\n(\nd\nd\n‚Äã\nt\nM\nt\n)\n0\n,\nt\n‚àí\n1\n\\displaystyle=(\\frac{\\mathrm{d}}{\\mathrm{d}t}{}_{0,t}^{-1})M_{t}+{}_{0,t}^{-1}(\\frac{\\mathrm{d}}{\\mathrm{d}t}M_{t})\n(H.12)\n=\n‚àí\nA\nt\n0\n,\nt\n‚àí\n1\n‚Äã\nM\nt\n+\nA\nt\n0\n,\nt\n‚àí\n1\n‚Äã\nM\nt\n+\nE\nt\n0\n,\nt\n‚àí\n1\n\\displaystyle=-{}_{0,t}^{-1}A_{t}M_{t}+{}_{0,t}^{-1}A_{t}M_{t}+{}_{0,t}^{-1}E_{t}\n(H.13)\n=\nE\nt\n0\n,\nt\n‚àí\n1\n.\n\\displaystyle={}_{0,t}^{-1}E_{t}.\n(H.14)\nNote that\n0,t\nis invertible by uniqueness of the ODE solution in\nEq.\nÀú\nH.9\n. Integrating both sides with respect to\nt\nt\ngives\nM\nt\n0\n,\nt\n‚àí\n1\n=\n‚à´\n0\nt\nE\ns\n0\n,\ns\n‚àí\n1\n‚Äã\nd\ns\n.\n\\displaystyle{}_{0,t}^{-1}M_{t}=\\int_{0}^{t}{}_{0,s}^{-1}E_{s}\\mathrm{d}s.\n(H.15)\nHence, we have\nM\nt\n=\n‚à´\n0\nt\n0\n,\nt\nE\ns\n0\n,\ns\n‚àí\n1\n‚Äã\nd\ns\n.\n\\displaystyle M_{t}={}_{0,t}\\int_{0}^{t}{}_{0,s}^{-1}E_{s}\\mathrm{d}s.\n(H.16)\nNote that\n=\n0\n,\ns\n‚àí\n1\ns\n,\n0\n{}_{0,s}^{-1}={}_{s,0}\nand\n‚ãÖ\n0\n,\nt\n=\ns\n,\n0\ns\n,\nt\n{}_{0,t}\\cdot{}_{s,0}={}_{s,t}\n, we obtain\nM\nt\n=\n‚à´\n0\nt\nE\ns\ns\n,\nt\n‚Äã\nd\ns\n.\n\\displaystyle M_{t}=\\int_{0}^{t}{}_{s,t}E_{s}\\mathrm{d}s.\n(H.17)\n‚àé\nAn immediate application of the triangle inequality and the property of\n‚à•\n‚ãÖ\n‚à•\n‚àò\n\\|\\cdot\\|_{\\circ}\nyields\n‚à•\nM\nt\n‚à•\n‚àò\n‚â§\n‚à´\n0\nt\n‚à•\n‚à•\nop\ns\n,\nt\n‚à•\nE\ns\n‚à•\n‚àò\nd\ns\n.\n\\displaystyle\\|M_{t}\\|_{\\circ}\\leq\\int_{0}^{t}\\|{}_{s,t}\\|_{\\mathrm{op}}\\|E_{s}\\|_{\\circ}\\mathrm{d}s.\n(H.18)\nMoreover,\n‚à•\n‚à•\nop\ns\n,\nt\n\\|{}_{s,t}\\|_{\\mathrm{op}}\nadmits the bound:\nLemma H.3\n.\n‚à•\n‚à•\nop\ns\n,\nt\n‚â§\nexp\n(\n‚à´\ns\nt\n‚à•\nA\ns\n‚Ä≤\n‚à•\nop\nd\ns\n‚Ä≤\n)\n.\n\\displaystyle\\|{}_{s,t}\\|_{\\mathrm{op}}\\leq\\exp\\left(\\int_{s}^{t}\\|A_{s^{\\prime}}\\|_{\\mathrm{op}}\\mathrm{d}s^{\\prime}\\right).\n(H.19)\nProof.\nDefine\nf\n(\ns\n,\nt\n)\n=\ns\n,\nt\nf(s,t)={}_{s,t}\\omega\n. We have\nd\nd\n‚Äã\nt\n‚Äã\n‚Äñ\nf\n‚Äã\n(\ns\n,\nt\n)\n‚Äñ\n2\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\|f(s,t)\\|_{2}\n=\n1\n‚Äñ\nf\n‚Äã\n(\ns\n,\nt\n)\n‚Äñ\n2\n‚Äã\nf\n‚Äã\n(\ns\n,\nt\n)\n‚ä§\n‚Äã\nd\nd\n‚Äã\nt\n‚Äã\nf\n‚Äã\n(\ns\n,\nt\n)\n\\displaystyle=\\frac{1}{\\|f(s,t)\\|_{2}}f(s,t)^{\\top}\\frac{\\mathrm{d}}{\\mathrm{d}t}f(s,t)\n(H.20)\n=\n1\n‚Äñ\nf\n‚Äã\n(\ns\n,\nt\n)\n‚Äñ\n2\nA\nt\n‚ä§\ns\n,\nt\n‚ä§\ns\n,\nt\n\\displaystyle=\\frac{1}{\\|f(s,t)\\|_{2}}{}^{\\top}{}_{s,t}^{\\top}A_{t}{}_{s,t}\\omega\n(H.21)\n‚â§\n‚Äñ\nA\nt\n‚Äñ\nop\n‚Äã\n‚Äñ\nf\n‚Äã\n(\ns\n,\nt\n)\n‚Äñ\n2\n.\n\\displaystyle\\leq\\|A_{t}\\|_{\\mathrm{op}}\\|f(s,t)\\|_{2}.\n(H.22)\nBy Gronwall‚Äôs theorem and\n‚à•\nf\n(\ns\n,\ns\n)\n‚à•\n2\n=\n‚à•\n‚à•\n2\n\\|f(s,s)\\|_{2}=\\|\\omega\\|_{2}\n, we obtain\n‚à•\nf\n(\ns\n,\nt\n)\n‚à•\n2\n‚â§\n‚à•\n‚à•\n2\nexp\n(\n‚à´\ns\nt\n‚à•\nA\ns\n‚Ä≤\n‚à•\nop\nd\ns\n‚Ä≤\n)\n.\n\\displaystyle\\|f(s,t)\\|_{2}\\leq\\|\\omega\\|_{2}\\exp(\\int_{s}^{t}\\|A_{s^{\\prime}}\\|_{\\mathrm{op}}\\mathrm{d}s^{\\prime}).\n(H.23)\n‚àé\nTo bound\nexp\n‚Å°\n(\n‚à´\ns\nt\n‚Äñ\nA\ns\n‚Ä≤\n‚Äñ\nop\n‚Äã\nd\ns\n‚Ä≤\n)\n\\exp\\left(\\int_{s}^{t}\\|A_{s^{\\prime}}\\|_{\\mathrm{op}}\\mathrm{d}s^{\\prime}\\right)\n, we use the following result from\n(\ndaniels2025contractivity\n)\n, included here for completeness.\nTheorem 3\n(Restated; Theorem 6 in\n(\ndaniels2025contractivity\n)\n)\n.\nSuppose\n‚àº\n0\nN\n(\n0\n,\nùêà\n)\n{}_{0}\\sim\\mathrm{N}(0,\\mathbf{I})\nand\n1\nis a -log-concave distribution with\n>\n0\n\\kappa>0\n. Define\nI\nt\n=\nX\n0\nt\n+\nX\n1\nt\n,\nX\n0\n‚àº\n,\n0\nX\n1\n‚àº\n,\n1\n\\displaystyle I_{t}={}_{t}X_{0}+{}_{t}X_{1},\\qquad X_{0}\\sim{}_{0},\\quad X_{1}\\sim{}_{1},\n(H.24)\nand let\nv\nt\n‚Äã\n(\nx\n)\nv_{t}(x)\ndenote the corresponding flow field.\nThen,\n‚àá\nx\nv\nt\n‚Äã\n(\nx\n)\n‚Äã\nt\nÀô\nt\n+\nt\nÀô\nt\n+\nt\n2\nt\n2\n‚Äã\nùêà\n.\n\\displaystyle\\nabla_{x}v_{t}(x)\\preccurlyeq\\frac{\\kappa{}_{t}\\dot{{}_{t}}+{}_{t}\\dot{{}_{t}}}{\\kappa{}_{t}^{2}+{}_{t}^{2}}\\mathbf{I}.\n(H.25)\nWith the result, we can bound\nexp\n‚Å°\n(\n‚à´\ns\nt\n‚Äñ\nA\ns\n‚Ä≤\n‚Äñ\nop\n‚Äã\nd\ns\n‚Ä≤\n)\n\\exp\\left(\\int_{s}^{t}\\|A_{s^{\\prime}}\\|_{\\mathrm{op}}\\mathrm{d}s^{\\prime}\\right)\nas follows.\nLemma H.4\n.\nb\nt\n‚ãÜ\n=\nE\n[\nI\nÀô\nt\n‚à£\nI\nt\n,\no\n]\n,\nwhere\nI\nt\n=\n(\n1\n‚àí\nt\n)\na\n0\n+\nt\na\n1\n,\na\n0\n‚àº\nN\n(\n0\n,\nùêà\n)\n,\na\n1\n‚àº\n,\n1\n\\displaystyle b^{\\star}_{t}=\\mdmathbb{E}[\\dot{I}_{t}\\mid I_{t},o],\\qquad\\text{where }I_{t}=(1-t)a_{0}+ta_{1},\\quad a_{0}\\sim\\mathrm{N}(0,\\mathbf{I}),\\quad a_{1}\\sim{}_{1},\n(H.26)\nwhere\n1\nis -log-concave. Then, we have\n‚à´\ns\nt\n‚à•\n‚àá\nx\nb\ns\n‚Ä≤\n‚ãÜ\n(\na\ns\n‚Ä≤\n‚à£\no\n)\n‚à•\nop\nd\ns\n‚Ä≤\n‚â§\nlog\n(\n1\n‚àí\nt\n)\n2\n+\nt\n2\n(\n1\n‚àí\ns\n)\n2\n+\ns\n2\n\\displaystyle\\int_{s}^{t}\\|\\nabla_{x}b^{\\star}_{s^{\\prime}}(a_{s^{\\prime}}\\mid o)\\|_{\\mathrm{op}}\\mathrm{d}s^{\\prime}\\leq\\log\\sqrt{\\frac{\\kappa(1-t)^{2}+t^{2}}{\\kappa(1-s)^{2}+s^{2}}}\n(H.27)\nProof.\nBy leveraging\nTheorem\nÀú\n3\nfor each condition\no\no\n, we have\n‚àá\na\nb\ns\n‚Ä≤\n‚ãÜ\n‚Äã\n(\na\ns\n‚Ä≤\n‚à£\no\n)\n‚Äã\ns\n‚Ä≤\nÀô\ns\n‚Ä≤\n+\ns\n‚Ä≤\nÀô\ns\n‚Ä≤\n+\ns\n‚Ä≤\n2\ns\n‚Ä≤\n2\n‚Äã\nùêà\n,\n\\displaystyle\\nabla_{a}b^{\\star}_{s^{\\prime}}(a_{s^{\\prime}}\\mid o)\\preccurlyeq\\frac{\\kappa{}_{s^{\\prime}}\\dot{{}_{s^{\\prime}}}+{}_{s^{\\prime}}\\dot{{}_{s^{\\prime}}}}{\\kappa{}_{s^{\\prime}}^{2}+{}_{s^{\\prime}}^{2}}\\mathbf{I},\n(H.28)\nthen we have\n‚à•\n‚àá\na\nb\ns\n‚Ä≤\n‚ãÜ\n(\na\ns\n‚Ä≤\n‚à£\no\n)\n‚à•\nop\n‚â§\ns\n‚Ä≤\nÀô\ns\n‚Ä≤\n+\ns\n‚Ä≤\nÀô\ns\n‚Ä≤\n+\ns\n‚Ä≤\n2\ns\n‚Ä≤\n2\n.\n\\displaystyle\\|\\nabla_{a}b^{\\star}_{s^{\\prime}}(a_{s^{\\prime}}\\mid o)\\|_{\\mathrm{op}}\\leq\\frac{\\kappa{}_{s^{\\prime}}\\dot{{}_{s^{\\prime}}}+{}_{s^{\\prime}}\\dot{{}_{s^{\\prime}}}}{\\kappa{}_{s^{\\prime}}^{2}+{}_{s^{\\prime}}^{2}}.\n(H.29)\nIntegrating both sides, we obtain\n‚à´\ns\nt\n‚à•\n‚àá\na\nb\ns\n‚Ä≤\n‚ãÜ\n(\na\ns\n‚Ä≤\n‚à£\no\n)\n‚à•\nop\nd\ns\n‚Ä≤\n\\displaystyle\\int_{s}^{t}\\|\\nabla_{a}b^{\\star}_{s^{\\prime}}(a_{s^{\\prime}}\\mid o)\\|_{\\mathrm{op}}\\mathrm{d}s^{\\prime}\n‚â§\n‚à´\ns\nt\ns\n‚Ä≤\nÀô\ns\n‚Ä≤\n+\n‚Ä≤\ns\nÀô\ns\n‚Ä≤\n+\ns\n‚Ä≤\n2\ns\n‚Ä≤\n2\n‚Äã\nd\ns\n‚Ä≤\n\\displaystyle\\leq\\int_{s}^{t}\\frac{\\kappa{}_{s^{\\prime}}\\dot{{}_{s^{\\prime}}}+{}_{s^{\\prime}}\\dot{{}_{s}^{\\prime}}}{\\kappa{}_{s^{\\prime}}^{2}+{}_{s^{\\prime}}^{2}}\\mathrm{d}s^{\\prime}\n(H.30)\n=\n1\n2\nlog\n(\n+\ns\n‚Ä≤\n2\n)\ns\n‚Ä≤\n2\n|\ns\nt\n\\displaystyle=\\frac{1}{2}\\log(\\kappa{}_{s^{\\prime}}^{2}+{}_{s^{\\prime}}^{2})\\Big|_{s}^{t}\n(H.31)\n=\nlog\n‚Å°\n+\nt\n2\nt\n2\n+\ns\n2\ns\n2\n.\n\\displaystyle=\\log\\sqrt{\\frac{\\kappa{}_{t}^{2}+{}_{t}^{2}}{\\kappa{}_{s}^{2}+{}_{s}^{2}}}.\n(H.32)\nBy substitute\n=\nt\n1\n‚àí\nt\n{}_{t}=1-t\nand\n=\nt\nt\n{}_{t}=t\n, we have\n‚à´\ns\nt\n‚à•\n‚àá\na\nb\ns\n‚Ä≤\n‚ãÜ\n(\na\ns\n‚Ä≤\n‚à£\no\n)\n‚à•\nop\nd\ns\n‚Ä≤\n‚â§\nlog\n(\n1\n‚àí\nt\n)\n2\n+\nt\n2\n(\n1\n‚àí\ns\n)\n2\n+\ns\n2\n.\n\\displaystyle\\int_{s}^{t}\\|\\nabla_{a}b^{\\star}_{s^{\\prime}}(a_{s^{\\prime}}\\mid o)\\|_{\\mathrm{op}}\\mathrm{d}s^{\\prime}\\leq\\log\\sqrt{\\frac{\\kappa(1-t)^{2}+t^{2}}{\\kappa(1-s)^{2}+s^{2}}}.\n(H.33)\n‚àé\nWith the preceding components in place, we now establish\nTheorem\nÀú\n2\n.\nH.3\nProof of Theorem\n2\nBy combining\nEq.\nÀú\nH.18\n,\nLemma\nÀú\nH.3\n, and\nLemma\nÀú\nH.4\n, we have\n‚à•\n‚àá\no\n(\na\n0\n,\no\n)\n0\n,\nt\n‚à•\n‚àò\n‚â§\n‚à´\n0\nt\n(\n1\n‚àí\nt\n)\n2\n+\nt\n2\n(\n1\n‚àí\ns\n)\n2\n+\ns\n2\n‚ãÖ\n‚à•\n‚àá\no\nb\ns\n‚ãÜ\n(\na\ns\n‚à£\no\n)\n‚à•\n‚àò\nd\ns\n.\n\\displaystyle\\|\\nabla_{o}{}_{0,t}(a_{0},o)\\|_{\\circ}\\leq\\int_{0}^{t}\\sqrt{\\frac{\\kappa(1-t)^{2}+t^{2}}{\\kappa(1-s)^{2}+s^{2}}}\\cdot\\|\\nabla_{o}b^{\\star}_{s}(a_{s}\\mid o)\\|_{\\circ}\\mathrm{d}s.\n(H.34)\nFor\nt\n=\n1\nt=1\n, the function\ns\n‚Ü¶\n(\n1\n‚àí\ns\n)\n2\n+\ns\n2\ns\\mapsto\\kappa(1-s)^{2}+s^{2}\nattains its minimum at\ns\n=\n+\n1\ns=\\tfrac{\\kappa}{\\kappa+1}\n. Applying Holder‚Äôs inequality then yields\n‚à•\n‚àá\no\n(\na\n0\n,\no\n)\n0\n,\n1\n‚à•\n‚àò\n\\displaystyle\\|\\nabla_{o}{}_{0,1}(a_{0},o)\\|_{\\circ}\n‚â§\n‚à´\n0\n1\n1\n(\n1\n‚àí\ns\n)\n2\n+\ns\n2\n‚ãÖ\n‚à•\n‚àá\no\nb\ns\n‚ãÜ\n(\na\ns\n‚à£\no\n)\n‚à•\n‚àò\nd\ns\n\\displaystyle\\leq\\int_{0}^{1}\\sqrt{\\frac{1}{\\kappa(1-s)^{2}+s^{2}}}\\cdot\\|\\nabla_{o}b^{\\star}_{s}(a_{s}\\mid o)\\|_{\\circ}\\mathrm{d}s\n(H.35)\n‚â§\nmax\ns\n‚àà\n[\n0\n,\n1\n]\n(\n1\n(\n1\n‚àí\ns\n)\n2\n+\ns\n2\n)\n‚ãÖ\n‚à´\n0\n1\n‚à•\n‚àá\no\nb\ns\n‚ãÜ\n(\na\ns\n‚à£\no\n)\n‚à•\n‚àò\nd\ns\n\\displaystyle\\leq\\max_{s\\in[0,1]}\\left(\\sqrt{\\frac{1}{\\kappa(1-s)^{2}+s^{2}}}\\right)\\cdot\\int_{0}^{1}\\|\\nabla_{o}b^{\\star}_{s}(a_{s}\\mid o)\\|_{\\circ}\\mathrm{d}s\n(H.36)\n=\n1\n+\n‚àí\n1\n‚à´\n0\n1\n‚à•\n‚àá\no\nb\ns\n‚ãÜ\n(\na\ns\n‚à£\no\n)\n‚à•\n‚àò\nd\ns\n.\n\\displaystyle=\\sqrt{1+{}^{-1}}\\int_{0}^{1}\\|\\nabla_{o}b^{\\star}_{s}(a_{s}\\mid o)\\|_{\\circ}\\mathrm{d}s.\n(H.37)\nAppendix I\nRegularization does not account for manifold adherence\nIn this section, we analyze a linear, population-level surrogate for\nMIP\nto test whether\nimplicit regularization\n, instantiated via ridge regression and a two-step\nMIP\n-like iteration, can explain the observed manifold adherence.\nWe mimic the two passes of\nMIP\nwith two ridge-regularized linear regressions: (i) a regression where the ridge penalty is applied to the observation-to-action map, and (ii) a regression where the ridge penalty is applied to the action-to-action map.\nWe then compose the two fitted maps to obtain the two-stage inference used by\nMIP\n.\nAs shown below, it instead yields smooth spectral shrinkage and does not make the manifold absorbing.\nThroughout we work in expectation (population covariances), so that conclusions reflect model structure rather than finite-sample effects. We assume independence of\no\no\n,\nz\nz\n, and the additive noise , and that the inverses we write exist (otherwise interpret as pseudoinverses on the relevant supports).\nSetup.\nObservations\no\n‚àà\nR\nd\no\\in\\mdmathbb{R}^{d}\nand actions\na\n‚àà\nR\nd\na\\in\\mdmathbb{R}^{d}\nfollow the linear model\na\n=\no\n‚àó\n+\n,\n‚àº\nN\n(\n0\n,\n=\nùêà\n2\n)\n,\n\\displaystyle a={}^{*}o+\\boldsymbol{\\eta},\\qquad\\boldsymbol{\\eta}\\sim\\mathrm{N}(0,={}^{2}\\mathbf{I}),\nwith\n‚üÇ\no\n\\boldsymbol{\\eta}\\perp o\n.\nLet\nz\n‚àº\nN\n(\n0\n,\n)\nz\nz\\sim\\mathrm{N}(0,{}_{z})\nbe an auxiliary signal, independent of\n(\no\n,\na\n,\n)\n(o,a,\\eta)\n, and define\nw\n:=\nc\n1\n‚Äã\na\n+\nc\n2\n‚Äã\nz\nw:=c_{1}a+c_{2}z\n.\nWe consider linear predictors\na\n^\n=\nB\n‚Äã\no\n+\nC\n‚Äã\nw\n\\hat{a}=Bo+Cw\nwith ridge regularization applied either to\nB\nB\n(\nSection\nÀú\nI.1\n) or to\nC\nC\n(\nSection\nÀú\nI.2\n). This reflects the use of both the observation\no\no\n, and the action\na\na\n, in prediction.\nI.1\nRidge regression for observation-to-action mapping (penalty on\nB\nB\n)\nWe solve\nmin\nB\n,\nC\n‚Å°\nE\n‚Äã\n‚Äñ\nB\n‚Äã\no\n+\nC\n‚Äã\nw\n‚àí\na\n‚Äñ\n2\n+\n‚Äñ\nB\n‚Äñ\nF\n2\n.\n\\displaystyle\\min_{B,C}\\mdmathbb{E}\\big\\|Bo+Cw-a\\big\\|^{2}+\\lambda\\|B\\|_{F}^{2}.\nDefine\nX\n:=\n[\no\nw\n]\n,\n:=\n[\nB\nC\n]\n,\n\\displaystyle X:=\\begin{bmatrix}o\\\\\nw\\end{bmatrix},\\qquad\\Psi:=\\begin{bmatrix}B&C\\end{bmatrix},\nand, at the population level, the second-moment blocks\n11\n:=\nE\n[\no\n‚äó\n2\n]\n=\n,\no\n\\displaystyle:=\\mdmathbb{E}[o^{\\otimes 2}]={}_{o},\n12\n:=\nE\n[\no\nw\n‚ä§\n]\n=\nE\n[\no\n(\nc\n1\na\n+\nc\n2\nz\n)\n‚ä§\n]\n=\nc\n1\n,\no\n‚àó\n‚ä§\n\\displaystyle:=\\mdmathbb{E}[ow^{\\top}]=\\mdmathbb{E}[o(c_{1}a+c_{2}z)^{\\top}]=c_{1}{}_{o}{}^{*\\top},\n21\n:=\n,\n12\n‚ä§\n\\displaystyle:={}_{12}^{\\top},\n22\n:=\nE\n[\nw\n‚äó\n2\n]\n=\nc\n1\n2\n(\n+\no\n‚àó\n‚àó\n‚ä§\n)\n+\nc\n2\n2\n.\nz\n\\displaystyle:=\\mdmathbb{E}[w^{\\otimes 2}]=c_{1}^{2}({}^{*}{}_{o}{}^{*\\top}+)+c_{2}^{2}{}_{z}.\na1\n:=\nE\n[\na\no\n‚ä§\n]\n=\n,\n‚àó\no\n\\displaystyle:=\\mdmathbb{E}[ao^{\\top}]={}^{*}{}_{o},\na2\n:=\nE\n[\na\nw\n‚ä§\n]\n=\nc\n1\n(\n+\no\n‚àó\n‚àó\n‚ä§\n)\n,\n\\displaystyle:=\\mdmathbb{E}[aw^{\\top}]=c_{1}({}^{*}{}_{o}{}^{*\\top}+),\nwhere, for any vector\nx\nx\n, we write\nx\n‚äó\n2\n:=\nx\n‚Äã\nx\n‚ä§\nx^{\\otimes 2}:=xx^{\\top}\n.\nThe objective can be written in trace form as\n‚Ñí\n\\displaystyle\\mathcal{L}\n=\nE\n‚Äã\n[\n(\na\n‚àí\nX\n)\n‚ä§\n‚Äã\n(\na\n‚àí\nX\n)\n]\n+\n‚Äñ\nB\n‚Äñ\nF\n2\n\\displaystyle=\\mdmathbb{E}[(a-\\Psi X)^{\\top}(a-\\Psi X)]+\\lambda\\|B\\|_{F}^{2}\n=\ntr\n‚Äã\n(\nE\n‚Äã\n[\n(\na\n‚àí\nX\n)\n‚Äã\n(\na\n‚àí\nX\n)\n‚ä§\n]\n)\n+\ntr\n‚Äã\n(\nB\n‚Äã\nB\n‚ä§\n)\n\\displaystyle=\\text{tr}(\\mdmathbb{E}[(a-\\Psi X)(a-\\Psi X)^{\\top}])+\\lambda\\text{tr}(BB^{\\top})\nDropping terms that are constant in\n(\nB\n,\nC\n)\n(B,C)\n, let\n:=\nX\nE\n[\nX\nX\n‚ä§\n]\n=\n[\n11\n12\n21\n22\n]\n,\n:=\na\n‚Äã\nX\nE\n[\na\nX\n‚ä§\n]\n=\n[\na\n‚Äã\n1\na\n‚Äã\n2\n]\n.\n\\displaystyle{}_{X}:=\\mdmathbb{E}[XX^{\\top}]=\\begin{bmatrix}{}_{11}&{}_{12}\\\\\n{}_{21}&{}_{22}\\end{bmatrix},\\qquad{}_{aX}:=\\mdmathbb{E}[aX^{\\top}]=\\begin{bmatrix}{}_{a1}&{}_{a2}\\end{bmatrix}.\nThen\n‚Ñí\n=\n‚àí\n2\ntr\n(\n)\na\n‚Äã\nX\n‚ä§\n+\ntr\n(\n)\nX\n‚ä§\n+\ntr\n(\nB\nB\n‚ä§\n)\n.\n\\displaystyle\\mathcal{L}=-2\\text{ tr}(\\Psi{}_{aX}^{\\top})+\\text{tr}(\\Psi{}_{X}{}^{\\top})+\\lambda\\text{ tr}(BB^{\\top}).\nDifferentiating gives\n‚àá\nB\n‚Ñí\n\\displaystyle\\nabla_{B}\\mathcal{L}\n=\n‚àí\n2\n+\na\n‚Äã\n1\n2\nB\n+\n11\n2\nC\n+\n21\n2\nB\n,\n\\displaystyle=-2{}_{a1}+2B{}_{11}+2C{}_{21}+\\lambda 2B,\n‚àá\nC\n‚Ñí\n\\displaystyle\\nabla_{C}\\mathcal{L}\n=\n‚àí\n2\n+\na\n‚Äã\n2\n2\nB\n+\n12\n2\nC\n.\n22\n\\displaystyle=-2{}_{a2}+2B{}_{12}+2C{}_{22}.\nSetting the gradients to zero yields the normal equations:\nB\n(\n+\n11\nùêà\n)\n\\displaystyle B({}_{11}+\\lambda\\mathbf{I})\n+\nC\n=\n21\na\n‚Äã\n1\n\\displaystyle+C{}_{21}={}_{a1}\nB\n12\n\\displaystyle B{}_{12}\n+\nC\n=\n22\n.\na\n‚Äã\n2\n\\displaystyle+C{}_{22}={}_{a2}.\nSolving the linear system (e.g., by block elimination) yields\nB\n\\displaystyle B\n=\n(\n‚àí\na\n‚Äã\n1\n21\n‚àí\n1\n22\n)\na\n‚Äã\n2\n‚èü\n(i)\n‚Äã\n[\n(\n+\n11\nùêà\n)\n‚àí\n21\n‚àí\n1\n22\n]\n12\n‚àí\n1\n‚èü\n(ii)\n,\n\\displaystyle=\\underbrace{({}_{a1}{}_{21}^{-1}{}_{22}-{}_{a2})}_{\\text{(i)}}\\underbrace{[({}_{11}+\\lambda\\mathbf{I}){}_{21}^{-1}{}_{22}-{}_{12}]^{-1}}_{\\text{(ii)}},\nC\n\\displaystyle C\n=\n[\n‚àí\na\n‚Äã\n1\nB\n(\n+\n11\nùêà\n)\n]\n.\n21\n‚àí\n1\n\\displaystyle=[{}_{a1}-B({}_{11}+\\lambda\\mathbf{I})]{}_{21}^{-1}.\nUsing\n=\n21\nc\n1\no\n‚àó\n{}_{21}=c_{1}{}^{*}{}_{o}\n, we have\n=\n21\n‚àí\n1\n1\nc\n1\n(\n)\n‚àó\no\n‚àí\n1\n{}_{21}^{-1}=\\frac{1}{c_{1}}({}^{*}{}_{o})^{-1}\n. For (i),\n(i)\n=\n‚àí\na\n‚Äã\n1\n21\n‚àí\n1\n22\na\n‚Äã\n2\n\\displaystyle={}_{a1}{}_{21}^{-1}{}_{22}-{}_{a2}\n=\n1\nc\n1\n‚àó\no\n(\n)\n‚àó\no\n‚àí\n1\n[\nc\n1\n2\n(\n+\no\n‚àó\n‚àó\n‚ä§\n)\n+\nc\n2\n2\n]\nz\n‚àí\nc\n1\n(\n+\no\n‚àó\n‚àó\n‚ä§\n)\n\\displaystyle={}^{*}{}_{o}\\frac{1}{c_{1}}({}^{*}{}_{o})^{-1}[c_{1}^{2}({}^{*}{}_{o}{}^{*\\top}+)+c_{2}^{2}{}_{z}]-c_{1}({}^{*}{}_{o}{}^{*\\top}+)\n=\nc\n1\n(\n+\no\n‚àó\n‚àó\n‚ä§\n)\n+\nc\n2\n2\nc\n1\n‚àí\nz\nc\n1\n(\n+\no\n‚àó\n‚àó\n‚ä§\n)\n\\displaystyle=c_{1}({}^{*}{}_{o}{}^{*\\top}+)+\\frac{c_{2}^{2}}{c_{1}}{}_{z}-c_{1}({}^{*}{}_{o}{}^{*\\top}+)\n=\nc\n2\n2\nc\n1\n.\nz\n\\displaystyle=\\frac{c_{2}^{2}}{c_{1}}{}_{z}.\nFor (ii),\n(ii)\n=\n[\n(\n+\n11\nùêà\n)\n‚àí\n21\n‚àí\n1\n22\n]\n12\n‚àí\n1\n\\displaystyle=[({}_{11}+\\lambda\\mathbf{I}){}_{21}^{-1}{}_{22}-{}_{12}]^{-1}\n=\n[\n(\n+\no\nùêà\n)\n1\nc\n1\n(\n)\n‚àó\no\n‚àí\n1\n[\nc\n1\n2\n(\n+\no\n‚àó\n‚àó\n‚ä§\n)\n+\nc\n2\n2\n]\nz\n‚àí\nc\n1\n(\n)\n‚àó\n‚ä§\no\n]\n‚àí\n1\n\\displaystyle=[({}_{o}+\\lambda\\mathbf{I})\\frac{1}{c_{1}}({}^{*}{}_{o})^{-1}[c_{1}^{2}({}^{*}{}_{o}{}^{*\\top}+)+c_{2}^{2}{}_{z}]-c_{1}{}_{o}({}^{*})^{\\top}]^{-1}\n=\n[\n(\n+\no\nùêà\n)\n[\nc\n1\n+\n‚àó\n‚ä§\nc\n1\n(\n)\n‚àó\no\n‚àí\n1\n+\nc\n2\n2\nc\n1\n(\n)\n‚àó\no\n‚àí\n1\n]\nz\n‚àí\nc\n1\n(\n)\n‚àó\n‚ä§\no\n]\n‚àí\n1\n\\displaystyle=[({}_{o}+\\lambda\\mathbf{I})[c_{1}{}^{*\\top}+c_{1}({}^{*}{}_{o})^{-1}+\\frac{c_{2}^{2}}{c_{1}}({}^{*}{}_{o})^{-1}{}_{z}]-c_{1}{}_{o}({}^{*})^{\\top}]^{-1}\n=\n[\nc\n1\n(\n)\n‚àó\n‚ä§\n+\n(\n+\no\nùêà\n)\n(\nc\n1\n(\n)\n‚àó\no\n‚àí\n1\n+\nc\n2\n2\nc\n1\n(\n)\n‚àó\no\n‚àí\n1\n)\nz\n]\n‚àí\n1\n\\displaystyle=[\\lambda c_{1}({}^{*})^{\\top}+({}_{o}+\\lambda\\mathbf{I})(c_{1}({}^{*}{}_{o})^{-1}+\\frac{c_{2}^{2}}{c_{1}}({}^{*}{}_{o})^{-1}{}_{z})]^{-1}\nIsotropic specialization.\nTake\n=\no\nùêà\n{}_{o}=\\mathbf{I}\n,\n=\nùêà\n2\n={}^{2}\\mathbf{I}\n,\n=\nz\nùêà\n{}_{z}=\\mathbf{I}\n, and\n=\n‚àó\ndiag\n(\ns\ni\n)\n{}^{*}=\\text{diag}(s_{i})\n. Then\n(i)\n=\nc\n2\n2\nc\n1\n‚Äã\nùêà\n,\n\\displaystyle=\\frac{c_{2}^{2}}{c_{1}}\\mathbf{I},\n(ii)\n=\n[\nc\n1\n(\n)\n‚àó\n‚ä§\n+\n(\n1\n+\n)\nc\n1\n(\n)\n‚àó\n‚àí\n1\n2\n+\n(\n1\n+\n)\nc\n2\n2\nc\n1\n(\n)\n‚àó\n‚àí\n1\n]\n‚àí\n1\n\\displaystyle=[\\lambda c_{1}({}^{*})^{\\top}+(1+\\lambda)c_{1}{}^{2}({}^{*})^{-1}+(1+\\lambda)\\frac{c_{2}^{2}}{c_{1}}({}^{*})^{-1}]^{-1}\n=\ndiag\n‚Äã\n(\n[\nc\n1\n‚Äã\ns\ni\n+\n(\n1\n+\n)\n(\nc\n1\n+\n2\nc\n2\n2\nc\n1\n)\ns\ni\n]\n‚àí\n1\n)\n\\displaystyle=\\text{diag}\\left([\\lambda c_{1}s_{i}+\\frac{(1+\\lambda)(c_{1}{}^{2}+\\frac{c_{2}^{2}}{c_{1}})}{s_{i}}]^{-1}\\right)\n=\ndiag\n‚Äã\n(\nc\n1\n‚Äã\ns\ni\nc\n1\n2\ns\ni\n2\n+\n(\n1\n+\n)\n(\nc\n1\n2\n+\n2\nc\n2\n2\n)\n)\n\\displaystyle=\\text{diag}\\left(\\frac{c_{1}s_{i}}{\\lambda c_{1}^{2}s_{i}^{2}+(1+\\lambda)(c_{1}^{2}{}^{2}+c_{2}^{2})}\\right)\nHence, we obtain\nB\n=\ndiag\n‚Äã\n(\nc\n2\n2\n‚Äã\ns\ni\nc\n1\n2\ns\ni\n2\n+\n(\n1\n+\n)\n(\nc\n1\n2\n+\n2\nc\n2\n2\n)\n)\n,\nC\n=\ndiag\n‚Äã\n(\n1\nc\n1\n‚Äã\n(\n1\n‚àí\n(\n1\n+\n)\n‚Äã\nc\n2\n2\nc\n1\n2\ns\ni\n2\n+\n(\n1\n+\n)\n(\nc\n1\n2\n+\n2\nc\n2\n2\n)\n)\n)\n.\n\\displaystyle B=\\text{diag}\\left(\\frac{c_{2}^{2}s_{i}}{\\lambda c_{1}^{2}s_{i}^{2}+(1+\\lambda)(c_{1}^{2}{}^{2}+c_{2}^{2})}\\right),\\quad C=\\text{diag}\\left(\\frac{1}{c_{1}}\\left(1-\\frac{(1+\\lambda)c_{2}^{2}}{\\lambda c_{1}^{2}s_{i}^{2}+(1+\\lambda)(c_{1}^{2}{}^{2}+c_{2}^{2})}\\right)\\right).\nDefine the shrinkage factor for the\ni\ni\n-th singular direction by\n:=\ni\nB\ni\n‚Äã\ni\n/\ns\ni\n{}_{i}:=B_{ii}/s_{i}\n. Then,\n=\ni\nc\n2\n2\nc\n1\n2\ns\ni\n2\n+\n(\n1\n+\n)\n(\nc\n1\n2\n+\n2\nc\n2\n2\n)\n.\n\\displaystyle{}_{i}=\\frac{c_{2}^{2}}{\\lambda c_{1}^{2}s_{i}^{2}+(1+\\lambda)(c_{1}^{2}{}^{2}+c_{2}^{2})}.\n(I.1)\nKey implications:\n‚Ä¢\nRidge on\nB\nB\n(\n>\n0\n\\lambda>0\n) makes shrinkage\ns\ni\ns_{i}\n-dependent.\nFrom (\nI.1\n), the factor decreases with\ns\ni\ns_{i}\n(because the denominator has\nc\n1\n2\n‚Äã\ns\ni\n2\n\\lambda c_{1}^{2}s_{i}^{2}\n). So\nlarger\nsingular directions are shrunk\nmore\nwhen\n>\n0\n\\lambda>0\n.\n‚Ä¢\nIf no ridge (\n=\n0\n\\lambda=0\n).\nB\n=\ndiag\n‚Äã\n(\nc\n2\n2\n‚Äã\ns\ni\nc\n1\n2\n+\n2\nc\n2\n2\n)\nB=\\text{diag}(\\frac{c_{2}^{2}s_{i}}{c_{1}^{2}{}^{2}+c_{2}^{2}})\n: shrinkage is constant across\ni\ni\n.\n‚Ä¢\nIf no ridge and no noise (\n=\n0\n,\n=\n0\n\\lambda=0,\\eta=0\n).\nB\n=\n‚àó\nB={}^{*}\n‚Äîno shrinkage (recovers the standard solution).\n‚Ä¢\nIf no auxiliary\nz\nz\n-signal (\nc\n2\n=\n0\nc_{2}=0\n).\nB\n=\n0\nB=0\n.\nI.2\nRidge regression for action-to-action mapping (penalty on\nC\nC\n)\nWe now solve\nmin\nB\n,\nC\n;\nE\n‚Äã\n‚Äñ\nB\n‚Äã\no\n+\nC\n‚Äã\nw\n‚àí\na\n‚Äñ\n2\n+\n‚Äñ\nC\n‚Äñ\nF\n2\n.\n\\displaystyle\\min_{B,C};\\mdmathbb{E}\\big\\|Bo+Cw-a\\big\\|^{2}+\\lambda\\|C\\|_{F}^{2}.\nThe normal equations become\nB\n+\n11\nC\n21\n\\displaystyle B{}_{11}+C{}_{21}\n=\na\n‚Äã\n1\n\\displaystyle={}_{a1}\nB\n+\n12\nC\n(\n+\n22\nùêà\n)\n\\displaystyle B{}_{12}+C({}_{22}+\\lambda\\mathbf{I})\n=\n.\na\n‚Äã\n2\n\\displaystyle={}_{a2}.\nSolving gives the closed-form estimators:\nB\n\\displaystyle B\n=\n(\n‚àí\na\n‚Äã\n1\nC\n)\n21\n,\n11\n‚àí\n1\n\\displaystyle=({}_{a1}-C{}_{21}){}_{11}^{-1},\nC\n\\displaystyle C\n=\n(\n‚àí\na\n‚Äã\n2\n)\na\n‚Äã\n1\n11\n‚àí\n1\n12\n‚èü\n(i)\n‚Äã\n(\n+\n22\nùêà\n‚àí\n)\n21\n11\n‚àí\n1\n12\n‚àí\n1\n‚èü\n(ii)\n.\n\\displaystyle=\\underbrace{({}_{a2}-{}_{a1}{}_{11}^{-1}{}_{12})}_{\\text{(i)}}\\underbrace{({}_{22}+\\lambda\\mathbf{I}-{}_{21}{}_{11}^{-1}{}_{12})^{-1}}_{\\text{(ii)}}.\nIn the isotropic specialization\n=\no\nùêà\n{}_{o}=\\mathbf{I}\n,\n=\nùêà\n2\n\\Sigma\\eta={}^{2}\\mathbf{I}\n,\n=\nz\nùêà\n{}_{z}=\\mathbf{I}\n,\n=\n‚àó\ndiag\n(\ns\ni\n)\n{}^{*}=\\text{diag}(s_{i})\n, one obtains\nB\n=\ndiag\n‚Äã\n(\n(\nc\n2\n2\n+\n)\n‚Äã\ns\ni\nc\n1\n2\n+\n2\nc\n2\n2\n+\n)\n,\nC\n=\nc\n1\n2\nc\n1\n2\n+\n2\nc\n2\n2\n+\n‚Äã\nùêà\n.\n\\displaystyle B=\\text{diag}\\left(\\frac{(c_{2}^{2}+\\lambda)s_{i}}{c_{1}^{2}{}^{2}+c_{2}^{2}+\\lambda}\\right),\\qquad C=\\frac{c_{1}{}^{2}}{c_{1}^{2}{}^{2}+c_{2}^{2}+\\lambda}\\mathbf{I}.\nI.3\nTwo-pass linear surrogate of\nMIP\nLet\n(\nB\n1\n,\nC\n1\n)\n(B_{1},C_{1})\ndenote the solution of\nSection\nÀú\nI.1\nwith ridge\n1\non\nB\nB\n, and\n(\nB\n2\n,\nC\n2\n)\n(B_{2},C_{2})\ndenote the solution of ¬ß\nI.2\nwith ridge\n2\non\nC\nC\n.\nTo mimic the\nMIP\ntwo-pass inference rule in (\n4.5\n), we consider\na\n^\n0\n‚Üê\nB\n1\n‚Äã\no\n,\na\n^\n‚Üê\nB\n2\n‚Äã\no\n+\nc\n1\n‚Äã\nC\n2\n‚Äã\na\n^\n0\n.\n\\displaystyle\\hat{a}_{0}\\leftarrow B_{1}o,\\qquad\\hat{a}\\leftarrow B_{2}o+c_{1}C_{2}\\hat{a}_{0}.\nWe obtain\na\n^\n‚Üê\n(\nB\n2\n+\nc\n1\n‚Äã\nC\n2\n‚Äã\nB\n1\n‚èü\n=\n‚Å£\n:\n^\n)\n‚Äã\no\n.\n\\displaystyle\\hat{a}\\leftarrow(\\underbrace{B_{2}+c_{1}C_{2}B_{1}}_{=:\\hat{\\Phi}})o.\nNote that\nc\n1\nc_{1}\nserves as the analogue of\nt\n‚ãÜ\nt_{\\star}\nfrom the main text. From\nSections\nÀú\nI.1\nand\nI.2\nwe then obtain\n^\n\\displaystyle\\hat{\\Phi}\n=\n(\nB\n2\n+\nc\n1\n‚Äã\nC\n2\n‚Äã\nB\n1\n)\n\\displaystyle=(B_{2}+c_{1}C_{2}B_{1})\n=\ndiag\n‚Äã\n(\n(\nc\n2\n2\n+\n)\n2\ns\ni\nc\n1\n2\n+\n2\nc\n2\n2\n+\n2\n+\nc\n1\n‚Äã\nc\n1\n2\nc\n1\n2\n+\n2\nc\n2\n2\n+\n2\n‚Äã\nc\n2\n2\n‚Äã\ns\ni\nc\n1\n2\n1\ns\ni\n2\n+\n(\n1\n+\n)\n1\n(\nc\n1\n2\n+\n2\nc\n2\n2\n)\n)\n\\displaystyle=\\text{diag}\\left(\\frac{(c_{2}^{2}+{}_{2})s_{i}}{c_{1}^{2}{}^{2}+c_{2}^{2}+{}_{2}}+c_{1}\\frac{c_{1}{}^{2}}{c_{1}^{2}{}^{2}+c_{2}^{2}+{}_{2}}\\frac{c_{2}^{2}s_{i}}{{}_{1}c_{1}^{2}s_{i}^{2}+(1+{}_{1})(c_{1}^{2}{}^{2}+c_{2}^{2})}\\right)\n=\ndiag\n(\ns\ni\nc\n1\n2\n+\n2\nc\n2\n2\n+\n2\n[\nc\n2\n2\n+\n+\n2\nc\n1\n2\n‚Äã\nc\n2\n2\n2\nc\n1\n2\n1\ns\ni\n2\n+\n(\n1\n+\n)\n1\n(\nc\n1\n2\n+\n2\nc\n2\n2\n)\n]\n)\n.\n\\displaystyle=\\text{diag}\\left(\\frac{s_{i}}{c_{1}^{2}{}^{2}+c_{2}^{2}+{}_{2}}\\left[c_{2}^{2}+{}_{2}+\\frac{c_{1}^{2}{}^{2}c_{2}^{2}}{{}_{1}c_{1}^{2}s_{i}^{2}+(1+{}_{1})(c_{1}^{2}{}^{2}+c_{2}^{2})}\\right]\\right).\n(I.2)\nMoreover, the shrink factor will be\n^\ni\n‚Äã\ni\ns\ni\n=\n1\nc\n1\n2\n+\n2\nc\n2\n2\n+\n2\n[\nc\n2\n2\n+\n+\n2\nc\n1\n2\n‚Äã\nc\n2\n2\n2\nc\n1\n2\n1\ns\ni\n2\n+\n(\n1\n+\n)\n1\n(\nc\n1\n2\n+\n2\nc\n2\n2\n)\n]\n.\n\\displaystyle\\frac{\\hat{\\Phi}_{ii}}{s_{i}}=\\frac{1}{c_{1}^{2}{}^{2}+c_{2}^{2}+{}_{2}}\\left[c_{2}^{2}+{}_{2}+\\frac{c_{1}^{2}{}^{2}c_{2}^{2}}{{}_{1}c_{1}^{2}s_{i}^{2}+(1+{}_{1})(c_{1}^{2}{}^{2}+c_{2}^{2})}\\right].\n(I.3)\nKey implications:\n‚Ä¢\nIf no ridge on B (\n=\n1\n0\n{}_{1}=0\n).\nNo\ns\ni\ns_{i}\n-dependent shrinkage.\n‚Ä¢\nIf no noise (\n=\n0\n\\eta=0\n).\nJust same as the regular case:\n^\n=\n‚àó\n\\hat{\\Phi}={}^{*}\n.\n‚Ä¢\nSignal-to-noise effect.\nThe quantity in (\nI.2\n) rises with\ns\ni\ns_{i}\nand falls with , mildly favoring signal over noise by damping noisy directions.\nWhy this composition is a plausible proxy.\nThe first stage applies a ridge penalty to the observation-to-action parameters\nB\nB\nand predicts an interpolant action from observations alone, as in the\nt\n=\n0\n,\nz\n=\n0\nt=0,z=0\npass of\nMIP\n.\nWe use ridge here as a canonical proxy for implicit regularization in the linear setting.\nThe first stage applies a ridge penalty to the action-to-action parameters\nC\nC\n. It takes the interpolant action input (near\nt\n=\n1\nt=1\n) together with\no\no\nand produces the final output. Composing the two yields the operator\n^\n\\hat{\\Phi}\nin (\nI.2\n), which is the linear analogue of the two-pass prediction of (\n4.5\n).\nWhy shrinkage does not yield manifold adherence.\nThe operator in (\nI.2\n) acts as a spectral shrinker: because the factors in (\nI.3\n) decrease with\ns\ni\ns_{i}\n(for\n>\n1\n0\n{}_{1}>0\n), it attenuates the dominant directions more than the weak ones‚Äîcontrary to a projection onto a manifold, which would preserve principal directions and damp small and noisy modes.\nSince these factors lie in\n(\n0\n,\n1\n]\n(0,1]\nand vary smoothly with\ns\ni\ns_{i}\n, , and\n,\n1\n2\n{}_{1},{}_{2}\n, the map lacks any projection-like behavior: once a point is off-manifold, it is neither returned to nor retained on any low-dimensional subspace.\nThus, implicit regularization alone, even with the two-pass composition of\nMIP\n, cannot account for the observed manifold adherence.\nAppendix J\nToy experiments: Testing the function approximation capabilities of regression and flow models\nJ.1\nOverview\nThis appendix summarizes an empirical comparison of training paradigms (regression, flow matching, straight flow, MIP) for function approximation with geometric constraints across three tasks: scalar reconstruction, high-dimensional projection with subspace constraints, and Lie algebra rotations. Experiments operate in low-data regimes (50 training samples) using concatenation and FiLM architectures, with results averaged across multiple random seeds.\nJ.2\nEvaluation Metrics\nReconstruction:\nL1 and L2 errors measure point-wise approximation quality between predictions\nf\n^\n‚Äã\n(\nc\n)\n\\hat{f}(c)\nand targets\nf\n‚Äã\n(\nc\n)\nf(c)\n.\nProjection:\nThree metrics assess geometric constraints in piecewise-constant projection structure:\nsubspace diagonal\nquantifies predictions outside correct subspace\nP\ni\nP_{i}\nfor interval\ni\ni\n,\noff-diagonal\ntests cross-interval generalization with mismatched projections, and\nboundary\nmeasures smoothness at interval transitions using combined adjacent subspace projections. All metrics use normalized form\n‚Äñ\n(\nI\n‚àí\nP\n)\n‚Äã\nf\n^\n‚Äñ\n/\n‚Äñ\nf\n^\n‚Äñ\n\\|(I-P)\\hat{f}\\|/\\|\\hat{f}\\|\n.\nLie Algebra:\nCosine similarity\nmeasures angular alignment between predicted and true rotation directions.\nProjection metric\nquantifies normalized perpendicular error relative to the rotation axis span.\nJ.3\nKey Findings\nTask-Dependent Performance:\nRegression-based approaches achieve lowest L2 reconstruction error (0.003197 ¬± 0.000525 with L2 loss and FiLM), consistently outperforming flow-based methods on point-wise approximation tasks.\nFlow Methods Excel at Projections:\nFlow-based training demonstrates superior geometric constraint satisfaction. Straight flow (flow matching without time conditioning) achieves best boundary projection (0.009769 ¬± 0.001630) and Lie algebra projection metrics (0.063612 ¬± 0.000952), indicating beneficial geometric biases from learning probability transport.\nMIP Competitive Performance:\nMIP combines direct regression with denoising regularization, achieving near-optimal reconstruction while maintaining reasonable geometric constraint satisfaction across tasks.\nJ.4\nTraining Loss Considerations\nResults focus on L2-trained models, providing mathematically grounded objectives for both regression and flow paradigms. While alternative loss functions were evaluated empirically, flow-based L1 training lacks principled derivation as conditional flow matching is naturally defined for squared error.\nJ.5\nArchitectural Observations\nBoth concatenation and FiLM architectures demonstrated competitive performance with no consistent dominance. FiLM showed marginal advantages on certain geometric metrics for flow-based methods, suggesting affine feature modulation may better capture conditional dependencies in probability transport.\nJ.6\nImplications for Method Selection\n‚Ä¢\nTasks prioritizing point-wise reconstruction: regression-based training with L2 loss offers superior accuracy and computational efficiency.\n‚Ä¢\nTasks requiring geometric constraint satisfaction: flow-based training provides significant advantages despite increased evaluation cost.\n‚Ä¢\nStraight flow‚Äôs success suggests time conditioning may be unnecessary, enabling simpler models with competitive performance.\nJ.7\nExperimental Details\nStudy encompasses 540 runs: 5 modes (regression, flow, straight flow, MIP, MIP one-step) √ó 2 losses √ó 2 architectures √ó 3 tasks √ó 3 seeds. Configuration: 256 hidden dimensions, 3 layers, ReLU, batch size 32, 50k epochs, Adam with lr=0.001. Evaluation: 100k test samples; flow methods use Euler integration with 9 ODE steps.\nFull Report:\nhttps://example.com/neural-manifold-report\nCode:\nhttps://example.com/code-repository\nAppendix K\nAppendix for Section\n2\nK.1\nMarkov Decision Processes Configuration\nWe consider a Markov Decision Process\n‚Ñ≥\n=\n(\nùíÆ\n,\nùíú\n,\nR\n,\nP\n,\nP\n0\n)\n\\mathcal{M}=(\\mathcal{S},\\mathcal{A},R,P,P_{0})\n7\n7\n7\nFor simplicity, we consider the MDP case in this context by identifying the state with the observation defined in\n2\n. More generally, one may consider a Partially Observable Markov Decision Process (POMDP), where the agent receives observation\no\no\nemitted by an underlying latent state\ns\ns\n.\nwith the state space\nùíÆ\n\\mathcal{S}\n, the action space\nùíú\n\\mathcal{A}\n, the reward\nR\n‚Äã\n(\ns\n,\na\n)\nR(s,a)\n8\n8\n8\nFor ease of exposition, we use the same notation for rewards defined on random variables and their distributions.\nobtained by taking action\na\na\nin state\ns\ns\n, the transition dynamics\nP\n:\nùíÆ\n√ó\nùíú\n‚Üí\n(\nùíÆ\n)\nP:\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(\\mathcal{S})\n, and the initial-state distribution\nP\n0\n‚àà\n(\nùíÆ\n)\nP_{0}\\in\\Delta(\\mathcal{S})\n. To formulate the success rate (i.e., performance) in this setting, we define the reward function as:\nR\n‚Äã\n(\ns\n,\na\n)\n=\n{\n1\n,\nif the task is successful under\n(\ns\n,\na\n)\n,\n0\n,\notherwise\n.\n\\displaystyle R(s,a)=\\begin{cases}1,&\\text{if the task is successful under $(s,a)$},\\\\\n0,&\\text{otherwise}.\\end{cases}\n(K.1)\nUnder this definition of rewards, the expected return of a policy  is\nJ\n‚Äã\n(\n)\n=\nE\n‚Äã\n[\n‚àë\nt\nR\n‚Äã\n(\ns\nt\n,\na\nt\n)\n]\nJ(\\pi)=\\mdmathbb{E}[\\sum_{t}R(s_{t},a_{t})]\n, which reduces to\nP\n‚Å°\n[\nsuccess under\n]\n\\operatorname{\\mdmathbb{P}}[\\text{success under }\\pi]\n. Hence,\nJ\n‚Äã\n(\n)\nJ(\\pi)\nexactly equals the success rate of policy .\nK.2\nIntegrated Flow Prediction\nFor completeness, we provide the flow ODE as\nd\nd\n‚Äã\nt\n‚Äã\na\nt\n=\nb\nt\n‚Äã\n(\na\nt\n‚à£\no\n)\nstarting from\na\n0\n=\nz\n.\n\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}a_{t}=b_{t}(a_{t}\\mid o)\\qquad\\text{starting from}\\qquad a_{0}=z.\n(K.2)\nThe associated integrated flow prediction is given by\n(\nz\n‚à£\no\n)\n=\nz\n+\n‚à´\n0\n1\nb\nt\n‚Äã\n(\na\nt\n‚à£\no\n)\n‚Äã\nd\nt\n.\n\\displaystyle(z\\mid o)=z+\\int_{0}^{1}b_{t}(a_{t}\\mid o)\\mathrm{d}t.\n(K.3)\nIn practice, to approximate the ODE solution for sampling, we employ the following discretized Euler integration.\nDefinition K.1\n(Discretized Euler Integration)\n.\nWe discretize the time interval\n[\n0\n,\n1\n]\n[0,1]\nto\nN\nN\nsteps with step size\nh\n=\n1\n/\nN\nh=1/N\n. The iterates are then updated according to\na\nk\n+\n1\n=\na\nk\n+\nh\n‚Äã\nb\nh\n‚Äã\nk\n‚Äã\n(\na\nk\n‚à£\no\n)\n,\nk\n=\n0\n,\n1\n,\n‚Ä¶\n,\nN\n‚àí\n1\n.\n\\displaystyle a_{k+1}=a_{k}+h\\,b_{hk}(a_{k}\\mid o),\\quad k=0,1,\\dots,N-1.\n(K.4)\nThe final iterate\na\nN\na_{N}\nserves as the Euler approximation\n(\nz\n‚à£\no\n)\n,\neul\n{}_{\\theta,\\mathrm{eul}}(z\\mid o)\n. We also refer to\nN\nN\nas the\nNumber of Function Evaluations (NFEs)\n.",
    "preview_text": "Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/\n\nMuch Ado About Noising: Dispelling the Myths of Generative Robotic Control\nChaoyi Pan\n1\n1\n1\n{chaoyip, giria, naichieh, clairej, gqu, nboffi, gshi, msimchow}@andrew.cmu.edu\n,\n$\n\\mathdollar\nGiri Anantharaman\n1\nNai-Chieh Huang\n1\nClaire Jin\n1\nDaniel Pfrommer\n2\n2\n2\ndpfrom@mit.edu\nChenyang Yuan\n3\n3\n3\n{chenyang.yuan,frank.permenter}tri.global\nFrank Permenter\n3\nGuannan Qu\n1\n,\n‚Ä†\n\\dagger\nNicholas Boffi\n1\n,\n‚Ä†\n\\dagger\nGuanya Shi\n1\n,\n‚Ä†\n\\dagger\nMax Simchowitz\n1\n,\n‚Ä†\n\\dagger\n$\nProject lead.\n‚Ä†\nEqual advising.\na\nCarnegie Mellon University\nb\nMassachusetts Institute of Technology\nc\nToyota Research Institute\nAbstract\nGenerative models, like flows and diffusions, have recently emerged as popular and efficacious policy para",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "diffusion",
        "generative models",
        "robotic control",
        "behavior cloning",
        "iterative computation"
    ],
    "one_line_summary": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÊ®°ÂûãÂú®Êú∫Âô®‰∫∫ÊéßÂà∂‰∏≠ÁöÑÊàêÂäüÂõ†Á¥†ÔºåËÆ§‰∏∫ÂÖ∂‰ºòÂäøÊ∫ê‰∫éËø≠‰ª£ËÆ°ÁÆóËÄåÈùûÂ§öÊ®°ÊÄÅÊçïËé∑Ôºå‰∏éËßÜÈ¢ëÊâ©Êï£„ÄÅÂ§öÊ®°ÊÄÅÁîüÊàêÁ≠âÂÖ≥ÈîÆËØçÁõ∏ÂÖ≥ÊÄßËæÉ‰Ωé„ÄÇ",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T15:44:53Z",
    "created_at": "2026-01-10T10:45:27.964598",
    "updated_at": "2026-01-10T10:45:27.964607",
    "flag": true
}