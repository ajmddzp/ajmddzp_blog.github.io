{
  "id": "2601.10379v1",
  "title": "Online identification of nonlinear time-varying systems with uncertain information",
  "authors": [
    "He Ren",
    "Gaowei Yan",
    "Hang Liu",
    "Lifeng Cao",
    "Zhijun Zhao",
    "Gang Dang"
  ],
  "abstract": "Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.",
  "url": "https://arxiv.org/abs/2601.10379v1",
  "html_url": "https://arxiv.org/html/2601.10379v1",
  "html_content": "Online identification of nonlinear time-varying systems with uncertain information\nHe Ren\nrh15848286658@outlook.com\nGaowei Yan\nyangaowei@tyut.edu.cn\nHang Liu\n2023310084@link.tyut.edu.cn\nLifeng Cao\n18636531698@163.com\nZhijun Zhao\nzhaozhijun1@tit.edu.cn\nGang Dang\ndzw627@126.com\nCollege of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, 030024, China\nDepartment of Automation, Taiyuan Institute of Technology, Taiyuan, 030024, China\nAbstract\nDigital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.\nkeywords:\nNonlinear time-varying systems; Bayesian recursive symbolic learning; Uncertainty analysis.\nâ€ \nâ€ \nthanks:\nThis paper was not presented at any conference.\n* Corresponding author\n, , , ,\n,\n1\nIntroduction\nThe rise of digital twins (DTs) as core enablers for the industrial metaverse and autonomous systems is driving a paradigm shift in the modeling, monitoring, and control of complex cyber-physical systems\n[\n17\n,\n20\n]\n. A high-fidelity DT is more than a static virtual model; it is a dynamic, evolving computational entity that continuously interacts with its physical counterpart via tight integration of data, models, and connectivity\n[\n11\n]\n. The performance of this coupling depends on the virtual modelâ€™s ability to fulfill three key requirements: predictive accuracy under uncertainty, interpretability of the underlying governing laws, and real-time adaptability to streaming data\n[\n22\n]\n.\nUncertainty in the physical world is a fundamental challenge in the construction and deployment of complex systems. In response, a substantial line of research has focused on real-time adaptability and predictive accuracy under uncertainty within a Bayesian framework. This approach includes techniques such as Bayesian compressive sensingâ€”which leverages sparsity-inducing priors (e.g., Laplace\n[\n10\n]\n, Horseshoe\n[\n5\n,\n13\n]\n) to enable uncertainty-aware signal recovery. It also encompasses recursive Bayesian inference methods that underpin adaptive filtering\n[\n7\n]\nand state estimation\n[\n18\n]\n, together providing a principled methodology for online learning. Nevertheless, these advanced methods are predominantly applied to state and parameter estimation within fixed model structures or to black-box function approximation\n[\n23\n,\n14\n]\n. As a result, although they excel in uncertainty quantification (thereby enhancing predictive accuracy) and computational efficiency (enabling real-time operation), they often sacrifice interpretability by failing to produce explicit symbolic descriptions of the underlying system dynamics\n[\n21\n,\n19\n]\n.\nIn contrast, state-of-the-art methods for deriving interpretable models, particularly those based on the Sparse Identification of Nonlinear Dynamics (SINDy) paradigm\n[\n8\n]\n, excel at discovering parsimonious governing equations from data. Variants like G-SINDy\n[\n15\n]\nand MGSINDy\n[\n16\n]\nhave extended this framework to multi-input multi-output systems and incorporated noise decomposition, thus addressing aspects of predictive accuracy. Despite these strengths, their core limitation lies in the real-time domain: they operate within an offline, batch-processing paradigm that inherently relies on iteratively solving non-convex, often NP-hard, optimization problems. This computational intractability creates a critical gap for DT applications that require online, continual adaptation\n[\n6\n]\n.\nConsequently, a core challenge persists: the lack of a unified framework that delivers interpretability, probabilistic predictive accuracy, and real-time efficiency. This work is motivated by two fundamental and interconnected scientific problems:\nThe Semantic Gap between Interpretability and Uncertainty Quantification.\nA fundamental divide exists between interpretable symbolic modeling and probabilistic uncertainty quantification. While the former produces transparent models lacking uncertainty bounds, the latter provides predictive distributions at the expense of interpretability. Bridging this gap requires a new formalism that intrinsically unifies both capabilities.\nThe Computational Gap between Batch and Streaming Inference.\nTranslating high-fidelity symbolic discovery from offline batch processing to online streaming poses a fundamental challenge, necessitating a shift from iterative optimization to recursive estimation while preserving numerical stability and statistical efficiency.\nTo this end, a novel Bayesian Regression-based Symbolic Learning (BRSL) framework is proposed. The contributions of this work are threefold:\nâ€¢\nA unified probabilistic state-space model for online symbolic discovery that bridges deterministic symbolic regression and Bayesian inference. By incorporating sparsity-inducing horseshoe priors, model-structure selection is cast as Bayesian inference, enabling simultaneous system identification and uncertainty quantification.\nâ€¢\nAn online recursive algorithm with a forgetting factor, together with precise recursive conditions that ensure posterior validity and well-posedness which also serve as a real-time data-utility monitor and enhance robustness.\nâ€¢\nA rigorous convergence analysis establishes the convergence of parameter estimates under persistent excitation, thereby ensuring provable stability and performance.\nThe remainder of this paper is organized as follows. Sec.\n2\nprovides the problem formulation. Sec.\n3\ndetails the main algorithm and provides supporting proofs. Sec.\n4\nvalidates the algorithm through two case studies. Finally, Sec.\n5\nconcludes the paper.\n2\nProblem formulation\nConsider a general nonlinear time-varying system\nğ’š\nd\n,\nÎ±\n=\nğ’‡\nÎ²\nâ€‹\n(\nğ’™\nd\n,\nÎ±\n,\nÎ±\n)\n,\n{{\\boldsymbol{y}}_{d,\\alpha}}={{\\boldsymbol{f}}_{\\beta}}\\left({{\\boldsymbol{x}}_{d,\\alpha}},\\alpha\\right),\n(1)\nwhere\nÎ±\n\\alpha\ndenotes continuous-time index;\nğ’š\nd\n,\nÎ±\nâˆˆ\nâ„\nn\ny\n{{\\boldsymbol{y}}_{d,\\alpha}}\\in{{\\mathbb{R}}^{{{n}_{y}}}}\ndenotes the observation vector;\nğ’™\nd\n,\nÎ±\nâˆˆ\nâ„\nn\nx\n{{\\boldsymbol{x}}_{d,\\alpha}}\\in{{\\mathbb{R}}^{{{n}_{x}}}}\ndenotes the state vector. For notational convenience, it is assumed that\nğ’™\nd\n,\nÎ±\n\\boldsymbol{x}_{d,\\alpha}\nencompasses both state variables and operational input variables, and\nğ’‡\nÎ²\n\\boldsymbol{f}_{\\beta}\nrepresents a nonlinear mapping.\nData sampled at discrete time instants\nÎ±\nt\nâˆˆ\n[\n0\n,\nâˆ\n)\n{{\\alpha}_{t}}\\in[0,\\infty)\n(with\nt\n=\n1\n,\n2\n,\nâ€¦\nt=1,2,...\nindexes the sampled data points) be denoted as\nğ’š\nd\n,\nÎ±\nt\n{{\\boldsymbol{y}}_{{{d,\\alpha}_{t}}}}\nand\nğ’™\nd\n,\nÎ±\nt\n{{\\boldsymbol{x}}_{{{d,\\alpha}_{t}}}}\n. For simplicity, define\nğ’š\nd\n,\nÎ±\nt\n=\nğ’š\nd\n,\nt\n{{\\boldsymbol{y}}_{{{d,\\alpha}_{t}}}}={{\\boldsymbol{y}}_{d,t}}\nand\nğ’™\nd\n,\nÎ±\nt\n=\nğ’™\nd\n,\nt\n{{\\boldsymbol{x}}_{{{d,\\alpha}_{t}}}}={{\\boldsymbol{x}}_{d,t}}\n. The dataset combining these observations and states is then given by\nğ’Ÿ\n=\n{\nğ’€\nd\n,\nt\n=\n[\nğ’š\nd\n,\n1\n;\nğ’š\nd\n,\n2\n;\nâ€¦\n;\nğ’š\nd\n,\nt\n]\n,\nğ‘¿\nd\n,\nt\n=\n[\nğ’™\nd\n,\n1\n;\nğ’™\nd\n,\n2\n;\nâ€¦\n;\nğ’™\nd\n,\nt\n]\n}\n,\n\\mathsf{\\mathcal{D}}=\\left\\{\\begin{aligned} {{\\boldsymbol{Y}}_{d,t}}=[{{\\boldsymbol{y}}_{d,1}};{{\\boldsymbol{y}}_{d,2}};\\ldots;{{\\boldsymbol{y}}_{d,t}}],\\\\\n{{\\boldsymbol{X}}_{d,t}}=[{{\\boldsymbol{x}}_{d,1}};{{\\boldsymbol{x}}_{d,2}};\\ldots;{{\\boldsymbol{x}}_{d,t}}]\\end{aligned}\\right\\},\n(2)\nwhere\nğ’€\nd\n,\nt\nâˆˆ\nâ„\nt\nÃ—\nn\ny\n{{\\boldsymbol{Y}}_{d,t}}\\in{{\\mathbb{R}}^{t\\times{{n}_{y}}}}\nrepresents observation matrix, and\nğ‘¿\nd\n,\nt\nâˆˆ\nâ„\nt\nÃ—\nn\nx\n{{\\boldsymbol{X}}_{d,t}}\\in{{\\mathbb{R}}^{t\\times{{n}_{x}}}}\nrepresents the state matrix, satisfying\nğ’€\nd\n,\nt\n=\nğ’‡\nÎ²\nâ€‹\n(\nğ‘¿\nd\n,\nt\n,\nt\n)\n.\n{{\\boldsymbol{Y}}_{d,t}}={{\\boldsymbol{f}}_{\\beta}}\\left({{\\boldsymbol{X}}_{d,t}},t\\right).\n(3)\nIn the symbolic learning framework, the structure of the governing equations is partially or completely unknown. Consequently, to learn interpretable dynamical equations, a common approach is to approximate the drift function by a sparse linear combination of predefined basis functions\nğ’‡\nÎ²\nâ€‹\n(\nğ‘¿\nd\n,\nt\n,\nt\n)\n=\nğ\n1\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\nâ€‹\nğœ·\n1\n+\nğ’‡\nÏ•\nâ€‹\n(\nğ‘¿\nd\n,\nt\n,\nt\n)\nâ€‹\nğœ·\n2\n,\n{{\\boldsymbol{f}}_{\\beta}}({{\\boldsymbol{X}}_{d,t}},t)={{\\boldsymbol{\\psi}}_{1}}({{\\boldsymbol{X}}_{d,t}}){{\\boldsymbol{\\beta}}^{1}}+{{\\boldsymbol{f}}_{\\phi}}({{\\boldsymbol{X}}_{d,t}},t){{\\boldsymbol{\\beta}}^{2}},\n(4)\nwhere\nğ’‡\nÏ•\n{{\\boldsymbol{f}}_{\\phi}}\ndenotes the known drift function of the dynamics, such as the first principles;\nğ\n1\n{{\\boldsymbol{\\psi}}_{1}}\ndenotes the library of basis functions defined by symbol learning, which is typically constructed from polynomials\nğ\n1\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\n=\n[\n1\nğ‘¿\nd\n,\nt\nğ‘¿\nd\n,\nt\nP\n2\nâ€¦\n]\n,\n{{\\boldsymbol{\\psi}}_{1}}({{\\boldsymbol{X}}_{d,t}})=\\left[\\begin{matrix}1&{{\\boldsymbol{X}}_{d,t}}&\\boldsymbol{X}_{d,t}^{{{P}_{2}}}&\\ldots\\\\\n\\end{matrix}\\right],\n(5)\nğ‘¿\nd\n,\nt\nP\ni\n\\boldsymbol{X}_{d,t}^{{{P}_{i}}}\nrepresents all polynomials of\ni\ni\ndegree in\nğ‘¿\nd\n,\nt\n{{\\boldsymbol{X}}_{d,t}}\n.\nIn order to keep notational consistency, the dictionary function is defined as\nğš¿\nd\n=\n[\nğ\n1\nğ’‡\nÏ•\n]\nâˆˆ\nâ„\nt\nÃ—\nn\np\n\\boldsymbol{\\Psi}_{d}=\\left[\\begin{array}[]{c|c}{{\\boldsymbol{\\psi}}_{1}}&{{\\boldsymbol{f}}_{\\phi}}\\end{array}\\right]\\in\\mathbb{R}^{t\\times n_{p}}\n.\nTo sum up, the problem of interest is to describe the point prediction problem as a probabilistic prediction problem and develop a probabilistic form of recurrence framework. Under the aforementioned assumptions, Eq.(\n3\n) can be transformed into a generalized linear representation, i.e.\nğ’€\nd\n,\nt\n=\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\nâ€‹\nğœ·\n,\n{{\\boldsymbol{Y}}_{d,t}}=\\boldsymbol{\\Psi}_{d}\\left({{\\boldsymbol{X}}_{d,t}}\\right)\\boldsymbol{\\beta},\n(6)\nwhere\nğœ·\nd\n=\n[\nğœ·\n1\nğœ·\n2\n]\n\\boldsymbol{\\beta}_{d}=[\\begin{array}[]{cc}{{\\boldsymbol{\\beta}}^{1}}\\\\\n\\hline\\cr{{\\boldsymbol{\\beta}}^{2}}\\\\\n\\end{array}]\nrepresents the sparse coefficient matrix to be identified.\n3\nMethodology\nThis section introduces an algorithm termed BRSL for online identification of nonlinear models in high-noise environments. Furthermore, the recursive conditions and convergence properties of the BRSL algorithm are investigated.\n3.1\nBayesian Regression-Based Symbolic Learning\nTo enhance robustness and parsimony, regression coefficients are often assumed to be sparse in practice, meaning that only a subset of features exerts significant influence on the outcome, thereby simplifying the model structure and improving interpretability\n[\n2\n,\n9\n,\n4\n]\n. In this study, a sparsity-inducing horseshoe prior is employed over the parameters,\nÎ²\nâˆ¼\np\nâ€‹\n(\nÎ²\n)\n\\beta\\sim p(\\beta)\n[\n3\n]\n. Without loss of generality, the regression process is formulated as in Eq.(\n7\n):\nğ’€\nd\n,\nt\n=\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\nâ€‹\nğœ·\nd\n+\nğœ·\n0\n,\n{{\\boldsymbol{Y}}_{d,t}}=\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,t}})\\boldsymbol{\\beta}_{d}+{{\\boldsymbol{\\beta}}_{0}},\n(7)\nLet\nvec\nâ€‹\n(\nâ‹…\n)\n\\mathrm{vec}(\\cdot)\ndenote the vectorization operation of matrix, we have\nvec\nâ€‹\n(\nğ’€\nd\n,\nt\n)\n=\nvec\nâ€‹\n(\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\nâ€‹\nğœ·\nd\n)\n+\nvec\nâ€‹\n(\nğœ·\n0\n)\n.\n\\mathrm{vec}({{\\boldsymbol{Y}}_{d,t}})=\\mathrm{vec}(\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,t}})\\boldsymbol{\\beta}_{d})+\\mathrm{vec}({{\\boldsymbol{\\beta}}_{0})}.\n(8)\nGiven\nğ’€\nt\n=\nvec\nâ€‹\n(\nğ’€\nd\n,\nt\n)\nâˆˆ\nâ„\nt\nâ‹…\nn\ny\nÃ—\n1\n\\boldsymbol{Y}_{t}=\\mathrm{vec}(\\boldsymbol{Y}_{d,t})\\in\\mathbb{R}^{t\\cdot n_{y}\\times 1}\nand\nğœ·\n=\nvec\nâ€‹\n(\nğœ·\nd\n)\nâˆˆ\nâ„\nn\np\nâ‹…\nn\ny\nÃ—\n1\n\\boldsymbol{\\beta}=\\mathrm{vec}(\\boldsymbol{\\beta}_{d})\\in\\mathbb{R}^{n_{p}\\cdot n_{y}\\times 1}\n, then,\nğ’€\nt\n=\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\nâ€‹\nğœ·\n+\nvec\nâ€‹\n(\nğœ·\n0\n)\n,\n{{\\boldsymbol{Y}}_{t}}=\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t}})\\boldsymbol{\\beta}+\\mathrm{vec}({{\\boldsymbol{\\beta}}_{0})},\n(9)\nwhere\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\n=\nğ‘°\nn\ny\nâŠ—\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\nâˆˆ\nâ„\nt\nâ‹…\nn\ny\nÃ—\nn\np\nâ‹…\nn\ny\n\\boldsymbol{\\Psi}(\\boldsymbol{X}_{t})=\\boldsymbol{I}_{n_{y}}\\otimes\\boldsymbol{\\Psi}_{d}(\\boldsymbol{X}_{d,t})\\in\\mathbb{R}^{t\\cdot n_{y}\\times n_{p}\\cdot n_{y}}\n, and\nâŠ—\n\\otimes\nrepresents Kronecker product.\nFrom the probabilistic perspective, Eq.(\n9\n) can be expressed as\np\nâ€‹\n(\nğ’€\nt\n|\nğœ·\n)\nâˆ¼\nğ’©\nâ€‹\n(\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\nâ€‹\nğœ·\n,\nğšº\np\nâŠ—\nğ‘°\nt\n)\n.\np({{\\boldsymbol{Y}}_{t}}\\left|\\boldsymbol{\\beta}\\right.)\\sim\\mathsf{\\mathcal{N}}(\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t})}\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}_{p}\\otimes\\boldsymbol{I}_{t}).\n(10)\nGenerally,\nğœ·\n\\boldsymbol{\\beta}\nis assumed to be sparse, and noise terms across the columns of\nğœ·\n0\n{\\boldsymbol{\\beta}}_{0}\nare assumed to be i.i.d., i.e\nvec\nâ€‹\n(\nğœ·\n0\n)\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nğšº\np\nâŠ—\nğ‘°\nt\n)\n{{\\mathrm{vec}(\\boldsymbol{\\beta}}_{0})}\\sim\\mathsf{\\mathcal{N}}(0,\\boldsymbol{\\Sigma}_{p}\\otimes\\boldsymbol{I}_{t})\n, where\nğšº\np\n=\nd\nâ€‹\ni\nâ€‹\na\nâ€‹\ng\nâ€‹\n(\nÏƒ\ni\n2\n)\n\\boldsymbol{\\Sigma}_{p}=diag(\\sigma_{i}^{2})\n,\ni\n=\n1\n,\n2\n,\nâ€¦\n,\nn\ny\ni=1,2,\\ldots,n_{y}\nand\nÏƒ\ni\n2\n\\sigma_{i}^{2}\ndenotes the measurement noise variance corresponding to the\ni\ni\n-th output.\nThe horseshoe prior assumes that each component\nÎ²\n\\beta\nof\nğœ·\n\\boldsymbol{\\beta}\nis conditionally independent, with its distribution given by a mixture of Gaussians\np\nâ€‹\n(\nÎ²\ni\n,\nj\n|\nÎ»\ni\n,\nj\n,\nÏ„\n)\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nÎ»\ni\n,\nj\n2\nâ€‹\nÏ„\n2\n)\n,\n\\displaystyle p({{\\beta}_{i,j}}\\left|{{\\lambda}_{i,j}},\\tau\\right.)\\sim\\mathsf{\\mathcal{N}}(0,\\lambda_{i,j}^{2}{{\\tau}^{2}}),\n(11)\np\nâ€‹\n(\nÎ»\ni\n,\nj\n)\nâˆ¼\nH\nâ€‹\na\nâ€‹\nl\nâ€‹\nf\nâ€‹\nC\nâ€‹\nu\nâ€‹\na\nâ€‹\nc\nâ€‹\nh\nâ€‹\ny\nâ€‹\n(\n0\n,\n1\n)\n,\n\\displaystyle p({{\\lambda}_{i,j}})\\sim HalfCuachy(0,1),\np\nâ€‹\n(\nÏ„\n)\nâˆ¼\nH\nâ€‹\na\nâ€‹\nl\nâ€‹\nf\nâ€‹\nC\nâ€‹\nu\nâ€‹\na\nâ€‹\nc\nâ€‹\nh\nâ€‹\ny\nâ€‹\n(\n0\n,\n1\n)\n,\n\\displaystyle p(\\tau)\\sim HalfCuachy(0,1),\nwhere\nÎ²\ni\n,\nj\n\\beta_{i,j}\ndenotes the element at the\n(\ni\n,\nj\n)\n(i,j)\n-th position of matrix\nğœ·\nd\n\\boldsymbol{\\beta}_{d}\n;\nÎ»\ni\n,\nj\n{{\\lambda}_{i,j}}\nis the local shrinkage parameter for each coefficient\nÎ²\ni\n,\nj\n\\beta_{i,j}\n;\nÏ„\n\\tau\nis the global shrinkage parameter.\nRemark 1\n.\nReliable uncertainty quantification remains a critical challenge in high-dimensional inference. However, the estimates derived from Lasso regression fail to provide meaningful distributional uncertainty for parameter estimates. The horseshoe prior demonstrates performance comparable to that of Lasso regression in sparse representation within high-dimensional spaces, while its heavy-tailed properties ensure that strong signals retain high posterior probability. More importantly, this design enables the mapping of deterministic point prediction problems into probabilistic forecasting frameworks. For further details, refer to\n[\n1\n]\n.\nAs in conventional Bayesian inference, the objective is to infer the posterior distribution of parameters\nğœ·\n\\boldsymbol{\\beta}\n, given the observed and input sequences. Incorporating the horseshoe prior assumptions and letting\nğœ½\n=\nd\nâ€‹\ni\nâ€‹\na\nâ€‹\ng\nâ€‹\n(\nÎ»\ni\n,\nj\n2\n)\nâ€‹\nÏ„\n2\n\\boldsymbol{\\theta}=diag(\\lambda_{i,j}^{2})\\tau^{2}\n, we obtain the following posterior\np\nâ€‹\n(\nğœ·\n|\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\n,\nğ’€\nt\n)\n\\displaystyle p(\\boldsymbol{\\beta}\\left|\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t}}),{{\\boldsymbol{Y}}_{t}}\\right.)\n(12)\nâˆ\nğ’©\nâ€‹\n(\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\nâ€‹\nğœ·\n,\nğšº\np\nâŠ—\nğ‘°\nt\n)\nâ‹…\nğ’©\nâ€‹\n(\nğŸ\n,\nğœ½\n)\n\\displaystyle\\propto\\mathsf{\\mathcal{N}}(\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t})}\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}_{p}\\otimes\\boldsymbol{I}_{t})\\cdot\\mathsf{\\mathcal{N}}(\\boldsymbol{0},\\boldsymbol{\\theta})\nâˆ\nâˆ\nj\n=\n1\nt\nğ’©\nâ€‹\n(\nğ’š\nj\n;\nğš¿\nâ€‹\n(\nğ’™\nj\n)\nâ€‹\nğœ·\n,\nğšº\np\n)\nâ‹…\nğ’©\nâ€‹\n(\nğŸ\n,\nğœ½\n)\n,\n\\displaystyle\\propto\\prod\\limits_{j=1}^{t}{\\mathsf{\\mathcal{N}}({\\boldsymbol{y}_{j}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}})\\boldsymbol{\\beta},{{\\boldsymbol{\\Sigma}}_{p}})}\\cdot\\mathsf{\\mathcal{N}}(\\boldsymbol{0},\\boldsymbol{\\theta}),\nwhere\nğš¿\nâ€‹\n(\nğ’™\ni\n)\n=\nğ‘°\nn\ny\nâŠ—\nğš¿\nd\nâ€‹\n(\nğ’™\nd\n,\ni\n)\n{\\boldsymbol{\\Psi}(\\boldsymbol{x}_{i})}=\\boldsymbol{I}_{n_{y}}\\otimes\\boldsymbol{\\Psi}_{d}(\\boldsymbol{x}_{d,i})\nand\nğ’š\ni\n=\nvec\nâ€‹\n(\nğ’š\nd\n,\ni\n)\n{\\boldsymbol{y}_{i}}=\\mathrm{vec}(\\boldsymbol{y}_{d,i})\n. Since the conjugate prior of the Gaussian distribution is also a Gaussian distribution, the resulting posterior distribution remains Gaussian,\nâˆ\nj\n=\n1\nt\nğ’©\nâ€‹\n(\nğ’š\nj\n;\nğš¿\nâ€‹\n(\nğ’™\nj\n)\nâ€‹\nğœ·\n,\nğšº\np\n)\n\\displaystyle\\prod\\limits_{j=1}^{t}{\\mathsf{\\mathcal{N}}({\\boldsymbol{y}_{j}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}})\\boldsymbol{\\beta},{{\\boldsymbol{\\Sigma}}_{p}})}\n(13)\n=\nâˆ\nj\n=\n1\nt\n1\n(\n2\nâ€‹\nÏ€\n)\nn\ny\n/\n2\nâ€‹\n|\nğšº\np\n|\n1\n/\n2\nexp\n(\nâˆ’\n1\n2\n(\nğ’š\nj\nâˆ’\nğš¿\n(\nğ’™\nj\n)\nğœ·\n)\nT\n\\displaystyle=\\prod\\limits_{j=1}^{t}\\frac{1}{{{(2\\pi)}^{n_{y}/2}}{{|\\boldsymbol{\\Sigma}}_{p}}|^{1/2}}\\exp(-\\frac{1}{2}{{{({\\boldsymbol{y}_{j}}-\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}})\\boldsymbol{\\beta})}^{T}}}\nâ‹…\nğšº\np\nâˆ’\n1\n(\nğ’š\nj\nâˆ’\nğš¿\n(\nğ’™\nj\n)\nğœ·\n)\n)\n\\displaystyle\\cdot\\boldsymbol{\\Sigma}_{p}^{-1}{({\\boldsymbol{y}_{j}}-\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}})\\boldsymbol{\\beta})})\n=\n1\n(\n2\nâ€‹\nÏ€\n)\nt\nâ€‹\nn\ny\n/\n2\nâ€‹\n|\nğšº\np\n|\nt\n/\n2\nexp\n(\nâˆ’\n1\n2\nâˆ‘\nj\n=\n1\nt\n(\nğ’š\nj\nâˆ’\nğš¿\n(\nğ’™\nj\n)\nğœ·\n)\nT\n\\displaystyle=\\frac{1}{{{(2\\pi)}^{tn_{y}/2}}{{|\\boldsymbol{\\Sigma}}_{p}}|^{t/2}}\\exp(-\\frac{1}{2}\\sum_{j=1}^{t}{{{({\\boldsymbol{y}_{j}}-\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}})\\boldsymbol{\\beta})}^{T}}}\nâ‹…\nğšº\np\nâˆ’\n1\n(\nğ’š\nj\nâˆ’\nğš¿\n(\nğ’™\nj\n)\nğœ·\n)\n)\n.\n\\displaystyle\\cdot\\boldsymbol{\\Sigma}_{p}^{-1}{({\\boldsymbol{y}_{j}}-\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}})\\boldsymbol{\\beta})}).\nSubstituting Eq.(\n13\n) into Eq.(\n12\n) leads to the following posterior distribution for the parameters\nğœ·\n\\boldsymbol{\\beta}\n:\np\nâ€‹\n(\nğœ·\n|\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\n,\nğ’€\nt\n)\n\\displaystyle p(\\boldsymbol{\\beta}\\left|\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t}}),{{\\boldsymbol{Y}}_{t}}\\right.)\n(14)\nâˆ\nexp\n(\nâˆ’\n1\n2\n(\nğ’€\nt\nâˆ’\nğš¿\n(\nğ‘¿\nt\n)\nğœ·\n)\nT\n(\nğšº\np\nâˆ’\n1\nâŠ—\nğ‘°\nt\n)\n\\displaystyle\\propto\\exp(-\\frac{1}{2}{{({{\\boldsymbol{Y}}_{t}}-\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t}})\\boldsymbol{\\beta})}^{T}}({{\\boldsymbol{\\Sigma}}^{-1}_{p}}\\otimes\\boldsymbol{I}_{t})\nâ‹…\n(\nğ’€\nt\nâˆ’\nğš¿\n(\nğ‘¿\nt\n)\nğœ·\n)\nâˆ’\n1\n2\nğœ·\nT\nğœ½\nâˆ’\n1\nğœ·\n)\n.\n\\displaystyle\\cdot({{\\boldsymbol{Y}}_{t}}-\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t}})\\boldsymbol{\\beta})-\\frac{1}{2}{{\\boldsymbol{\\beta}}^{T}}{\\boldsymbol{\\theta}^{-1}}\\boldsymbol{\\beta}).\nAssuming that the final result obtained by Bayesian inference follows the distribution\nğ’©\nâ€‹\n(\nğ\nw\n,\nğšµ\nw\n)\n\\mathsf{\\mathcal{N}}({{\\boldsymbol{\\mu}}_{w}},{{\\boldsymbol{\\Xi}}_{w}})\n, we can then write\n{\nğ\nw\n=\nğšµ\nw\nâ€‹\nğš¿\nT\nâ€‹\n(\nğ‘¿\nt\n)\nâ€‹\n(\nğšº\np\nâˆ’\n1\nâŠ—\nğ‘°\nt\n)\nâ€‹\nğ’€\nt\n,\nğšµ\nw\nâˆ’\n1\n=\nğš¿\nT\nâ€‹\n(\nğ‘¿\nt\n)\nâ€‹\n(\nğšº\np\nâˆ’\n1\nâŠ—\nğ‘°\nt\n)\nâ€‹\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\n+\nğœ½\nâˆ’\nğŸ\n.\n\\left\\{\\begin{aligned} &{{\\boldsymbol{\\mu}}_{w}}={{\\boldsymbol{\\Xi}}_{w}}{{\\boldsymbol{\\Psi}}^{T}}({{\\boldsymbol{X}}_{t}})({{\\boldsymbol{\\Sigma}}^{-1}_{p}}\\otimes\\boldsymbol{I}_{t}){{\\boldsymbol{Y}}_{t}},\\\\\n&\\boldsymbol{\\Xi}_{w}^{-1}={{\\boldsymbol{\\Psi}}^{T}}({{\\boldsymbol{X}}_{t}})({{\\boldsymbol{\\Sigma}}^{-1}_{p}}\\otimes\\boldsymbol{I}_{t})\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t}})+\\boldsymbol{\\theta^{-1}}.\\end{aligned}\\right.\n(15)\nFurther, since\nğš¿\nâ€‹\n(\nğ‘¿\nt\n)\n=\nğ‘°\nn\ny\nâŠ—\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\n\\boldsymbol{\\Psi}(\\boldsymbol{X}_{t})=\\boldsymbol{I}_{n_{y}}\\otimes\\boldsymbol{\\Psi}_{d}(\\boldsymbol{X}_{d,t})\n, Eq.(\n15\n) can be rewriting as\n{\nğ\nw\n=\nğšµ\nw\nâ€‹\nvec\nâ€‹\n(\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\nâ€‹\nğ’€\nd\n,\nt\nâ€‹\nğšº\np\nâˆ’\n1\n)\n,\nğšµ\nw\nâˆ’\n1\n=\nğšº\np\nâˆ’\n1\nâŠ—\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\nt\n)\n+\nğœ½\nâˆ’\nğŸ\n.\n\\left\\{\\begin{aligned} &{{\\boldsymbol{\\mu}}_{w}}={{\\boldsymbol{\\Xi}}_{w}}\\mathrm{vec}({{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,t}}){{\\boldsymbol{Y}}_{d,t}}{{\\boldsymbol{\\Sigma}}^{-1}_{p}}),\\\\\n&\\boldsymbol{\\Xi}_{w}^{-1}=\\boldsymbol{\\boldsymbol{\\Sigma}}_{p}^{-1}\\otimes{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,t}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,t}})+\\boldsymbol{\\theta^{-1}}.\\end{aligned}\\right.\n(16)\nThe above procedure constitutes the offline training phase. To address the requirement for online parameter updates in DT systems, the entire methodology must be extended to incorporate a recursive online update strategy. Within the framework of this study, the generalized expression for online updating is given by\np\nâ€‹\n(\nğœ·\nt\n+\n1\n|\nğš¿\nâ€‹\n(\nğ‘¿\nt\n+\n1\n)\n,\nğ’€\nt\n+\n1\n)\n\\displaystyle p({{\\boldsymbol{\\beta}}_{t+1}}\\left|\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t+1}}),{{\\boldsymbol{Y}}_{t+1}}\\right.)\n(17)\n=\nğ’©\nâ€‹\n(\nğ\nw\n,\nt\n+\n1\n,\nğšµ\nw\n,\nt\n+\n1\n)\n\\displaystyle=\\mathsf{\\mathcal{N}}({{\\boldsymbol{\\mu}}_{w,t+1}},{{\\boldsymbol{\\Xi}}_{w,t+1}})\n=\nf\nâ€‹\n[\nğ’©\nâ€‹\n(\nğ\nw\n,\nt\n,\nğšµ\nw\n,\nt\n)\n,\nğš¿\nâ€‹\n(\nğ’™\nt\n+\n1\n)\n,\ny\nt\n+\n1\n]\n.\n\\displaystyle=f[\\mathsf{\\mathcal{N}}({{\\boldsymbol{\\mu}}_{w,t}},{{\\boldsymbol{\\Xi}}_{w,t}}),\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{t+1}}),{{y}_{t+1}}].\nConsidering Eq.(\n12\n)-(\n14\n),\np\nâ€‹\n(\nğœ·\nt\n+\n1\n|\nğš¿\nâ€‹\n(\nğ‘¿\nt\n+\n1\n)\n,\nğ’€\nt\n+\n1\n)\n\\displaystyle p({{\\boldsymbol{\\beta}}_{t+1}}\\left|\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{t+1}}),{{\\boldsymbol{Y}}_{t+1}}\\right.)\n(18)\nâˆ\nâˆ\nj\n=\n1\nt\nğ’©\nâ€‹\n(\nğ’š\nj\n;\nğš¿\nâ€‹\n(\nğ’™\nj\n)\nâ€‹\nğœ·\n,\nğšº\np\n)\nâ‹…\nğ’©\nâ€‹\n(\nğŸ\n,\nğœ½\n)\n\\displaystyle\\propto\\prod\\limits_{j=1}^{t}{\\mathsf{\\mathcal{N}}({{\\boldsymbol{y}}_{j}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}})\\boldsymbol{\\beta},{\\boldsymbol{\\Sigma}_{p}})}\\cdot\\mathsf{\\mathcal{N}}(\\boldsymbol{0},\\boldsymbol{\\theta})\nâ‹…\nğ’©\nâ€‹\n(\nğ’š\nt\n+\n1\n;\nğš¿\nâ€‹\n(\nğ’™\nt\n+\n1\n)\nâ€‹\nğœ·\n,\nğšº\np\n)\n\\displaystyle\\cdot\\mathsf{\\mathcal{N}}({{\\boldsymbol{y}}_{t+1}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{t+1}})\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}_{p})\nâˆ\nğ’©\nâ€‹\n(\nğ\nw\n,\nt\n,\nğšµ\nw\n,\nt\n)\nâ‹…\nğ’©\nâ€‹\n(\nğ’š\nt\n+\n1\n;\nğš¿\nâ€‹\n(\nğ’™\nt\n+\n1\n)\nâ€‹\nğœ·\n,\nÏƒ\n2\n)\n\\displaystyle\\propto\\mathsf{\\mathcal{N}}({{\\boldsymbol{\\mu}}_{w,t}},{{\\boldsymbol{\\Xi}}_{w,t}})\\cdot\\mathsf{\\mathcal{N}}({{\\boldsymbol{y}}_{t+1}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{t+1}})\\boldsymbol{\\beta},{{\\sigma}^{2}})\nâˆ\nğ’©\nâ€‹\n(\nğ\nw\n,\nt\n+\n1\n,\nğšµ\nw\n,\nt\n+\n1\n)\n.\n\\displaystyle\\propto\\mathsf{\\mathcal{N}}({{\\boldsymbol{\\mu}}_{w,t+1}},{{\\boldsymbol{\\Xi}}_{w,t+1}}).\nwhere\n{\nğ\nw\n,\nt\n+\n1\n=\nğšµ\nw\n,\nt\n+\n1\n[\nğšµ\nw\n,\nt\nâˆ’\n1\nğ\nw\n,\nt\n+\nvec\n(\nğš¿\nd\nT\n(\nğ’™\nd\n,\nt\n+\n1\n)\nğ’š\nd\n,\nt\n+\n1\nğšº\np\nâˆ’\n1\n)\n]\n,\nğšµ\nw\n,\nt\n+\n1\nâˆ’\n1\n=\nğšµ\nw\n,\nt\nâˆ’\n1\n+\nğšº\np\nâˆ’\n1\nâŠ—\n(\nğš¿\nd\nT\nâ€‹\n(\nğ’™\nd\n,\nt\n+\n1\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ’™\nd\n,\nt\n+\n1\n)\n)\n.\n\\left\\{\\begin{aligned} &{{\\boldsymbol{\\mu}}_{w,t+1}}={{\\boldsymbol{\\Xi}}_{w,t+1}}[\\boldsymbol{\\Xi}_{w,t}^{-1}{{\\boldsymbol{\\mu}}_{w,t}}\\\\\n&+\\mathrm{vec}({{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{x}}_{d,t+1}})\\boldsymbol{y}_{d,t+1}\\boldsymbol{\\Sigma}_{p}^{-1})],\\\\\n&\\boldsymbol{\\Xi}_{w,t+1}^{-1}=\\boldsymbol{\\Xi}_{w,t}^{-1}+{\\boldsymbol{\\Sigma}^{-1}_{p}}\\\\\n&\\otimes({{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{x}}_{d,t+1}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{x}}_{d,t+1})}).\\end{aligned}\\right.\n(19)\nIn recursive processes, it is typically desirable for new data to exert greater influence on the parameter posteriors, while the contribution of historical data gradually diminishes over time. Consequently, a forgetting factor must be introduced into Eq.(\n19\n) to control the weighting of historical data. Unlike conventional recursive algorithms, this method operates within a Bayesian inference framework where simply multiplying coefficients cannot achieve historical data forgetting for posterior inference. In this framework, the forgetting factor\nÏ…\n\\upsilon\nfunctions analogously to a sliding window length. The corresponding target posterior distribution\np\nâ€‹\n(\nğœ·\nt\n+\n1\n|\nğš¿\nâ€‹\n(\nğ‘¿\n(\nt\nâˆ’\nÏ…\n+\n1\n:\nt\n+\n1\n)\n)\n,\nğ’€\n(\nt\nâˆ’\nÏ…\n+\n1\n:\nt\n+\n1\n)\n)\np({{\\boldsymbol{\\beta}}_{t+1}}\\left|\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{(t-\\upsilon+1:t+1)}}),{{\\boldsymbol{Y}}_{(t-\\upsilon+1:t+1)}}\\right.)\nis given by Eq.(\n20\n). Here the subscript\n(\nt\nâˆ’\nÏ…\n+\n1\n:\nt\n+\n1\n)\n(t-\\upsilon+1:t+1)\ndenotes data from time\n(\nt\nâˆ’\nÏ…\n+\n1\n)\n(t-\\upsilon+1)\nto\n(\nt\n+\n1\n)\n(t+1)\n. The symbol â€œ/â€ represents division in probability computation.\np\nâ€‹\n(\nğœ·\nt\n+\n1\n|\nğš¿\nâ€‹\n(\nğ‘¿\n(\nt\nâˆ’\nÏ…\n+\n1\n:\nt\n+\n1\n)\n)\n,\nğ’€\n(\nt\nâˆ’\nÏ…\n+\n1\n:\nt\n+\n1\n)\n)\n\\displaystyle p({{\\boldsymbol{\\beta}}_{t+1}}\\left|\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{(t-\\upsilon+1:t+1)}}),{{\\boldsymbol{Y}}_{(t-\\upsilon+1:t+1)}}\\right.)\n(20)\nâˆ\nâˆ\nj\n=\n1\nt\n+\n1\nğ’©\nâ€‹\n(\nğ’š\nj\n;\nğš¿\nâ€‹\n(\nğ’™\nj\n)\nâ€‹\nğœ·\nt\n,\nğšº\np\n)\nâ‹…\nğ’©\nâ€‹\n(\nğŸ\n,\nğœ½\n)\n\\displaystyle\\propto\\prod\\limits_{j=1}^{t+1}{\\mathsf{\\mathcal{N}}({{\\boldsymbol{y}}_{j}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{j}}){{\\boldsymbol{\\beta}}_{t}},{{\\boldsymbol{\\Sigma}_{p}}})}\\cdot\\mathsf{\\mathcal{N}}(\\boldsymbol{0},{{\\boldsymbol{\\theta}}})\n/\nâˆ\nk\n=\n1\nt\nâˆ’\nÏ…\nğ’©\n(\nğ’š\nk\n;\nğš¿\n(\nğ’™\nk\n)\nğœ·\nt\n,\nğšº\np\n)\n\\displaystyle/\\prod\\limits_{k=1}^{t-\\upsilon}{\\mathsf{\\mathcal{N}}({{\\boldsymbol{y}}_{k}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{k}}){{\\boldsymbol{\\beta}}_{t}},{{\\boldsymbol{\\Sigma}_{p}}})}\nâˆ\nğ’©\nâ€‹\n(\nğ\nw\n,\nt\n,\nğšµ\nw\n,\nt\n)\nâ‹…\nğ’©\nâ€‹\n(\nğ’š\nt\n+\n1\n;\nğš¿\nâ€‹\n(\nğ’™\nt\n+\n1\n)\nâ€‹\nğœ·\nt\n,\nğšº\np\n)\n\\displaystyle\\propto\\mathsf{\\mathcal{N}}({{\\boldsymbol{\\mu}}_{w,t}},{{\\boldsymbol{\\Xi}}_{w,t}})\\cdot\\mathsf{\\mathcal{N}}({{\\boldsymbol{y}}_{t+1}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{t+1}}){{\\boldsymbol{\\beta}}_{t}},{{\\boldsymbol{\\Sigma}_{p}}})\n/\nâˆ\nk\n=\n1\nt\nâˆ’\nÏ…\nğ’©\n(\nğ’š\nk\n;\nğš¿\n(\nğ’™\nk\n)\nğœ·\nt\n,\nğšº\np\n)\n\\displaystyle/\\prod\\limits_{k=1}^{t-\\upsilon}{\\mathsf{\\mathcal{N}}({{\\boldsymbol{y}}_{k}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{k}}){{\\boldsymbol{\\beta}}_{t}},{{\\boldsymbol{\\Sigma}_{p}}})}\nâˆ\nğ’©\nÏ…\nâ€‹\n(\nğ\nw\n,\nt\n+\n1\nÏ…\n,\nğšµ\nw\n,\nt\n+\n1\nÏ…\n)\n,\n\\displaystyle\\propto{{\\mathsf{\\mathcal{N}}}_{\\upsilon}}(\\boldsymbol{\\mu}_{w,t+1}^{\\upsilon},\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}),\nwhere\n{\nğ\nw\n,\nt\n+\n1\nÏ…\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\n{\nğšµ\nw\n,\nt\nâˆ’\n1\nğ\nw\n,\nt\n+\nvec\n[\n(\nğš¿\nd\nT\n(\nğ’™\nd\n,\nt\n+\n1\n)\nğ’š\nd\n,\nt\n+\n1\nâˆ’\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\nt\nâˆ’\nÏ…\n)\nğ’€\nd\n,\nt\nâˆ’\nÏ…\n)\nğšº\np\nâˆ’\n1\n]\n}\n,\n(\nğšµ\nw\n,\nt\n+\n1\nÏ…\n)\nâˆ’\n1\n=\nğšµ\nw\n,\nt\nâˆ’\n1\n+\nğšº\np\nâˆ’\n1\nâŠ—\n[\nğš¿\nd\nT\n(\nğ’™\nd\n,\nt\n+\n1\n)\nğš¿\nd\n(\nğ’™\nd\n,\nt\n+\n1\n)\nâˆ’\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\nt\nâˆ’\nÏ…\n)\nğš¿\nd\n(\nğ‘¿\nd\n,\nt\nâˆ’\nÏ…\n)\n]\n.\n\\left\\{\\begin{aligned} &\\boldsymbol{\\mu}_{w,t+1}^{\\upsilon}=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\{\\boldsymbol{\\Xi}_{w,t}^{-1}{{\\boldsymbol{\\mu}}_{w,t}}\\\\\n&+\\mathrm{vec}[{{(\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{x}}_{d,t+1}}){\\boldsymbol{y}_{d,t+1}}\\\\\n&-{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,t-\\upsilon}}){{\\boldsymbol{Y}}_{d,t-\\upsilon}})\\boldsymbol{\\Sigma}_{p}^{-1}]\\},\\\\\n&{{(\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon})}^{-1}}=\\boldsymbol{\\Xi}_{w,t}^{-1}+{\\boldsymbol{\\Sigma}^{-1}_{p}}\\\\\n&\\otimes[{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{x}}_{d,t+1}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{x}}_{d,t+1}})\\\\\n&-{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,t-\\upsilon}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,t-\\upsilon}})].\\end{aligned}\\right.\n(21)\nRemark 2\n.\nThe division operation â€œ/â€ is not a primitive operation in probabilistic theory, implying that probability spaces are not closed under the division operation. The following subsection will discuss the establishment conditions for â€œ/â€ and further provide recursive rules and parameter design guidelines.\nIndeed, Eq.(\n21\n) does not constitute a standard recursive formulation. This is because the terms\nğšµ\nw\n,\nt\nâˆ’\n1\n\\boldsymbol{\\Xi}_{w,t}^{-1}\n,\nğ\nw\n,\nt\n{{\\boldsymbol{\\mu}}_{w,t}}\n,\nğ‘¿\nd\n,\nt\nâˆ’\nÏ…\n{{\\boldsymbol{X}}_{d,t-\\upsilon}}\nand\nğ’€\nd\n,\nt\nâˆ’\nÏ…\n{{\\boldsymbol{Y}}_{d,t-\\upsilon}}\nmust be recomputed from time zero during each update, contradicting the core objective of recursive efficiency. For a system with an update period of\nÎ¹\n\\iota\n, Eq.(\n20\n) is extended as\np\nâ€‹\n(\nğœ·\nt\n+\nÎ¹\n|\nğš¿\nâ€‹\n(\nğ‘¿\n(\nt\nâˆ’\nÏ…\n+\nÎ¹\n:\nt\n+\nÎ¹\n)\n)\n,\nğ’€\n(\nt\nâˆ’\nÏ…\n+\nÎ¹\n:\nt\n+\nÎ¹\n)\n)\n\\displaystyle p({{\\boldsymbol{\\beta}}_{t+\\iota}}\\left|\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{(t-\\upsilon+\\iota:t+\\iota)}}),{{\\boldsymbol{Y}}_{(t-\\upsilon+\\iota:t+\\iota)}}\\right.)\n(22)\nâˆ\nğ’©\nÏ…\nâ€‹\n(\nğ\nw\n,\nt\nÏ…\n,\nğšµ\nw\n,\nt\nÏ…\n)\nâ‹…\nâˆ\nk\n=\nt\n+\n1\nt\n+\nÎ¹\nğ’©\nâ€‹\n(\ny\nk\n;\nğš¿\nâ€‹\n(\nğ’™\nk\n)\nâ€‹\nğœ·\nt\n,\nğšº\np\n)\n\\displaystyle\\propto{{\\mathsf{\\mathcal{N}}}_{\\upsilon}}(\\boldsymbol{\\mu}_{w,t}^{\\upsilon},\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})\\cdot\\prod\\limits_{k=t+1}^{t+\\iota}{\\mathsf{\\mathcal{N}}({{y}_{k}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{k}}){{\\boldsymbol{\\beta}}_{t}},{{\\boldsymbol{\\Sigma}_{p}}})}\n/\nâˆ\nk\n=\nt\nâˆ’\nÏ…\nt\nâˆ’\nÏ…\n+\nÎ¹\nâˆ’\n1\nğ’©\n(\ny\nk\n;\nğš¿\n(\nğ’™\nk\n)\nğœ·\nt\n,\nğšº\np\n)\n\\displaystyle/\\prod\\limits_{k=t-\\upsilon}^{t-\\upsilon+\\iota-1}{\\mathsf{\\mathcal{N}}({{y}_{k}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{k}}){{\\boldsymbol{\\beta}}_{t}},{{\\boldsymbol{\\Sigma}_{p}}})}\nâˆ\nğ’©\nÏ…\nâ€‹\n(\nğ\nw\n,\nt\n+\n1\nÏ…\n,\nğšµ\nw\n,\nt\n+\n1\nÏ…\n)\n.\n\\displaystyle\\propto{{\\mathsf{\\mathcal{N}}}_{\\upsilon}}(\\boldsymbol{\\mu}_{w,t+1}^{\\upsilon},\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}).\nThus, Eq.(\n21\n) is updated as\n{\nğ\nw\n,\nt\n+\n1\nÏ…\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\n{\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nğ\nw\n,\nt\nÏ…\n+\nvec\n[\n(\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nğ’€\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nÎ¹\nâˆ’\n1\n)\n)\nâ‹…\nğ’€\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nÎ¹\nâˆ’\n1\n)\n)\nğšº\np\nâˆ’\n1\n]\n}\n,\n(\nğšµ\nw\n,\nt\n+\n1\nÏ…\n)\nâˆ’\n1\n=\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\n+\nğšº\np\nâˆ’\n1\nâŠ—\n[\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ‹…\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nÎ¹\nâˆ’\n1\n)\n)\nâ‹…\nğš¿\nd\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nÎ¹\nâˆ’\n1\n)\n)\n]\n.\n\\left\\{\\begin{aligned} &\\boldsymbol{\\mu}_{w,t+1}^{\\upsilon}=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\{{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\\boldsymbol{\\mu}_{w,t}^{\\upsilon}\\\\\n&+\\mathrm{vec}[{{(\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{Y}}_{d,(t+1:t+\\iota)}}\\\\\n&-{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+\\iota-1)}})\\\\\n&\\cdot{{\\boldsymbol{Y}}_{d,(t-\\upsilon:t-\\upsilon+\\iota-1)}})\\boldsymbol{\\Sigma}_{p}^{-1}]\\},\\\\\n&{{(\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon})}^{-1}}={{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\\\\\n&+\\boldsymbol{\\Sigma}_{p}^{-1}\\otimes[{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\\\\\n&\\cdot\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\\\\\n&-{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+\\iota-1)}})\\\\\n&\\cdot\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+\\iota-1)}})].\\end{aligned}\\right.\n(23)\nThe central innovation of the proposed BRSL framework lies in its integration of a recursive structure with traditional probabilistic methods, notably through a novel probabilistic recursive scheme that incorporates a forgetting factor. However, unlike traditional point-estimation methods, the practical implementation of Bayesian recursion is subject to specific constraints. The following subsection details these recursive conditions.\n3.2\nRecursive condition of BRSL\nLet the initial time be\nt\n0\nt_{0}\n, window width be\nÏ…\n\\upsilon\n, number of new samples per update be\nÎ¹\n\\iota\n, and historical information forgotten per update be\nk\nk\n.\nTheorem 1\n.\nGiven Gaussian distributions\nğ’©\nâ€‹\n(\nÎ¼\n1\n,\nÏƒ\n1\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{1}},\\sigma_{1}^{2}\\right)\n,\nğ’©\nâ€‹\n(\nÎ¼\n2\n,\nÏƒ\n2\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{2}},\\sigma_{2}^{2}\\right)\n,\nğ’©\nâ€‹\n(\nÎ¼\n3\n,\nÏƒ\n3\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{3}},\\sigma_{3}^{2}\\right)\nsatisfying\nğ’©\nâ€‹\n(\nÎ¼\n1\n,\nÏƒ\n1\n2\n)\nâˆ\nğ’©\nâ€‹\n(\nÎ¼\n2\n,\nÏƒ\n2\n2\n)\nâ€‹\nğ’©\nâ€‹\n(\nÎ¼\n3\n,\nÏƒ\n3\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{1}},\\sigma_{1}^{2}\\right)\\propto\\mathsf{\\mathcal{N}}\\left({{\\mu}_{2}},\\sigma_{2}^{2}\\right)\\mathsf{\\mathcal{N}}\\left({{\\mu}_{3}},\\sigma_{3}^{2}\\right)\n(24)\n(i.e.,\nğ’©\nâ€‹\n(\nÎ¼\n1\n,\nÏƒ\n1\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{1}},\\sigma_{1}^{2}\\right)\nis the normalized product of\nğ’©\nâ€‹\n(\nÎ¼\n2\n,\nÏƒ\n2\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{2}},\\sigma_{2}^{2}\\right)\nand\nğ’©\nâ€‹\n(\nÎ¼\n3\n,\nÏƒ\n3\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{3}},\\sigma_{3}^{2}\\right)\n). If the precision condition holds\nÏƒ\n1\n2\n<\nÏƒ\n2\n2\n,\n\\sigma_{1}^{2}<\\sigma_{2}^{2},\n(25)\nthere exists a unique Gaussian distribution\nğ’©\nâ€‹\n(\nÎ¼\n3\n,\nÏƒ\n3\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{3}},\\sigma_{3}^{2}\\right)\n, with parameters given by\n{\nÏƒ\n3\nâˆ’\n2\n=\nÏƒ\n1\nâˆ’\n2\nâˆ’\nÏƒ\n2\nâˆ’\n2\n,\nÎ¼\n3\n=\nÎ¼\n1\nâ€‹\nÏƒ\n1\nâˆ’\n2\nâˆ’\nÎ¼\n2\nâ€‹\nÏƒ\n2\nâˆ’\n2\nÏƒ\n1\nâˆ’\n2\nâˆ’\nÏƒ\n2\nâˆ’\n2\n.\n\\left\\{\\begin{aligned} &\\sigma_{3}^{-2}=\\sigma_{1}^{-2}-\\sigma_{2}^{-2},\\\\\n&{{\\mu}_{3}}=\\frac{{{\\mu}_{1}}\\sigma_{1}^{-2}-{{\\mu}_{2}}\\sigma_{2}^{-2}}{\\sigma_{1}^{-2}-\\sigma_{2}^{-2}}.\\\\\n\\end{aligned}\\right.\n(26)\nDefine â€œ/â€ as\nğ’©\nâ€‹\n(\nÎ¼\n3\n,\nÏƒ\n3\n2\n)\n=\nğ’©\nâ€‹\n(\nÎ¼\n1\n,\nÏƒ\n1\n2\n)\n/\nğ’©\nâ€‹\n(\nÎ¼\n2\n,\nÏƒ\n2\n2\n)\n\\mathsf{\\mathcal{N}}\\left({{\\mu}_{3}},\\sigma_{3}^{2}\\right)=\\mathsf{\\mathcal{N}}\\left({{\\mu}_{1}},\\sigma_{1}^{2}\\right)/\\mathsf{\\mathcal{N}}\\left({{\\mu}_{2}},\\sigma_{2}^{2}\\right)\n.\nTheorem 2\n.\nLet\nğ€\nâˆˆ\nâ„\nn\nÃ—\nm\n\\boldsymbol{A}\\in{{\\mathbb{R}}^{n\\times m}}\n,\nğ\nâˆˆ\nâ„\np\nÃ—\nm\n\\boldsymbol{B}\\in{{\\mathbb{R}}^{p\\times m}}\n, and\nğ‚\nâˆˆ\nâ„\nl\nÃ—\nm\n\\boldsymbol{C}\\in{{\\mathbb{R}}^{l\\times m}}\nbe arbitrary matrices, with\nn\n=\np\n+\nl\nn=p+l\n. If\nA\nA\nand\nB\nB\nsatisfy\nğ‘¨\n=\n[\nğ‘©\nğ‘ª\n]\n,\n\\boldsymbol{A}=\\left[\\begin{matrix}\\boldsymbol{B}\\\\\n\\boldsymbol{C}\\\\\n\\end{matrix}\\right],\n(27)\nğ‘¨\nT\nâ€‹\nğ‘¨\nâˆ’\nğ‘©\nT\nâ€‹\nğ‘©\n{{\\boldsymbol{A}}^{T}}\\boldsymbol{A}-{{\\boldsymbol{B}}^{T}}\\boldsymbol{B}\nis necessarily positive semi-definite.\nSince\nğ’©\n(\nğ’€\n(\nt\n0\n:\nt\n0\n+\nÏ…\n)\n;\nğš¿\n(\nğ‘¿\n(\nt\n0\n:\nt\n0\n+\nÏ…\n)\n)\nğœ·\n(\nt\n0\n:\nt\n0\n+\nÏ…\n)\n,\nğšº\np\nâŠ—\n\\mathsf{\\mathcal{N}}(\\boldsymbol{Y}_{\\left({{t}_{0}}:{{t}_{0}}+\\upsilon\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left({{t}_{0}}:{{t}_{0}}+\\upsilon\\right)}\\right){{\\boldsymbol{\\beta}}_{({{t}_{0}}:{{t}_{0}}+\\upsilon)}},\\boldsymbol{\\Sigma}_{p}\\otimes\nğ‘°\n(\nt\n0\n:\nt\n0\n+\nÏ…\n)\n)\n\\boldsymbol{I}_{({{t}_{0}}:{{t}_{0}+\\upsilon})})\n, we first address the initialization at\nt\n0\n=\n1\nt_{0}=1\n, the following result is obtained:\nğ’©\nâ€‹\n(\nğ’€\n(\n1\n:\nÏ…\n)\n;\nğš¿\nâ€‹\n(\nğ‘¿\n(\n1\n:\nÏ…\n)\n)\nâ€‹\nğœ·\n(\n1\n:\nÏ…\n)\n,\nğšº\np\nâŠ—\nğ‘°\n(\n1\n:\nÏ…\n)\n)\n\\displaystyle\\mathsf{\\mathcal{N}}\\left(\\boldsymbol{Y}_{\\left(1:\\upsilon\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left(1:\\upsilon\\right)}\\right){{\\boldsymbol{\\beta}}_{(1:\\upsilon)}},{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(1:\\upsilon)}\\right)\n(28)\n=\nâˆ\nj\n=\n1\nk\nğ’©\nâ€‹\n(\ny\nj\n;\nğš¿\nâ€‹\n(\nğ’™\nj\n)\nâ€‹\nğœ·\nj\n,\nğšº\np\n)\n\\displaystyle=\\prod\\limits_{j=1}^{k}{\\mathsf{\\mathcal{N}}\\left({{y}_{j}};\\boldsymbol{\\Psi}\\left({{\\boldsymbol{x}}_{j}}\\right){{\\boldsymbol{\\beta}}_{j}},{\\boldsymbol{\\Sigma}_{p}}\\right)}\nâ‹…\nâˆ\np\n=\nk\n+\n1\nÏ…\nğ’©\n(\ny\np\n;\nğš¿\n(\nğ’™\np\n)\nğœ·\np\n,\nğšº\np\n)\nğ’©\n(\nğŸ\n,\nğœ½\n)\n.\n\\displaystyle\\cdot\\prod\\limits_{p=k+1}^{\\upsilon}{\\mathsf{\\mathcal{N}}\\left({{y}_{p}};\\boldsymbol{\\Psi}\\left({{\\boldsymbol{x}}_{p}}\\right){{\\boldsymbol{\\beta}}_{p}},\\boldsymbol{\\Sigma}_{p}\\right)}\\mathsf{\\mathcal{N}}\\left(\\boldsymbol{0},\\boldsymbol{\\theta}\\right).\nTo achieve unbiased recovery of\nâˆ\np\n=\nk\n+\n1\nÏ…\nğ’©\nâ€‹\n(\ny\np\n;\nğš¿\nâ€‹\n(\nğ’™\np\n)\nâ€‹\nğœ·\np\n,\nğšº\np\n)\nâ€‹\nğ’©\nâ€‹\n(\nğŸ\n,\nğœ½\n)\n\\prod\\limits_{p=k+1}^{\\upsilon}\\mathsf{\\mathcal{N}}({{y}_{p}};\\boldsymbol{\\Psi}({{\\boldsymbol{x}}_{p}}){{\\boldsymbol{\\beta}}_{p}},\\\\\n{\\boldsymbol{\\Sigma}_{p}})\\mathsf{\\mathcal{N}}(\\boldsymbol{0},\\boldsymbol{\\theta})\naccording to Theorem\n1\n, the following conditions must hold simultaneously, i.e.\nğšº\np\nâˆ’\n1\nâŠ—\n[\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\n1\n:\nÏ…\n)\n)\nğš¿\nd\n(\nğ‘¿\nd\n,\n(\n1\n:\nÏ…\n)\n)\n\\displaystyle\\boldsymbol{\\boldsymbol{\\Sigma}}_{p}^{-1}\\otimes[{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(1:\\upsilon)}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(1:\\upsilon)}})\n(29)\nâˆ’\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\n1\n:\nk\n)\n)\nğš¿\nd\n(\nğ‘¿\nd\n,\n(\n1\n:\nk\n)\n)\n]\n+\nğœ½\nâˆ’\nğŸ\nâ‰»\nğŸ\n.\n\\displaystyle-{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(1:k)}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(1:k)}})]+\\boldsymbol{\\theta^{-1}}\\succ\\boldsymbol{0}.\nBecause the matrix\nğœ½\n\\boldsymbol{\\theta}\n, defined by the horseshoe prior, is positive definite, the following condition must hold\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\n1\n:\nÏ…\n)\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\n1\n:\nÏ…\n)\n)\n\\displaystyle{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(1:\\upsilon)}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(1:\\upsilon)}})\n(30)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\n1\n:\nk\n)\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\n1\n:\nk\n)\n)\nâ‰»\nğŸ\n.\n\\displaystyle-{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(1:k)}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(1:k)}})\\succ\\boldsymbol{0}.\nBy using Theorem\n2\n, it is straightforward to implement, we have\nğ’©\n(\nğ’€\n(\nk\n+\n1\n:\nÏ…\n)\n;\nğš¿\n(\nğ‘¿\n(\nk\n+\n1\n:\nÏ…\n)\n)\nğœ·\n(\nk\n+\n1\n:\nÏ…\n)\n\\displaystyle\\mathsf{\\mathcal{N}}(\\boldsymbol{Y}_{\\left(k+1:\\upsilon\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left(k+1:\\upsilon\\right)}\\right){{\\boldsymbol{\\beta}}_{(k+1:\\upsilon)}}\n(31)\n,\nğšº\np\nâŠ—\nğ‘°\n(\nk\n+\n1\n:\nÏ…\n)\n)\n\\displaystyle,{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(k+1:\\upsilon)})\n=\nğ’©\nâ€‹\n(\nğ’€\n(\n1\n:\nÏ…\n)\n;\nğš¿\nâ€‹\n(\nğ‘¿\n(\n1\n:\nÏ…\n)\n)\nâ€‹\nğœ·\n(\n1\n:\nÏ…\n)\n,\nğšº\np\nâŠ—\nğ‘°\n(\n1\n:\nÏ…\n)\n)\n\\displaystyle=\\mathsf{\\mathcal{N}}(\\boldsymbol{Y}_{\\left(1:\\upsilon\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left(1:\\upsilon\\right)}\\right){{\\boldsymbol{\\beta}}_{(1:\\upsilon)}},{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(1:\\upsilon)})\n/\nğ’©\nâ€‹\n(\nğ’€\n(\n1\n:\nk\n)\n;\nğš¿\nâ€‹\n(\nğ‘¿\n(\n1\n:\nk\n)\n)\nâ€‹\nğœ·\n(\n1\n:\nk\n)\n,\nğšº\np\nâŠ—\nğ‘°\n(\n1\n:\nk\n)\n)\n,\n\\displaystyle/\\mathsf{\\mathcal{N}}\\left(\\boldsymbol{Y}_{\\left(1:k\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left(1:k\\right)}\\right){{\\boldsymbol{\\beta}}_{(1:k)}},{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(1:k)}\\right),\nand\n\\displaystyle\\mathrm{and}\nğ’©\n(\nğ’€\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n;\nğš¿\n(\nğ‘¿\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\nğœ·\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n\\displaystyle\\mathsf{\\mathcal{N}}(\\boldsymbol{Y}_{\\left(k+1:\\upsilon+\\iota\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left(k+1:\\upsilon+\\iota\\right)}\\right){{\\boldsymbol{\\beta}}_{(k+1:\\upsilon+\\iota)}}\n,\nğšº\np\nâŠ—\nğ‘°\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\n\\displaystyle,{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(k+1:\\upsilon+\\iota)})\n=\nğ’©\n(\nğ’€\n(\nk\n+\n1\n:\nÏ…\n)\n;\nğš¿\n(\nğ‘¿\n(\nk\n+\n1\n:\nÏ…\n)\n)\nğœ·\n(\nk\n+\n1\n:\nÏ…\n)\n\\displaystyle=\\mathsf{\\mathcal{N}}(\\boldsymbol{Y}_{\\left(k+1:\\upsilon\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left(k+1:\\upsilon\\right)}\\right){{\\boldsymbol{\\beta}}_{(k+1:\\upsilon)}}\n,\nğšº\np\nâŠ—\nğ‘°\n(\nk\n+\n1\n:\nÏ…\n)\n)\nğ’©\n(\nğ’€\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n;\nğš¿\n(\nğ‘¿\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\n\\displaystyle,{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(k+1:\\upsilon)})\\mathsf{\\mathcal{N}}(\\boldsymbol{Y}_{\\left(\\upsilon+1:\\upsilon+\\iota\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left(\\upsilon+1:\\upsilon+\\iota\\right)}\\right)\nğœ·\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n,\nğšº\np\nâŠ—\nğ‘°\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\n.\n\\displaystyle{{\\boldsymbol{\\beta}}_{(\\upsilon+1:\\upsilon+\\iota)}},{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(\\upsilon+1:\\upsilon+\\iota)}).\nCombining the preceding equations, we obtain the following constraint, which is equivalent to Eq.(\n30\n),\nğ’©\n(\nğ’€\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n;\nğš¿\n(\nğ‘¿\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\nğœ·\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n\\displaystyle\\mathsf{\\mathcal{N}}({{\\boldsymbol{Y}}_{\\left(k+1:\\upsilon+\\iota\\right)}};\\boldsymbol{\\Psi}\\left({{\\boldsymbol{X}}_{\\left(k+1:\\upsilon+\\iota\\right)}}\\right){{\\boldsymbol{\\beta}}_{(k+1:\\upsilon+\\iota)}}\n(32)\n,\nğšº\np\nâŠ—\nğ‘°\n(\nk\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\n\\displaystyle,{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(k+1:\\upsilon+\\iota)})\n=\nğ’©\nâ€‹\n(\nğ’€\n(\n1\n:\nÏ…\n)\n;\nğš¿\nâ€‹\n(\nğ‘¿\n(\n1\n:\nÏ…\n)\n)\nâ€‹\nğœ·\n(\n1\n:\nÏ…\n)\n,\nğšº\np\nâŠ—\nğ‘°\n(\n1\n:\nÏ…\n)\n)\n\\displaystyle=\\mathsf{\\mathcal{N}}({{\\boldsymbol{Y}}_{\\left(1:\\upsilon\\right)}};\\boldsymbol{\\Psi}\\left({{\\boldsymbol{X}}_{\\left(1:\\upsilon\\right)}}\\right){{\\boldsymbol{\\beta}}_{(1:\\upsilon)}},{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(1:\\upsilon)})\n/\nğ’©\nâ€‹\n(\nğ’€\n(\n1\n:\nk\n)\n;\nğš¿\nâ€‹\n(\nğ‘¿\n(\n1\n:\nk\n)\n)\nâ€‹\nğœ·\n(\n1\n:\nk\n)\n,\nğšº\np\nâŠ—\nğ‘°\n(\n1\n:\nk\n)\n)\n\\displaystyle/\\mathsf{\\mathcal{N}}({{\\boldsymbol{Y}}_{\\left(1:k\\right)}};\\boldsymbol{\\Psi}\\left({{\\boldsymbol{X}}_{\\left(1:k\\right)}}\\right){{\\boldsymbol{\\beta}}_{(1:k)}},{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(1:k)})\nâ‹…\nğ’©\n(\nğ’€\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n;\nğš¿\n(\nğ‘¿\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\nğœ·\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n\\displaystyle\\cdot\\mathsf{\\mathcal{N}}({{\\boldsymbol{Y}}_{\\left(\\upsilon+1:\\upsilon+\\iota\\right)}};\\boldsymbol{\\Psi}\\left({{\\boldsymbol{X}}_{\\left(\\upsilon+1:\\upsilon+\\iota\\right)}}\\right){{\\boldsymbol{\\beta}}_{(\\upsilon+1:\\upsilon+\\iota)}}\n,\nğšº\np\nâŠ—\nğ‘°\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\n,\n\\displaystyle,{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(\\upsilon+1:\\upsilon+\\iota)}),\ni.e.\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nÏ…\n+\n1\n:\nÏ…\n+\nÎ¹\n)\n)\n\\displaystyle{{\\boldsymbol{\\Psi}}^{T}_{d}}\\left({{\\boldsymbol{X}}_{d,\\left(\\upsilon+1:\\upsilon+\\iota\\right)}}\\right)\\boldsymbol{\\Psi}_{d}\\left({{\\boldsymbol{X}}_{d,\\left(\\upsilon+1:\\upsilon+\\iota\\right)}}\\right)\n(33)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\n1\n:\nk\n)\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\n1\n:\nk\n)\n)\nâ‰»\nğŸ\n.\n\\displaystyle-{{\\boldsymbol{\\Psi}}^{T}_{d}}\\left({{\\boldsymbol{X}}_{d,\\left(1:k\\right)}}\\right)\\boldsymbol{\\Psi}_{d}\\left({{\\boldsymbol{X}}_{d,\\left(1:k\\right)}}\\right)\\succ\\boldsymbol{0}.\nPrecisely, the equivalent constraint holds universally at any time\nt\nt\n. Consider the case when\nt\n0\n>\nÏ…\n{{t}_{0}}>\\upsilon\n:\nğ’©\n(\nğ’€\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\n)\n;\nğš¿\n(\nğ‘¿\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\n)\n)\n\\displaystyle\\mathsf{\\mathcal{N}}(\\boldsymbol{Y}_{\\left({{t}_{0}}-\\upsilon+1:{{t}_{0}}\\right)};\\boldsymbol{\\Psi}\\left(\\boldsymbol{X}_{\\left({{t}_{0}}-\\upsilon+1:{{t}_{0}}\\right)}\\right)\n(34)\nâ‹…\nğœ·\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\n)\n,\nğšº\np\nâŠ—\nğ‘°\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\n)\n)\n\\displaystyle\\cdot{{\\boldsymbol{\\beta}}_{({{t}_{0}}-\\upsilon+1:{{t}_{0}})}},{\\boldsymbol{\\Sigma}_{p}}\\otimes\\boldsymbol{I}_{(t_{0}-\\upsilon+1:t_{0})})\n=\nâˆ\nj\n=\nt\n0\nâˆ’\nÏ…\n+\n1\nt\n0\nâˆ’\nÏ…\n+\nk\nğ’©\nâ€‹\n(\ny\nj\n;\nğš¿\nâ€‹\n(\nx\nj\n)\nâ€‹\nğœ·\nj\n,\nğšº\np\n)\n\\displaystyle=\\prod\\limits_{j={{t}_{0}}-\\upsilon+1}^{{{t}_{0}}-\\upsilon+k}{\\mathsf{\\mathcal{N}}\\left({{y}_{j}};\\boldsymbol{\\Psi}\\left({{x}_{j}}\\right){{\\boldsymbol{\\beta}}_{j}},\\boldsymbol{\\Sigma}_{p}\\right)}\nâ‹…\nâˆ\nq\n=\nt\n0\nâˆ’\nÏ…\n+\nk\n+\n1\nt\n0\nğ’©\n(\ny\nq\n;\nğš¿\n(\nx\nq\n)\nğœ·\nq\n,\nğšº\np\n)\n.\n\\displaystyle\\cdot\\prod\\limits_{q={{t}_{0}}-\\upsilon+k+1}^{{{t}_{0}}}{\\mathsf{\\mathcal{N}}\\left({{y}_{q}};\\boldsymbol{\\Psi}\\left({{x}_{q}}\\right){{\\boldsymbol{\\beta}}_{q}},\\boldsymbol{\\Sigma}_{p}\\right)}.\nBy applying Theorem\n2\n, the invertibility of the operations involved in the preceding expression is mathematically ensured. Following the same reasoning as in the initial case, a recursive condition is derived that holds uniformly over the entire time domain, as shown below:\nğš¿\nT\nâ€‹\n(\nğ‘¿\n(\nt\n0\n+\n1\n:\nt\n0\n+\nÎ¹\n)\n)\nâ€‹\nğš¿\nâ€‹\n(\nğ‘¿\n(\nt\n0\n+\n1\n:\nt\n0\n+\nÎ¹\n)\n)\nâˆ’\n\\displaystyle{{\\boldsymbol{\\Psi}}^{T}}({{\\boldsymbol{X}}_{({{t}_{0}}+1:{{t}_{0}}+\\iota)}})\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{({{t}_{0}}+1:{{t}_{0}}+\\iota)}})-\n(35)\nğš¿\nT\nâ€‹\n(\nğ‘¿\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\nâˆ’\nÏ…\n+\nk\n)\n)\nâ€‹\nğš¿\nâ€‹\n(\nğ‘¿\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\nâˆ’\nÏ…\n+\nk\n)\n)\n\\displaystyle{{\\boldsymbol{\\Psi}}^{T}}({{\\boldsymbol{X}}_{({{t}_{0}}-\\upsilon+1:{{t}_{0}}-\\upsilon+k)}})\\boldsymbol{\\Psi}({{\\boldsymbol{X}}_{({{t}_{0}}-\\upsilon+1:{{t}_{0}}-\\upsilon+k)}})\nâ‰»\nğŸ\n.\n\\displaystyle\\succ\\boldsymbol{0}.\nTherefore, Eq.(\n23\n) should be revised as\n{\nğ\nw\n,\nt\n+\n1\nÏ…\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\n{\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nğ\nw\n,\nt\nÏ…\n+\nvec\n[\n(\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nğ’€\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ‹…\nğ’€\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nğšº\np\nâˆ’\n1\n]\n}\n,\n(\nğšµ\nw\n,\nt\n+\n1\nÏ…\n)\nâˆ’\n1\n=\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\n+\nğšº\np\nâˆ’\n1\nâŠ—\n[\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ‹…\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ‹…\nğš¿\nd\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\n]\n.\n\\left\\{\\begin{aligned} &\\boldsymbol{\\mu}_{w,t+1}^{\\upsilon}=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\{{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\\boldsymbol{\\mu}_{w,t}^{\\upsilon}\\\\\n&+\\mathrm{vec}[{{(\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{Y}}_{d,(t+1:t+\\iota)}}\\\\\n&-{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\\\\\n&\\cdot{{\\boldsymbol{Y}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\\boldsymbol{\\Sigma}_{p}^{-1}]\\},\\\\\n&{{(\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon})}^{-1}}={{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\\\\\n&+\\boldsymbol{\\Sigma}_{p}^{-1}\\otimes[{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\\\\\n&\\cdot\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\\\\\n&-{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\\\\\n&\\cdot\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})].\\end{aligned}\\right.\n(36)\nRemark 3.1\n.\nThe variance matrix\nğšµ\nw\n,\nt\nÏ…\n\\boldsymbol{\\Xi}_{w,t}^{\\upsilon}\nin Eq.(\n36\n) must remain strictly positive definite (i.e.,\nğšµ\nw\n,\nt\nÏ…\nâ‰»\nğŸ\n\\boldsymbol{\\Xi}_{w,t}^{\\upsilon}\\succ\\boldsymbol{0}\n) at all times during the recursive process. This requirement is guaranteed by the recursive condition specified in Eq.(\n35\n).\nFurthermore, defining\nğ›„\n\\boldsymbol{\\gamma}\nas the evaluation matrix, it has\nğœ¸\n=\nd\nâ€‹\ni\nâ€‹\na\nâ€‹\ng\nâ€‹\n(\nÎº\ni\n)\nâ‰»\nğŸ\n,\n\\boldsymbol{\\gamma}=diag({{\\kappa}_{i}})\\succ\\boldsymbol{0},\n(37)\nwhere\nÎº\ni\n{\\kappa}_{i}\ndenotes the\ni\ni\n-th eigenvalue of\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n0\n+\n1\n:\nt\n0\n+\nÎ¹\n)\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n0\n+\n1\n:\nt\n0\n+\nÎ¹\n)\n)\n{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,({{t}_{0}}+1:{{t}_{0}}+\\iota)}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,{({t}_{0}}+1:{{t}_{0}}+\\iota)}})\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\nâˆ’\nÏ…\n+\nk\n)\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n0\nâˆ’\nÏ…\n+\n1\n:\nt\n0\nâˆ’\nÏ…\n+\nk\n)\n)\n-{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,{({t}_{0}}-\\upsilon+1:{{t}_{0}}-\\upsilon+k)}})\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,{({t}_{0}}-\\upsilon+1:{{t}_{0}}-\\upsilon+k)}})\n.\nThe recursive condition can be interpreted as follows: the informational contribution of new data must consistently exceed that of old data. This implies that upon the arrival of new data, the computation of\nÎº\ni\n{\\kappa}_{i}\nenables online assessment of data utility. Specifically:\nâ€¢\nIf\nâˆƒ\nÎº\ni\n=\n0\n\\exists{\\kappa}_{i}=0\n, the incoming data is redundant with historical data;\nâ€¢\nIf\nâˆƒ\nÎº\ni\n<\n0\n\\exists{\\kappa}_{i}<0\n, the new data may contain significant noise that could compromise model stability.\nThis mechanism effectively supports real-time data quality screening during the recursive update process.\nIndeed, during recursive updates, parameters\nÎ¹\n\\iota\nand\nk\nk\nare inherently distinct:\nÎ¹\n\\iota\ncan be adaptively determined, while\nk\nk\nis typically fixed as a hyperparameter(Typically,\n(\nÏ…\n+\nÎ¹\nâˆ’\nk\n)\n(\\upsilon+\\iota-k)\nis bigger than the number of unknown parameters, to enhance the long-term stability of the model, the value of\nk\nk\ncan be adjusted according to specific practical requirements). This causes the effective window size\nÏ…\n\\upsilon\nto expand indefinitely. Artificially fixing\nÏ…\n\\upsilon\nresolves uncontrolled growth but introduces historical information remanence, leading to model divergence over time. Therefore, under a fixed window size\nÏ…\n\\upsilon\n, a forgetting factor\nÎ¾\nâˆˆ\n(\n0\n,\n1\n)\n\\xi\\in\\left(0,1\\right)\nis incorporated for historical information, yielding the final recursive expression\n{\nğ\nw\n,\nt\n+\n1\nÏ…\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\n{\nÎ¾\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nğ\nw\n,\nt\nÏ…\n+\nvec\n[\n(\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nğ’€\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ‹…\nğ’€\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nğšº\np\nâˆ’\n1\n]\n}\n,\n(\nğšµ\nw\n,\nt\n+\n1\nÏ…\n)\nâˆ’\n1\n=\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\n+\nğšº\np\nâˆ’\n1\nâŠ—\n[\nğš¿\nd\nT\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ‹…\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ‹…\nğš¿\nd\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\n]\n.\n\\left\\{\\begin{aligned} &\\boldsymbol{\\mu}_{w,t+1}^{\\upsilon}=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\{\\xi{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\\boldsymbol{\\mu}_{w,t}^{\\upsilon}\\\\\n&+\\mathrm{vec}[{{(\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{Y}}_{d,(t+1:t+\\iota)}}\\\\\n&-{{\\boldsymbol{\\Psi}}_{d}^{T}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\\\\\n&\\cdot{{\\boldsymbol{Y}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\\boldsymbol{\\Sigma}_{p}^{-1}]\\},\\\\\n&{{(\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon})}^{-1}}=\\xi{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\\\\\n&+\\boldsymbol{\\Sigma}_{p}^{-1}\\otimes[{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\\\\\n&\\cdot\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\\\\\n&-{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\\\\\n&\\cdot\\boldsymbol{\\Psi}_{d}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})].\\end{aligned}\\right.\n(38)\nThe recursive framework proposed in this paper is designed to enhance the modelâ€™s adaptability to new data by dynamically updating regression parameters, thereby accelerating convergence. A key feature of this approach is the ability to flexibly balance between incorporating new information and retaining historical stability through adjustable parameters (\nÏ…\n\\upsilon\n,\nk\nk\n,\nÎ¹\n\\iota\n,\nÎ¾\n\\xi\n). The recursive condition formally establishes a weighted information differential between new and old data, allowing the model to emphasize recent trends without compromising overall robustness. This enables a tunable trade-off between convergence speed and long-term stability, making the framework suitable for applications where responsive adaptation is valued.\n3.3\nConvergence\nTheorem 3\n.\nFor a discrete-time system, let\nğš²\nâ€‹\n(\nt\n)\nâˆˆ\nâ„\nn\n\\boldsymbol{\\Lambda}\\left(t\\right)\\in{{\\mathbb{R}}^{n}}\ndenotes the regression vector. If there exists constant\nÎ±\n1\n>\n0\n{{\\alpha}_{1}}>0\n, and a positive integer\nN\nN\n, such that for all time instants\nt\nâ‰¥\n0\nt\\geq 0\nand any unit vector\nğ¯\nT\nâˆˆ\nâ„\nn\n{{\\boldsymbol{v}}^{T}}\\in{{\\mathbb{R}}^{n}}\n, the following inequality holds:\n1\nN\nâ€‹\nâˆ‘\nk\n=\nt\nt\n+\nN\nâˆ’\n1\n(\nğš²\nT\nâ€‹\n(\nk\n)\nâ€‹\nğ’—\n)\n2\nâ‰¥\nÎ±\n1\n,\n\\frac{1}{N}\\sum\\limits_{k=t}^{t+N-1}{{{\\left({{\\boldsymbol{\\Lambda}}^{T}}\\left(k\\right)\\boldsymbol{v}\\right)}^{2}}}\\geq{{\\alpha}_{1}},\n(39)\nthen the signal\nğš²\nâ€‹\n(\nt\n)\n\\boldsymbol{\\Lambda}\\left(t\\right)\nis said to satisfy the Persistent Excitation (PE) condition.\nCrucially, this PE condition is essentially equivalent to the conclusions established in the Sec.\n3.2\n.\nLemma 3.2\n.\nUnder the conditions of Theorem\n3\n, there exists a constant\nÎ±\n2\n>\n0\n{{\\alpha}_{2}}>0\nsuch that\nÎ±\n1\nâ€‹\nğ‘°\nm\nâ€‹\nâ‰º\n-\nâ€‹\n1\nN\nâ€‹\nâˆ‘\nk\n=\nt\nt\n+\nN\nâˆ’\n1\nğš²\nT\nâ€‹\n(\nk\n)\nâ€‹\nğš²\nâ€‹\n(\nk\n)\nâ€‹\nâ‰º\n-\nâ€‹\nÎ±\n2\nâ€‹\nğ‘°\nm\n,\n{{\\alpha}_{1}}{{\\boldsymbol{I}}_{m}}\\underset{\\scriptscriptstyle-}{\\prec}\\frac{1}{N}\\sum\\limits_{k=t}^{t+N-1}{{{\\boldsymbol{\\Lambda}}^{T}}\\left(k\\right)}\\boldsymbol{\\Lambda}\\left(k\\right)\\underset{\\scriptscriptstyle-}{\\prec}{{\\alpha}_{2}}{{\\boldsymbol{I}}_{m}},\n(40)\nthen the signal\nğš²\nâ€‹\n(\nt\n)\n\\boldsymbol{\\Lambda}\\left(t\\right)\nis said to satisfy the PE condition.\nTheorem 4\n.\nIf\nğ€\nâˆˆ\nâ„\nn\nÃ—\nn\n\\boldsymbol{A}\\in{{\\mathbb{R}}^{n\\times n}}\nis a symmetric positive definite matrix, then there exists a unique triangular matrix\nğš²\nâˆˆ\nâ„\nn\nÃ—\nn\n\\boldsymbol{\\Lambda}\\in{{\\mathbb{R}}^{n\\times n}}\nwith strictly positive diagonal elements such that\nğ€\n=\nğš²\nT\nâ€‹\nğš²\n\\boldsymbol{A}={{\\boldsymbol{\\Lambda}}^{T}}\\boldsymbol{\\Lambda}\n. This factorization is termed the Cholesky decomposition.\nTo reinforce the PE condition, this work requires the information matrix to be positive definite. Let\nğš²\nT\nâ€‹\n(\nt\n+\n1\n)\nâ€‹\nğš²\nâ€‹\n(\nt\n+\n1\n)\n=\n\\displaystyle{{\\boldsymbol{\\Lambda}}^{T}}(t+1)\\boldsymbol{\\Lambda}(t+1)=\n(41)\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ€‹\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\n\\displaystyle{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\n\\displaystyle-{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\nâ‹…\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\n.\n\\displaystyle\\cdot{{\\boldsymbol{\\Psi}}^{T}_{d}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}}).\nAssuming the observed data satisfies the PE condition and the observation noise\nÎµ\nâ€‹\n(\nt\n)\n\\varepsilon(t)\nis i.i.d. with\nğ”¼\nâ€‹\n[\nğœ·\n0\n]\n=\nğŸ\n\\mathbb{E}\\left[\\boldsymbol{\\beta}_{0}\\right]=\\boldsymbol{0}\nand\nV\nâ€‹\na\nâ€‹\nr\nâ€‹\n[\nğœ·\n0\nâ€‹\nğœ·\n0\nT\n]\n=\nğšº\np\n<\nâˆ\nVar\\left[\\boldsymbol{\\beta}_{0}\\boldsymbol{\\beta}_{0}^{T}\\right]={\\boldsymbol{\\Sigma}_{p}}<\\infty\n, the information matrix\nğ‘º\nt\n+\n1\n=\n(\nğšµ\nw\n,\nt\n+\n1\nÏ…\n)\nâˆ’\n1\n{{\\boldsymbol{S}}_{t+1}}={{\\left(\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\right)}^{-1}}\nadmits the recursive relation\nğ‘º\nt\n+\n1\n=\nÎ¾\nâ€‹\nğ‘º\nt\n+\nğšº\np\nâˆ’\n1\nâŠ—\nğš²\nT\nâ€‹\n(\nt\n+\n1\n)\nâ€‹\nğš²\nâ€‹\n(\nt\n+\n1\n)\n\\displaystyle{{\\boldsymbol{S}}_{t+1}}=\\xi{{\\boldsymbol{S}}_{t}}+{\\boldsymbol{\\Sigma}^{-1}_{p}}\\otimes{{\\boldsymbol{\\Lambda}}^{T}}(t+1)\\boldsymbol{\\Lambda}(t+1)\n(42)\n=\nÎ¾\nt\n+\n1\nâ€‹\nğ‘º\n0\n+\nğšº\np\nâˆ’\n1\nâŠ—\nâˆ‘\ni\n=\n1\nt\n+\n1\nÎ¾\nt\nâˆ’\ni\n+\n1\nâ€‹\nğš²\nT\nâ€‹\n(\ni\n)\nâ€‹\nğš²\nâ€‹\n(\ni\n)\n,\n\\displaystyle={{\\xi}^{t+1}}{{\\boldsymbol{S}}_{0}}+{\\boldsymbol{\\Sigma}^{-1}_{p}}\\otimes\\sum\\limits_{i=1}^{t+1}{\\xi^{t-i+1}{{\\boldsymbol{\\Lambda}}^{T}}(i)\\boldsymbol{\\Lambda}(i)},\nwhere\nğ‘º\n0\n=\nğšº\np\nâˆ’\n1\nâ€‹\nğš¿\nd\nT\nâ€‹\n(\nğ’™\nd\n,\nt\n0\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ’™\nd\n,\nt\n0\n)\n+\nğœ½\nâˆ’\nğŸ\n{{\\boldsymbol{S}}_{0}}={\\boldsymbol{\\Sigma}^{-1}_{p}}{{\\boldsymbol{\\Psi}}^{T}_{d}}\\left({{\\boldsymbol{x}}_{{d,{t}_{0}}}}\\right)\\boldsymbol{\\Psi}_{d}\\left({{\\boldsymbol{x}}_{{d,{t}_{0}}}}\\right)+\\boldsymbol{\\theta^{-1}}\n, and\nğš²\n\\boldsymbol{\\Lambda}\nis triangular matrix.\nUnder the PE condition and with\nÎ¾\nâˆˆ\n(\n0\n,\n1\n)\n\\xi\\in\\left(0,1\\right)\n, as\nt\nâ†’\nâˆ\nt\\to\\infty\n, the eigenvalues\nÎ»\nâ€‹\n(\nğ‘º\nt\n)\n{{\\lambda}}\\left({{\\boldsymbol{S}}_{t}}\\right)\nof the matrix\nğ‘º\nt\n{{\\boldsymbol{S}}_{t}}\nsatisfy\nÎ»\nmax\nâ€‹\n(\nÎ£\np\nâˆ’\n1\n)\nâ€‹\nÎ±\n2\n/\n(\n1\nâˆ’\nÎ¾\n)\nâ‰¥\nÎ»\nâ€‹\n(\nğ‘º\nt\n)\nâ‰¥\nÎ»\nmin\nâ€‹\n(\nÎ£\np\nâˆ’\n1\n)\nâ€‹\nÎ±\n1\n/\n(\n1\nâˆ’\nÎ¾\n)\n\\lambda_{\\max}({\\Sigma}^{-1}_{p})\\alpha_{2}/(1-\\xi)\\geq\\lambda\\left({{\\boldsymbol{S}}_{t}}\\right)\\geq\\lambda_{\\min}({\\Sigma}^{-1}_{p})\\alpha_{1}/(1-\\xi)\n, which is uniformly positive definite. Thus\nğšµ\nw\n,\nt\n+\n1\nÏ…\n\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\nexhibits bounded convergence over time.\nDefining the parameter estimation error\nğœ·\n~\n=\nğ\nw\n,\nt\nÏ…\nâˆ’\nğœ·\nâˆ—\n,\n\\boldsymbol{\\tilde{\\beta}}=\\boldsymbol{\\mu}_{w,t}^{\\upsilon}-{{\\boldsymbol{\\beta}}^{*}},\n(43)\nwhere\nğœ·\nâˆ—\n=\nvec\nâ€‹\n(\nğœ·\nd\nâˆ—\n)\n{{\\boldsymbol{\\beta}}^{*}}=\\mathrm{vec}(\\boldsymbol{\\beta}_{d}^{*})\n,\nğœ·\nd\nâˆ—\n\\boldsymbol{\\beta}_{d}^{*}\nrepresents the true values of the coefficients.\nReferring to Eq.\n8\n, the following can be derived:\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ€‹\nğ’€\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n\\displaystyle\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{Y}}_{d,(t+1:t+\\iota)}}\n(44)\n=\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ€‹\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ€‹\nğœ·\nd\nâˆ—\n\\displaystyle=\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{\\Psi}}_{d}}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}})\\boldsymbol{\\beta}_{d}^{*}\n+\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ€‹\nğœ·\n0\n,\nn\nâ€‹\ne\nâ€‹\nw\n,\n\\displaystyle+\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{\\beta}}_{0,new}},\nand\n\\displaystyle\\mathrm{and}\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ€‹\nğ’€\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n\\displaystyle\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}}){{\\boldsymbol{Y}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}}\n=\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\n\\displaystyle=\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\nâ‹…\nğš¿\nd\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ€‹\nğœ·\nd\nâˆ—\n\\displaystyle\\cdot{{\\boldsymbol{\\Psi}}_{d}}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}})\\boldsymbol{\\beta}_{d}^{*}\n+\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ€‹\nğœ·\n0\n,\no\nâ€‹\nl\nâ€‹\nd\n,\n\\displaystyle+\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}}){{\\boldsymbol{\\beta}}_{0,old}},\nwhere\nğœ·\n0\n,\nn\nâ€‹\ne\nâ€‹\nw\n{{\\boldsymbol{\\beta}}_{0,new}}\nand\nğœ·\n0\n,\no\nâ€‹\nl\nâ€‹\nd\n{{\\boldsymbol{\\beta}}_{0,old}}\nrepresent the noise at different time instances. Then,\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ€‹\nğ’€\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\nâ€‹\nğšº\np\nâˆ’\n1\n\\displaystyle\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{Y}}_{d,(t+1:t+\\iota)}}\\boldsymbol{\\Sigma}_{p}^{-1}\n(45)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ€‹\nğ’€\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n\\displaystyle-\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}}){{\\boldsymbol{Y}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}}\nâ‹…\nğšº\np\nâˆ’\n1\n\\displaystyle\\cdot\\boldsymbol{\\Sigma}_{p}^{-1}\n=\nğš²\nT\nâ€‹\n(\nt\n+\n1\n)\nâ€‹\nğš²\nâ€‹\n(\nt\n+\n1\n)\nâ€‹\nğœ·\nd\nâˆ—\nâ€‹\nğšº\np\nâˆ’\n1\n+\nğœº\n,\n\\displaystyle={{\\boldsymbol{\\Lambda}}^{T}}(t+1)\\boldsymbol{\\Lambda}(t+1)\\boldsymbol{\\beta}_{d}^{*}\\boldsymbol{\\Sigma}_{p}^{-1}+\\boldsymbol{\\varepsilon},\nwhere\nğœº\n=\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\n+\n1\n:\nt\n+\nÎ¹\n)\n)\nâ€‹\nğœ·\n0\n,\nn\nâ€‹\ne\nâ€‹\nw\nâ€‹\nğšº\np\nâˆ’\n1\n\\displaystyle\\boldsymbol{\\varepsilon}=\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t+1:t+\\iota)}}){{\\boldsymbol{\\beta}}_{0,new}}\\boldsymbol{\\Sigma}_{p}^{-1}\n(46)\nâˆ’\nğš¿\nd\nT\nâ€‹\n(\nğ‘¿\nd\n,\n(\nt\nâˆ’\nÏ…\n:\nt\nâˆ’\nÏ…\n+\nk\nâˆ’\n1\n)\n)\nâ€‹\nğœ·\n0\n,\no\nâ€‹\nl\nâ€‹\nd\nâ€‹\nğšº\np\nâˆ’\n1\n,\n\\displaystyle-\\boldsymbol{\\Psi}_{d}^{T}({{\\boldsymbol{X}}_{d,(t-\\upsilon:t-\\upsilon+k-1)}}){{\\boldsymbol{\\beta}}_{0,old}}\\boldsymbol{\\Sigma}_{p}^{-1},\nSince\nvec\nâ€‹\n(\nğš²\nT\nâ€‹\n(\nt\n+\n1\n)\nâ€‹\nğš²\nâ€‹\n(\nt\n+\n1\n)\nâ€‹\nğœ·\nd\nâˆ—\nâ€‹\nğšº\np\nâˆ’\n1\n+\nğœº\n)\n\\displaystyle\\mathrm{vec}({{\\boldsymbol{\\Lambda}}^{T}}(t+1)\\boldsymbol{\\Lambda}(t+1)\\boldsymbol{\\beta}_{d}^{*}\\boldsymbol{\\Sigma}_{p}^{-1}+\\boldsymbol{\\varepsilon})\n(47)\n=\n(\nğšº\np\nâˆ’\n1\nâŠ—\nğš²\nT\nâ€‹\n(\nt\n+\n1\n)\nâ€‹\nğš²\nâ€‹\n(\nt\n+\n1\n)\n)\nâ€‹\nğœ·\nâˆ—\n+\nvec\nâ€‹\n(\nğœº\n)\n,\n\\displaystyle=(\\boldsymbol{\\Sigma}_{p}^{-1}\\otimes{{\\boldsymbol{\\Lambda}}^{T}}(t+1)\\boldsymbol{\\Lambda}(t+1)){{\\boldsymbol{\\beta}}^{*}}+\\mathrm{vec}(\\boldsymbol{\\varepsilon}),\nconsequently,\nğœ·\n~\nt\n+\n1\n=\nğ\nw\n,\nt\n+\n1\nÏ…\nâˆ’\nğœ·\nâˆ—\n\\displaystyle{{{\\boldsymbol{\\tilde{\\beta}}}}_{t+1}}=\\boldsymbol{\\mu}_{w,t+1}^{\\upsilon}-{{\\boldsymbol{\\beta}}^{*}}\n(48)\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nâ€‹\n(\nğ\nw\n,\nt\nÏ…\nâˆ’\nğœ·\nâˆ—\n)\n+\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nvec\nâ€‹\n(\nğœº\n)\n\\displaystyle=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\xi{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}(\\boldsymbol{\\mu}_{w,t}^{\\upsilon}-{{\\boldsymbol{\\beta}}^{*}})+\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\mathrm{vec}(\\boldsymbol{\\varepsilon})\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nâ€‹\nğœ·\n~\nt\n+\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nvec\nâ€‹\n(\nğœº\n)\n.\n\\displaystyle=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\xi{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}{{{\\boldsymbol{\\tilde{\\beta}}}}_{t}}+\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\mathrm{vec}(\\boldsymbol{\\varepsilon}).\nNotice that\nğ”¼\nâ€‹\n[\nvec\nâ€‹\n(\nğœº\n)\n]\n=\nğŸ\n\\mathbb{E}[\\mathrm{vec}(\\boldsymbol{\\varepsilon})]=\\boldsymbol{0}\n, yield\nğ”¼\nâ€‹\n[\nğœ·\n~\nt\n+\n1\n]\n=\nğ”¼\nâ€‹\n[\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nâ€‹\nğœ·\n~\nt\n]\n\\displaystyle\\mathbb{E}[{{\\boldsymbol{\\tilde{\\beta}}}_{t+1}}]=\\mathbb{E}[\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\xi{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}{{\\boldsymbol{\\tilde{\\beta}}}_{t}}]\n(49)\n=\nğ”¼\nâ€‹\n[\nÎ¾\nt\n+\n1\nâ€‹\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\n(\nğšµ\nw\n,\n0\nÏ…\n)\nâˆ’\n1\nâ€‹\nÎ²\n~\n0\n]\n,\n\\displaystyle=\\mathbb{E}[\\xi^{t+1}\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\boldsymbol{(\\Xi}_{w,0}^{\\upsilon})^{-1}\\tilde{\\beta}_{0}],\nhence,\nlim\nt\nâ†’\nâˆ\nâ€‹\nğ”¼\nâ€‹\n[\nğœ·\n~\nt\n+\n1\n]\n=\nğŸ\n.\n\\underset{t\\to\\infty}{\\mathop{\\lim}}\\,\\mathbb{E}[{{\\boldsymbol{\\tilde{\\beta}}}_{t+1}}]=\\boldsymbol{0}.\n(50)\nRemark 3.3\n.\nIn online identification methods, the true parameters are generally time-varying. However, due to the inherent constraints of process industries, abrupt changes in operating conditions are uncommon, i.e., the temporal variation of parameters is bounded:\nâ€–\nğœ·\nt\nâˆ—\nâˆ’\nğœ·\nt\n+\n1\nâˆ—\nâ€–\nâ‰¤\nÎ´\n.\n\\left\\|\\boldsymbol{\\beta}_{t}^{*}-\\boldsymbol{\\beta}_{t+1}^{*}\\right\\|\\leq\\delta.\n(51)\nUnder this assumption, Eq.(\n48\n) transforms into\nğœ·\n~\nt\n+\n1\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nâ€‹\nğœ·\n~\nt\n\\displaystyle{{\\boldsymbol{\\tilde{\\beta}}}_{t+1}}=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\xi{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}{{\\boldsymbol{\\tilde{\\beta}}}_{t}}\n(52)\n+\n(\nğœ·\nt\nâˆ—\nâˆ’\nğœ·\nt\n+\n1\nâˆ—\n)\n+\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nvec\nâ€‹\n(\nğœº\n)\n.\n\\displaystyle+(\\boldsymbol{\\beta}_{t}^{*}-\\boldsymbol{\\beta}_{t+1}^{*})+\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\mathrm{vec}(\\boldsymbol{\\varepsilon}).\nDefine\nğ†\nâ€‹\n(\nt\n,\nk\n)\n=\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\nâ€‹\nğšµ\nw\n,\nt\nâˆ’\n1\nÏ…\nâ€‹\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nt\nâˆ’\n2\nÏ…\n)\nâˆ’\n1\n\\displaystyle\\boldsymbol{\\rho}(t,k)=\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\xi{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\\boldsymbol{\\Xi}_{w,t-1}^{\\upsilon}\\xi{{(\\boldsymbol{\\Xi}_{w,t-2}^{\\upsilon})}^{-1}}\n(53)\nâ€¦\nâ€‹\nğšµ\nw\n,\nk\n+\n1\nÏ…\nâ€‹\nÎ¾\nâ€‹\n(\nğšµ\nw\n,\nk\nÏ…\n)\nâˆ’\n1\n\\displaystyle.\\boldsymbol{\\Xi}_{w,k+1}^{\\upsilon}\\xi{{(\\boldsymbol{\\Xi}_{w,k}^{\\upsilon})}^{-1}}\n=\nÎ¾\nt\nâˆ’\nk\n+\n1\nâ€‹\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\n(\nğšµ\nw\n,\nk\nÏ…\n)\nâˆ’\n1\n,\n\\displaystyle={{\\xi}^{t-k+1}}\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}{{(\\boldsymbol{\\Xi}_{w,k}^{\\upsilon})}^{-1}},\nwe obtain\nğœ·\n~\nt\n+\n1\n=\nğ†\nâ€‹\n(\nt\n,\n0\n)\nâ€‹\nğœ·\n~\n0\n+\nâˆ‘\nk\n=\n0\nt\nâˆ’\n1\nğ†\nâ€‹\n(\nt\n,\nk\n+\n1\n)\nâ€‹\nğ‘ª\nk\n+\nğ‘ª\nt\n,\n{{\\boldsymbol{\\tilde{\\beta}}}_{t+1}}=\\boldsymbol{\\rho}(t,0){{\\boldsymbol{\\tilde{\\beta}}}_{0}}+\\sum\\limits_{k=0}^{t-1}{\\boldsymbol{\\rho}(t,k+1){{\\boldsymbol{C}}_{k}}}+{{\\boldsymbol{C}}_{t}},\n(54)\nwhere\nğ‚\nk\n=\n(\nğ›ƒ\nt\nâˆ—\nâˆ’\nğ›ƒ\nt\n+\n1\nâˆ—\n)\n+\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\nvec\nâ€‹\n(\nğ›†\n)\n{{\\boldsymbol{C}}_{k}}=(\\boldsymbol{\\beta}_{t}^{*}-\\boldsymbol{\\beta}_{t+1}^{*})+\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}\\mathrm{vec}(\\boldsymbol{\\varepsilon})\n.\nUnder the PE condition and assuming the input data is stationary, the matrix\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\n(\nğšµ\nw\n,\nt\nÏ…\n)\nâˆ’\n1\n\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}{{(\\boldsymbol{\\Xi}_{w,t}^{\\upsilon})}^{-1}}\nis bounded\n[\n12\n]\n. According to the recursive stability condition, we have\nâ€–\nğšµ\nw\n,\nt\n+\n1\nÏ…\nâ€‹\n(\nğšµ\nw\n,\nk\nÏ…\n)\nâˆ’\n1\nâ€–\nâ‰¤\nH\n\\left\\|\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}{{(\\boldsymbol{\\Xi}_{w,k}^{\\upsilon})}^{-1}}\\right\\|\\leq H\n(since\nğšµ\nw\n,\nt\nÏ…\n\\boldsymbol{\\Xi}_{w,t}^{\\upsilon}\nis bounded). Therefore, it can be inferred that\nlim\nt\nâ†’\nâˆ\nğ”¼\n[\nâˆ¥\nğœ·\n~\nt\n+\n1\nâˆ¥\n]\nâ‰¤\nlim\nt\nâ†’\nâˆ\nğ”¼\n[\nâˆ¥\nÎ¾\nt\n+\n1\nğšµ\nw\n,\nt\n+\n1\nÏ…\n(\nğšµ\nw\n,\n0\nÏ…\n)\nâˆ’\n1\n\\displaystyle\\underset{t\\to\\infty}{\\mathop{\\lim}}\\,\\mathbb{E}[{{\\boldsymbol{\\|\\tilde{\\beta}}}_{t+1}\\|}]\\leq\\underset{t\\to\\infty}{\\mathop{\\lim}}\\,\\mathbb{E}[\\|{{\\xi}^{t+1}}\\boldsymbol{\\Xi}_{w,t+1}^{\\upsilon}{{(\\boldsymbol{\\Xi}_{w,0}^{\\upsilon})}^{-1}}\n(55)\nâ‹…\n\\displaystyle\\cdot\nğœ·\n~\n0\nâˆ¥\n+\nÎ´\n1\nâˆ’\n(\nÎ¾\n)\nt\n+\n1\n1\nâˆ’\nÎ¾\nH\n]\n=\nÎ´\n1\n1\nâˆ’\nÎ¾\nH\n.\n\\displaystyle{{\\boldsymbol{\\tilde{\\beta}}}_{0}\\|}+\\delta\\frac{1-{{(\\xi)}^{t+1}}}{1-\\xi}H]=\\delta\\frac{1}{1-\\xi}H.\n4\nNumerical simulations\nThis section presents a numerical experiment to validate the accuracy and convergence of the proposed recursive scheme, along with a complex system case study to demonstrate the practical utility of the BRSL framework.\n4.1\nCase study 1\nSymbol learning forms the core of the entire framework, where the accuracy of system identification and the sparsity of the regression parameters critically determine its downstream performance. The sparsity-inducing horseshoe prior applied within the symbol learning framework has been proven effective for system identification\n[\n3\n]\n. While Carvalho et al. extensively discussed horseshoe priors in traditional supervised learning settingsâ€”including linear regression, generalized linear models, and functional estimationâ€”this section validates their efficacy, and that of the broader BRSL framework, through a concise numerical case study.\nThe dataset is generated from\nğ’š\nâˆ¼\nğ’©\nâ€‹\n(\nğ‘¿\nâ€‹\nğœ·\n,\nÏƒ\n2\nâ€‹\nğ‘°\n)\n\\boldsymbol{y}\\sim\\mathsf{\\mathcal{N}}(\\boldsymbol{X\\beta},{{\\sigma}^{2}}\\boldsymbol{I})\n, where the design matrix\nğ‘¿\nâˆˆ\nâ„\nn\nÃ—\nm\n\\boldsymbol{X}\\in{{\\mathbb{R}}^{n\\times m}}\nis drawn from a uniform distribution on\n(\nâˆ’\n0.5\n,\n0.5\n]\n\\left(-0.5,0.5\\right]\n, and the regression coefficient vector\nğœ·\n\\boldsymbol{\\beta}\nis sparse. In this probabilistic model, the coefficients\nğœ·\n\\boldsymbol{\\beta}\nrepresent the mean parameters of the Gaussian distribution. This experiment evaluates the estimation accuracy of this\nm\nm\n-dimensional mean vector. We simulated 100 datasets with each\nğœ·\n\\boldsymbol{\\beta}\ncontaining approximately 30% non-zero parameters. The non-zero parameters were generated from a uniform distribution on\n(\n5\n,\n10\n]\n\\left(5,10\\right]\n, and additive Gaussian noise with zero mean and variance 0.1 was applied to the data.\nFig.\n1\nand Fig.\n2\nshow the distributions of the absolute estimation errors for the non-zero and zero-valued parameters, respectively. The results demonstrate that the horseshoe prior provides strong performance in system identification under noisy conditions. It achieves high precision in estimating the true values of non-zero parameters within systems containing multiple stochastic elements, while effectively suppressing the estimates of zero-valued parameters toward zero throughout the sparse recovery process.\nFigure 1:\nAbsolute error statistics for non-zero parameters\nThe horseshoe prior employs a novel scale-mixture scheme of multivariate normal distributions, yielding estimation results that exhibit robustness against unknown sparsity patterns and significant outlier signals. This characteristic is exquisitely aligned with the fundamental requirements of sparse representation in symbol learning and the online probabilistic recursion of Bayesian symbolic learning.\nFigure 2:\nAbsolute error statistics for zero-valued parameters\nTo validate the convergence and robustness of the recursive algorithm, an abrupt change in operating conditions was introduced at a random point during the simulation. Fig.\n3\nshows the resulting distribution of non-zero parameters before and after the change, where triangles and pentagrams represent the true post-change and pre-change coefficients, respectively. Fig.\n4\nillustrates the convergence behavior of the means and variances during the iterative process. After the change, the coefficients do not converge gradually but exhibit a step-like decline, which is due to the recursive condition in the BRSL method that ensures absolute convergence. Both figures visually demonstrate the strong convergence capability and robustness of the BRSL method.\nFigure 3:\nThe prediction of BRSL before and after the change of working conditions\nFigure 4:\nThe convergence of the means and variances in the recursive process\n4.2\nCase study 2\nIn this illustrative example, a state-space model of Lorenz is utilized to validate the performance of the BRSL. The model is described as\n{\nx\nË™\n1\n=\nk\n1\nâ€‹\n(\nx\n2\nâˆ’\nx\n1\n)\n+\nv\n1\nx\nË™\n2\n=\nx\n1\nâ€‹\n(\n28\nâˆ’\nx\n3\n)\nâˆ’\nx\n2\n+\nv\n2\nx\nË™\n3\n=\nx\n1\nâ€‹\nx\n2\nâˆ’\nk\n3\nâ€‹\nx\n3\n+\nv\n3\nx\n0\n=\n[\nâˆ’\n8\n7\n2\n7\n]\n,\n\\begin{matrix}\\left\\{\\begin{aligned} &{{{\\dot{x}}}_{1}}={{k}_{1}}({{x}_{2}}-{{x}_{1}})+{{v}_{1}}\\\\\n&{{{\\dot{x}}}_{2}}={{x}_{1}}(28-{{x}_{3}})-{{x}_{2}}+{{v}_{2}}\\\\\n&{{{\\dot{x}}}_{3}}={{x}_{1}}{{x}_{2}}-{{k}_{3}}{{x}_{3}}+{{v}_{3}}\\\\\n\\end{aligned}\\right.&{{x}_{0}}=\\left[\\begin{aligned} -&8\\\\\n&7\\\\\n2&7\\\\\n\\end{aligned}\\right]\\\\\n\\end{matrix},\n(56)\nwhere\nv\n1\nv_{1}\n,\nv\n2\nv_{2}\nand\nv\n3\nv_{3}\nrepresent system noise, assumed to follow a standard normal distribution, i.e.,\nv\nâˆ¼\nğ’©\nâ€‹\n(\nğŸ\n,\nğŸ\n)\nv\\sim\\mathsf{\\mathcal{N}}(\\boldsymbol{0},\\boldsymbol{1})\n. The coefficients to be identified,\nk\n1\nk_{1}\nand\nk\n2\nk_{2}\n, are defined as time-varying functions:\nk\n1\n=\n0.5\nâ€‹\ns\nâ€‹\ni\nâ€‹\nn\nâ€‹\n(\n0.1\nâ€‹\nt\n)\n+\n10\n,\n\\displaystyle k_{1}=5sin(1t)+0,\n(57)\nk\n3\n=\na\nâ€‹\nr\nâ€‹\nc\nâ€‹\nt\nâ€‹\na\nâ€‹\nn\nâ€‹\n(\n0.1\nâ€‹\nt\n)\n+\n3\n.\n\\displaystyle k_{3}=arctan(1t)+3.\nThe effectiveness of the proposed method is validated under time-varying parameter scenarios. During modeling, two cases are considered: one where the system model is fully known, and another where it is partially unknown (with\nx\n2\nx_{2}\nbeing a known state, while\nx\n1\nx_{1}\nand\nx\n3\nx_{3}\nare entirely unknown). For sparse identification, a library of second-order polynomial basis functions of the state variables is constructed.\nFig.\n5\nand Fig.\n6\npresent the SHAP (SHapley Additive exPlanations) value analysis, which quantifies the contribution of each basis function to the system output. The results demonstrate that the BRSL method effectively drives the model to converge to the true complex system over time, confirming its strong identification capability. Furthermore, Fig.\n7\nand Fig.\n8\nshow the simulation results for state variables\nx\n1\nx_{1}\nand\nx\n3\nx_{3}\n. Even under noise disturbances, the BRSL method accurately tracks the system states, verifying its robustness and effectiveness in complex noisy environments.\nFigure 5:\nSHAP analysis of feature contributions for\nx\n1\nx_{1}\nFigure 6:\nSHAP analysis of feature contributions for\nx\n3\nx_{3}\nFigure 7:\nSystem state\nx\n1\nx_{1}\ntrajectory: Actual vs. Predicted(BRSL)\nFigure 8:\nSystem state\nx\n3\nx_{3}\ntrajectory: Actual vs. Predicted(BRSL)\n5\nConclusions\nIn conclusion, this work unifies interpretability with uncertainty-aware online learning via a Bayesian symbolic regression framework, establishing a new paradigm for system identification that delivers full posterior distributions over parsimonious governing equations. The proposed BRSL method provides a robust foundation for real-time symbolic learning through stable recursive conditions, a forgetting mechanism, and provable convergence guarantees. This approach enables dynamic, trustworthy models critical for applications such as DTs, bridging long-standing gaps between computational efficiency, probabilistic rigor, and interpretability for deployment in real-world streaming data environments. Future work will extend the framework to distributed learning and non-stationary dynamics.\n{ack}\nThis work was supported by the Shanxi Province General Program of Natural Science Research (20240302122\n1055), Shanxi Province Major Special Program of Science and Technology (202201090301013), and Gemeng Group Technology Innovation Fund Project (2024-05, 2025-01).\nReferences\n[1]\nA. Bhadra, J. Datta, N. G. Polson, and B. Willard\n(2019)\nLasso meets horseshoe: a survey\n.\nStatistical Science\n34\n(\n3\n),\npp.Â 405â€“427\n.\nCited by:\nRemark 1\n.\n[2]\nS. L. Brunton, J. L. Proctor, and J. N. Kutz\n(2016)\nDiscovering governing equations from data by sparse identification of nonlinear dynamical systems\n.\nProceedings of the National Academy of Sciences\n113\n(\n15\n),\npp.Â 3932â€“3937\n.\nCited by:\nÂ§3.1\n.\n[3]\nC. M. Carvalho, N. G. Polson, and J. G. Scott\n(2009)\nHandling sparsity via the horseshoe\n.\nIn\nProceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)\n,\nClearwater Beach, FL, USA\n,\npp.Â 73â€“80\n.\nCited by:\nÂ§3.1\n,\nÂ§4.1\n.\n[4]\nT. Chen, M. S. Andersen, L. Ljung, A. Chiuso, and G. Pillonetto\n(2014)\nSystem identification via sparse multiple kernel-based regularization using sequential convex optimization techniques\n.\nIEEE Transactions on Automatic Control\n59\n(\n11\n),\npp.Â 2933â€“2945\n.\nCited by:\nÂ§3.1\n.\n[5]\nK. Course and P. B. Nair\n(2023-10)\nState estimation of a physical system with unknown governing equations\n.\nNature\n622\n(\n7982\n),\npp.Â 261â€“267\n.\nCited by:\nÂ§1\n.\n[6]\nL. Edington, N. Dervilis, A. B. Abdessalem, and D. Wagg\n(2023)\nA time-evolving digital twin tool for engineering dynamics applications\n.\nMechanical Systems and Signal Processing\n188\n,\npp.Â 109971\n.\nCited by:\nÂ§1\n.\n[7]\nZ. Hua, J. Wu, and N. Li\n(2026)\nAn efficient second-order filtered characteristic method for navierâ€“stokes equations with its adaptive optimization\n.\nApplied Mathematics Letters\n172\n,\npp.Â 109723\n.\nCited by:\nÂ§1\n.\n[8]\nE. Kaiser, J. N. Kutz, and S. L. Brunton\n(2017)\nSparse identification of nonlinear dynamics for model predictive control in the low-data limit\n.\nProceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences\n473\n(\n2206\n).\nCited by:\nÂ§1\n.\n[9]\nK. Li, J.-X. Peng, and G. W. Irwin\n(2005)\nA fast nonlinear model identification method\n.\nIEEE Transactions on Automatic Control\n50\n(\n8\n),\npp.Â 1211â€“1216\n.\nCited by:\nÂ§3.1\n.\n[10]\nH. Liao, Z.-R. Lu, L. Wang, J. Liu, and D. Yang\n(2025)\nData-driven modeling of bolted joints by iwan dictionary and laplace prior-enhanced sparse bayesian learning\n.\nInternational Journal of Non-Linear Mechanics\n175\n,\npp.Â 105099\n.\nCited by:\nÂ§1\n.\n[11]\nW. Luo, T. Hu, Y. Ye, C. Zhang, and Y. Wei\n(2020)\nA hybrid predictive maintenance approach for cnc machine tool driven by digital twin\n.\nRobotics and Computer-Integrated Manufacturing\n65\n,\npp.Â 101974\n.\nCited by:\nÂ§1\n.\n[12]\nB. Pasik-Duncan\n(1996)\n[Review of the book adaptive control (2nd ed.), by k. j. Ã…strÃ¶m and b. wittenmark]\n.\nIEEE Control Systems Magazine\n16\n(\n2\n),\npp.Â 87\n.\nCited by:\nRemark 3.3\n.\n[13]\nG. Pillonetto and A. Yazdani\n(2022)\nSparse estimation in linear dynamic networks using the stable spline horseshoe prior\n.\nAutomatica\n146\n,\npp.Â 110666\n.\nCited by:\nÂ§1\n.\n[14]\nB. A. Surya\n(2024)\nMaximum likelihood recursive state estimation: an incomplete-information based approach\n.\nAutomatica\n168\n,\npp.Â 111820\n.\nCited by:\nÂ§1\n.\n[15]\nJ. Wang, J. Moreira, Y. Cao, and B. Gopaluni\n(2022)\nTime-variant digital twin modeling through the kalman-generalized sparse identification of nonlinear dynamics\n.\nIn\n2022 American Control Conference (ACC)\n,\nAtlanta, GA, USA\n,\npp.Â 5217â€“5222\n.\nCited by:\nÂ§1\n.\n[16]\nJ. Wang, J. Moreira, Y. Cao, and R. B. Gopaluni\n(2023)\nSimultaneous digital twin identification and signal-noise decomposition through modified generalized sparse identification of nonlinear dynamics\n.\nComputers & Chemical Engineering\n177\n,\npp.Â 108294\n.\nCited by:\nÂ§1\n.\n[17]\nY. Wang, Z. Su, S. Guo, M. Dai, T. H. Luan, and Y. Liu\n(2023)\nA survey on digital twins: architecture, enabling technologies, security and privacy, and future prospects\n.\nIEEE Internet of Things Journal\n10\n(\n17\n),\npp.Â 14965â€“14987\n.\nCited by:\nÂ§1\n.\n[18]\nX. Yang, W.-A. Zhang, and L. Yu\n(2025)\nFractional kalman filters\n.\nAutomatica\n178\n,\npp.Â 112383\n.\nCited by:\nÂ§1\n.\n[19]\nF. Yu, D. He, R. Jia, and Z. Mao\n(2025)\nStructure identification of time delay polynomial hammerstein models\n.\nAutomatica\n179\n,\npp.Â 112386\n.\nCited by:\nÂ§1\n.\n[20]\nQ. Zhang, W. Long, R. Wang, Z. Cao, Z. Wang, Y. Yan, and Y. Wen\n(2025)\nCaper: dual-level physics-data fusion with modular metamodels for reliable generalization in predictive digital twins\n.\nApplied Energy\n398\n,\npp.Â 126393\n.\nCited by:\nÂ§1\n.\n[21]\nY. Zhang, C. Yu, and F. Fabiani\n(2025)\nIdentification of non-causal systems with random switching modes\n.\nAutomatica\n182\n,\npp.Â 112532\n.\nCited by:\nÂ§1\n.\n[22]\nJ. Zhu, Y. Yang, M. Xi, S. Ji, L. Jia, and T. Hu\n(2025)\nThe next-generation digital twin: from advanced sensing towards artificial intelligence-assisted physical-virtual system\n.\nJournal of Industrial Information Integration\n48\n,\npp.Â 100942\n.\nCited by:\nÂ§1\n.\n[23]\nL. Zou, Z. Wang, B. Shen, and H. Dong\n(2025)\nRecursive state estimation in relay channels with enhanced security against eavesdropping: an innovative encryptionâ€“decryption framework\n.\nAutomatica\n174\n,\npp.Â 112159\n.\nCited by:\nÂ§1\n.",
  "preview_text": "Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.\n\nOnline identification of nonlinear time-varying systems with uncertain information\nHe Ren\nrh15848286658@outlook.com\nGaowei Yan\nyangaowei@tyut.edu.cn\nHang Liu\n2023310084@link.tyut.edu.cn\nLifeng Cao\n18636531698@163.com\nZhijun Zhao\nzhaozhijun1@tit.edu.cn\nGang Dang\ndzw627@126.com\nCollege of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, 030024, China\nDepartment of Autom",
  "is_relevant": false,
  "relevance_score": 0.0,
  "extracted_keywords": [
    "Bayesian Regression",
    "Symbolic Learning",
    "online identification",
    "nonlinear time-varying systems",
    "uncertain information",
    "Digital twins",
    "interpretability",
    "uncertainty quantification"
  ],
  "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯å›å½’çš„ç¬¦å·å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨çº¿è¯†åˆ«å…·æœ‰ä¸ç¡®å®šä¿¡æ¯çš„éçº¿æ€§æ—¶å˜ç³»ç»Ÿï¼Œä»¥æé«˜æ•°å­—å­ªç”Ÿæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§å’Œè‡ªé€‚åº”èƒ½åŠ›ã€‚",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-15T13:33:48Z",
  "created_at": "2026-01-20T17:49:55.710377",
  "updated_at": "2026-01-20T17:49:55.710385"
}