{
    "id": "2601.14445v1",
    "title": "Robust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery",
    "authors": [
        "Aiden Mazidi",
        "Majid Roshanfar",
        "Amir Sayadi",
        "Javad Dargahi",
        "Jake Barralet",
        "Liane S. Feldman",
        "Amir Hooshiar"
    ],
    "abstract": "èƒŒæ™¯ï¼šåœ¨æœºå™¨äººè¾…åŠ©å¾®åˆ›æ‰‹æœ¯ä¸­ï¼Œè§¦è§‰åé¦ˆçš„é›†æˆé•¿æœŸä»¥æ¥å—é™äºåŠ›åé¦ˆç²¾ç¡®æ¸²æŸ“ä¸ç³»ç»Ÿå®‰å…¨ä¿éšœæ–¹é¢çš„æŒ‘æˆ˜ã€‚æ„å»ºé²æ£’ä¸”é«˜ä¿çœŸçš„è§¦è§‰ç³»ç»Ÿå¯¹äºæå‡è¿œç¨‹æ“æ§æ‰‹æœ¯å·¥å…·çš„ç²¾ç¡®æ€§ä¸å¯é æ€§è‡³å…³é‡è¦ã€‚æ–¹æ³•ï¼šæœ¬ç ”ç©¶æå‡ºä¸€ç§éçº¿æ€§é˜»æŠ—åŒ¹é…æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç²¾ç¡®å»ºæ¨¡å¤æ‚çš„å™¨æ¢°-ç»„ç»‡äº¤äº’ä½œç”¨æ¥æ”¹å–„åŠ›æ¸²æŸ“æ•ˆæœã€‚è¯¥æ–¹æ³•åŸºäºæˆ‘ä»¬å…ˆå‰éªŒè¯çš„é˜»æŠ—åŒ¹é…æ–¹æ³•ï¼Œå¼•å…¥äº†éçº¿æ€§åŠ¨åŠ›å­¦ä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰å¹¶å‘ˆç°å™¨æ¢°-ç»„ç»‡ä½œç”¨åŠ›ã€‚ç»“æœï¼šéçº¿æ€§é˜»æŠ—åŒ¹é…æ–¹æ³•å°†åŠ›åé¦ˆçš„å¹³å‡ç»å¯¹è¯¯å·®é™è‡³0.01ç‰›ï¼ˆæ ‡å‡†å·®0.02ç‰›ï¼‰ï¼Œè¾ƒåŸé˜»æŠ—åŒ¹é…æ–¹æ³•é™ä½95%ï¼Œæ˜¾è‘—æå‡äº†åŠ›åé¦ˆç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½ç¡®ä¿ç”¨æˆ·åœ¨é‡Šæ”¾æ“ä½œæ‰‹æŸ„æ—¶è§¦è§‰è®¾å¤‡ä¸å¯¹æ‰‹éƒ¨æ–½åŠ ä»»ä½•ä½œç”¨åŠ›ï¼Œä»è€Œæœ‰æ•ˆæ¶ˆé™¤äº†è§¦è§‰\"å›å†²\"ç°è±¡ï¼Œæ—¢ä¿éšœäº†æ‚£è€…å®‰å…¨ï¼Œä¹Ÿæå‡äº†æ“ä½œèˆ’é€‚åº¦ã€‚ç»“è®ºï¼šéçº¿æ€§é˜»æŠ—åŒ¹é…æ–¹æ³•é€šè¿‡è€ƒé‡å™¨æ¢°-ç»„ç»‡äº¤äº’ä¸­çš„éçº¿æ€§ç‰¹å¾ï¼Œåœ¨ä¸åŒæ‰‹æœ¯æ¡ä»¶ä¸‹å‡å®ç°äº†åŠ›åé¦ˆä¿çœŸåº¦ã€å“åº”é€Ÿåº¦ä¸æ“ä½œç²¾åº¦çš„å…¨é¢æå‡ã€‚æœ¬ç ”ç©¶æ¨åŠ¨äº†æœºå™¨äººæ‰‹æœ¯è§¦è§‰åé¦ˆç³»ç»Ÿçš„å‘å±•ï¼Œä¸ºæœºå™¨äººè¾…åŠ©æ‰‹æœ¯æä¾›äº†æ›´çœŸå®å¯é çš„äººæœºäº¤äº’ç•Œé¢ã€‚",
    "url": "https://arxiv.org/abs/2601.14445v1",
    "html_url": "https://arxiv.org/html/2601.14445v1",
    "html_content": "\\copyyear\n2026\n\\startpage\n1\n\\authormark\nA. Mazidi\net al.\n\\titlemark\nRobust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery\n\\corres\nAmir Hooshiar (amir.hooshiar@mcgill.ca)\nRobust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery\nAiden Mazidi\nMajid Roshanfar\nAmir Sayadi\nJavad Dargahi\nJake Barralet\nLiane S. Feldman\nAmir Hooshiar\n\\orgdiv\nSurgical Robotics Laboratory (SRL), Mechanical Engineering Department,\n\\orgname\nConcordia University,\n\\orgaddress\n\\state\nMontreal, QC,\n\\country\nCanada\n\\orgdiv\nSurgical Performance Enhancement and Robotics (SuPER) Centre, Department of Surgery,\n\\orgname\nMcGill University,\n\\orgaddress\n\\state\nMontreal, QC,\n\\country\nCanada\n\\orgdiv\nThe Wilfred and Joyce Posluns Centre for Image Guided Innovation & Therapeutic Intervention (PCIGITI),\n\\orgname\nThe Hospital for Sick Children (SickKids),\n\\orgaddress\n\\state\nToronto, ON,\n\\country\nCanada\n(\nxxx 2026\n)\nAbstract\n[Abstract]\nBackground:\nThe integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools.\nMethods:\nIn this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively.\nResults:\nNIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01\nÂ±\n\\pm\n0.02 N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic \"kickback\" by ensuring no force is applied by the haptic device to the userâ€™s hand when they release the handle, enhancing both patient safety and user comfort.\nConclusion:\nNIMAâ€™s ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.\nkeywords:\nHaptics, Impedance Matching Approach, Force Rendering, Human-in-the-loop, Surgical Robotics, Minimally Invasive Surgery, Laparoscopy.\nâ€ \nâ€ \narticletype:\nOriginal Article\nâ€ \nâ€ \njournal:\nThe International Journal of Medical Robotics and Computer Assisted Surgery\nâ€ \nâ€ \nvolume:\n00\n1\nIntroduction\nAdvances in haptic technology are central to enhancing patient safety, surgical precision, and operator performance in robot-assisted minimally invasive surgery (RAMIS)\ncolan2024tactile\n,\nAmirabdollahian2017\n. The integration of haptic or force feedback into teleoperated RAMIS platforms provides substantial clinical and scientific advantages, including real-time assistance to surgeons, improved perception of tissue consistency, and automatic acquisition of tissue mechanical properties\nOkamura2004\n,\ntorkaman2023embedded\n,\nroshanfar2025learning\n. Several studies have shown that haptic feedback enhances the consistency, accuracy, and efficiency of tasks such as knot tying, thereby reducing the likelihood of inadvertent tissue damage\nMei2011\n. Moreover, it improves overall surgical outcomes by enabling finer motor control and augmenting visual feedback, ultimately reducing patient risk\nhooshiar2019haptic\n,\nMeijden2009\n,\nZhou2020\n. Despite these benefits, most current teleoperated surgical systems still operate without direct haptic feedback due to challenges in maintaining closed-loop stability and ensuring patient safety\nPacchierotti2016\n. For instance, while next-generation RAMIS platforms such as the da Vinci 5 system have begun integrating limited force-sensing capabilities for enhanced situational awareness, they do not yet provide continuous force reflection to the operator. Consequently, the integration of robust and safe haptic feedback remains one of the most critical technological challenges in achieving full sensory immersion and automation in robotic surgery\nMeijden2009\n,\nGumbs2021\n.\nThe regulatory approval of haptic systems as\nhuman-in-the-loop\ncomponents in RAMIS continues to face significant challenges, primarily due to the difficulty of accounting for variations in human dynamics during validation and testing\nPacchierotti2017\n,\nOkamura2004\n. Stability concerns in force-feedback teleoperation and uncertainties in force estimation further complicate the safe deployment of such systems in clinical environments\nBahar2020\n. Designing haptic devices that simultaneously ensure stability, transparency, and intuitive interaction while satisfying the diverse ergonomic and performance requirements of surgeons remains a major engineering challenge\nSelim2023\n. Moreover, variations in grip strength, reaction time, and motion control among individual users introduce additional uncertainties that hinder regulatory standardization and certification\nChae2018\n,\nFu2012\n. The persistent absence of high-fidelity haptic sensation in most commercial RAMIS platforms continues to limit their ability to fully replicate the tactile experience of open surgery\nLai2022\n. As the adoption of robotic surgery expands globally, the integration of safe, robust, and validated haptic feedback systems (HFS) into next-generation surgical robots has become increasingly imperative to bridge this sensory gap and enhance both training and operative performance\nAbiri2019\n.\n1.1\nRelated Studies\nIn the past decade, advances in commercial platforms and AI-enabled sensing methods have significantly advanced the provision of robust haptic feedback in RAMIS. Recent reviews and meta-analyses consistently show that augmenting vision with force or tactile information improves task performance, reduces peak forces, and can reduce errors in simulated and pre-clinical settings\npatel2022haptic\n,\nbergholz2023benefits\n,\ncolan2024tactile\n,\nboul2025role\n,\ndagnino2024robot\n. In Table\n1\nwe have synthesized a non-exhaustive list of current main approaches, i.e., direct tip sensing, proximal or joint-level sensing, model-based sensor-free estimation, vision-based force estimation, and purely visual or pseudo-haptic cueing, and compared them in terms of algorithmic complexity, hardware-agnostic deployment, sensor dependence, and whether they rely on proximal or distal measurements relative to the toolâ€“tissue interface.\nInstrument-integrated force sensing represents the most intuitive route to robust haptic feedback, and several recent studies have quantified its impact in realistic surgical tasks. Awad\net al.\nevaluated a new generation of robotic instruments with tip-mounted force sensors and demonstrated that force-feedback substantially reduced mean and maximal forces during retraction, dissection, and suturing on inanimate and ex-vivo models across experience levels\nawad2024evaluation\n. Systematic reviews of tactile and kinesthetic feedback in RAMIS confirm that such â€œtrueâ€ haptics can improve precision and reduce error, but they also highlight integration complexity, sterilization constraints, and platform-specific hardware as major barriers to widespread adoption\ncolan2024tactile\n,\nboul2025role\n. These solutions offer high-fidelity distal sensing at the cost of high mechanical and electronic complexity and low hardware agnosticity.\nTo avoid modifying the distal instrument, several groups have pursued proximal or sensorless approaches that reconstruct interaction forces from joint torques, actuator currents, or high-level motion descriptors. Pisla\net al.\nproposed an AI-based sensorless force-feedback strategy for robot-assisted esophagectomy that infers interaction forces from robot-side signals and uses them to generate haptic cues without any additional sensors on the surgical tools\npisla2025ai\n. Yan\net al.\ndeveloped a deep-learning model based on an ISSA-optimized backpropagation Neural Network (NN) to predict clamp forces on soft tissue using clamp motion parameters (contact area, speed, displacement, time) measured during compression tests\nyan2025robust\n. These methods, summarized under â€œsensorless/model-basedâ€ in Table\n1\n, tend to be more hardware-agnostic at the tool level and easier to retrofit to existing robots, but they require accurate dynamic models or extensive training data, careful calibration, and explicit handling of tissue variability to remain robust in vivo.\nA complementary trend is the use of vision-based force estimation, where endoscopic images (sometimes combined with robot kinematics) are used to infer contact forces or detect over-force events. Early work by Chua\net al.\ncombined vision and robot state to learn a mapping to interaction forces in RAMIS\nchua2021toward\n, and subsequent studies have refined these ideas. Masui\net al.\nshowed that deep learning models trained on laparoscopic images of a porcine kidney can estimate manipulation forces and detect over-force events when the region of interest is restricted around the tool tips\nmasui2024vision\n. Yang\net al.\nintroduced a contact-conditional framework that uses vision with local stiffness models and optionally joint torque information to predict forces in minimally invasive telesurgery, achieving high accuracy on contact detection and sub-10% force prediction errors\nyang2024vision\n. Ding\net al.\nsurveyed this emerging field and emphasized the potential of vision-based contact force detection to reduce hardware complexity and enable platform-agnostic haptic augmentation\nding2025vision\n. In Table\n1\n, these approaches are characterized by high algorithmic complexity but minimal additional hardware, with distal sensing achieved indirectly through image analysis.\nVision-based methods naturally connect to pseudo-haptic and visual-only feedback strategies, where the goal is not to reconstruct forces precisely but to shape the surgeonâ€™s perception of tissue stiffness or danger through visual deformation cues or other modalities. Masui\net al.\nexplicitly frame their vision-based force estimation as an attempt to make surgeonsâ€™ existing â€œpseudo-hapticâ€ impressions more explicit and actionable\nmasui2024vision\n. Trute\net al.\nstudied how soft-tissue visual cues in minimal-invasive and robotic surgery can support haptic perception and inform the design of visual augmentations\ntrute2024visual\n. Broader reviews of haptic technologies for surgical simulation and robotics also stress that pseudo-haptic and multi-modal sensory substitution (e.g., vibrotactile cues) can reduce hardware demands while still improving training and performance\nboul2025role\n. These approaches are highly hardware-agnostic and low-cost but rely heavily on robust computer vision and careful human factor design rather than physically accurate force reconstruction.\nTable 1:\nComparison of representative studies for haptic-feedback methods in robotic surgery\nMethod Class\nKey Works\nComplexity\nHardware Agnosticity\nSensor Dependence\nProx/Distal\nDistal force sensing\nawad2024evaluation\n,\ncolan2024tactile\n,\nbergholz2023benefits\nHigh (H)\nLow (L)\nHigh (H)\nDistal (D)\nProximal/Joint-space sensing\npisla2025ai\n,\nyan2025robust\n,\npatel2022haptic\nMed (M)\nMed (M)\nRobot-side only (R)\nProximal (P)\nVision-based force estimation\nchua2021toward\n,\nmasui2024vision\n,\nyang2024vision\n,\nding2025vision\nHigh (H)\nHigh (H)\nLow physical (L) + High data (H\nd\n)\nDistal (D\nind\n)\nPseudo-haptics / visual cues\nmasui2024vision\n,\ntrute2024visual\n,\nboul2025role\nLowâ€“Med (Lâ€“M)\nVery High (VH)\nLow (L)\nVisual distal (V)\nHybrid multi-modal fusion\npatel2022haptic\n,\nding2025vision\n,\nboul2025role\n,\ndagnino2024robot\nHigh (H)\nMed (M)\nMixed (Mx)\nP + D\nTaken together, recent literature suggests that there is no single â€œbestâ€ method for robust haptic feedback, instead, different clinical, regulatory, and engineering constraints naturally map to different regions of the design space. For new robotic platforms where instrument redesign is feasible and the cost per-unit is acceptable, instrument-integrated force sensors provide the most direct, distal measurements and the strongest evidence of performance benefits\nawad2024evaluation\n,\ncolan2024tactile\n,\nbergholz2023benefits\n. For retrofitting legacy systems or ensuring cross-platform deployability, proximal sensing and sensorless learning-based models offer a pragmatic compromise between hardware-agnostic deployment and robustness, especially when trained on data that span the expected range of tasks and tissues\npisla2025ai\n,\nyan2025robust\n,\npatel2022haptic\n. Vision-based and pseudo-haptic methods, finally, are attractive when hardware changes are impossible (e.g., closed commercial systems) or when force information is primarily needed for training, monitoring, or safety alerts rather than for continuous kinesthetic feedback\nmasui2024vision\n,\nyang2024vision\n,\nding2025vision\n,\ntrute2024visual\n.\nMore recently, Impedance Matching Approach (IMA) for force feedback has emerged as an effective strategy for achieving stable and transparent haptic interaction in RAMIS. It operates as a form of sensory substitution, enabling indirect force rendering by bypassing actuators on the leader side and excluding human components from the control loop\nYin2018\n,\nMunawar2016\n,\nPrattichizzo2012\n. Unlike conventional direct force feedback methods, where force error directly commands actuator output, IMA employs closed-loop position control, converting the force error into a corresponding position error that drives the robot\ngolahmadi2021tool\n,\nhaouchine2018vision\n,\nWilfinger1994\n. This formulation allows precise regulation of toolâ€“tissue contact forces while maintaining system stability through an inner position-control loop\nSiciliano1996\n. Previous studies have validated the IMA framework through simulation and experimental evaluations, demonstrating its potential for stable and accurate haptic rendering\nParsi2023\n. Complementary work has explored integrating tactile object recognition and purposeful haptic exploration to extract object features and improve force perception during teleoperation\nPezzementi2011\n. Moreover, advanced haptic rendering algorithms have been shown to minimize feedback oscillations and enhance the fidelity of force cues, leading to more realistic and responsive interaction\nLiu2022\n. Building on this, the linear IMA method has recently demonstrated high precision and stability for single-axis force components in surgical teleoperation tasks\nsayadi2020impedance\n, motivating further development of nonlinear extensions for more complex toolâ€“tissue interactions.\n1.2\nContributions\nThis study introduces a generalized Nonlinear Impedance Matching Approach (NIMA), which extends the conventional IMA framework by incorporating nonlinear impedance parameters to more accurately capture the complex dynamics of instrumentâ€“tissue interactions encountered in RAMIS. As illustrated in Fig.\n1\n, the proposed NIMA framework continuously identifies nonlinear impedance characteristics in real-time to enable precise rendering of multi-axis contact forces and stable bidirectional communication between the leader and follower modules. The framework was implemented and experimentally validated using commercially available laparoscopic tools, confirming its robustness in controlling three-dimensional contact forces and executing motion commands with high fidelity. The main contributions of this work are:\n1.\nA novel NIMA that generalizes the traditional IMA framework to model nonlinear toolâ€“tissue dynamics for improved force rendering in teleoperated surgery.\n2.\nA complete leaderâ€“follower control architecture enabling real-time estimation and transmission of impedance parameters for stable and responsive haptic feedback.\n3.\nExperimental validation demonstrating accurate 3D force control.\n4.\nElimination of haptic kickback through adaptive nonlinear modeling, ensuring enhanced safety and comfort during manipulation.\nFigure 1:\nProposed Nonlinear Impedance Matching Approach (NIMA). The leader module (left) receives motion commands\nğ—\n\\mathbf{X}\nfrom the operator and renders the estimated feedback force\nğŸ\nğ\n=\nğŒğ—\n\\mathbf{f_{d}}=\\mathbf{M}\\mathbf{X}\n, while the follower module (right) executes\nğ—\n\\mathbf{X}\n, measures\n(\nğŸ\n,\nğ—\n)\n(\\mathbf{f},\\mathbf{X})\n, and identifies nonlinear impedance parameters\nğŒ\n\\mathbf{M}\nin real time. This closed-loop structure enables stable, high-fidelity force feedback in robotic laparoscopy\nmazidi2024nonlinear\n.\n2\nMethodology\n2.1\nSystem Design\n(a) Leader module\n(b) Follower module\nFigure 2:\nExperimental platform used to evaluate the proposed NIMA framework.\n(a) The leader module consists of a dual-haptic console (Omega.7, Force Dimension) used by the surgeon to teleoperate the robotic arms through motion commands, with real-time feedback displayed on a monitor. (b) The follower module comprises two Kinova Gen3 robotic arms equipped with 6-DoF Bota forceâ€“torque sensors, custom instrument adapters, and a translucent mannequin containing a soft-tissue surrogate.\nThe setup replicates a realistic minimally invasive surgical environment for evaluating toolâ€“tissue interaction forces, motion tracking, and haptic feedback performance.\nThe experimental apparatus was designed to simulate a realistic surgical environment by integrating advanced technologies and components, as depicted in Fig.\n2\n. The setup included a custom mannequin embedded with tissue-representative objects, specially designed surgical tools, two Kinova Gen3 robotic arms (7 DOF), dedicated carts for robotic arm stability, two Omega.7 haptic controllers (Force Dimension), three 6-axis forceâ€“torque sensors (SensONE, Bota Systems), and an adaptable, in-house-designed surgical console. The Kinova Gen3 robotic arms were essential for delivering precise and controlled movements of the surgical instruments, replicating the dexterity of human hands to ensure an authentic experimental environment for surgical tasks. Specialized carts were engineered to support the robotic arms, providing both stability and flexibility in positioning the system during experiments. The surgical tools were custom-designed to integrate seamlessly with the robotic arms while adhering to clinical standards and were powered by four Dynamixel actuators (ROBOTIS) to achieve fine-grained control and precision during operation. The Omega.7 haptic controllers enabled the operator to intuitively interact with the robotic arms, translating hand movements into precise surgical motions. Their high positional accuracy played a crucial role in creating realistic and user-friendly surgical simulations. A translucent mannequin, as shown in Fig.\n2\na, incorporated tissue-representative objects to replicate the structural complexities of real surgical scenarios, including pick-and-place and suturing tasks, creating an environment that closely mirrors actual surgical challenges. Force data from the simulated tasks were captured using Bota force sensors mounted on the tips of the robotic arms. These sensors provided valuable insights into the mechanical interactions between the surgical tools and the tissue-representative objects. The custom-designed surgical console, shown in Fig.\n2\nb, acted as the central control unit, seamlessly integrating the robotic arms, haptic controllers, surgical tools, Dynamixel actuators, and force sensors, ensuring efficient communication and coordination among all components.\nFigure 3:\nSystem architecture of the proposed NIMA-enabled robot-assisted laparoscopy setup. The leader module (left) includes dual haptic devices (Omega.7) that transmit motion commands and receive force feedback estimated by the NIMA. The follower module (right) consists of two robotic arms equipped with 6-DoF forceâ€“torque sensors, an endoscope, and optical markers tracked by an NDI system for spatial registration. Force and motion data are processed through a NN for tool-tip force extraction and used by NIMA to render stable, high-fidelity haptic.\nThe system architecture, illustrated in Fig.\n3\n, enabled coordinated control by transmitting motion commands from the haptic devices to both the robot and instrument controllers. The robotic arms executed position commands, while the Dynamixel actuators controlled the orientation of the surgical tool tip, with each task managed by a dedicated control module. A PID controller was implemented for precise position and velocity regulation, ensuring accurate tool-tip alignment during operation. The robot controller consisted of two main components: one for handâ€“eye coordination and another for robot communication. This structure synchronized the robotic arms, optical tracker, endoscope, and haptic devices with the surgeonâ€™s visual perspective to maintain cohesive and intuitive teleoperation. Bota 6-DoF forceâ€“torque sensors, mounted between the robotsâ€™ end effectors and the custom instrument adapters, measured the interaction forces between the surgical tools and tissue, as well as the frictional forces generated at the Remote Center of Motion (RCM). To maintain a stable entry point on the patientâ€™s body, translational movements of the surgeonâ€™s hand were converted into rotational motions about the RCM. The friction forces induced during these translational motions were calculated and subtracted from the total measured forces to isolate the true tool-tip forces. A NN module was then developed to estimate these tool-tip forces based on the signals acquired from the force sensors at the robot end effectors, as described in Section\n2.2\n. The resulting toolâ€“tissue interaction forces were transmitted to the NIMA framework, which generated adaptive haptic feedback to the surgeonâ€™s hands, allowing the feedback intensity to be adjusted according to user preference.\nFor incorporating force feedback, accurately identifying the force sensor system is essential for applying gravitational biasing, which ensures precise robotic instrument movements. This study employs system identification using IMU data to determine the spatial orientation of the end effector, enabling effective gravitational biasing methods that counteract the effects of gravity on the robotic tools. To achieve this, an Integral-Free Spatial Orientation Estimation Method is introduced, which utilizes IMU sensors to calculate the end effectorâ€™s orientation in real time without relying on data integration from other sources. Using this IMU data, the system effectively calculates the end effectorâ€™s roll, pitch, and yaw, critical for compensating gravitational forces acting on the robotic instruments. For simplicity, this method computes roll and pitch angles using accelerations in three directions, avoiding the integration of angular velocity over time\n9397346\n. Real-time angles are calculated as:\nÎ±\n=\natan2\nâ€‹\n(\na\ny\n,\na\nz\n)\n\\alpha=\\text{atan2}(a_{y},a_{z})\n(1)\nÎ²\n=\natan2\nâ€‹\n(\nâˆ’\na\nx\n,\na\ny\n2\n+\na\nz\n2\n)\n\\beta=\\text{atan2}(-a_{x},\\sqrt{a_{y}^{2}+a_{z}^{2}})\n(2)\nHere,\nÎ±\n\\alpha\nrepresents the angle along the worldâ€™s x-axis,\nÎ²\n\\beta\nis the angle along the y-axis, and\na\nx\na_{x}\n,\na\ny\na_{y}\n, and\na\nz\na_{z}\nare acceleration values along the\nx\nx\n,\ny\ny\n, and\nz\nz\ndirections, respectively. After deriving the spatial orientation of the end effector from the IMU data, the next step involves modeling gravitational biasing. This model is crucial for implementing techniques to counteract gravityâ€™s effects on the robotic tools. The extracted model is expressed as follows:\nAX\n=\nB\n\\textbf{AX}=\\textbf{B}\n(3)\nA\nrepresents the system dynamics,\nX\nis the matrix of unknown coefficients, and\nB\nis sensor readings. The structure of\nA\nis:\nA\n=\n(\nsin\nâ¡\nÎ±\nt\n1\ncos\nâ¡\nÎ±\nt\n1\nsin\nâ¡\nÎ²\nt\n1\ncos\nâ¡\nÎ²\nt\n1\n1\nsin\nâ¡\nÎ±\nt\n2\ncos\nâ¡\nÎ±\nt\n2\nsin\nâ¡\nÎ²\nt\n2\ncos\nâ¡\nÎ²\nt\n2\n1\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nsin\nâ¡\nÎ±\nt\nn\ncos\nâ¡\nÎ±\nt\nn\nsin\nâ¡\nÎ²\nt\nn\ncos\nâ¡\nÎ²\nt\nn\n1\n)\nn\nÃ—\n5\n\\textbf{A}=\\begin{pmatrix}\\sin\\alpha_{t_{1}}&\\cos\\alpha_{t_{1}}&\\sin\\beta_{t_{1}}&\\cos\\beta_{t_{1}}&1\\\\\n\\sin\\alpha_{t_{2}}&\\cos\\alpha_{t_{2}}&\\sin\\beta_{t_{2}}&\\cos\\beta_{t_{2}}&1\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n\\sin\\alpha_{t_{n}}&\\cos\\alpha_{t_{n}}&\\sin\\beta_{t_{n}}&\\cos\\beta_{t_{n}}&1\\end{pmatrix}_{n\\times 5}\n(4)\nX\n, the matrix of unknown coefficients, is defined as:\nX\n=\n(\nC\n1\nâ€‹\nx\nC\n1\nâ€‹\ny\nC\n1\nâ€‹\nz\nC\n2\nâ€‹\nx\nC\n2\nâ€‹\ny\nC\n2\nâ€‹\nz\nC\n3\nâ€‹\nx\nC\n3\nâ€‹\ny\nC\n3\nâ€‹\nz\nC\n4\nâ€‹\nx\nC\n4\nâ€‹\ny\nC\n4\nâ€‹\nz\nC\n5\nâ€‹\nx\nC\n5\nâ€‹\ny\nC\n5\nâ€‹\nz\n)\n5\nÃ—\n3\n\\textbf{X}=\\begin{pmatrix}C_{1x}&C_{1y}&C_{1z}\\\\\nC_{2x}&C_{2y}&C_{2z}\\\\\nC_{3x}&C_{3y}&C_{3z}\\\\\nC_{4x}&C_{4y}&C_{4z}\\\\\nC_{5x}&C_{5y}&C_{5z}\\end{pmatrix}_{5\\times 3}\n(5)\nFinally,\nB\n, containing the force readings, is expressed as:\nB\n=\n(\nF\nX\nt\n1\nF\nY\nt\n1\nF\nZ\nt\n1\nF\nX\nt\n2\nF\nY\nt\n2\nF\nZ\nt\n2\nâ‹®\nâ‹®\nâ‹®\nF\nX\nt\nn\nF\nY\nt\nn\nF\nZ\nt\nn\n)\nn\nÃ—\n3\n\\textbf{B}=\\begin{pmatrix}F_{X_{t_{1}}}&F_{Y_{t_{1}}}&F_{Z_{t_{1}}}\\\\\nF_{X_{t_{2}}}&F_{Y_{t_{2}}}&F_{Z_{t_{2}}}\\\\\n\\vdots&\\vdots&\\vdots\\\\\nF_{X_{t_{n}}}&F_{Y_{t_{n}}}&F_{Z_{t_{n}}}\\end{pmatrix}_{n\\times 3}\n(6)\nHere,\nC\nC\nrepresents the unknown coefficients, and\nF\nF\ndenotes the measured forces for a specific robot position. To determine the coefficient matrix\nX\n, which is fundamental to the model, the equation\nAX\n=\nB\n\\textbf{AX}=\\textbf{B}\nis solved using the pseudo-inverse of\nA\n:\nX\n=\nA\nâ€ \nâ€‹\nB\n\\textbf{X}=\\textbf{A}^{\\dagger}\\textbf{B}\n(7)\nwhere\nA\nâ€ \n\\textbf{A}^{\\dagger}\nis the pseudo-inverse of\nA\n, defined as:\nA\nâ€ \n=\nA\nT\nâ€‹\n(\nA\nA\nT\n)\nâˆ’\n1\n\\textbf{A}^{\\dagger}=\\textbf{A}^{T}(\\textbf{A}\\textbf{A}^{T})^{-1}\n(8)\nOnce the coefficient matrix is obtained, the gravitational biasing techniques can be implemented based on the extracted model from the IMU data. These techniques allow robotic instruments to counteract gravitational forces, ensuring precise and reliable movements.\n2.2\nTool-tissue Force Extraction\n2.2.1\nCoordinate Systems Correspondence\nAfter eliminating all gravitational forces due to the weight of the 3D-printed adapter, the tool-tissue forces at the tip of the surgical instrument need to be isolated from the total forces measured by the sensing element. This isolation ensures that only the tool-tip forces are rendered to the surgeon. To achieve this, two experiments were designed to gather data for training a NN capable of estimating the tool-tip forces solely using force sensors positioned outside the patientâ€™s body. As illustrated in Fig.\n2\n, the experimental setup includes two Bota FT sensors, two Kinova robotic arms (one managing the endoscope and the other controlling the surgical instrument), an instrument adaptor, a mannequin, and an NDI optical tracker. Additionally, a silicon-based flexible tissue surrogate, representing human tissue, was mounted on one of the force sensors. These experiments aimed to capture the robotic arm sensor readings, the forces at the tool tip measured via the sensor beneath the flexible tissue surrogate, and the robotic arm end-effector configuration. This data set was subsequently used to train the model.\nFor simplicity, we refer to the force sensor attached to the robotic arm as\nS\n1\nS_{1}\nand the force sensor attached to the flexible tissue representative for the capture of tool-tissue forces as\nS\n2\nS_{2}\n. To use the measured tool-tip forces and those at\nS\n1\nS_{1}\nfor training, both force readings must be represented within a unified coordinate system. Thus, the first experiment aimed to transform all force readings to the coordinate system of\nS\n1\nS_{1}\n(Fig.\n4\n). In a first experiment, the upper section of the mannequin was removed to eliminate friction forces at the RCM. The robotic arm was then manipulated using haptic devices to engage the tool with the tissue surrogate. This setup ensured that\nS\n1\nS_{1}\nonly measured the interaction forces with the tissue (\nF\nr\nF_{r}\n), while\nS\n2\nS_{2}\nrecorded the same forces (\nF\nt\nF_{t}\n). Consequently, the forces\nF\nt\nF_{t}\n, expressed in the coordinate system of\nS\n1\nS_{1}\n, should match:\nf\nr\n{\nS\n1\n}\n=\n{\nS\n1\n}\nf\nt\n{}^{\\{\\textbf{S}_{1}\\}}\\textbf{f}_{r}=^{\\{\\textbf{S}_{1}\\}}\\textbf{f}_{t}\n(9)\nwhere\nF\nr\n{\nS\n1\n}\n{}^{\\{S_{1}\\}}F_{r}\nrepresents the measured forces of\nS\n1\nS_{1}\nexpressed in its own coordinate system, and\nF\nt\n{\nS\n1\n}\n{}^{\\{S_{1}\\}}F_{t}\nrepresents the measured forces from\nS\n2\nS_{2}\nexpressed in the coordinate system of\nS\n1\nS_{1}\n. Hence:\nf\nr\n{\nS\n1\n}\n=\n{\nS\n1\n}\nR\n{\nS\n2\n}\n{\nS\n2\n}\nâ€‹\nf\nt\n{}^{\\{\\textbf{S}_{1}\\}}\\textbf{f}_{r}=^{\\{\\textbf{S}_{1}\\}}\\textbf{R}_{\\{\\textbf{S}_{2}\\}}^{\\{\\textbf{S}_{2}\\}}\\textbf{f}_{t}\n(10)\nwhere the rotation matrix between the coordinate systems can be expressed as:\n{\nS\n1\n}\nR\n{\nS\n2\n}\n=\n{\nS\n1\n}\nR\n{\nKBF\n}\n{\nKBF\n}\nR\n{\nM\n}\n{\nM\n}\nR\n{\nC\n}\n{\nC\n}\nR\n{\nS\n2\n}\n^{\\{\\textbf{S}_{1}\\}}\\textbf{R}_{\\{\\textbf{S}_{2}\\}}=^{\\{\\textbf{S}_{1}\\}}\\textbf{R}_{\\{\\textbf{KBF}\\}}{\\penalty 10000\\ }^{\\{\\textbf{KBF}\\}}\\textbf{R}_{\\{M\\}}{\\penalty 10000\\ }^{\\{\\textbf{M}\\}}\\textbf{R}_{\\{\\textbf{C}\\}}{\\penalty 10000\\ }^{\\{\\textbf{C}\\}}\\textbf{R}_{\\{\\textbf{S}_{2}\\}}\n(11)\nHere, the coordinate systems are defined as follows:\nKBF\nis Kinova Base Frame,\nS\n1\nS_{1}\nis the end effector of the robot holding the instrument,\nM\nis the optical marker attached to the base of the surgical instrument,\nC\nis the optical tracking camera, and\nS\n2\nS_{2}\nis the coordinate system of the sensor measuring forces at the tip. In Eq.\n11\n, the only unknown matrix is\nR\n{\nS\n2\n}\n{\nC\n}\n{}^{\\{C\\}}R_{\\{S_{2}\\}}\n, which must be determined. This matrix can be obtained by solving an optimization problem with the objective:\nR\nR\nT\n=\nI\n\\textbf{R}\\textbf{R}^{T}=I\n(12)\nsubject to the equality constraints:\nR\n=\nR\nZ\nâ€‹\n(\nÎ¸\nZ\n)\nÃ—\nR\nY\nâ€‹\n(\nÎ¸\nY\n)\nÃ—\nR\nX\nâ€‹\n(\nÎ¸\nX\n)\n\\textbf{R}=\\textbf{R}_{Z}(\\theta_{Z})\\times\\textbf{R}_{Y}(\\theta_{Y})\\times\\textbf{R}_{X}(\\theta_{X})\n(13)\nR\n{\nC\n}\n{\nS\n1\n}\nÃ—\nR\nÃ—\n{\nS\n2\n}\nF\nt\nâˆ’\n{\nS\n1\n}\nF\nr\n=\n0\n{}^{\\{S_{1}\\}}R_{\\{C\\}}\\times\\textbf{R}\\times^{\\{S_{2}\\}}F_{t}-^{\\{S_{1}\\}}F_{r}=0\n(14)\nwhere\nÎ¸\nX\n\\theta_{X}\n,\nÎ¸\nY\n\\theta_{Y}\n, and\nÎ¸\nZ\n\\theta_{Z}\nare the Euler angles along the\nX\nX\n,\nY\nY\n, and\nZ\nZ\naxes, respectively. By optimizing these angles,\nR\n{\nS\n2\n}\n{\nC\n}\n{}^{\\{C\\}}R_{\\{S_{2}\\}}\ncan be computed using Eq.\n13\n.\nFigure 4:\nSetup of Experiment 1, designed to determine the transformation between the coordinate systems of the two force sensors (\nS\n1\nS_{1}\nand\nS\n2\nS_{2}\n). A flexible tissue representative was mounted on a Bota 6-DoF forceâ€“torque sensor (\nS\n2\nS_{2}\n), while the second sensor (\nS\n1\nS_{1}\n) was attached to the robotic armâ€™s end effector. The upper section of the mannequin was removed to eliminate friction forces at the RCM, allowing the measurement of pure toolâ€“tissue interaction forces for coordinate calibration.\n2.2.2\nNeural Tool-Tissue Force Estimation\nAfter determining the required rotation matrix to map\nS\n1\nS_{1}\nand\nS\n2\nS_{2}\n, the next step involved isolating the forces at the tool tip from those acting at the RCM. To achieve this, the upper section of the mannequin was reinstalled, and the same experimental procedure was performed on the flexible tissue representative while covering the full range of possible robot configurations and movements. By neglecting inertial effects and assuming quasi-static motion of the robotic arms, the system equations of force equilibrium on the surgical instrument were expressed as:\nâˆ‘\nF\n=\nF\n1\n+\nF\n2\n+\nF\n3\n=\n0\n\\sum{\\textbf{F}}=\\textbf{F}_{1}+\\textbf{F}_{2}+\\textbf{F}_{3}=0\n(15)\nwhere\nF\n1\nâˆˆ\nâ„\n1\nÃ—\n3\n\\textbf{F}_{1}\\in\\mathbb{R}^{1\\times 3}\n,\nF\n2\nâˆˆ\nâ„\n1\nÃ—\n3\n\\textbf{F}_{2}\\in\\mathbb{R}^{1\\times 3}\n, and\nF\n3\nâˆˆ\nâ„\n1\nÃ—\n3\n\\textbf{F}_{3}\\in\\mathbb{R}^{1\\times 3}\nare the forces at the robotic arm, RCM point, and tip forces. During the experiment,\nF\n1\n\\textbf{F}_{1}\n,\nF\n3\n\\textbf{F}_{3}\n, and quaternions of the robotic arm\nq\nwere captured. These parameters create a dataset of\n30000\nÃ—\n10\n30000\\times 10\nfor a fully connected feedforward NN designed for this regression task where\n(\nF\n1\n)\n3\nÃ—\n30000\n(\\textbf{F}_{1})_{3\\times 30000}\n,\n(\nq\n)\n4\nÃ—\n30000\n(\\textbf{q})_{4\\times 30000}\nwith 7 features and 30000 observations were the inputs and\n(\nF\n3\n)\n3\nÃ—\n30000\n(\\textbf{F}_{3})_{3\\times 30000}\nwith 3 features and 30000 observations were the output.\nFigure 5:\nInternal view of the mannequin used in Experiment 2, illustrating the setup for acquiring toolâ€“tissue interaction data to train the NN model. A surgical instrument, inserted through the upper access port, interacts with a flexible tissue representative mounted on a 6-DoF Bota forceâ€“torque sensor.\nAn endoscope provides visual feedback to monitor tool motion and contact inside the surgical workspace.\n2.3\nNIMA-based Force Reconstruction\nThis work presents a novel NIMA as an alternative to Direct Force Reflection (DFR) for delivering force feedback in remote surgical robotics. NIMA operates by identifying nonlinear tool-tissue contact impedance parameters, denoted as\nM\n, at the follower module in real-time and transmitting these parameters to the leader module. Simultaneously, motion commands\nX\nare sent to the follower module, where a representative laparoscopic tool interacts with a mannequin simulating soft tissue. For simplicity and generality, a polynomial nonlinear impedance model is employed to determine the NIMA parameters\nM\n. The contact force\nf\nâˆˆ\nâ„\n3\nÃ—\n1\n\\textbf{f}\\in\\mathbb{R}^{3\\times 1}\nis modeled as the response of a nonlinear impedance hyper-surface to the given motion command\nX\n:\nf\n=\nM\nX\n=\nM\nâ€‹\n(\nx\nN\nâ‹†\ny\nN\nâ‹†\nz\nN\nâ‹†\n)\n1\nÃ—\n9\nâ€‹\nN\nT\n\\displaystyle\\textbf{f}=\\textbf{M}\\textbf{X}=\\textbf{M}\\begin{pmatrix}\\textbf{x}_{N}^{\\star}&\\textbf{y}_{N}^{\\star}&\\textbf{z}_{N}^{\\star}\\end{pmatrix}^{T}_{1\\times 9N}\n(16)\nwhere\nT\ndenotes the transpose operator,\nx\nx\n,\ny\ny\n, and\nz\nz\nrepresent the motion commands (i.e., incremental positional changes of the instrument within a time interval\nÎ´\nâ€‹\nt\n\\delta t\n), and\nâ‹†\nN\n{}_{N}^{\\star}\nsignifies the augmented state operator of degree\nN\nN\n, defined as:\nu\nN\nâ‹†\n=\n(\nu\nu\nË™\nu\nÂ¨\nâ‹¯\nu\nN\nu\nË™\nN\nu\nÂ¨\nN\n)\n3\nâ€‹\nN\nÃ—\n1\n\\textbf{u}_{N}^{\\star}=\\begin{pmatrix}u&\\dot{u}&\\ddot{u}&\\cdots&u^{N}&\\dot{u}^{N}&\\ddot{u}^{N}\\end{pmatrix}_{3N\\times 1}\n(17)\nThe NIMA impedance parameter matrix\nM\nis structured as:\nM\n=\n(\nm\nx\n0\n0\n0\nm\ny\n0\n0\n0\nm\nz\n)\n3\nÃ—\n9\nâ€‹\nN\n\\textbf{M}=\\begin{pmatrix}\\textbf{m}_{x}&\\textbf{0}&\\textbf{0}\\\\\n\\textbf{0}&\\textbf{m}_{y}&\\textbf{0}\\\\\n\\textbf{0}&\\textbf{0}&\\textbf{m}_{z}\\end{pmatrix}_{3\\times 9N}\n(18)\nwhere\nm\ni\nâˆˆ\nâ„\n1\nÃ—\n3\nâ€‹\nN\n\\textbf{m}_{i}\\in\\mathbb{R}^{1\\times 3N}\nis a vector of impedance parameters and\n0\nâˆˆ\nâ„\n1\nÃ—\n3\nâ€‹\nN\n\\textbf{0}\\in\\mathbb{R}^{1\\times 3N}\nis a zero vector. This model assumes no cross-talk between the orthogonal tool-tissue forces along\nx\nx\n,\ny\ny\n, and\nz\nz\naxes. Cross-talk effects, if needed, can be modeled by including additional non-zero elements in\nM\n. To compute\nM\nin real-time, a rolling time window of\nÎ´\nâ€‹\nt\n=\n300\n\\delta t=300\nms was adopted. Delays under 300 ms are generally imperceptible to surgeons in leader-follower setups\nPerez2007\n, making this an effective worst-case scenario for updating impedance parameters. The NIMA parameters\nM\nare continuously updated using a rolling dataset of\nn\nn\nsample forces\nF\n^\nâˆˆ\nâ„\n3\nÃ—\nn\n\\hat{\\textbf{F}}\\in\\mathbb{R}^{3\\times n}\nand motion commands\nX\n^\n\\hat{\\textbf{X}}\n:\nM\n=\nF\n^\nâ€‹\nX\n^\n+\n\\displaystyle\\textbf{M}=\\hat{\\textbf{F}}\\hat{\\textbf{X}}^{+}\n(19)\nF\n^\n=\n(\nf\nt\nâˆ˜\nâ‹¯\nf\nt\nâˆ˜\n+\nÎ´\nâ€‹\nt\n)\n3\nÃ—\nn\n\\displaystyle\\hat{\\textbf{F}}=\\begin{pmatrix}\\textbf{f}_{t_{\\circ}}&\\cdots&\\textbf{f}_{t_{\\circ}+\\delta t}\\end{pmatrix}_{3\\times n}\n(20)\nX\n^\n=\n(\nX\nt\nâˆ˜\nâ‹¯\nX\nt\nâˆ˜\n+\nÎ´\nâ€‹\nt\n)\n9\nâ€‹\nN\nÃ—\nn\n\\displaystyle\\hat{\\textbf{X}}=\\begin{pmatrix}\\textbf{X}_{t_{\\circ}}&\\cdots&\\textbf{X}_{t_{\\circ}+\\delta t}\\end{pmatrix}_{9N\\times n}\n(21)\nHere,\nX\n^\n+\n\\hat{\\textbf{X}}^{+}\nrepresents the pseudo-inverse of\nX\n^\n\\hat{\\textbf{X}}\n, computed as:\nX\n^\n+\n=\nX\n^\nT\nâ€‹\n(\nX\n^\nâ€‹\nX\n^\nT\n)\nâˆ’\n1\n\\hat{\\textbf{X}}^{+}=\\hat{\\textbf{X}}^{T}(\\hat{\\textbf{X}}\\hat{\\textbf{X}}^{T})^{-1}\n(22)\nThe peg transfer task, a core element of the Fundamentals of Laparoscopic Surgery (FLS) curriculum\nFried2007\n, was chosen for evaluation. This task involved capturing the robotic armsâ€™ positions, orientations, and velocities using two Omega.7 haptic controllers, synchronized to the operatorâ€™s motion commands at a refresh rate of 1 kHz with a programmed delay of 300 ms. Force feedback, essential for realistic haptic interaction, was recorded via force sensors (SensOne, Bota Systems) at 2 kHz. These data were used to identify NIMA parameters and apply Eq.\n16\nto compute and render the desired 3D force,\nf\nd\n\\textbf{f}_{d}\n, on the haptic device. To optimize the polynomial degree\nN\nN\nfor NIMA, five parallel threads with\nN\n=\n1\nâ€‹\nâ‹¯\nâ€‹\n5\nN=1\\cdots 5\nwere executed, with the optimal\nN\nN\nfor each time window selected based on minimizing the 3D force reconstruction error.\n(a)\n(b)\nFigure 6:\n(a) Comparison of forces measured by the tip sensor (ground truth) and those estimated by the robot-mounted sensor in the\nY\nY\ndirection, showing strong agreement between the two datasets. The lower subplot illustrates the corresponding force estimation error over time. (b) Linear regression analysis between the measured and estimated forces in the\nY\nY\ndirection, demonstrating a strong correlation (\nR\n2\n=\n0.95\nR^{2}=0.95\n) and validating the accuracy of the coordinate system calibration.\n(a)\nX\nX\ndirection\n(b)\nY\nY\ndirection\n(c)\nZ\nZ\ndirection\nFigure 7:\nPerformance of the trained NN model in predicting tool-tip forces after removing frictional effects at the RCM. The plots show measured forces (ground truth) and model predictions along the (a)\nX\nX\n, (b)\nY\nY\n, and (c)\nZ\nZ\ndirections, with shaded regions representing the 95% confidence intervals. The close overlap between measured and predicted forces demonstrates the NN modelâ€™s high accuracy and consistency across all axes, validating its capability to capture nonlinear toolâ€“tissue interaction dynamics.\n3\nValidation Studies\nThis section presents a comprehensive validation of the proposed NIMA through a series of controlled experiments and performance evaluations. The results section is divided into three sections: results of Experiment 1, results of Experiment 2, and the results indicating the effectiveness of the presented NIMA model.\n(a)\nX\nX\ndirection\n(b)\nY\nY\ndirection\n(c)\nZ\nZ\ndirection\nFigure 8:\nResults of the no-contact experiment demonstrating the NN modelâ€™s capability to capture and subtract frictional forces at the RCM. Measured and predicted forces are compared along the (a)\nX\nX\n, (b)\nY\nY\n, and (c)\nZ\nZ\ndirections. The NN model effectively suppresses sensor noise and isolates minimal residual forces, confirming its ability to eliminate frictional artifacts and ensure precise estimation of tool-tip forces.\n3.1\nValidation I: correspondence\nThe first validation experiment, performed using the setup shown in Fig.\n4\n, evaluated whether the forces measured at the robot-mounted sensor (\nS\n1\nS_{1}\n) corresponded to the ground-truth toolâ€“tissue forces from the tissue-mounted sensor (\nS\n2\nS_{2}\n). Removing the upper half of the mannequin eliminated RCM friction, ensuring that both sensors captured only pure interaction forces. As illustrated in Fig.\n6\n, the force traces follow the same indentationâ€“release pattern, with both sensors exhibiting closely matched peaks, transitions, and overall waveform shape. The error signal remains tightly bounded around zero, indicating stable correspondence throughout the trial. Quantitatively, the Mean Absolute Error (MAE) between the two sensors was 0.11 N, 0.09 N, and 0.13 N for the\nX\nX\n,\nY\nY\n, and\nZ\nZ\ndirections, with Standard Deviations (SD) values of 0.11 N, 0.12 N, and 0.18 N. The strong linear correlation in Fig.\n6\n(b) (\nR\n2\n=\n0.95\nR^{2}=0.95\n) confirms that the calibrated transformation accurately aligns the two sensor frames.\n3.2\nValidation II: Tip Force Isolation\nFigure\n7\npresents the results of the trained NN model used to subtract friction forces at the RCM and isolate the tool-tip forces applied by the surgical instrument. In all three directions, the measured and predicted traces follow the same loading and unloading patterns, with close overlap during both rising and falling phases of the interaction. The shaded bands show the 95% confidence intervals, within which the predicted forces remain for the vast majority of the trial. Quantitatively, the MAE between the measured and predicted forces is 0.19 N, 0.16 N, and 0.16 N for the\nX\nX\n,\nY\nY\n, and\nZ\nZ\naxes, respectively, with SD values of 0.2 N across all axes. These errors are mainly localized around rapid changes in force and sign reversals, while plateau regions and quasi-static segments show almost complete agreement, indicating that the NN captures the underlying toolâ€“tissue dynamics and primarily leaves residuals associated with measurement noise and unmodeled transients. To further characterize the baseline error and verify that the NN does not introduce additional artifacts, an additional no-contact experiment was conducted (Fig.\n8\n). In this configuration, the surgical instrument did not interact with the tissue surrogate, so the recorded forces represent only frictional components at the RCM and the intrinsic sensor noise. The raw sensor measurements exhibit high-frequency fluctuations, whereas the NN predictions remain close to zero throughout the trial. The mean predicted forces were 0.02 N for all three axes, with SD values of 0.03 N, showing that the model effectively suppresses noise and frictional bias. The distribution of the predicted forces along one axis, shown in Fig.\n9\n, is tightly centered around zero, further confirming the stability of the estimator and its suitability for providing clean tool-tip force estimates for subsequent force rendering.\nFigure 9:\nA histogram illustrating the effectiveness of the NN model in accurately capturing forces at the RCM, facilitating the precise extraction of tool-tissue interaction forces.\nFigure 10:\nA comparison between the rendered force using NIMA and the measured force with DFR as the ground truth.\n3.3\nValidation III: NIMA-based Force Reconstruction\nThe effectiveness of our NIMA in accurately rendering forces on haptic devices is illustrated in Fig.\n10\n. Figure\n10\ncompares the forces rendered by the haptic device to the actual forces measured by force sensors. The implementation of NIMA achieved a MAE of 0.01 N, demonstrating high fidelity in force feedback. The errors followed a normal distribution with a SD of 0.02 N. This performance significantly exceeds that of the Linear IMA, which recorded an MAE of 0.2 N and an SD of 0.4 N. A comparative assessment reveals an 95% improvement in accuracy with NIMA. This enhancement underscores the superior precision of the nonlinear approach in generating force feedback, setting a new benchmark for realism and immersion in haptic interactions. The substantial reduction in MAE highlights the effectiveness of integrating nonlinear dynamics into impedance matching processes, improving both the quality and reliability of haptic feedback.\nFurthermore, Fig.\n10\ndemonstrates how NIMA addresses the kick-back behavior by rendering no forces for movements with velocities below 1 mm/s. Consequently, when the user releases the haptic device, no force is applied to the handles, ensuring they remain stationary. However, resisting forces are rendered with high accuracy once user input exceeds the velocity threshold, preserving NIMAâ€™s precision. A detailed analysis of the algorithmâ€™s performance throughout the experiment revealed a noteworthy preference for nonlinear models, with a nonlinear fit being selected in over 64% of the evaluated time windows. This reliance on nonlinear approaches highlights the limitations of the Linear IMA model in capturing the intricate, dynamic interactions between surgical tools and tissue. The complexity and variability of these interactions exceed the representational capabilities of linear models, emphasizing the nuanced nature of tool-tissue forces encountered during laparoscopic procedures. These findings suggest that temporal variations in these forces, which are crucial for realistic haptic feedback, are better captured through nonlinear modeling, enabling a more accurate and responsive simulation of surgical scenarios.\nThe experimental findings confirm that the proposed NIMA significantly enhances the stability, transparency, and realism of force feedback in RAMIS. A particularly noteworthy outcome across all trials was the complete absence of the phenomenon commonly known as the haptic kick, a sudden and undesirable jerk experienced by the operator when releasing the haptic interface. This behavior, often observed in conventional force feedback systems, typically results from the delayed response of the controller or the residual energy stored within the actuatorâ€“sensor loop. In the case of NIMA, the elimination of this artifact stems from the rapid convergence of the systemâ€™s output force vector\nX\nto zero within a short time interval\nÎ´\nâ€‹\nt\n\\delta t\nonce the operator releases the haptic device. Consequently, the desired force\nf\nd\n\\textbf{f}^{d}\nalso decays smoothly to zero, ensuring that no reactive forces are transmitted back to the user. This dynamic behavior guarantees passive stability and prevents unintended force reflections, thereby improving both operator comfort and system safety. These results are consistent with the impedance-based framework previously described by Sayadi\net al.\nsayadi2020impedance\n, but the nonlinear formulation implemented in NIMA allows a more robust and adaptive response across a broader range of toolâ€“tissue interaction conditions.\nBeyond mitigating haptic kickback, NIMA demonstrates strong adaptability and precision in representing nonlinear contact dynamics between the surgical instrument and the tissue phantom. Traditional linear impedance control approaches often oversimplify these interactions, limiting their ability to reproduce complex soft-tissue behaviors and variable stiffness profiles encountered during real surgical procedures. By incorporating higher-order nonlinear terms and updating impedance parameters in real time, NIMA effectively maintains consistent performance under varying operational and material conditions. The enhanced transparency of the system allows the operator to perceive genuine toolâ€“tissue forces without interference from control-induced artifacts, improving tactile perception during teleoperation. This capability is crucial in delicate surgical manipulations, where even small unintentional force fluctuations can lead to tissue deformation or procedural errors. The robustness and stability demonstrated by NIMA across multiple experimental conditions suggest its strong potential for integration into clinical robotic systems and surgical training simulators. By delivering stable, high-fidelity, and realistic haptic feedback, NIMA moves one step closer toward achieving the level of tactile awareness required for safe, precise, and intuitive humanâ€“robot interaction in next-generation RAMIS platforms.\n4\nConclusions\nThis study established NIMA as a stable and adaptable framework for delivering high-fidelity haptic feedback in RAMIS. By addressing key challenges in force rendering and system stability, NIMA provides surgeons with a realistic and safe interaction experience while maintaining smooth teleoperation. The framework integrates nonlinear impedance control with real-time force estimation, ensuring consistent and reliable haptic feedback under varying surgical conditions. Its design philosophy emphasizes simplicity, precision, and intrinsic safety, making it suitable for both clinical applications and surgical training environments.\nLooking forward, several directions can further enhance NIMAâ€™s capabilities and clinical readiness. Extending the framework to include torque feedback would provide surgeons with a richer and more intuitive sense of toolâ€“tissue interaction, improving precision in rotational maneuvers. Integration with physics-informed neural networks (PINNs) could enable model-free, self-supervised haptic rendering, allowing the system to adapt dynamically to varying surgical environments without explicit modeling. Structural optimization of the haptic interface could ensure consistent zero-force rendering when the surgeon releases the handle, improving passivity and long-term reliability. Expanding the system to multiple degrees of freedom for combined force and torque control would make it suitable for advanced bimanual surgical tasks such as suturing or dissection.\nFuture work should also focus on integrating NIMA into commercial surgical robots to accelerate clinical translation and regulatory validation. Real-time optimization and clinical trials will be essential to confirm system safety and performance under operative conditions. Also, incorporating surgeon training modules and collecting user feedback will facilitate smoother adoption in both educational and clinical settings. The adaptability of NIMA also opens avenues for applications beyond laparoscopic surgery, including neurosurgical and endovascular procedures, where precise force feedback is critical for safety and efficacy. By addressing these future directions, NIMA has the potential to set a new standard for haptic feedback in robotic-assisted interventions, bridging the gap between human dexterity and robotic precision to advance the next generation of surgical robotics.\n\\bmsection\n*Author contributions\nA. Mazidi: conceptualization, methodology, software, validation, formal analysis, investigation, data curation, writing â€“ original draft, writing â€“ review and editing, visualization.\nM. Roshanfar: formal analysis, writing â€“ review and editing.\nA. Sayadi: methodology, validation, formal analysis, writing â€“ review and editing.\nJ. Dargahi: conceptualization, resources, supervision, writing â€“ review and editing, funding acquisition.\nJ. Barralet: clinical guidance, methodology, writing â€“ review and editing.\nL. Feldman: clinical guidance, supervision, writing â€“ review and editing.\nA. Hooshiar: conceptualization, methodology, resources, supervision, writing â€“ review and editing, project administration, funding acquisition.\n\\bmsection\n*Acknowledgments\nThis research was supported by the Natural Science and Engineering Research Council (NSERC) of Canada through the NSERC CREATE Grant for Innovation at the Cutting Edge (ICE), the Fonds de Recherche du QuÃ©bec pour la Nature et les Technologies (FRQNT), Concordia University, Research Institute of McGill University Health Centre (RI-MUHC), Montreal General Hospital Foundation (MGHF), and McGill University.\n\\bmsection\n*Ethics Statement\nThe authors have nothing to report.\n\\bmsection\n*Conflict of interest\nThe authors declare no conflicts of interest.\n\\bmsection\n*Data Availability Statement\nThe data that support the findings of this study are available from the corresponding author upon reasonable request.\n\\bmsection\n*Permission to Reproduce Material From Other Sources\nAll figures, images, and content in this paper are original works created by the authors. No material has been reproduced from other sources.\nReferences\n\\bmsection\n*Author Biography\n{biography}\nAiden Mazidi\nis a Master of Science candidate at Concordia University, where he serves as a Research Assistant at both McGill Universityâ€™s Surgical Performance Enhancement and Robotics Center (SuPER) and the Surgical Robotic Laboratory (SRL) at Concordia University. Aidenâ€™s research is focused on force rendering and haptic feedback, with a particular emphasis on RAMIS. His work involves advancing precision surgical interventions through innovative technologies in collaboration with the Surgical Innovation Program at McGill University.\n{biography}\nMajid Roshanfar\nis a Postdoctoral Research Fellow at the Wilfred and Joyce Posluns Centre for Image-Guided Innovation and Therapeutic Intervention (PCIGITI) at The Hospital for Sick Children (SickKids) and the University of Toronto, Canada. He received his Ph.D. in Mechanical Engineering from Concordia University, where his research focused on hybrid-actuated soft robots with stiffness adaptation for surgical applications. His current work focuses on continuum manipulators, magnetic actuation, and force sensing for MIS.\n{biography}\nAmir Sayadi\nis a Ph.D. candidate and researcher at the Surgical Performance Enhancement and Robotics Centre (SuPER) at McGill University, Montreal, Canada. His research focuses on medical robotics, haptic interfaces, and intelligent control systems for minimally invasive and image-guided surgical applications. His work aims to advance the development of robotic platforms that improve surgical precision.\n{biography}\nJavad Dargahi\nreceived a Ph.D. degree in robotic tactile sensing from Glasgow Caledonian University. He joined the Department of Mechanical and Industrial Engineering, at Concordia University, Montreal, QC, where he is currently a Professor with the Department of Mechanical Engineering. He has co-authored three books in the scope of mechatronics and tactile sensing and has published more than 200 peer-reviewed articles. His research focuses on haptic sensors and feedback for MIS and robotics, as well as micro-manufacturing of sensors and actuators.\n{biography}\nJake Barralet\nis currently the Vice Chair of Research and Alan Thompson Chair of Surgical Research in the Department of Surgery at McGill University, Montreal, Canada. He also serves as the Scientific Director of the McGill Clinical Innovation Platform and the Associate Director of the Surgical and Interventional Sciences Program at the Research Institute of the McGill University Health Centre. His research focuses on surgical innovation and material-mediated tissue regeneration, with an emphasis on the role of inorganic ions in bone and skin healing.\n{biography}\nLiane S. Feldman\nis the Edward W. Archibald Professor and Chair of Surgery at McGill University and the Surgeon-in-Chief at the McGill University Health Centre. Her clinical focus is on minimally invasive gastrointestinal surgery, and her research aims to improve recovery and outcomes following abdominal operations. She leads a multidisciplinary team implementing evidence-based Enhanced Recovery care plans recognized by Accreditation Canada. She has served as the President of the Society of American Gastrointestinal and Endoscopic Surgeons and has held leadership positions in the Canadian Association of General Surgeons and the American College of Surgeons.\n{biography}\nAmir Hooshiar\nis an Assistant Professor, Edwards Distinguished Scientist, and the Director of Surgical Performance Enhancement and Robotics Centre (SuPER), at the Department of Surgery, McGill University. He specializes in the development of surgical robots and associated technologies such as surgical planning and navigation, sensing and actuation with smart materials, learning-based modelling and control of surgical robots.",
    "preview_text": "Background: The integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools. Methods: In this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively. Results: NIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01 (SD 0.02) N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic \"kickback\" by ensuring no force is applied by the haptic device to the user's hand when they release the handle, enhancing both patient safety and user comfort. Conclusion: NIMA's ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.\n\n\\copyyear\n2026\n\\startpage\n1\n\\authormark\nA. Mazidi\net al.\n\\titlemark\nRobust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery\n\\corres\nAmir Hooshiar (amir.hooshiar@mcgill.ca)\nRobust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery\nAiden Mazidi\nMajid Roshanfar\nAmir Sayadi\nJavad Dargahi\nJake Barralet\nLiane S. Feldman\nAmir Hooshiar\n\\orgdiv\nSurgical Robotics Laboratory (SRL), Mechanical Engineering Department,\n\\orgname\nConcordia University,\n\\orgaddress\n\\state\nMontreal, QC,\n\\country\nCana",
    "is_relevant": false,
    "relevance_score": 0.0,
    "extracted_keywords": [
        "haptic feedback",
        "robotic surgery",
        "impedance matching",
        "force rendering",
        "tool-tissue interaction"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§éçº¿æ€§é˜»æŠ—åŒ¹é…æ–¹æ³•ï¼ˆNIMAï¼‰ï¼Œç”¨äºæé«˜æœºå™¨äººè…¹è…”é•œæ‰‹æœ¯ä¸­çš„è§¦è§‰åé¦ˆç²¾åº¦å’Œå®‰å…¨æ€§ï¼Œä¸å¼ºåŒ–å­¦ä¹ ã€æ‰©æ•£æ¨¡å‹ã€å…¨èº«æ§åˆ¶ç­‰å…³é”®è¯æ— å…³ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T19:57:50Z",
    "created_at": "2026-01-27T15:53:14.943611",
    "updated_at": "2026-01-27T15:53:14.943618",
    "recommend": 0
}