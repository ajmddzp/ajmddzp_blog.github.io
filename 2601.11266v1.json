{
  "id": "2601.11266v1",
  "title": "Skill-Aware Diffusion for Generalizable Robotic Manipulation",
  "authors": [
    "Aoshen Huang",
    "Jiaming Chen",
    "Jiyu Cheng",
    "Ran Song",
    "Wei Pan",
    "Wei Zhang"
  ],
  "abstract": "Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.",
  "url": "https://arxiv.org/abs/2601.11266v1",
  "html_url": "https://arxiv.org/html/2601.11266v1",
  "html_content": "Skill-Aware Diffusion for Generalizable Robotic Manipulation\nAoshen Huang\n‚àó\n1\n, Jiaming Chen\n‚àó\n2\n, Jiyu Cheng\nüñÇ\n‚Äã\n1\n{}^{\\text{\\Letter}1}\n, Ran Song\n1\n, Wei Pan\n2\n, Wei Zhang\n1\n* Contributed equally to this work.\nüñÇ\n{}^{\\text{\\Letter}}\nCorresponding author.\n1\nSchool of Control Science and Engineering, Shandong University, Jinan 250061, China. (email:\naoshenhuang@mail.sdu.edu.cn; jycheng@sdu.edu.cn; ransong@sdu.edu.cn; info@vsislab.com)\n2\nDepartment of Computer Science, The University of Manchester, M13 9PL Manchester, U.K. (e-mail:\nppjmchen@gmail.com; wei.pan@manchester.ac.uk)\nAbstract\nRobust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at\nhttps://sites.google.com/view/sa-diff\n.\nI\nIntroduction\nLanguage-conditioned robotic manipulation\n[\n1\n,\n2\n,\n3\n]\n, which involves planning executable actions according to visual observation and language instruction, has gained increasing attention.\nAs a primary approach in this field, imitation learning has demonstrated powerful capability as it enables robots to learn directly from collected demonstrations\n[\n4\n,\n5\n]\n.\nHowever, achieving robust generalization across various objects and environments continues to be a persistent challenge\n[\n6\n]\n, as the classic imitation learning paradigm\n[\n7\n,\n8\n]\ndepends heavily on task-specific data and struggles to adapt to distributional shifts, thus affecting its adaptability and flexibility in unfamiliar scenarios.\nTo improve generalization in robotic manipulation, previous research has explored two main directions.\nOne direction focuses on enhancing visual representations in the end-to-end imitation learning framework by leveraging large-scale pre-training with incorporated auxiliary objectives\n[\n9\n,\n10\n,\n11\n]\n. For instance, Nair et al.\n[\n10\n]\nadopted time-contrastive learning to capture robust visual patterns, while Jia et al.\n[\n11\n]\nleveraged interaction prediction to model behavioral dynamics. However, these methods typically require large-scale data and extracted visual features inevitably include task-irrelevant elements (e.g., background details or textures), reducing their efficiency and precision.\nThe other direction formulates imitation learning as a two-stage framework, which first learns to predict future task-relevant motion representations (e.g., task video\n[\n12\n]\n, goal image\n[\n13\n]\n, or scene flow\n[\n14\n]\n) from easily accessible video data, and then leverages these representations as auxiliary inputs for visuomotor policy learning or directly maps them to executable actions. Among various motion representations, flow-based approaches\n[\n15\n,\n16\n]\nstand out for effectively emphasizing task-relevant motion dynamics while ignoring task-irrelevant visual distractions. However, attaining strong generalization with these methods remains dependent on large-scale training data. Moreover, such approaches face challenges in achieving a balance between robustness and precision when translating predicted 2D motion representations into executable 3D actions. For instance, learning-based strategies\n[\n14\n,\n15\n]\nare sensitive to intrinsic and extrinsic camera parameters, while vanilla heuristic-based strategies\n[\n16\n,\n17\n]\nsuffer from unstable performance due to sensor noise.\nIn contrast, this work proposes to extract and leverage the correlations across diverse tasks for data-efficient generalization rather than relying on large-scale data.\nInspired by the fact that different tasks within the same skill domain exhibit similar motion patterns\n[\n18\n,\n19\n,\n20\n]\n, we exploit skill-level information as additional guidance and propose the Skill-Aware Diffusion (SADiff) framework to improve generalization in robotic manipulation. First, a skill-aware encoding module equips learnable skill tokens to capture skill-specific information during interacting with multimodal inputs. Then, a skill-constrained diffusion model generates object-centric motion flow conditioned on the produced skill-aware token sequences, allowing the model to capture unified features across diverse tasks within the same skill domain while preserving task-specific information. Two skill-specific auxiliary losses, i.e., skill classification and skill contrastive losses, are introduced to distinguish motion patterns across diverse skills to enhance precision and robustness.\nFinally, we propose a skill-retrieval transformation strategy that translates the generated 2D motion flow to executable 3D actions by retrieving skill-specific priors to refine the 2D-to-3D mapping. Through systematically incorporating skill-specific information, SADiff achieves superior generalization across diverse tasks and environments without relying on large-scale data.\nFurthermore, existing robotic manipulation datasets\n[\n21\n,\n22\n,\n23\n]\nprimarily focus on task completion rates, lacking the granularity required to evaluate specific manipulation skills. To address this limitation, we construct IsaacSkill, a new dataset specifically designed for skill-centric evaluation. Built upon the high-fidelity NVIDIA Isaac Lab platform\n[\n24\n]\n, IsaacSkill covers fundamental robotic skills applied to a variety of tasks. It not only enables a comprehensive assessment of specific skill capabilities, but also provides the realistic dynamics required to support robust sim-to-real transfer.\nIn summary, the contributions of this work are as follows:\n1)\nWe propose the SADiff framework to improve generalization in robotic manipulation by explicitly modeling skill-level representations and systematically integrating them into the encoding, generation, and execution phases.\n2)\nWe develop a skill-constrained diffusion model that generates object-centric motion flow conditioned on skill-aware token sequences, concurrently reinforced by skill contrastive learning for robust flow generation.\n3)\nWe design a skill-retrieval transformation strategy that uses skill-specific priors to refine the mapping from predicted object-centric 2D motion flow to executable 3D actions, improving precision and consistency without additional training.\n4)\nWe construct IsaacSkill, a new dataset on the high-fidelity NVIDIA Isaac Lab platform, covering fundamental skills across diverse tasks to enable skill-centric evaluation and facilitate zero-shot sim-to-real transfer.\nThe rest of this article is organized as follows. Section\nII\nreviews related work. Section\nIII\npresents the problem formulation. Section\nIV\ndetails the proposed method. Section\nV\npresents experimental results and comparisons. Section\nVI\nconcludes this article.\nII\nRelated Work\nIn this section, we review recent works in robotic manipulation that aim at learning from demonstrations and enhancing generalization to novel tasks and environments.\nII-A\nLearning from Demonstrations in Robotic Manipulation\nImitation learning, which enables robots to directly learn from collected demonstrations, has recently become a primary approach in robotic manipulation\n[\n8\n,\n25\n,\n26\n,\n27\n,\n28\n,\n29\n]\n.\nIn the standard imitation learning framework, a policy network is trained to replicate expert behavior by learning a mapping from state observations to the corresponding actions\n[\n28\n]\n.\nFor instance, ACT\n[\n8\n]\nemployed a CVAE-transformer to predict k-step action chunks from visual observations, enabling effective manipulation with a few demonstrations. Chi et al.\n[\n29\n]\nproposed diffusion policy to model complex action distributions through a denoising diffusion process, efficiently handling the multimodality of trajectories in collected demonstrations.\nTo enhance human-robot interaction, language-conditioned imitation learning has emerged to enable robots to execute tasks following instructions, where visual inputs and language instructions are integrated to plan executable actions\n[\n1\n,\n30\n,\n31\n]\n. For instance, CLIPort\n[\n32\n]\nleveraged a pretrained CLIP\n[\n33\n]\nmodel to align visual and language features, allowing robots to directly interpret and perform language-instructed tasks. PerAct\n[\n34\n]\nemployed a perceiver transformer\n[\n35\n]\nto integrate language features with voxelized RGB-D observations, enabling precise action prediction. However, these methods typically suffer from poor generalization in unseen scenarios due to their heavy reliance on extensive, task-specific demonstrations. Crucially, they often process tasks in isolation, ignoring the shared motion patterns that cut across diverse tasks and thus struggle to adapt efficiently to new environments.\nII-B\nImproving Generalization in Imitation Learning\nTo enhance the generalization in imitation learning for robotic manipulation, one intuitive direction is enhancing visual encoders by pre-training on large-scale manipulation-related datasets\n[\n36\n,\n37\n]\nin an end-to-\nend imitation learning framework\n[\n9\n,\n10\n,\n11\n,\n38\n]\n. For instance, R3M\n[\n10\n]\npre-trained visual encoders on human video data, using time-contrastive learning and video-language alignment to obtain robust visual features. MVP\n[\n38\n]\nleveraged masked image reconstruction to develop visual representations for generalized robotic motion control. In addition, MPI\n[\n11\n]\nintroduced interaction prediction as an auxiliary objective to model behavioral patterns and physical interactions, improving the generalization of the visuomotor policy. However, these methods typically rely on large-scale datasets for pre-training, which requires labor-intensive demonstration collection and massive computational resources. More importantly, extracted visual representations often contain task-irrelevant details, such as background clutter and textures, which inevitably affect performance.\nAnother key direction formulates imitation learning as a two-stage framework\n[\n12\n,\n14\n,\n39\n]\n. The first stage focuses on predicting future task-relevant motion representations, such as task video\n[\n12\n]\n, goal image\n[\n13\n]\n, and scene flow\n[\n14\n]\n. In the second stage, these representations are used as auxiliary inputs to facilitate visuomotor policy learning or directly transformed into executable actions. Among diverse motion representations, motion flow in 2D pixel space has become a powerful intermediate representation in motion planning since it can be easily extracted from raw video data with advancing point tracking methods and learned at scale\n[\n39\n]\n. Representatively, Track2Act\n[\n14\n]\nemployed a goal-conditioned grid flow model to generate future trajectories for query points and translated these trajectories into residual actions via a policy network. Im2Flow2Act\n[\n15\n]\nlearned object-centric motion flow to capture the dynamics of the target objects and translated the motion flow into actions via a flow-conditioned policy network, facilitating embodiment-agnostic manipulation. However, these methods still rely heavily on large-scale datasets. In general, previous methods tend to treat tasks in isolation, thereby overlooking shared prior across diverse tasks and ignoring the explicit modeling of skill-level information. As a consequence, such methods often struggle to effectively transfer knowledge to unseen scenarios. In contrast, we explicitly incorporate skill-specific representations to bridge different tasks, thereby enhancing generalization to new scenarios with similar underlying mechanics.\nII-C\nDiffusion Model in Robotic Manipulation\nRecently, the diffusion model has shown remarkable potential in robotic manipulation\n[\n40\n,\n41\n,\n42\n]\n. By iteratively denoising noise into structured actions or trajectories, diffusion-based approaches can effectively model the variability in demonstrations, enabling smooth action generation\n[\n43\n]\n. Previous works have pioneered the integration of denoising diffusion probabilistic models (DDPMs) into visuomotor control, achieving superior performance by modeling the conditional probability of actions given visual inputs\n[\n29\n,\n44\n]\n. Subsequent research has further refined this paradigm by incorporating richer multimodal information (e.g., 3D visual representations\n[\n45\n]\nand tactile feedback\n[\n46\n]\n) together with more expressive geometric and semantic conditioning mechanisms\n[\n47\n]\n, thereby improving spatial reasoning and controllability of visuomotor policies.\nSome methods\n[\n17\n,\n48\n,\n49\n]\nleveraged diffusion models to generate future videos that serve as guidance for manipulation. For instance, UniPi\n[\n49\n]\nemployed a language-conditioned video diffusion model to synthesize future frames, subsequently using an inverse dynamics model to infer actions for execution. Unlike video-based methods that capture environmental details, flow-based methods focus on task-relevant motion dynamics, enabling more direct modeling of action-related movement patterns\n[\n14\n,\n15\n,\n50\n]\n. Following Im2Flow2Act\n[\n15\n]\n, SADiff adopts object-centric flow as the intermediate motion representation of task dynamics. Benefiting from the systematic integration of skill information into both the flow generation process and the 2D-to-3D action transformation, SADiff achieves improved robustness and generalization in various scenarios.\nFigure 1:\nOverview of the proposed Skill-Aware Diffusion (SADiff) framework. It is structured into three phases: (1) The encoding phase, where the skill-aware encoding module uses learnable skill tokens to interact with multimodal inputs and extract skill-specific information; (2) The generation phase, in which a skill-constrained diffusion model generates object-centric motion flow conditioned on the skill-aware token sequences, optimized by both denoising loss and two skill-specific auxiliary losses; and (3) The execution phase, which employs a skill-retrieval transformation strategy to translate the generated 2D motion flow into executable 3D trajectories by leveraging skill-specific priors.\nIII\nPreliminaries\nIII-A\nProblem Formulation\nFollowing existing works such as Im2Flow2Act\n[\n15\n]\nand Track2Act\n[\n14\n]\n, we decompose the robotic manipulation problem into two sub-problems. The first is flow generation, which aims to interpret the task from image observation and language instruction, and produce dense 2D motion flow that describes the pixel-level movement of the target object over a time horizon. The second sub-problem is to lift the 2D motion flow into a 3D trajectory and convert it into a sequence of executable actions through a flow transformation stage.\nFormally, given the task input of initial image observation\nI\nI\nand language instruction\nL\nL\n, the first sub-problem is to learn a flow generator\nùí¢\n\\mathcal{G}\nthat maps them to a 2D motion flow\nF\nF\n:\nF\n=\nùí¢\n‚Äã\n(\nI\n,\nL\n)\nF=\\mathcal{G}(I,L)\n(1)\nwhere\nF\n=\n{\nP\n1\n,\nP\n2\n,\n‚Ä¶\n,\nP\nT\n}\nF=\\{P_{1},P_{2},\\dots,P_{T}\\}\nrepresents the pixel-wise motion of the target object across\nT\nT\ntime steps,\nP\nt\nP_{t}\ndenotes a set of the 2D coordinates of keypoints on the target object at time step\nt\nt\n. The second sub-problem is to design a transformation strategy\nùíØ\n\\mathcal{T}\nthat maps the predicted flow\nF\nF\nto an executable action sequence\nA\nA\n:\nA\n=\nùíØ\n‚Äã\n(\nF\n)\nA=\\mathcal{T}(F)\n(2)\nwhere\nA\n=\n{\na\n1\n,\na\n2\n,\n‚Ä¶\n,\na\nT\n}\nA=\\{a_{1},a_{2},\\dots,a_{T}\\}\ndenotes the action sequence of end-effector over the task horizon, where each action\na\nt\na_{t}\nrepresents the 6-DoF end-effector pose at time step\nt\nt\n. We detailed the proposed approach in the following section.\nIII-B\nMotion Flow Extraction\nWe adopt the similar paradigm of motion flow extration as Im2Flow2Act\n[\n15\n]\n.\nFirst, we employ Qwen-VL\n[\n51\n]\nas an open-set detector to identify and locate the target object in the first frame of the video demonstration and obtain the corresponding bounding box. Then we uniformly sample a set of key points\nP\n1\n‚àà\n‚Ñù\n2\n√ó\nN\nx\n√ó\nN\ny\nP_{1}\\in\\mathbb{R}^{2\\times N_{x}\\times N_{y}}\nwithin the bounding box,\nwhere the first channel represents the 2D coordinates of the points, and\nN\nx\n√ó\nN\ny\nN_{x}\\times N_{y}\nindicates the number of sampled points. Next, we employ TAPIR\n[\n52\n]\nas a key-point tracker, which tracks the motion of each key-point across the entire video demonstration, yielding the motion flow of the target object. The extracted motion flow can be denoted as\nF\n=\n{\nP\n1\n,\nP\n2\n,\n‚Ä¶\n,\nP\nT\n}\n‚àà\n‚Ñù\n2\n√ó\nN\nx\n√ó\nN\ny\n√ó\nT\nF=\\{P_{1},P_{2},\\dots,P_{T}\\}\\in\\mathbb{R}^{2\\times N_{x}\\times N_{y}\\times T}\n, where each\nP\nt\nP_{t}\ncorresponds to the tracked key points in frame\nt\nt\n, and\nT\nT\nis the total number of frames in the demonstration.\nThe extracted motion flow serves as the training sample for the diffusion model.\nIV\nMethod\nFig.\n1\npresents the overview of SADiff. Its key components are detailed as follows: Section\nIV-A\nintroduces the skill-aware encoding module, which employs a set of learnable skill-aware tokens to capture skill-specific\ninformation while interacting with multimodal task inputs.\nSection\nIV-B\nintroduces the skill-constrained diffusion model that generates the motion flow of the target object conditioned on the skill-aware token sequences. We design a multi-objective training paradigm for flow generation that particularly introduces a skill contrastive training method for enhancing robustness. Finally, Section\nIV-C\ndetails the skill-retrieval transformation strategy, which uses skill-specific trajectory priors to guide the optimization-based transformation process, accurately converting the 2D motion flow into executable 3D actions.\nIV-A\nSkill-Aware Encoding\nWe design a skill-aware encoding module that performs interaction between multimodal inputs and extracts skill-specific information, as illustrated in Fig.\n2\n. The encoding process begins with an RGB image captured by a camera, represented as\nI\n‚àà\n‚Ñù\nH\n√ó\nW\n√ó\n3\nI\\in\\mathbb{R}^{H\\times W\\times 3}\n. Given a language instruction\nL\nL\n, such as\n‚ÄúPour water from the black cup into the porcelain bowl‚Äù\n, we use an advanced vision-language model, i.e., Qwen-VL\n[\n51\n]\n, to identify and locate target objects\n‚Äúblack cup‚Äù\nand\n‚Äúporcelain bowl‚Äù\n, extracting their bounding boxes\nB\n‚àà\n‚Ñù\nN\no\n√ó\n4\nB\\in\\mathbb{R}^{N_{o}\\times 4}\n, where\nN\no\nN_{o}\nrepresents the number of related objects.\nTo capture skill-specific information and facilitate dynamic interactions across multimodal inputs, we introduce a set of learnable skill tokens, which are randomly initialized and represented as\nS\n‚àà\n‚Ñù\nN\ns\n√ó\nD\nS\\in\\mathbb{R}^{N_{s}\\times D}\n, where\nN\ns\nN_{s}\ndenotes the number of skills. For a given task, skill-relevant tokens\nS\ni\n‚àà\n‚Ñù\nD\nS_{i}\\in\\mathbb{R}^{D}\n, are selected from\nS\nS\nbased on image observation\nI\nI\nand language instruction\nL\nL\nwith an MLP-based skill classifier, where\ni\ni\nis the index of the relevant skill. Furthermore, we encode\nI\nI\n,\nL\nL\n,\nB\nB\n, and\nS\ni\nS_{i}\nindependently to obtain initial token sequences\nv\nI\nv_{I}\n,\nv\nL\nv_{L}\n,\nv\nB\nv_{B}\n,\nv\nS\ni\nv_{S_{i}}\n, all transformed to a common encoding dimension\nD\nD\n. Specifically, we use the pre-trained CLIP\n[\n33\n]\nto encode both image and language. The bounding boxes are encoded using 2D sinusoidal positional encoding\n[\n53\n]\n, capturing spatial relationships. And the skill tokens are encoded using a simple fully-connected layer.\nTo generate semantically rich skill representations and enable effective information exchange across multimodal token sequences, we employ multi-head self-attention (MHSA) and multi-head cross-attention (MHCA)\n[\n54\n]\n. These mechanisms enhance the interaction of image, text, and bounding box tokens with skill tokens, facilitating the alignment and fusion of multimodal information. For conciseness, the MHSA and MHCA mechanisms are formally defined as follows:\nMHSA\n‚Äã\n(\nX\n)\n=\nœÉ\n‚Äã\n(\nf\nQ\n‚Äã\n(\nX\n)\n‚Äã\nf\nK\n‚Äã\n(\nX\n)\nùñ≥\n)\n‚Äã\nf\nV\n‚Äã\n(\nX\n)\n\\text{MHSA}(X)=\\sigma(f_{Q}(X)f_{K}(X)^{\\mathsf{T}})f_{V}(X)\n(3)\nand\nMHCA\n‚Äã\n(\nX\n,\nY\n)\n=\nœÉ\n‚Äã\n(\nf\nQ\n‚Äã\n(\nX\n)\n‚Äã\nf\nK\n‚Äã\n(\nY\n)\nùñ≥\n)\n‚Äã\nf\nV\n‚Äã\n(\nY\n)\n\\text{MHCA}(X,Y)=\\sigma(f_{Q}(X)f_{K}(Y)^{\\mathsf{T}})f_{V}(Y)\n(4)\nwhere\nœÉ\n\\sigma\nis the softmax function, while\nf\nQ\nf_{Q}\n,\nf\nK\nf_{K}\n, and\nf\nV\nf_{V}\nrepresent the query, key, and value mapping functions, respectively.\nThe multimodal token sequencse\nv\nI\n,\nv\nL\n,\nv\nB\nv_{I},v_{L},v_{B}\nfirst undergoes initial self-encoding via MHSA, then the skill token sequences act as the query inputs, while the remaining token sequences serve as key and value inputs in MHCA. This process is followed by a feed-forward network (FFN) implemented with multi-layer perceptrons (MLPs). The interaction process is expressed as:\nv\nx\n‚Ä≤\n=\nFFN\n‚Äã\n(\nMHCA\n‚Äã\n(\nv\nS\ni\n,\nMHSA\n‚Äã\n(\nv\nx\n)\n)\n)\nv^{\\prime}_{x}=\\text{FFN}(\\text{MHCA}(v_{S_{i}},\\text{MHSA}(v_{x})))\n(5)\nwhere\nx\n‚àà\n{\nI\n,\nL\n,\nB\n}\nx\\in\\{I,L,B\\}\n. The resulting skill-aware token sequences\nv\nS\ni\n,\nv\nI\n‚Ä≤\n,\nv\nL\n‚Ä≤\n,\nv\nB\n‚Ä≤\nv_{S_{i}},v^{\\prime}_{I},v^{\\prime}_{L},v^{\\prime}_{B}\nare used as conditioning inputs for the diffusion model in the subsequent flow generation stage. Unlike existing methods\n[\n15\n]\nthat often process multimodal inputs independently and rely on static feature alignment, our skill-aware encoding module introduces learnable skill tokens and leverages MHSA and MHCA mechanisms to dynamically align and fuse skill-specific features across modalities. Through this encoding process, the resulting skill-aware token sequences capture shared features across diverse tasks within the same skill category, while preserving task-specific details, thereby providing an effective condition for the following skill-constrained diffusion model.\nFigure 2:\nArchitecture of the skill-aware encoding module. The skill-aware encoding module integrates image, language, and bounding boxes of relevant objects with learnable skill tokens through attention-based interactions, producing skill-aware token sequences.\nIV-B\nSkill-Constrained Flow Generation\nTo generate a precise 2D object motion flow that is aligned with a specific skill, the current task, and the target objects, we propose a novel skill-constrained diffusion model. This model is equipped with a denoising loss to supervise flow generation and two auxiliary skill-specific losses that guide the model to capture semantic clues related to the skill. We first introduce the diffusion model for flow generation, and then explain how we design and leverage denoising loss and two auxiliary losses to train the model.\nIV-B\n1\nDiffusion Model for Flow Generation\nFirst, the motion flow\nF\nF\nis encoded into a latent representation space using a pre-trained VAE\n[\n55\n]\nencoder\nE\nœï\n‚Äã\n(\n‚ãÖ\n)\nE_{\\phi}(\\cdot)\n, denoted as:\nz\n0\n=\nE\nœï\n‚Äã\n(\nF\n)\nz_{0}=E_{\\phi}(F)\n(6)\nwhere\nz\n0\nz_{0}\nis the initial latent representation of the flow. Then the latent representation\nz\n0\nz_{0}\nis perturbed over\nk\nk\nsteps using the forward diffusion process to derive the perturbed latent representation\nz\nk\nz_{k}\n:\nz\nk\n=\nŒ±\n¬Ø\nk\n‚Äã\nz\n0\n+\n1\n‚àí\nŒ±\n¬Ø\nk\n‚Äã\nœµ\n,\nœµ\n‚àº\nùí©\n‚Äã\n(\n0\n,\nI\n)\nz_{k}=\\sqrt{\\overline{\\alpha}_{k}}z_{0}+\\sqrt{1-\\overline{\\alpha}_{k}}\\epsilon,\\epsilon\\sim\\mathcal{N}(0,I)\\\n(7)\nwhere\nŒ±\n¬Ø\nk\n\\bar{\\alpha}_{k}\nis the cumulative noise schedule at step\nk\nk\nand\nùí©\n‚Äã\n(\n0\n,\nI\n)\n\\mathcal{N}(0,I)\nis Gaussian noise distribution.\nSubsequently,\nz\nk\nz_{k}\nis processed by a UNet-based noise prediction network\nœµ\nŒ∏\n\\epsilon_{\\theta}\nto predict perturbed noise, conditioned on skill-aware token sequences\nv\nS\ni\n=\n[\nv\nS\ni\n,\nv\nI\n‚Ä≤\n,\nv\nL\n‚Ä≤\n,\nv\nB\n‚Ä≤\n]\n\\textbf{v}_{S_{i}}=[v_{S_{i}},v^{\\prime}_{I},v^{\\prime}_{L},v^{\\prime}_{B}]\n:\nœµ\n^\n=\nœµ\nŒ∏\n‚Äã\n(\nz\nk\n,\nv\nS\ni\n)\n\\hat{\\epsilon}=\\epsilon_{\\theta}(z_{k},\\textbf{v}_{S_{i}})\n(8)\nwhere\n[\n‚ãÖ\n]\n[\\cdot]\nis the concatenation operation, and\nœµ\n^\n\\hat{\\epsilon}\nis the predicted noise.\nAligning with AnimatedDiff\n[\n56\n]\nand Im2Flow2Act\n[\n15\n]\n, we integrate an MHSA layer as a motion module into\nœµ\nŒ∏\n\\epsilon_{\\theta}\nthat performs information exchange on each spatial feature along the temporal dimension to capture temporal dependencies across frames.\nDuring inference, the reverse diffusion process starts by sampling random noise\nz\n^\nm\n‚àº\nùí©\n‚Äã\n(\n0\n,\nI\n)\n\\hat{z}_{m}\\sim\\mathcal{N}(0,I)\nin the latent space. Then we iteratively denoise\nm\nm\ntimes to generate the latent\nz\n^\n0\n\\hat{z}_{0}\n. Finally, a pre-trained decoder\nD\nœï\n‚Äã\n(\n‚ãÖ\n)\nD_{\\phi}(\\cdot)\nmaps the latent\nz\n^\n0\n\\hat{z}_{0}\nback to the pixel-space to generate object motion flow\nF\n^\n\\hat{F}\n:\nF\n^\n=\nD\nœï\n‚Äã\n(\nz\n^\n0\n)\n.\n\\hat{F}=D_{\\phi}(\\hat{z}_{0}).\n(9)\nIV-B\n2\nSkill-Constrained Training\nTo ensure that the generated motion flow is precise and aligned with the intended skill, we train the diffusion model with a combination of the primary denoising loss for flow generation and two skill-specific auxiliary losses. Fig.\n3\nillustrates the overview of the skill-constrained diffusion model and its optimization objectives.\nDenoising Loss\nConsistent with previous diffusion-based approaches\n[\n15\n,\n56\n,\n57\n]\n, we adopt a mean squared error (MSE) loss between the predicted noise\nœµ\n^\n\\hat{\\epsilon}\nand the ground-truth noise\nœµ\n\\epsilon\n:\n‚Ñí\nMSE\n=\n‚Äñ\nœµ\n^\n‚àí\nœµ\n‚Äñ\n2\n.\n\\mathcal{L}_{\\text{MSE}}=\\left\\|\\hat{\\epsilon}-\\epsilon\\right\\|^{2}.\n(10)\nThis objective facilitates precise denoising by minimizing the divergence between predicted and ground-truth noise.\nSkill Classification Loss\nTo enable the skill-aware encoding module to select the optimal skill-relevant tokens from the full set of learnable skill tokens\nS\n‚àà\n‚Ñù\nN\ns\n√ó\nD\nS\\in\\mathbb{R}^{N_{s}\\times D}\n, we design an auxiliary classification branch based on MLPs. Specifically, the image feature\nv\nI\nv_{I}\nand the language feature\nv\nL\nv_{L}\nencoded by the pre-trained CLIP encoder are concatenated and passed through the MLP-based skill classifier to predict the skill category\ny\n^\ni\n\\hat{y}_{i}\n, which indicates the most relevant skill tokens\nS\ni\n‚àà\n‚Ñù\nD\nS_{i}\\in\\mathbb{R}^{D}\n. The selected tokens\nS\ni\nS_{i}\nare then used in the subsequent multimodal encoding stage as described in Section\nIV-A\n, ensuring that the skill-aware token sequences are produced correctly. To supervise this skill classification, we adopt a standard cross-entropy (CE) loss:\n‚Ñí\nCE\n=\n‚àí\n‚àë\ni\n=\n1\nN\ns\ny\ni\n‚Äã\nlog\n‚Å°\n(\ny\n^\ni\n)\n\\mathcal{L}_{\\text{CE}}=-\\sum_{i=1}^{N_{s}}y_{i}\\log(\\hat{y}_{i})\n(11)\nwhere\ny\ni\ny_{i}\nrepresents the one-hot encoded ground truth label for the skill category and\ny\n^\ni\n\\hat{y}_{i}\ndenotes the predicted logits.\nFigure 3:\nOverview of the skill-constrained diffusion model. The diffusion model generates motion flow by jointly optimizing skill classification loss, skill contrastive loss, and denoising loss to ensure accurate skill selection, semantic alignment, and precise flow generation.\nSkill Contrastive Loss\nTo further ensure that the diffusion model distinguishes between motion patterns of different skills and aligns them with their semantic descriptions, we propose a skill contrastive loss that aligns the intermediate features of\nœµ\nŒ∏\n\\epsilon_{\\theta}\nwith the semantic representations of the pre-defined skill prompts. Specifically, we construct a skill prompt bank\nv\nC\n‚àà\n‚Ñù\nN\ns\n√ó\nD\nv_{C}\\in\\mathbb{R}^{N_{s}\\times D}\nby encoding high-level skill categories (e.g.,\n‚ÄúPouring‚Äù\n,\n‚ÄúPicking and Placing‚Äù\n) using the CLIP text encoder.\nFor the target skill token\nS\ni\nS_{i}\n, the skill-aware token sequences are computed according to Eq.\n(\n5\n)\n, yielding the positive skill-aware sequence\nv\nS\ni\n=\n[\nv\nS\ni\n,\nv\nI\n‚Ä≤\n,\nv\nL\n‚Ä≤\n,\nv\nB\n‚Ä≤\n]\n\\textbf{v}_{S_{i}}=[v_{S_{i}},v^{\\prime}_{I},v^{\\prime}_{L},v^{\\prime}_{B}]\n, which is then feed as the condition for the UNet\nœµ\nŒ∏\n\\epsilon_{\\theta}\n. We then extract the hidden feature representation from the middle layer of the UNet and project it to match the dimensionality of the CLIP features, which produces\nh\ni\nh_{i}\n.\nTo construct negative pairs, we treat all other skill tokens\nS\nj\n‚àà\n‚Ñù\nD\nS_{j}\\in\\mathbb{R}^{D}\n(\nj\n‚â†\ni\nj\\neq i\n) as negative instances which\nare processed identically to the positive sample to generate mismatching intermediate features\nh\nj\nh_{j}\n. Consequently, the positive pair\n(\nh\ni\n,\nv\nC\ni\n)\n(h_{i},v_{C_{i}})\nis contrasted against the negative pairs\n(\nh\nj\n,\nv\nC\ni\n)\n(h_{j},v_{C_{i}})\nto enforce semantic alignment in the feature space. We utilize cosine similarity to quantify the alignment:\nŒ±\ni\n,\nj\n=\ncos\n‚Äã\n(\nh\ni\n,\nv\nC\nj\n)\n\\alpha_{i,j}=\\text{cos}(h_{i},v_{C_{j}})\n(12)\nThe skill contrastive loss is formulated as:\n‚Ñí\nContrastive\n=\n‚àí\nlog\n‚Å°\nexp\n‚Å°\n(\nŒ±\ni\n,\ni\n/\nœÑ\n)\nexp\n‚Å°\n(\nŒ±\ni\n,\ni\n/\nœÑ\n)\n+\n‚àë\nj\n‚â†\ni\nexp\n‚Å°\n(\nŒ±\ni\n,\nj\n/\nœÑ\n)\n\\mathcal{L}_{\\text{Contrastive}}=-\\log\\frac{\\exp(\\alpha_{i,i}/\\tau)}{\\exp(\\alpha_{i,i}/\\tau)+\\sum_{j\\neq i}\\exp(\\alpha_{i,j}/\\tau)}\n(13)\nwhere\nœÑ\n\\tau\nis a temperature parameter. This objective encourages the model to align the intermediate feature\nh\ni\nh_{i}\nof the correct skill token with its corresponding skill prompt\nv\nC\ni\nv_{C_{i}}\n, while pushing the features\nh\nj\nh_{j}\nderived from incorrect skill tokens away from that prompt. This skill contrastive loss effectively constrains the diffusion model to distinguish between motion patterns of different skills during the denoising process.\nOverall Objective\nThe overall training objective is the weighted combination of the denoising loss and the two skill-constrained objectives:\n‚Ñí\ntotal\n=\n‚Ñí\nMSE\n+\nœâ\n1\n‚Äã\n‚Ñí\nCE\n+\nœâ\n2\n‚Äã\n‚Ñí\nContrastive\n\\mathcal{L}_{\\text{total}}=\\mathcal{L}_{\\text{MSE}}+\\omega_{1}\\mathcal{L}_{\\text{CE}}+\\omega_{2}\\mathcal{L}_{\\text{Contrastive}}\n(14)\nwhere\nœâ\n1\n\\omega_{1}\nand\nœâ\n2\n\\omega_{2}\nare hyperparameters that balance contributions of respective loss. By jointly optimizing these losses, the model learns to predict motion flow that is spatially precise and semantically aligned with the intended skill, thereby achieving robust and skill-aware motion flow generation.\nIV-C\nSkill-Retrieval Transformation from Flow to Actions\nMapping the predicted object-centric 2D motion flow\nF\n^\n=\n{\nP\n1\n,\nP\n2\n,\n‚Ä¶\n,\nP\nT\n}\n\\hat{F}=\\{P_{1},P_{2},\\dots,P_{T}\\}\ninto 3D space and transform it into executable actions is a critical step. In this section, we first formulate the conventional geometric optimization approach typically used, which serves as the foundation for our method. Subsequently, we will introduce our skill-retrieval strategy designed to refine this process.\nIV-C\n1\nMotion Estimation via Geometric Optimization\nGiven the initial depth observation captured by the RGB-D camera, the key-point set\nP\n1\nP_{1}\nin the first frame of\nF\n^\n\\hat{F}\nis back-projected into 3D space, resulting in\nùí´\n1\n‚àà\n‚Ñù\nN\n√ó\n3\n\\mathcal{P}_{1}\\in\\mathbb{R}^{N\\times 3}\n, where\nN\nN\nrepresents the total number of tracked points. To obtain the 3D trajectory of the task-related object, we follow the standard formulation in AVDC\n[\n17\n]\n, modeling the rigid motion from the initial frame\nt\n1\nt_{1}\nto any subsequent frame\nt\nt\nusing a transformation matrix\nùëª\nt\n1\n‚Üí\nt\n‚àà\n‚Ñù\n4\n√ó\n4\n\\boldsymbol{T}_{t_{1}\\rightarrow t}\\in\\mathbb{R}^{4\\times 4}\n. This matrix consists of a rotational component\nùëπ\nt\n1\n‚Üí\nt\n\\boldsymbol{R}_{t_{1}\\rightarrow t}\nand a translational component\nùíï\nt\n1\n‚Üí\nt\n\\boldsymbol{t}_{t_{1}\\rightarrow t}\n, describing the object‚Äôs change in position and orientation within the camera coordinate system. To estimate\nùëª\nt\n1\n‚Üí\nt\n\\boldsymbol{T}_{t_{1}\\rightarrow t}\ncorresponding to the predicted 2D motion flow\nF\n^\n\\hat{F}\n, we initialize it as an identity matrix and optimize the parameters by minimizing the reprojection error:\nE\nt\n=\n‚àë\nn\n=\n1\nN\n‚Äñ\nP\nt\n,\nn\n‚àí\nK\n‚Äã\nùëª\nt\n1\n‚Üí\nt\n‚Äã\nùí´\n1\n,\nn\n‚Äñ\n2\nE_{t}=\\sum_{n=1}^{N}||P_{t,n}-K\\boldsymbol{T}_{t_{1}\\rightarrow t}\\mathcal{P}_{1,n}||^{2}\n(15)\nwhere\nE\nt\nE_{t}\ndenotes the optimization objective,\nP\nt\n,\nn\nP_{t,n}\nis the\nn\nn\n-th point in frame\nt\nt\nof the predicted flow,\nK\nK\ndenotes the camera intrinsic matrix, and\nùí´\n1\n,\nn\n\\mathcal{P}_{1,n}\nrepresents the 3D coordinates of the\nn\nn\n-th point in\nùí´\n1\n\\mathcal{P}_{1}\n. By minimizing\nE\nt\nE_{t}\nusing a non-linear least squares algorithm, such as Levenberg‚ÄìMarquardt\n[\n58\n]\n, we can obtain the optimized transformation matrix\nùëª\n^\nt\n1\n‚Üí\nt\n\\boldsymbol{\\hat{T}}_{t_{1}\\rightarrow t}\n.\n1\n2\nInput :\nPredicted 2D motion flow\nF\n^\n=\n{\nP\n1\n,\n‚Ä¶\n,\nP\nT\n}\n\\hat{F}=\\{P_{1},\\dots,P_{T}\\}\n, initial depth frame, camera intrinsics\nK\nK\n, skill templates\nœà\n\\psi\n, skill category\ns\ns\nOutput :\nOptimized 3D transformations matrix\n{\nùëª\n^\nt\n1\n‚Üí\nt\n}\nt\n=\n1\nT\n\\{\\boldsymbol{\\hat{T}}_{t_{1}\\rightarrow t}\\}_{t=1}^{T}\n3\n4\nBack-project keypoints\nP\n1\nP_{1}\nusing initial depth to obtain initial 3D points\nùí´\n1\n‚àà\n‚Ñù\nN\n√ó\n3\n\\mathcal{P}_{1}\\in\\mathbb{R}^{N\\times 3}\n;\n5\nRetrieve skill template\nœà\ns\n‚àà\n‚Ñù\nT\n√ó\n3\n\\psi_{s}\\in\\mathbb{R}^{T\\times 3}\ncorresponding to skill category\ns\ns\n;\n6\nAlign\nœà\ns\n‚àà\n‚Ñù\nT\n√ó\n3\n\\psi_{s}\\in\\mathbb{R}^{T\\times 3}\nwith initial object configuration to obtain trajectory prior\nœÜ\n‚àà\n‚Ñù\nT\n√ó\n3\n\\varphi\\in\\mathbb{R}^{T\\times 3}\n;\n7\n8\nfor\nt\n=\n1\nt=1\nto\nT\nT\ndo\n9\nInitialize transformation\nùëª\nt\n1\n‚Üí\nt\n‚Üê\nùêà\n4\n√ó\n4\n\\boldsymbol{T}_{t_{1}\\rightarrow t}\\leftarrow\\mathbf{I}_{4\\times 4}\n;\n10\nDefine joint optimization objective\nE\nt\nE_{t}\nby Eq.\n(\n16\n)\n;\n11\nOptimize\nE\nt\nE_{t}\nusing a non-linear least\nsquares algorithm (e.g., Levenberg‚ÄìMarquardt) to obtain\nùëª\n^\nt\n1\n‚Üí\nt\n\\boldsymbol{\\hat{T}}_{t_{1}\\rightarrow t}\n;\n12\n13\n14\nreturn\n{\nùêì\n^\nt\n1\n‚Üí\nt\n}\nt\n=\n1\nT\n\\{\\boldsymbol{\\hat{T}}_{t_{1}\\rightarrow t}\\}_{t=1}^{T}\nAlgorithm¬†1\nSkill-Retrieval Transformation Strategy\nIV-C\n2\nSkill-Retrieval Transformation Strategy\nWhile minimizing reprojection error theoretically allows for the mapping of predicted 2D motion flow to 3D trajectories, practical deployments are often hindered by depth ambiguities in subsequent frames and sensor noise, which can lead to trajectory inaccuracies and discontinuities. To address these limitations, we introduce skill-specific trajectory priors into the optimization framework. These priors act as high-level guides, constraining the optimization process to motion patterns that are consistent with the intended skill, thereby enhancing both precision and physical consistency.\nSpecifically, we first construct a bank of trajectory templates offline. Utilizing monocular depth estimation (e.g., DepthAnythingV2\n[\n59\n]\n), we obtain pseudo-depth maps from video demonstrations to lift the extracted 2D motion flows into 3D space. For each skill category, we compute the average 3D trajectory from relevant demonstrations and normalize it spatially. The resulting normalized trajectories form a set of templates\nœà\n‚àà\n‚Ñù\nN\ns\n√ó\nT\n√ó\n3\n\\psi\\in\\mathbb{R}^{N_{s}\\times T\\times 3}\n, where\nN\ns\nN_{s}\ndenotes the number of skills and\nT\nT\nrepresents the temporal sequence length. During the deployment phase, we retrieve the template\nœà\ni\n‚àà\n‚Ñù\nT\n√ó\n3\n\\psi_{i}\\in\\mathbb{R}^{T\\times 3}\ncorresponding to the predicted skill category of the current task. This normalized template is spatially aligned through translation and scaling based on the initial configuration of the task-related object, yielding a task-specific trajectory prior denoted as\nœÜ\n‚àà\n‚Ñù\nT\n√ó\n3\n\\varphi\\in\\mathbb{R}^{T\\times 3}\n.\nWe introduce this prior into the optimization process by refining the objective function defined in Eq.\n(\n15\n)\n. The modified objective encourages the estimated 3D trajectory to minimize geometric reprojection error while simultaneously aligning with the retrieved skill prior:\nE\nt\n=\n‚àë\nn\n=\n1\nN\n‚Äñ\nP\nt\n,\nn\n‚àí\nK\n‚Äã\nùëª\nt\n1\n‚Üí\nt\n‚Äã\nùí´\n1\n,\nn\n‚Äñ\n2\n+\nŒª\n‚Äã\n‚àë\nn\n=\n1\nN\n‚Äñ\nùí´\nt\n,\nn\n‚àí\nœÜ\nt\n‚Äñ\n2\nE_{t}=\\sum_{n=1}^{N}||P_{t,n}-K\\boldsymbol{T}_{t_{1}\\rightarrow t}\\mathcal{P}_{1,n}||^{2}+\\lambda\\sum_{n=1}^{N}||\\mathcal{P}_{t,n}-\\varphi_{t}||^{2}\n(16)\nwhere\nùí´\nt\n,\nn\n=\nùëª\nt\n1\n‚Üí\nt\n‚Äã\nùí´\n1\n,\nn\n\\mathcal{P}_{t,n}=\\boldsymbol{T}_{t_{1}\\rightarrow t}\\mathcal{P}_{1,n}\nrepresents the transformed 3D coordinate in frame\nt\nt\n,\nœÜ\nt\n\\varphi_{t}\ndenotes the corresponding waypoint from the skill prior\nœÜ\n\\varphi\n, and\nŒª\n\\lambda\nis a hyperparameter balancing the two terms. By optimizing this joint objective, the system produces a transformation matrix\nùëª\n^\nt\n1\n‚Üí\nt\n\\boldsymbol{\\hat{T}}_{t_{1}\\rightarrow t}\nthat are not only geometrically plausible but also robust to noise and faithful to the characteristic motion patterns of the skill. The complete pipeline of skill-retrieval transformation strategy is summarized in Algorithm\n1\n.\nIV-C\n3\nMapping to Executable Actions\nThe optimized transformation matrix is further transformed from the camera coordinate system to the robot base coordinate system. By applying the relative offset between the end-effector and the object, we derive the target 3D end-effector path. Inverse Kinematics (IK) is then applied to generate smooth joint position and velocity commands, completing the mapping from 2D flow to executable robotic actions.\nV\nExperiments\nIn this section, we first introduce the dataset construction in Section\nV-A\n, followed by the training details in Section\nV-B\n. Section\nV-C\npresents comparative evaluations across within-distribution, challenging generalization, and skill adaptation settings, alongside component-wise ablation studies in simulation. Section\nV-D\nverifies the method‚Äôs generalization and zero-shot sim-to-real transfer capabilities in real-world scenarios. Finally, Section\nV-E\nevaluates the scalability and composability of the proposed approach.\nFigure 4:\nOverview of the constructed IsaacSkill dataset. The dataset comprises five fundamental manipulation skills: ‚Äú\nPouring\n‚Äù, ‚Äú\nPicking&Placing\n‚Äù, ‚Äú\nPushing\n‚Äù, ‚Äú\nSlide Opening\n‚Äù, and ‚Äú\nHinge Opening\n‚Äù. Each skill includes three different tasks involving different objects.\nV-A\nDataset Construction\nWhile datasets such as Meta-World\n[\n21\n]\n, RLBench\n[\n22\n]\n, and LIBERO\n[\n23\n]\nhave advanced the field of robotic manipulation, they rely on simplistic simulations with coarse physics and limited visual fidelity. Crucially, these simplified environments fail to capture the complex dynamics required for real-world interaction, rendering them inadequate for rigorous generalization evaluation and zero-shot sim-to-real transfer. To bridge this fidelity gap, we constructed IsaacSkill, a novel dataset built on the high-fidelity NVIDIA Isaac Lab\n[\n24\n]\nplatform, ensuring physically accurate dynamics and photorealistic scenarios. Furthermore, unlike prior works\n[\n15\n,\n14\n,\n17\n]\nthat focus on a narrow set of isolated tasks, IsaacSkill is structured around 5 fundamental robotic skills: ‚Äú\nPouring\n‚Äù, ‚Äú\nPicking & Placing\n‚Äù, ‚Äú\nPushing\n‚Äù, ‚Äú\nSlide Opening\n‚Äù, and ‚Äú\nHinge Opening\n‚Äù. This skill-centric approach allows us to encompass a wide variety of tasks involving diverse objects and scenarios, as shown in Fig.\n4\n. To ensure robustness, we design 3 different tasks per skill, introducing high variability in object geometry, texture, and placement, alongside randomized distractor objects. In total, the dataset comprises 2,400 trajectory demonstrations (160 per task), providing a comprehensive benchmark for skill-generalizable manipulation.\nIn our dataset, the robotic platform consists of a Franka Emika Panda manipulator equipped with a parallel-jaw gripper. A single RGB camera is mounted in a third-person perspective, positioned at an elevated frontal angle to capture the entire workspace. During data collection, we employ an oracle policy to ensure successful execution for each task. The resulting demonstrations are recorded as single-view RGB videos and temporally downsampled to 32 frames to maintain consistency. To extract the motion flow of task-relevant objects, we leverage Qwen-VL\n[\n51\n]\nto first localize target objects based on language instructions. Subsequently, keypoints are uniformly sampled within the detected bounding boxes and tracked throughout the video sequence using the TAPIR tracker\n[\n52\n]\n.\nV-B\nTraining Details\nThe training process is decoupled into two stages. In the first stage, the encoder\nE\nœï\n‚Äã\n(\n‚ãÖ\n)\nE_{\\phi}(\\cdot)\nand the decoder\nD\nœï\n‚Äã\n(\n‚ãÖ\n)\nD_{\\phi}(\\cdot)\nare trained within an autoencoder framework. We initialize both components using pre-trained Stable Diffusion\n[\n60\n]\nweights, but keep the encoder frozen while fine-tuning the decoder on our dataset.\nIn the second stage, the noise prediction network\nœµ\nŒ∏\n\\epsilon_{\\theta}\nis trained. Its parameters are also initialized from Stable Diffusion\n[\n60\n]\n, except for the motion module, which is trained from scratch. The remaining layers are fine-tuned using Low-Rank Adaptation\n[\n61\n]\n(LoRA) to ensure parameter efficiency. We employ the AdamW optimizer with an initial learning rate of\n1\n√ó\n10\n‚àí\n5\n1\\times 10^{-5}\n, utilizing a cosine decay schedule. The hyperparameters for the loss function in Eq.\n(\n14\n)\nare set to\nœâ\n1\n=\n0.01\n\\omega_{1}=0.01\nand\nœâ\n2\n=\n0.02\n\\omega_{2}=0.02\n. All training is conducted on four NVIDIA A100 GPUs.\nV-C\nSimulation Experiments\nV-C\n1\nExperiment Setup\nWe conduct simulation experiments on the proposed IsaacSkill dataset. Four baseline methods representing diverse paradigms are selected for comparison:\n‚Ä¢\nR3M\n[\n10\n]\n: A behavior cloning approach utilizing pre-trained visual representations. It leverages temporal contrastive learning and video-language alignment to extract semantically rich features for robust policy learning.\n‚Ä¢\nAVDC\n[\n17\n]\n: A video prediction framework that synthesizes task-specific future frames and derives executable actions by analyzing dense pixel correspondences between adjacent predicted frames.\n‚Ä¢\nTrack2Act\n[\n14\n]\n: A trajectory-centric method that employs a goal-conditioned grid flow model to generate future trajectories for query points. These trajectories are subsequently mapped to residual actions via a specific policy network.\n‚Ä¢\nIm2Flow2Act\n[\n15\n]\n: A flow-based approach that generates object-centric motion flows, which serve as conditions for a policy network to output executable actions.\nTo comprehensively evaluate the effectiveness and robustness of the proposed SADiff, we structure our simulation experiments into four main parts:\n‚Ä¢\nWithin-Distribution Comparative Experiment\n: We first assess the fundamental imitation capabilities of SADiff and baselines on tasks consistent with the training distribution.\n‚Ä¢\nRobustness and Generalization Analysis\n: We rigorously test the model‚Äôs robustness against variations in visual context, object instances, categories, and robotic embodiments to evaluate out-of-distribution performance.\n‚Ä¢\nInstruction-Guided Skill Adaptation Experiment\n: We examine the model‚Äôs flexibility in adapting to changing language instructions while the scene configuration remains constant.\n‚Ä¢\nAblation Studies\n: Finally, we isolate and quantify the contributions of key components, including learnable skill tokens, skill contrastive loss, and skill-specific trajectory priors.\nIn all the aforementioned experiments, we compare our method against baselines over 25 independent rollouts, reporting the average Success Rate (SR) as the primary metric. To ensure robust evaluation, each rollout features randomly initialized positions for task-related objects, as well as randomized types and placements for interfering objects. In the following sections, we present the detailed results and analysis for each experimental setting.\nV-C\n2\nWithin-Distribution Comparative Experiment\nTo evaluate the fundamental imitation capability of the proposed method, we conduct a standard within-distribution comparative experiment, serving as a primary baseline for imitation performance. The scenes and task-related objects are sampled from the exact same distributions as the collected dataset, ensuring a rigorous assessment of the system‚Äôs ability to recall and execute demonstrated skills.\nTABLE I:\nComparison of Success Rates (%) Across Different Skills in Within-Distribution Comparative Experiment\nMethod\nPouring\nPicking&\nPlacing\nPushing\nSlide\nOpening\nHinge\nOpening\nAverage\nSR\nR3M\n[\n10\n]\n64.0\n64.0\n52.0\n60.0\n60.0\n60.0\nAVDC\n[\n17\n]\n64.0\n68.0\n84.0\n92.0\n72.0\n76.0\nTrack2Act\n[\n14\n]\n88.0\n80.0\n76.0\n80.0\n88.0\n82.4\nIm2Flow2Act\n[\n15\n]\n84.0\n92.0\n88.0\n92.0\n84.0\n88.0\nSADiff (Ours)\n92.0\n96.0\n92.0\n96.0\n88.0\n92.8\nThe experimental results are presented in Table\nI\n. SADiff consistently outperforms all baselines across the five skills, achieving a state-of-the-art average success rate of 92.8%. Notably, SADiff surpasses the closest competitor, Im2Flow2Act (88.0%), by a margin of 4.8%. Since Im2Flow2Act also leverages object-centric flow, this performance gain directly highlights the efficacy of explicitly modeling skill-level information, thereby refining the quality of generated motion flows and improving the accuracy of action transformation. Conversely, methods modeling global scene dynamics, including Track2Act (82.4%) and AVDC (76.0%), achieve lower success rates because their full-frame synthesis or dense grid tracking increases susceptibility to background perturbations. SADiff addresses this limitation by centering strictly on task-relevant objects, effectively filtering out visual noise. Moreover, R3M (60.0%) yields the lowest success rate, indicating that pre-trained general-purpose visual representations struggle to disentangle fine-grained, task-specific dynamics from environmental distractors. Collectively, these results confirm that integrating skill-aware priors with focused object dynamics is essential for robust manipulation in within-distribution settings.\nFigure 5:\nOverview of the generalization evaluation settings. Left: The original collected demonstration. Right: The model is evaluated under four different types of variations involving distribution shifts in backgrounds, intra-category instances, cross-category objects, and embodiment.\nV-C\n3\nRobustness and Generalization Analysis\nTo rigorously evaluate the robustness and generalization of SADiff, we design experiments across diverse scenarios that deviate significantly from the training distribution. Specifically, we investigate the performance under four different types of variations involving backgrounds, object instances, object categories, and embodiment. As illustrated in Fig.\n5\n, the four generalization settings are designed as follows:\n‚Ä¢\nBackground Generalization\n: This setting introduces variations in lighting conditions and background while keeping the task-related objects consistent with the training set.\n‚Ä¢\nIntra-Category Instance Generalization\n: This setting evaluates the model on unseen instances of task-related objects that differ in color, shape, and physical appearance within the same category.\n‚Ä¢\nCross-Category Generalization\n: This setting challenges the model by replacing task-related objects with functionally similar objects from entirely different categories (e.g., replacing a cup with a bottle).\n‚Ä¢\nCross-Embodiment Generalization\n: This setting assesses cross-embodiment generalization by replacing the Franka Panda manipulator with a UR10 robotic arm equipped with a Robotiq two-finger gripper.\nThe quantitative results averaged across five skills in Fig.\n6\ndemonstrate that SADiff consistently surpasses all baselines across every generalization setting. In the background and intra-category instance generalization scenarios, SADiff achieves robust average success rates of 89.6% and 86.4% by leveraging skill-level guidance to overcome visual variations. This mechanism prevents the incoherent flows observed in Im2Flow2Act and mitigates the visual distractions that degrade the performance of R3M, AVDC, and Track2Act.\nIn the cross-category generalization setting, SADiff maintains an 82.4% success rate while AVDC and R3M fail completely and other baselines suffer performance drops exceeding 20%. These results indicate that skill priors effectively bridge the gap between different object categories. Furthermore, the cross-embodiment setting reveals that R3M fails due to proprioception mismatches and AVDC cannot synthesize plausible future frames for the unseen robot. While Track2Act and Im2Flow2Act also suffer significant degradation due to limited adaptability, SADiff incurs only a 5.6% performance drop. This stability arises from the skill-specific trajectory priors that enable effective adaptation to novel robotic platforms. Overall, these experiments validate that the skill-aware design of SADiff is essential for robust generalization across visual, object, and embodiment variations.\nFigure 6:\nAverage success rate (%) of baselines and SADiff under different generalization scenarios. SADiff consistently achieves the highest average success rate across all four generalization settings.\nV-C\n4\nInstruction-Guided Skill Adaptation Experiment\nTo further investigate whether the proposed SADiff can overcome spurious scene-task correlations and prioritize semantic instructions over visual priors, we design a instruction-guided adaptation experiment. In this setup, the physical scene configuration and object placements remain identical to the training distribution, but the language instruction is altered to command a different manipulation skill. As illustrated in Fig.\n7\n, a scene originally associated with a ‚Äú\nPicking&Placing\n‚Äù task is redefined as a ‚Äú\nPushing\n‚Äù task during testing. This protocol rigorously evaluates whether the model can flexibly adapt its behavior to new semantic instructions rather than simply memorizing scene-task correlations. We define four specific cross-skill adaptation tasks:\n‚Ä¢\nTask 1\n: ‚Äú\nPicking&Placing\n‚Äù\n‚Üí\n\\rightarrow\n‚Äú\nPushing\n‚Äù\n‚Ä¢\nTask 2\n: ‚Äú\nPushing\n‚Äù\n‚Üí\n\\rightarrow\n‚Äú\nPicking&Placing\n‚Äù\n‚Ä¢\nTask 3\n: ‚Äú\nPouring\n‚Äù\n‚Üí\n\\rightarrow\n‚Äú\nPicking&Placing\n‚Äù\n‚Ä¢\nTask 4\n: ‚Äú\nPouring\n‚Äù\n‚Üí\n\\rightarrow\n‚Äú\nPushing\n‚Äù\nThe experimental results in Table\nII\nhighlight the superior flexibility of SADiff, which achieves an average success rate of 85.0% and significantly outperforms all baselines. R3M and AVDC exhibit a complete inability to adapt with 0% success rates, because they rigidly rely on the specific scene-task correlations learned during training. Similarly, Im2Flow2Act performs poorly with only 16.0% success as its flow generation is overly constrained by the visual scene rather than the language instruction. Although Track2Act fares slightly better with 41.0% due to goal-image conditioning, it still lacks the semantic grasp necessary to consistently execute the new skill. SADiff successfully overcomes these limitations by explicitly classifying the modified instruction into a known skill category during inference. This mechanism allows the model to leverage learned skill representations and generate appropriate actions that align with the new command even when the visual environment suggests a different task.\nFigure 7:\nExamples of the instruction-guided adaptation scenario. The model is evaluated on its ability to switch manipulation skills within an unchanged visual environment solely by following modified language instructions.\nTABLE II:\nComparison of Success Rates (%) of Instruction-Guided Skill Adaptation Experiment\nMethod\nTask 1\nTask 2\nTask 3\nTask 4\nAverage SR\nR3M\n[\n10\n]\n0.0\n0.0\n0.0\n0.0\n0.0\nAVDC\n[\n17\n]\n0.0\n0.0\n0.0\n0.0\n0.0\nTrack2Act\n[\n14\n]\n44.0\n52.0\n32.0\n36.0\n41.0\nIm2Flow2Act\n[\n15\n]\n20.0\n16.0\n12.0\n16.0\n16.0\nSADiff (Ours)\n88.0\n84.0\n88.0\n80.0\n85.0\nV-C\n5\nAblation Studies\nTo systematically assess the individual contributions of each core component, we perform ablation experiments following the identical evaluation protocol utilized for the Robustness and Generalization Analysis in Section\nV-C\n3\n. Specifically, we evaluate performance across background, intra-category instance, and cross-category generalization settings. The three ablated variants are defined as follows:\n‚Ä¢\nw/o Learnable Skill Tokens (LST)\n: This variant removes the learnable skill tokens, along with the related skill classification loss and skill contrastive losses.\n‚Ä¢\nw/o Skill Contrastive Loss (SCL)\n: In this variant, the skill contrastive loss in Eq.\n(\n14\n)\nis removed.\n‚Ä¢\nw/o Skill Trajectory Priors (STP)\n: In this variant, the skill-specific trajectory prior in Eq.\n(\n16\n)\nis disabled.\nThe results are shown in Table\nIII\n.\nRemoving the Learnable Skill Tokens (LST) causes the average success rate to decline from 86.1% to 75.7%. This drop indicates that without explicit skill representations, the model loses the ability to dynamically modulate multimodal inputs with skill-specific context, severely hampering adaptability in unseen scenarios. Excluding the Skill Contrastive Loss (SCL) results in a decrease to 81.1%, suggesting that without skill-level constraints during the denoising process, the diffusion model generates motion flows that drift from the intended manipulation semantics. Most critically, removing the Skill Trajectory Priors (STP) leads to the sharpest performance degradation to 66.9%. This underscores that trajectory priors are essential for stabilizing the transformation from 2D motion flow to 3D executable actions. Collectively, these findings demonstrate that SADiff relies on the tight coupling of skill-aware encoding, constrained diffusion, and prior-guided transformation to achieve robust generalization.\nTABLE III:\nComparison of Success Rates (%) of Ablation Studies\nMethod\nBackground\nGeneralization\nInstance\nGeneralization\nCross-Category\nGeneralization\nAverage\nSR\nSADiff\n89.6\n86.4\n82.4\n86.1\nw/o LST\n84.8\n74.4\n68.0\n75.7\nw/o SCL\n86.4\n81.6\n75.2\n81.1\nw/o STP\n73.6\n62.4\n64.8\n66.9\nV-D\nReal-World Experiments\nV-D\n1\nExperiment Setup\nIn real-world experiments, we employ a UR5 robotic manipulator equipped with a Robotiq gripper and an Intel RealSense D435i camera, which is mounted on the front-upper side of the workspace to maintain consistency with the simulation setup. To evaluate the generalization ability and zero-shot sim-to-real transfer capability of the proposed SADiff, we directly deploy the model trained purely in simulation without any real-world fine-tuning.\nWe compare our approach against Track2Act\n[\n14\n]\nand Im2Flow2Act\n[\n15\n]\n, as other baselines like R3M\n[\n10\n]\nand AVDC\n[\n17\n]\nlack the capability for direct transfer.\nFor each skill, we conduct 25 independent rollouts. To rigorously test robustness, we follow the simulation setting by randomly initializing the positions of task-related objects and introducing randomized types and placements for visual distractors in each trial.\nFigure 8:\nVisualization of real-world experiments. This figure demonstrates the predicted object-centric motion flow and the corresponding execution trajectories for five manipulation skills in the real world.\nFigure 9:\nQualitative evaluation under environmental variations. We subject the model to a variety of test scenarios featuring diverse backgrounds, complex lighting conditions, and unseen object instances to assess its visual robustness.\nV-D\n2\nExperiment Results\nThe results are summarized in Table\nIV\n. SADiff achieves an average success rate of 76.0%, outperforming Im2Flow2Act by 21.6% and Track2Act by 25.6% across all five skills. The primary challenge to achieving robust zero-shot sim-to-real transfer lies in pervasive domain discrepancies between simulation and physical environments. These discrepancies include substantial changes in visual scenes and illumination conditions, sensor noise, variations in object types, and embodiment differences.\nThese factors typically cause severe performance degradation for policies trained exclusively in simulation.\nBaseline methods struggle considerably under these conditions. Although Track2Act can generate query-point trajectories conditioned on goal images, it is highly sensitive to visual domain shifts, which degrades trajectory quality. Furthermore, its residual policy, which was trained to map 2D trajectories to actions for a specific simulated agent, generalizes poorly to the physical robot. Similarly, Im2Flow2Act leverages Grounding-DINO\n[\n62\n]\nto identify task-relevant objects, yet it falters when generating object-centric motion flows for novel scenes or unseen instructions. Crucially, its learned policy for converting 2D flow into executable actions relies heavily on fixed camera parameters, limiting its adaptability to real-world setups.\nTABLE IV:\nComparison of Success Rates (%) Across Different Skills in Real-World Experiment\nMethod\nPouring\nPicking&\nPlacing\nPushing\nSlide\nOpening\nHinge\nOpening\nAverage\nSR\nTrack2Act\n[\n14\n]\n44.0\n52.0\n56.0\n48.0\n52.0\n50.4\nIm2Flow2Act\n[\n15\n]\n56.0\n60.0\n56.0\n58.0\n42.0\n54.4\nSADiff (Ours)\n72.0\n80.0\n80.0\n76.0\n72.0\n76.0\nIn contrast, SADiff effectively bridges the sim-to-real gap by injecting skill-level knowledge into both the flow generation and action transformation stages. As demonstrated by the real-world results, SADiff retains strong performance without requiring domain randomization or fine-tuning. This consistent superiority validates two key advantages: (1) explicit skill modeling stabilizes diffusion-based motion generation against visual noise, and (2) the skill-retrieval transformation module reliably converts predicted motion flow into executable actions, even across different robotic embodiments. These findings confirm that SADiff not only excels in simulation but also robustly handles embodiment changes and physical variations in real-world environments. Fig.\n8\nvisualizes the predicted object-centric motion flows and corresponding execution trajectories for five skills, providing further qualitative evidence of the model‚Äôs robustness.\nIn addition, we investigate the time efficiency of the proposed framework in real-world deployment. We measured the average planning time for flow generation, 2D-to-3D mapping, and the average execution time, as shown in Table\nV\n. Despite the latency introduced by the utilization of the VLM and the iterative sampling process of the diffusion model, the total time remains within a reasonable range, demonstrating the system‚Äôs capability for practical real-time deployment.\nTABLE V:\nPlanning and Execution Time for Five Skills in Real-World Experiments\nTime Phases\nPouring\nPicking&\nPlacing\nPushing\nSlide\nOpening\nHinge\nOpening\nPlanning Time (s)\n18.4\n23.1\n17.2\n16.9\n17.8\nExecution Time (s)\n28.2\n21.9\n10.6\n14.5\n15.2\nV-D\n3\nQualitative Analysis under Environmental Variations\nBeyond standard performance metrics, we further qualitatively evaluate how the policy behaves under extreme visual distractions and domain discrepancies.\nWe extended the experimental evaluation to include real-world scenarios characterized by significant environmental variations. As illustrated in Fig.\n9\n, we introduced three different types of domain shifts to challenge the visual robustness:\n‚Ä¢\nDifferent Backgrounds\n: We altered the workspace environment by changing tablecloths with varying textures and colors to test robustness against background clutter.\n‚Ä¢\nDifferent Lighting\n: We introduced complex lighting conditions, including drastic changes in color and intensity, to evaluate the model‚Äôs insensitivity to illumination shifts.\n‚Ä¢\nDifferent Objects\n: We substituted the manipulation targets with unseen object instances that differ in shape, size, and appearance.\nWe constructed a diverse evaluation set comprising 30 different tasks spanning all five manipulation skills. Despite domain shifts and visual distractions, SADiff demonstrates remarkable adaptability. The qualitative results indicate that our method effectively disentangles target object dynamics from environmental noise. This resilience is attributed to the object-centric nature of the predicted motion flow, which, when combined with the structural constraints of skill-specific priors, enables SADiff to maintain high success rates even in highly unstructured and visually novel environments. These findings further validate the potential of SADiff for robust open-world deployment.\nV-E\nScalability and Composability Experiments\nTo bridge the gap between specialized manipulation policies and general-purpose robotic agents, it is crucial to verify capabilities beyond standard task repetition. Specifically, real-world deployment requires an agent to not only generalize to new, untrained behaviors but also to seamlessly sequence mastered skills to solve complex, multi-stage problems. To further validate the scalability and robustness of the proposed SADiff in these advanced settings, we consider two essential properties: (1) scalability to unseen skills absent from the training dataset, demonstrating the transferability of our skill representations; and (2) composability for completing long-horizon tasks by chaining multiple skill executions. All experiments utilize the pre-trained SADiff model trained on the proposed IsaacSkill dataset without any additional fine-tuning, evaluated in both simulated and real-world environments.\nV-E\n1\nScalability to Unseen Skills\nWe validate scalability by introducing two unseen skills, ‚Äú\nStacking\n‚Äù and ‚Äú\nWiping\n‚Äù, which were entirely absent from the training phase.\nAs visualized in Fig.\n10\n, SADiff successfully generates coherent and temporally consistent motion flows for both tasks, enabling reliable translation into stable action trajectories. This zero-shot generalization is achievable because the fundamental skills in our training set (e.g.,\n‚ÄúPicking & Placing‚Äù and ‚ÄúPushing‚Äù\n) share underlying motion patterns with these novel skills. Consequently, SADiff can effectively approximate the required behaviors for unseen skills like ‚Äú\nStacking\n‚Äù and ‚Äú\nWiping\n‚Äù by leveraging the learned primitives from these foundational skills. These results demonstrate that the proposed SADiff framework can seamlessly extend to semantically and kinematically related behaviors without additional training. Furthermore, this experiment suggests that as the category of fundamental skills in IsaacSkill expands, the model‚Äôs capacity to master increasingly complex and diverse tasks will scale correspondingly.\nV-E\n2\nComposability for Long-Horizon Tasks\nTo verify the composability of our approach, we evaluate SADiff on long-horizon tasks that require sequential execution of multiple skills. For a complex task instruction such as\n‚ÄúPut the apple in the lower drawer‚Äù\n, we leverage Qwen-VL\n[\n51\n]\nas a VLM-based planner, prompting it to decompose the high-level goal into actionable subtasks conditioned on visual observations and the language command. Subsequently, SADiff generates object-centric motion flows for each subtask and maps them into executable actions. As illustrated in the representative rollouts in Fig.\n11\n, this decoupling strategy enables the flexible composition of diverse skills while leveraging the robust generalization capabilities of SADiff to handle task complexity. The successful execution in both simulation and real-world settings demonstrates the seamless composability of our framework and validates its potential for completing complex, long-horizon manipulation tasks.\nFigure 10:\nDemonstration of the scalability of SADiff to unseen skills. We showcase that SADiff can successfully perform new skills (e.g., ‚Äú\nStacking\n‚Äù and ‚Äú\nWiping\n‚Äù) unseen during training in both simulated and real-world environments.\nFigure 11:\nDemonstration of the composability of SADiff for long-horizon tasks. We assess the model‚Äôs capability to execute multi-stage manipulation sequences by decomposing complex instructions into sequential subtasks.\nVI\nConclusion\nIn this work, we propose SADiff, a skill-aware diffusion framework designed to improve generalization in robotic manipulation through explicit modeling of skill-level information. By integrating a skill-aware encoding module equipped with learnable skill tokens, a skill-constrained diffusion model, and a skill-retrieval transformation strategy, SADiff effectively captures shared skill-level information within the same skill domain and systematically integrates it across the encoding, generation, and execution phases. This allows it to precisely generate object-centric motion flow and robustly map the 2D flow to executable 3D actions in diverse scenarios and environments. Furthermore, we construct a high-fidelity IsaacSkill dataset, which not only enables a comprehensive assessment of specific skill capabilities but also provides the realistic dynamics required to support robust sim-to-real transfer. Extensive experiments in both simulation and real-world settings demonstrate that SADiff significantly outperforms existing methods in generalization to unseen objects, varying environments, and distinct embodiments, and achieves zero-shot sim-to-real transfer. These results underscore the pivotal role of explicitly modeling skill-level knowledge and integrating it throughout all phases of the pipeline. Moreover, SADiff demonstrates scalability and composability by successfully generalizing to unseen skills and composing multiple learned skills to execute long-horizon manipulation tasks.\nAlthough SADiff demonstrates strong generalization and zero-shot sim-to-real transfer, it still faces several limitations. Firstly, the skill taxonomy is manually specified, which constrains the expressiveness of the skill space when extending the framework to more complex or fine-grained manipulation tasks. Future work could explore unsupervised skill discovery, such as using VQ-VAE\n[\n63\n]\nto extract and structure semantic skills from various demonstrations. Secondly, the 2D motion flow representation struggles to fully resolve spatial dynamics in tasks with out-of-plane rotations (e.g., unscrewing a bottle cap). Future work could extend our framework to utilize 3D motion flow\n[\n64\n]\nfor more precise robotic manipulation.\nReferences\n[1]\nH.¬†Zhou, X.¬†Yao, Y.¬†Meng, S.¬†Sun, Z.¬†Bing, K.¬†Huang, and A.¬†Knoll, ‚ÄúLanguage-conditioned learning for robotic manipulation: A survey,‚Äù\narXiv preprint arXiv:2312.10807\n, 2023.\n[2]\nW.¬†Zhao, J.¬†Chen, Z.¬†Meng, D.¬†Mao, R.¬†Song, and W.¬†Zhang, ‚ÄúVlmpc: Vision-language model predictive control for robotic manipulation,‚Äù in\nRobotics: Science and Systems\n, 2024.\n[3]\nO.¬†Mees, L.¬†Hermann, and W.¬†Burgard, ‚ÄúWhat matters in language conditioned robotic imitation learning over unstructured data,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†7, no.¬†4, pp. 11‚Äâ205‚Äì11‚Äâ212, 2022.\n[4]\nT.¬†Ma, J.¬†Zhou, Z.¬†Wang, R.¬†Qiu, and J.¬†Liang, ‚ÄúContrastive imitation learning for language-guided multi-task robotic manipulation,‚Äù in\nConference on Robot Learning\n.‚ÄÉPMLR, 2025, pp. 4651‚Äì4669.\n[5]\nF.¬†Lin, Y.¬†Hu, P.¬†Sheng, C.¬†Wen, J.¬†You, and Y.¬†Gao, ‚ÄúData scaling laws in imitation learning for robotic manipulation,‚Äù in\nThe Thirteenth International Conference on Learning Representations\n.\n[6]\nZ.¬†Chen, Z.¬†Mandi, H.¬†Bharadhwaj, M.¬†Sharma, S.¬†Song, A.¬†Gupta, and V.¬†Kumar, ‚ÄúSemantically controllable augmentations for generalizable robot learning,‚Äù\nThe International Journal of Robotics Research\n, vol.¬†44, no. 10-11, pp. 1705‚Äì1726, 2025.\n[7]\nT.¬†Zhang, Z.¬†McCarthy, O.¬†Jow, D.¬†Lee, X.¬†Chen, K.¬†Goldberg, and P.¬†Abbeel, ‚ÄúDeep imitation learning for complex manipulation tasks from virtual reality teleoperation,‚Äù in\n2018 IEEE international conference on robotics and automation (ICRA)\n.‚ÄÉIeee, 2018, pp. 5628‚Äì5635.\n[8]\nT.¬†Z. Zhao, V.¬†Kumar, S.¬†Levine, and C.¬†Finn, ‚ÄúLearning fine-grained bimanual manipulation with low-cost hardware,‚Äù\narXiv preprint arXiv:2304.13705\n, 2023.\n[9]\nY.¬†J. Ma, S.¬†Sodhani, D.¬†Jayaraman, O.¬†Bastani, V.¬†Kumar, and A.¬†Zhang, ‚ÄúVip: Towards universal visual reward and representation via value-implicit pre-training,‚Äù in\nThe Eleventh International Conference on Learning Representations\n.\n[10]\nS.¬†Nair, A.¬†Rajeswaran, V.¬†Kumar, C.¬†Finn, and A.¬†Gupta, ‚ÄúR3m: A universal visual representation for robot manipulation,‚Äù in\nConference on Robot Learning\n.‚ÄÉPMLR, 2023, pp. 892‚Äì909.\n[11]\nJ.¬†Zeng, Q.¬†Bu, B.¬†Wang, W.¬†Xia, L.¬†Chen, H.¬†Dong, H.¬†Song, D.¬†Wang, D.¬†Hu, P.¬†Luo\net¬†al.\n, ‚ÄúLearning manipulation by predicting interaction,‚Äù in\nRobotics: Science and Systems\n, 2024.\n[12]\nH.¬†Bharadhwaj, D.¬†Dwibedi, A.¬†Gupta, S.¬†Tulsiani, C.¬†Doersch, T.¬†Xiao, D.¬†Shah, F.¬†Xia, D.¬†Sadigh, and S.¬†Kirmani, ‚ÄúGen2act: Human video generation in novel scenarios enables generalizable robot manipulation,‚Äù\narXiv preprint arXiv:2409.16283\n, 2024.\n[13]\nF.¬†Ni, J.¬†Hao, S.¬†Wu, L.¬†Kou, J.¬†Liu, Y.¬†Zheng, B.¬†Wang, and Y.¬†Zhuang, ‚ÄúGenerate subgoal images before act: Unlocking the chain-of-thought reasoning in diffusion model for robot manipulation with multimodal prompts,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2024, pp. 13‚Äâ991‚Äì14‚Äâ000.\n[14]\nH.¬†Bharadhwaj, R.¬†Mottaghi, A.¬†Gupta, and S.¬†Tulsiani, ‚ÄúTrack2act: Predicting point tracks from internet videos enables generalizable robot manipulation,‚Äù in\nEuropean Conference on Computer Vision\n.‚ÄÉSpringer, 2024, pp. 306‚Äì324.\n[15]\nM.¬†Xu, Z.¬†Xu, Y.¬†Xu, C.¬†Chi, G.¬†Wetzstein, M.¬†Veloso, and S.¬†Song, ‚ÄúFlow as the cross-domain manipulation interface,‚Äù in\n8th Annual Conference on Robot Learning\n, 2024.\n[16]\nY.¬†Chen, P.¬†Li, Y.¬†Huang, J.¬†Yang, K.¬†Chen, and L.¬†Wang, ‚ÄúEc-flow: Enabling versatile robotic manipulation from action-unlabeled videos via embodiment-centric flow,‚Äù\narXiv preprint arXiv:2507.06224\n, 2025.\n[17]\nP.¬†C. Ko, J.¬†Mao, Y.¬†Du\net¬†al.\n, ‚ÄúLearning to act from actionless videos through dense correspondences,‚Äù in\nProceedings of the Twelfth International Conference on Learning Representations\n, 2023.\n[18]\nK.¬†Fang, Y.¬†Zhu, S.¬†Savarese, and L.¬†Fei-Fei, ‚ÄúDiscovering generalizable skills via automated generation of diverse tasks,‚Äù\narXiv preprint arXiv:2106.13935\n, 2021.\n[19]\nZ.¬†Liang, Y.¬†Mu, H.¬†Ma, M.¬†Tomizuka, M.¬†Ding, and P.¬†Luo, ‚ÄúSkilldiffuser: Interpretable hierarchical planning via skill abstractions in diffusion-based task execution,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2024, pp. 16‚Äâ467‚Äì16‚Äâ476.\n[20]\nY.¬†Yao, S.¬†Liu, H.¬†Song, D.¬†Qu, Q.¬†Chen, Y.¬†Ding, B.¬†Zhao, Z.¬†Wang, X.¬†Li, and D.¬†Wang, ‚ÄúThink small, act big: Primitive prompt learning for lifelong robot manipulation,‚Äù in\nProceedings of the Computer Vision and Pattern Recognition Conference\n, 2025, pp. 22‚Äâ573‚Äì22‚Äâ583.\n[21]\nT.¬†Yu, D.¬†Quillen, Z.¬†He, R.¬†Julian, K.¬†Hausman, C.¬†Finn, and S.¬†Levine, ‚ÄúMeta-world: A benchmark and evaluation for multi-task and meta reinforcement learning,‚Äù in\nConference on robot learning\n.‚ÄÉPMLR, 2020, pp. 1094‚Äì1100.\n[22]\nS.¬†James, Z.¬†Ma, D.¬†R. Arrojo, and A.¬†J. Davison, ‚ÄúRlbench: The robot learning benchmark & learning environment,‚Äù\nIEEE Robotics and Automation Letters\n, vol.¬†5, no.¬†2, pp. 3019‚Äì3026, 2020.\n[23]\nB.¬†Liu, Y.¬†Zhu, C.¬†Gao, Y.¬†Feng, Q.¬†Liu, Y.¬†Zhu, and P.¬†Stone, ‚ÄúLibero: Benchmarking knowledge transfer for lifelong robot learning,‚Äù\nAdvances in Neural Information Processing Systems\n, vol.¬†36, pp. 44‚Äâ776‚Äì44‚Äâ791, 2023.\n[24]\nM.¬†Mittal, P.¬†Roth, J.¬†Tigue, A.¬†Richard, O.¬†Zhang, P.¬†Du, A.¬†Serrano-Mu√±oz, X.¬†Yao, R.¬†Zurbr√ºgg, N.¬†Rudin\net¬†al.\n, ‚ÄúIsaac lab: A gpu-accelerated simulation framework for multi-modal robot learning,‚Äù\narXiv preprint arXiv:2511.04831\n, 2025.\n[25]\nO.¬†Kroemer, S.¬†Niekum, and G.¬†Konidaris, ‚ÄúA review of robot learning for manipulation: Challenges, representations, and algorithms,‚Äù\nJournal of machine learning research\n, vol.¬†22, no.¬†30, pp. 1‚Äì82, 2021.\n[26]\nM.¬†Drolet, S.¬†Stepputtis, S.¬†Kailas, A.¬†Jain, J.¬†Peters, S.¬†Schaal, and H.¬†B. Amor, ‚ÄúA comparison of imitation learning algorithms for bimanual manipulation,‚Äù\nIEEE Robotics and Automation Letters\n, 2024.\n[27]\nZ.¬†Li, A.¬†Chapin, E.¬†Xiang, R.¬†Yang, B.¬†Machado, N.¬†Lei, E.¬†Dellandrea, D.¬†Huang, and L.¬†Chen, ‚ÄúRobotic manipulation via imitation learning: Taxonomy, evolution, benchmark, and challenges,‚Äù\narXiv preprint arXiv:2508.17449\n, 2025.\n[28]\nA.¬†Mandlekar, D.¬†Xu, J.¬†Wong, S.¬†Nasiriany, C.¬†Wang, R.¬†Kulkarni, L.¬†Fei-Fei, S.¬†Savarese, Y.¬†Zhu, and R.¬†Mart√≠n-Mart√≠n, ‚ÄúWhat matters in learning from offline human demonstrations for robot manipulation,‚Äù\narXiv preprint arXiv:2108.03298\n, 2021.\n[29]\nC.¬†Chi, Z.¬†Xu, S.¬†Feng, E.¬†Cousineau, Y.¬†Du, B.¬†Burchfiel, R.¬†Tedrake, and S.¬†Song, ‚ÄúDiffusion policy: Visuomotor policy learning via action diffusion,‚Äù\nThe International Journal of Robotics Research\n, p. 02783649241273668, 2023.\n[30]\nS.¬†Stepputtis, J.¬†Campbell, M.¬†Phielipp, S.¬†Lee, C.¬†Baral, and H.¬†Ben¬†Amor, ‚ÄúLanguage-conditioned imitation learning for robot manipulation tasks,‚Äù\nAdvances in Neural Information Processing Systems\n, vol.¬†33, pp. 13‚Äâ139‚Äì13‚Äâ150, 2020.\n[31]\nX.¬†Yao, T.¬†Blei, Y.¬†Meng, Y.¬†Zhang, H.¬†Zhou, Z.¬†Bing, K.¬†Huang, F.¬†Sun, and A.¬†Knoll, ‚ÄúLong-horizon language-conditioned imitation learning for robotic manipulation,‚Äù\nIEEE/ASME Transactions on Mechatronics\n, 2025.\n[32]\nM.¬†Shridhar, L.¬†Manuelli, and D.¬†Fox, ‚ÄúCliport: What and where pathways for robotic manipulation,‚Äù in\nConference on robot learning\n.‚ÄÉPMLR, 2022, pp. 894‚Äì906.\n[33]\nA.¬†Radford, J.¬†W. Kim, C.¬†Hallacy, A.¬†Ramesh, G.¬†Goh, S.¬†Agarwal, G.¬†Sastry, A.¬†Askell, P.¬†Mishkin, J.¬†Clark\net¬†al.\n, ‚ÄúLearning transferable visual models from natural language supervision,‚Äù in\nInternational conference on machine learning\n.‚ÄÉPMLR, 2021, pp. 8748‚Äì8763.\n[34]\nM.¬†Shridhar, L.¬†Manuelli, and D.¬†Fox, ‚ÄúPerceiver-actor: A multi-task transformer for robotic manipulation,‚Äù in\nConference on Robot Learning\n.‚ÄÉPMLR, 2023, pp. 785‚Äì799.\n[35]\nA.¬†Jaegle, S.¬†Borgeaud, J.-B. Alayrac, C.¬†Doersch, C.¬†Ionescu, D.¬†Ding, S.¬†Koppula, D.¬†Zoran, A.¬†Brock, E.¬†Shelhamer\net¬†al.\n, ‚ÄúPerceiver io: A general architecture for structured inputs & outputs,‚Äù\narXiv preprint arXiv:2107.14795\n, 2021.\n[36]\nK.¬†Grauman, A.¬†Westbury, E.¬†Byrne, Z.¬†Chavis, A.¬†Furnari, R.¬†Girdhar, J.¬†Hamburger, H.¬†Jiang, M.¬†Liu, X.¬†Liu\net¬†al.\n, ‚ÄúEgo4d: Around the world in 3,000 hours of egocentric video,‚Äù in\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, 2022, pp. 18‚Äâ995‚Äì19‚Äâ012.\n[37]\nA.¬†O‚ÄôNeill, A.¬†Rehman, A.¬†Maddukuri, A.¬†Gupta, A.¬†Padalkar, A.¬†Lee, A.¬†Pooley, A.¬†Gupta, A.¬†Mandlekar, A.¬†Jain\net¬†al.\n, ‚ÄúOpen x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0,‚Äù in\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n.‚ÄÉIEEE, 2024, pp. 6892‚Äì6903.\n[38]\nT.¬†Xiao, I.¬†Radosavovic, T.¬†Darrell, and J.¬†Malik, ‚ÄúMasked visual pre-training for motor control,‚Äù\narXiv preprint arXiv:2203.06173\n, 2022.\n[39]\nC.¬†Wen, X.¬†Lin, J.¬†So, K.¬†Chen, Q.¬†Dou, Y.¬†Gao, and P.¬†Abbeel, ‚ÄúAny-point trajectory modeling for policy learning,‚Äù\narXiv preprint arXiv:2401.00025\n, 2023.\n[40]\nR.¬†P. Wolf, Y.¬†Shi, S.¬†Liu, and R.¬†Rayyes, ‚ÄúDiffusion models for robotic manipulation: A survey,‚Äù\nFrontiers in Robotics and AI\n, vol.¬†12, p. 1606247, 2025.\n[41]\nK.¬†Zhang, P.¬†Yun, J.¬†Cen, J.¬†Cai, D.¬†Zhu, H.¬†Yuan, C.¬†Zhao, T.¬†Feng, M.¬†Y. Wang, Q.¬†Chen\net¬†al.\n, ‚ÄúGenerative artificial intelligence in robotic manipulation: A survey,‚Äù\narXiv preprint arXiv:2503.03464\n, 2025.\n[42]\nX.¬†Ma, S.¬†Patidar, I.¬†Haughton, and S.¬†James, ‚ÄúHierarchical diffusion policy for kinematics-aware multi-task robotic manipulation,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2024, pp. 18‚Äâ081‚Äì18‚Äâ090.\n[43]\nT.¬†Pearce, T.¬†Rashid, A.¬†Kanervisto, D.¬†Bignell, M.¬†Sun, R.¬†Georgescu, S.¬†V. Macua, S.¬†Z. Tan, I.¬†Momennejad, K.¬†Hofmann\net¬†al.\n, ‚ÄúImitating human behaviour with diffusion models,‚Äù\narXiv preprint arXiv:2301.10677\n, 2023.\n[44]\nM.¬†Reuss, M.¬†Li, X.¬†Jia, and R.¬†Lioutikov, ‚ÄúGoal conditioned imitation learning using score-based diffusion policies,‚Äù in\nRobotics: Science and Systems\n, 2023.\n[45]\nY.¬†Ze, G.¬†Zhang, K.¬†Zhang, C.¬†Hu, M.¬†Wang, and H.¬†Xu, ‚Äú3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations,‚Äù in\nProceedings of Robotics: Science and Systems (RSS)\n, 2024.\n[46]\nH.¬†Xue, J.¬†Ren, W.¬†Chen, G.¬†Zhang, Y.¬†Fang, G.¬†Gu, H.¬†Xu, and C.¬†Lu, ‚ÄúReactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation,‚Äù in\nProceedings of Robotics: Science and Systems (RSS)\n, 2025.\n[47]\nT.¬†Chen, Y.¬†Mu, Z.¬†Liang, Z.¬†Chen, S.¬†Peng, Q.¬†Chen, M.¬†Xu, R.¬†Hu, H.¬†Zhang, X.¬†Li\net¬†al.\n, ‚ÄúG3flow: Generative 3d semantic flow for pose-aware and generalizable object manipulation,‚Äù in\nProceedings of the Computer Vision and Pattern Recognition Conference\n, 2025, pp. 1735‚Äì1744.\n[48]\nH.¬†Zhen, Q.¬†Sun, H.¬†Zhang, J.¬†Li, S.¬†Zhou, Y.¬†Du, and C.¬†Gan, ‚ÄúTesseract: learning 4d embodied world models,‚Äù\narXiv preprint arXiv:2504.20995\n, 2025.\n[49]\nY.¬†Du, S.¬†Yang, B.¬†Dai, H.¬†Dai, O.¬†Nachum, J.¬†Tenenbaum, D.¬†Schuurmans, and P.¬†Abbeel, ‚ÄúLearning universal policies via text-guided video generation,‚Äù\nAdvances in neural information processing systems\n, vol.¬†36, pp. 9156‚Äì9172, 2023.\n[50]\nC.¬†Wen, X.¬†Lin, J.¬†So, K.¬†Chen, Q.¬†Dou, Y.¬†Gao, and P.¬†Abbeel, ‚ÄúAny-point trajectory modeling for policy learning,‚Äù\nRobotics Science and Systems 2024\n, 2024.\n[51]\nJ.¬†Bai, S.¬†Bai, S.¬†Yang, S.¬†Wang, S.¬†Tan, P.¬†Wang, J.¬†Lin, C.¬†Zhou, and J.¬†Zhou, ‚ÄúQwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond,‚Äù\narXiv preprint arXiv:2308.12966\n, 2023.\n[52]\nC.¬†Doersch, Y.¬†Yang, M.¬†Vecerik\net¬†al.\n, ‚ÄúTapir: Tracking any point with per-frame initialization and temporal refinement,‚Äù in\nProceedings of the IEEE/CVF International Conference on Computer Vision\n, 2023, pp. 10‚Äâ061‚Äì10‚Äâ072.\n[53]\nZ.¬†Wang and J.-C. Liu, ‚ÄúTranslating math formula images to latex sequences using deep neural networks with sequence-level training,‚Äù\nInternational Journal on Document Analysis and Recognition (IJDAR)\n, vol.¬†24, no.¬†1, pp. 63‚Äì75, 2021.\n[54]\nA.¬†Vaswani, ‚ÄúAttention is all you need,‚Äù\nAdvances in Neural Information Processing Systems\n, 2017.\n[55]\nD.¬†P. Kingma, M.¬†Welling\net¬†al.\n, ‚ÄúAn introduction to variational autoencoders,‚Äù\nFoundations and Trends¬Æ in Machine Learning\n, vol.¬†12, no.¬†4, pp. 307‚Äì392, 2019.\n[56]\nY.¬†Guo, C.¬†Yang, A.¬†Rao, Z.¬†Liang, Y.¬†Wang, Y.¬†Qiao, M.¬†Agrawala, D.¬†Lin, and B.¬†Dai, ‚ÄúAnimatediff: Animate your personalized text-to-image diffusion models without specific tuning,‚Äù\narXiv preprint arXiv:2307.04725\n, 2023.\n[57]\nR.¬†Rombach, A.¬†Blattmann, D.¬†Lorenz, P.¬†Esser, and B.¬†Ommer, ‚ÄúHigh-resolution image synthesis with latent diffusion models,‚Äù in\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, 2022, pp. 10‚Äâ684‚Äì10‚Äâ695.\n[58]\nJ.¬†J. Mor√©, ‚ÄúThe levenberg-marquardt algorithm: implementation and theory,‚Äù in\nNumerical analysis: proceedings of the biennial Conference held at Dundee, June 28‚ÄìJuly 1, 1977\n.‚ÄÉSpringer, 2006, pp. 105‚Äì116.\n[59]\nL.¬†Yang, B.¬†Kang, Z.¬†Huang, Z.¬†Zhao, X.¬†Xu, J.¬†Feng, and H.¬†Zhao, ‚ÄúDepth anything v2,‚Äù\nAdvances in Neural Information Processing Systems\n, vol.¬†37, pp. 21‚Äâ875‚Äì21‚Äâ911, 2024.\n[60]\nR.¬†Rombach, A.¬†Blattmann, D.¬†Lorenz, P.¬†Esser, and B.¬†Ommer, ‚ÄúHigh-resolution image synthesis with latent diffusion models,‚Äù in\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, 2022, pp. 10‚Äâ684‚Äì10‚Äâ695.\n[61]\nE.¬†J. Hu, Y.¬†Shen, P.¬†Wallis, Z.¬†Allen-Zhu, Y.¬†Li, S.¬†Wang, L.¬†Wang, and W.¬†Chen, ‚ÄúLoRA: Low-rank adaptation of large language models,‚Äù in\nInternational Conference on Learning Representations\n, 2022.\n[62]\nS.¬†Liu, Z.¬†Zeng, T.¬†Ren, F.¬†Li, H.¬†Zhang, J.¬†Yang, Q.¬†Jiang, C.¬†Li, J.¬†Yang, H.¬†Su\net¬†al.\n, ‚ÄúGrounding dino: Marrying dino with grounded pre-training for open-set object detection,‚Äù in\nEuropean conference on computer vision\n.‚ÄÉSpringer, 2024, pp. 38‚Äì55.\n[63]\nA.¬†Van Den¬†Oord, O.¬†Vinyals\net¬†al.\n, ‚ÄúNeural discrete representation learning,‚Äù\nAdvances in neural information processing systems\n, vol.¬†30, 2017.\n[64]\nC.¬†Yuan, C.¬†Wen, T.¬†Zhang, and Y.¬†Gao, ‚ÄúGeneral flow as foundation affordance for scalable robot learning,‚Äù\narXiv preprint arXiv:2401.11439\n, 2024.",
  "preview_text": "Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.\n\nSkill-Aware Diffusion for Generalizable Robotic Manipulation\nAoshen Huang\n‚àó\n1\n, Jiaming Chen\n‚àó\n2\n, Jiyu Cheng\nüñÇ\n‚Äã\n1\n{}^{\\text{\\Letter}1}\n, Ran Song\n1\n, Wei Pan\n2\n, Wei Zhang\n1\n* Contributed equally to this work.\nüñÇ\n{}^{\\text{\\Letter}}\nCorresponding author.\n1\nSchool of Control Science and Engineering, Shandong University, Jinan 250061, China. (email:\naoshenhuang@mail.sdu.edu.cn; jycheng@sdu.edu.cn; ransong@sdu.edu.cn; info@vsislab.com)\n2\nDepartment of Computer Science, The University of Manchester, M13 9PL Manchester, U.K. (e-mail:\nppjmchen@gmail.com; wei.pan@manchester.ac.uk)\nAbstract\nRobust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks inde",
  "is_relevant": true,
  "relevance_score": 8.0,
  "extracted_keywords": [
    "diffusion",
    "robotic manipulation",
    "generalization",
    "skill-aware",
    "motion flow",
    "sim-to-real"
  ],
  "one_line_summary": "ËØ•ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊäÄËÉΩÊÑüÁü•Êâ©Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÔºåÈÄöËøáÊï¥ÂêàÊäÄËÉΩÁ∫ß‰ø°ÊÅØÊù•ÊèêÂçáÊú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂ÂºïÂÖ•‰∫ÜÈ´ò‰øùÁúüÊï∞ÊçÆÈõÜËøõË°åÈ™åËØÅ„ÄÇ",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-16T13:14:40Z",
  "created_at": "2026-01-20T17:50:00.345419",
  "updated_at": "2026-01-20T17:50:00.345427"
}