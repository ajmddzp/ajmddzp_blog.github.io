{
    "id": "2601.15459v1",
    "title": "Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation",
    "authors": [
        "Sarvin Ghiasi",
        "Majid Roshanfar",
        "Jake Barralet",
        "Liane S. Feldman",
        "Amir Hooshiar"
    ],
    "abstract": "æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªé›†æˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è§£å†³è…¹è…”é•œæ‰‹æœ¯ä¸­æœºå™¨äººè‡‚ç¢°æ’æ£€æµ‹ä¸æœ€å°è·ç¦»ä¼°è®¡çš„å…³é”®æŒ‘æˆ˜ï¼Œæå‡å…¶å®‰å…¨æ€§ä¸æ“ä½œæ•ˆç‡ã€‚è¯¥æ¡†æ¶èåˆäº†åˆ†æå»ºæ¨¡ã€å®æ—¶ä»¿çœŸä¸æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä¸ºä¿éšœæœºå™¨äººæ‰‹æœ¯å®‰å…¨æä¾›äº†ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶é¦–å…ˆæ„å»ºäº†åŸºäºå…³èŠ‚æ„å‹çš„æœºå™¨äººè‡‚é—´æœ€å°è·ç¦»åˆ†ææ¨¡å‹ï¼Œé€šè¿‡ç²¾ç¡®çš„ç†è®ºè®¡ç®—æ—¢å¯ä½œä¸ºéªŒè¯å·¥å…·ï¼Œä¹Ÿå¯ä½œä¸ºæ€§èƒ½åŸºå‡†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼€å‘äº†æ¨¡æ‹Ÿä¸¤å°ä¸ƒè‡ªç”±åº¦Kinovaæœºå™¨äººè‡‚çš„ä¸‰ç»´ä»¿çœŸç¯å¢ƒï¼Œç”Ÿæˆæ¶µç›–ç¢°æ’æ£€æµ‹ä¸è·ç¦»ä¼°è®¡çš„å¤šæ ·åŒ–æ„å‹æ•°æ®é›†ã€‚åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œç ”ç©¶ä»¥æœºå™¨äººå…³èŠ‚æ‰§è¡Œå™¨å‚æ•°ä¸ç›¸å¯¹ä½ç½®ä½œä¸ºè¾“å…¥è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæœ€ç»ˆå®ç°å¹³å‡ç»å¯¹è¯¯å·®282.2æ¯«ç±³ã€å†³å®šç³»æ•°0.85çš„é¢„æµ‹æ€§èƒ½ã€‚é¢„æµ‹è·ç¦»ä¸å®é™…è·ç¦»çš„é«˜åº¦å»åˆè¡¨æ˜è¯¥ç½‘ç»œåœ¨ç©ºé—´å…³ç³»æ³›åŒ–èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œå°†åˆ†æå»ºæ¨¡çš„ç²¾ç¡®æ€§ä¸æœºå™¨å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œèƒ½æœ‰æ•ˆæå‡æœºå™¨äººç³»ç»Ÿçš„ç²¾åº¦ä¸å¯é æ€§ã€‚",
    "url": "https://arxiv.org/abs/2601.15459v1",
    "html_url": "https://arxiv.org/html/2601.15459v1",
    "html_content": "1]\n\\orgdiv\nSurgical Performance Enhancement and Robotics (SuPER) Centre, Department of Surgery,\n\\orgname\nMcGill University,\n\\orgaddress\n\\city\nMontreal,\n\\state\nQC,\n\\country\nCanada\n2]\n\\orgdiv\nThe Wilfred and Joyce Posluns Centre for Image Guided Innovation & Therapeutic Intervention (PCIGITI),\n\\orgname\nThe Hospital for Sick Children (SickKids),\n\\orgaddress\n\\city\nToronto,\n\\state\nON,\n\\country\nCanada\nNeural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation\n\\fnm\nSarvin\n\\sur\nGhiasi\n\\fnm\nMajid\n\\sur\nRoshanfar\n\\fnm\nJake\n\\sur\nBarralet\n\\fnm\nLiane S.\n\\sur\nFeldman\n\\fnm\nAmir\n\\sur\nHooshiar\namir.hooshiar@mcgill.ca\n[\n[\nAbstract\nThis study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2Â mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the networkâ€™s accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.\nkeywords:\nMulti-robot laparoscopy surgery, collision detection, learning from simulation, deep neural networks\n1\nIntroduction\n1.1\nBackground\nMinimally invasive techniques such as laparoscopy have transformed modern surgery, improving safety and patient outcomes\n[\njeganathan2025minimally\n]\n. However, laparoscopic procedures remain limited by restricted instrument dexterity, two-dimensional visualization, and suboptimal surgeon ergonomics. Robotic systems have emerged to address these challenges by enhancing precision, range of motion, and control, thereby overcoming key limitations of conventional laparoscopy\n[\njung2015robotic\n,\ncochetti2020combined\n,\nkwon2022comparison\n,\nroshanfar2025advanced\n]\n. The advanced design of robotic surgical systems plays a crucial role in reducing the physical strain and operational difficulties associated with traditional laparoscopy. As demonstrated by Thomas etÂ al., the Versius Surgical System (CMR Surgical Ltd, Cambridge, United Kingdom) replicates the articulation of the human arm, offering improved surgical access and maneuverability compared to standard laparoscopic tools\n[\nthomas2021preclinical\n]\n. Similarly, robotic surgery has been shown to provide a more effective alternative by mitigating ergonomic and operational challenges faced by surgeons\n[\nwehrmann2023clinical\n]\n. The work of Soumpasis etÂ al.\n[\nsoumpasis2023safe\n]\nfurther emphasizes that robotic systems enhance flexibility, visualization, and access, directly addressing the limitations of conventional instruments. According to Silva etÂ al.\n[\nsilva2018introduction\n]\n, these systems can significantly reduce surgeon fatigue while providing intuitive instrument control. This innovation not only improves procedural efficiency but also accelerates the training process for novice surgeons, enabling them to acquire complex surgical skills more quickly. Ruzzenente etÂ al.\n[\nruzzenente2020robotic\n]\nreported that the learning curve for robotic surgery is generally shorter than that for laparoscopy, particularly for surgeons already proficient in open procedures. Likewise, robotic systems facilitate the transfer of skills between robotic-assisted and laparoscopic surgery, simplifying training and skill retention\n[\nhardon2023crossover\n,\nschmidt2024laparoscopic\n]\n.\nBeyond ergonomic and training advantages, robotic surgery has also been associated with improved postoperative outcomes. Studies have shown that patients undergoing robotic-assisted procedures experience fewer complications and shorter hospital stays than those treated with conventional laparoscopy\n[\nroh2020comparison\n]\n. Gerdes etÂ al.\n[\ngerdes2022results\n]\nreported superior postoperative recovery, including reduced pain and faster rehabilitation, while Chen etÂ al.\n[\nchen2022vinci\n]\nassociated robotic approaches with enhanced recovery protocols. These findings highlight the potential of robotic systems to optimize patient outcomes, particularly in gynecological procedures\n[\nmanchanda2024comprehensive\n]\n. Moreover, robotic surgery holds significant value in surgical education, where the complexity of laparoscopic techniques poses challenges for trainees. By providing a stable camera platform and advanced 3D visualization, robotic systems improve spatial awareness and facilitate skill acquisition, as illustrated in Fig.\n1\n.\n(a)\n(b)\nFigure 1\n:\n(a) Robot-controlled laparoscopic instruments with an integrated laparoscope camera providing a magnified operative view. (b) A surgeon operating through a bimanual haptic interface within a multi-robot robotic laparoscopic system (RLS), with real-time visualization displayed on the monitor and overhead surgical display. These images illustrate the typical setup of a robot-assisted laparoscopic surgery environment, demonstrating both the operative field and the surgeon console. Images courtesy of CMR Surgical (Versius Surgical System).\n1.2\nClinical Motivation for Collision-Aware Robotic Systems\nRobotic-assisted surgery enhances precision and ergonomics by providing improved dexterity, three-dimensional visualization, and tremor filtration, which together lead to better outcomes and reduced fatigue for surgeons\n[\nli2018robotic\n,\nli2019distal\n]\n. Despite these advantages, challenges persist that can compromise safety and efficiency, particularly the risk of instrument collisions and workspace constraints\n[\nmontagne2021robotic\n]\n. Sun etÂ al.\n[\nsun2020visual\n]\nreported that coordinating multiple robotic arms within confined surgical environments often leads to reduced maneuverability and unintentional collisions, potentially causing tissue damage or mechanical interference. Similarly, Abiri etÂ al.\n[\nabiri2017visual\n]\nemphasized the need for optimizing robotic system design to improve flexibility and reduce the likelihood of tissue trauma, while Kim etÂ al.\n[\nkim2016development\n]\nidentified arm misalignment as a frequent source of accidental contact during robotic procedures.\nUnintended collisions remain a critical yet underexplored challenge across several minimally invasive domains, including laparoscopic, endonasal, and microsurgical procedures\n[\nueda2017toward\n]\n. In these operations, surgeons often face limited visibility near the instrument tip, increasing the likelihood of inadvertent contact with surrounding tissues or instruments. Kelly etÂ al.\n[\nkelly2020single\n]\nand Nigicser etÂ al.\n[\nnigicser2022magnetic\n]\nboth highlighted that restricted visibility and workspace constraints hinder precise instrument coordination in multi-arm robotic surgery. Mendelsohn etÂ al.\n[\nmendelsohn2020transoral\n]\nsimilarly noted that limited visual access can result in unexpected tool collisions in transoral procedures. In microsurgery, spatial restrictions further limit dexterity and elevate procedural complexity\n[\nmarinho2019dynamic\n]\n, while Yoshimura etÂ al.\n[\nyoshimura2020single\n]\nobserved that collision zones near the tool tip often fall outside the surgeonâ€™s visual field. Such limitations are especially consequential in high-precision interventions, where even minor contact can jeopardize surgical accuracy and patient safety\n[\nminig2017minimally\n]\n. These challenges underscore the pressing need for reliable, real-time collision detection and avoidance systems to enhance safety and performance in robotic-assisted surgery\n[\nhe2019human\n,\nroshanfar2022stiffness\n]\n.\n1.3\nPrior Work\nAccurate detection and prevention of collisions in robotic systems require an understanding of workspace geometry and often rely on computationally intensive motion-planning algorithms\n[\njimenez2005collision\n]\n. To address this challenge, researchers have proposed a range of sensing and modeling approaches to enhance safety and control in robotic-assisted environments. Cirillo etÂ al.\n[\ncirillo2015conformable\n]\ndeveloped a sensorized flexible skin capable of detecting contact position and applied forces for managing both intentional and unintentional human-robot interactions. While promising, the methodâ€™s dependence on a single sensing modality limits adaptability in dynamic environments and warrants further comparison with alternative force-sensing approaches, such as torque or residual-based methods. Marinho etÂ al.\n[\nmarinho2019dynamic\n]\nintroduced a vector-field-inequalities approach for autonomous motion guidance, minimizing the risk of collisions during surgical tasks. However, the methodâ€™s reliance on Jacobian-based modeling restricts its ability to capture nonlinear dynamics in complex procedures. Other studies employed virtual fixtures as safety boundaries to prevent instruments from entering restricted zones\n[\nvitrani2016applying\n,\nmarinho2020smartarm\n]\n. Although effective, these approaches face challenges related to the fulcrum constraint, which introduces multiple possible solutions for replicating distal forces during proximal co-manipulation.\nSubsequent work extended these concepts to improve teleoperation and safety. Marinho etÂ al.\n[\nmarinho2019unified\n]\nproposed a unified vector-field framework for smooth teleoperation and collision avoidance, though its performance in flexible or dynamic geometries remains unverified. Similarly, Marinho etÂ al.\n[\nmarinho2020virtual\n]\napplied guidance virtual fixturesâ€”such as looping trajectories and cylindrical constraintsâ€”to enhance precision and safety, but noted minimal improvement in task completion time. Xin etÂ al.\n[\nxin6research\n]\nintegrated virtual fixtures with impedance-conductance control to prevent human-robot co-surgery errors; however, experiments were limited to two-dimensional settings, reducing applicability to real-world three-dimensional surgical environments. Lin etÂ al.\n[\nlin2019virtual\n]\nfurther employed a human-arm kinematic model to construct guided virtual fixtures that restrict robot motion during dragging, though validation was confined to simulation studies. Beyond virtual fixtures, path-planning and control strategies have been explored for safe human-robot collaboration. Wang etÂ al.\n[\nwang2023path\n]\nreduced oscillations in robot trajectories using velocity potential fields, achieving real-time performance but focusing only on single-arm manipulators. Herbster etÂ al.\n[\nherbster2023modeling\n]\nmodeled contact forces in constrained human-robot collisions to advance collaborative robot (cobot) safety, though the modelâ€™s generalizability is limited by simplified contact mechanics and testing across a few platforms.\nZhu etÂ al.\n[\nzhu2012model\n]\nproposed a geometric collision detection method for a maxillofacial multi-arm surgical robot (MMSR) using cylindrical and spherical primitives to represent arms and obstacles. While computationally efficient, the simplified geometry limits applicability in anatomically complex surgical spaces. Hao etÂ al.\n[\nhao2021application\n]\npresented an adaptive genetic algorithm based on collision detection (AGACD) that improved convergence and computation time in planning tasks, though the approach neglected mechanical constraints and dynamic environments. In parallel, sensorless detection techniques have gained attention for reducing hardware complexity. Lee etÂ al.\n[\nlee2015sensorless\n]\nand Li etÂ al.\n[\nli2019virtual\n]\ndeveloped sensorless collision detection systems that identified external torques using friction modeling and joint motor currents. While these methods avoid costly sensors, they remain less accurate than traditional sensor-based systems and require refinement of friction models for improved precision. These limitations highlight the need for lightweight, accurate, and real-time collision detection frameworks capable of adapting to the constrained and dynamic conditions of surgical workspaces. A summary of some of the related studies are presented in Table\n1\n.\nTable 1\n:\nSummary of related studies on collision detection and avoidance methods.\nRef.\nApplication\nMethod\nAdvantages\nLimitations\n[\ncirillo2015conformable\n]\nHuman-robot interaction\nSensorized skin\nMeasures contact directly; adaptable\nSingle-sensor reliance; needs comparative studies\n[\nmarinho2019dynamic\n]\nSafe guidance in robotic surgery\nVector-field inequalities\nReduces collision risks\nJacobian model limitations\n[\nvitrani2016applying\n]\nEnforcing safe zones\nVirtual fixtures\nPrevents unsafe movement\nFulcrum constraints\n[\nmarinho2019unified\n]\nTeleoperation\nUnified vector-field framework\nSmooth teleoperation; collision avoidance\nUnverified for dynamic settings\n[\nxin6research\n]\nHuman-machine co-surgery\nImpedance control with virtual fixtures\nPrevents errors; integrates force feedback\n2D testing only\n[\nlin2019virtual\n]\nGuided movement restriction\nHuman arm kinematics\nEnhances precision; simulation-tested\nNo real-world validation\n[\nzhu2012model\n]\nSelf-collision detection for MMSR\nGeometric modeling\nComputational efficiency\nLimited for complex shapes\n[\nhao2021application\n]\nPath planning\nAdaptive Genetic Algorithm\nResolves convergence issues\nIgnores mechanical constraints\n[\nlee2015sensorless\n]\nCollision detection in human-robot tasks\nSensorless (friction/current)\nCost-effective; no sensors needed\nRequires model refinement\n[\nwen2014study\n]\nReal-time collision detection in robotic surgery\nLIDAR-based sensing system\nAccurate collision avoidance\nHigh computational power\n1.4\nContributions\nTo address the challenges of collision detection and workspace safety in multi-arm robotic systems, this study presents a comprehensive framework that integrates analytical modeling, real-time simulation, and learning-based prediction. A novel analytical algorithm is introduced for modeling robotic arms through a multi-linear shape representation, allowing efficient estimation of inter-arm distances and detection of potential collisions during operation. The proposed method was validated through real-time experiments across multiple robotic configurations and further examined within a custom 3D simulation environment developed to visualize and analyze dynamic system behavior. In addition, a deep neural network (NN) was trained and validated on a large dataset of simulated configurations covering a wide range of surgical scenarios. Comparative analyses between the analytical model, simulation outcomes, and experimental measurements demonstrated strong agreement, confirming the robustness and accuracy of the proposed framework for safe and reliable robotic operation. The main contributions of this work are summarized as follows:\n1.\nDevelopment and validation of a robust analytical geometric framework for accurately predicting the minimum distance between two serial robotic arms.\n2.\nDesign and implementation of a real-time simulation platform for visualizing and validating minimum-distance estimation between robotic arms under varying configurations.\n3.\nTraining and deployment of a NN model for fast and accurate real-time prediction of minimum inter-arm distances in multi-arm robotic systems.\n2\nMaterials and Methods\n2.1\nCollision Detection Framework\nThis study proposes a collision detection framework that integrates simulation-based learning, analytical modeling, and deep NN prediction to enhance the safety and situational awareness of multi-arm robotic systems. The framework follows a learning from simulation approach, combining large-scale synthetic data generation with real-time inference for collision prediction and warning. Simulation-driven optimization with task-driven constraints has also been used in surgical robotics to improve safe and robust operation, motivating the use of virtual environments for systematic design and validation\n[\nroshanfar2024design\n]\n. In the first stage, a physics-based simulation environment was developed in Unity to model two 7 DOF robotic arms across a wide range of spatial configurations. The simulation platform enabled the generation of 75,655 random configurations by varying joint angles and relative poses between the robots. An analytical proximity estimation algorithm was embedded into the simulator to compute the minimum distance (\nd\nm\nâ€‹\ni\nâ€‹\nn\nd_{min}\n) between the arms for each configuration. The resulting dataset, consisting of joint configurations and relative pose parameters, was used to train a deep NN designed to predict\nd\nm\nâ€‹\ni\nâ€‹\nn\nd_{min}\nin real time. During the inference phase, experimental configurations obtained from two real Kinova Gen3 robotic arms were used as input to the trained network. The model receives the joint angles and relative position of the two robots and predicts the minimum inter-arm distance. When the predicted\nd\nm\nâ€‹\ni\nâ€‹\nn\nd_{min}\nfalls below 0.2 m, corresponding to the approximate outer diameter of the robotic arms, the system triggers an audio warning to alert the operator of a potential collision. The overall learning from simulation workflow, including dataset generation, NN architecture, and real-time inference, is illustrated in Fig.\n2\n.\nFigure 2\n:\nOverview of the proposed learning from simulation collision detection framework. The Unity-based simulation generates 75,655 robot configurations for training, using joint configurations and relative poses as inputs. A deep NN predicts the minimum distance (\nd\nm\nâ€‹\ni\nâ€‹\nn\nd_{min}\n) between the arms during inference on real robotic setups, issuing an audio warning when\nd\nm\nâ€‹\ni\nâ€‹\nn\n<\n0.2\nâ€‹\nm\nd_{min}<0.2~\\text{m}\n.\n2.2\nKinematic Modeling of the Robotic System\nThe overall schematic of the experimental setup used in this study is shown in Fig.\n3\n. The system consists of a bimanual haptic surgeon interface, a multibody simulator with integrated collision detection, and multiple collaborative robotic manipulators. The bimanual interface enables intuitive control of the dual robotic arms using left and right hand devices, while a foot pedal allows the user to actively select which robot to operate. The simulator receives motion commands from the haptic interface and computes the corresponding kinematic parameters and joint angles for each robotic arm. It provides a comprehensive model of the robotsâ€™ motion by calculating and visualizing their positions, velocities, and accelerations based on the input joint configurations. The simulator also includes a collision detection module that continuously monitors potential interactions between the robotic arms and surrounding objects within the workspace. A redundancy resolution algorithm is implemented to determine optimal joint configurations that achieve the desired end-effector trajectory while avoiding singularities or workspace conflicts. Internal control mechanisms ensure precise execution of motion commands while enforcing joint limits, torque constraints, and synchronized movement between the robots. Furthermore, the simulator delivers haptic feedback cues through the interface, allowing the operator to feel interaction forces and improving awareness of the robotsâ€™ behavior during operation. The integration of kinematic modeling, collision detection, redundancy resolution, and haptic feedback supports the safe and accurate assessment of robotic motion in complex surgical environments. A forward kinematic model was developed using the Denavit-Hartenberg (DH) convention to describe the spatial configuration of each robot. As shown in Fig.\n4(a)\n, each robotic arm includes seven revolute joints (\ni\nâˆˆ\n1\n,\n2\n,\nâ€¦\n,\n7\ni\\in 1,2,\\ldots,7\n) arranged in a spherical wrist configuration. The naming convention and corresponding axes of rotation are depicted in Fig.\n4(b)\n. The coordinate frames for each joint were defined according to the Kinova Gen3 Ultra Lightweight Robot User Guide. The DH parameters including link length (\na\ni\na_{i}\n), twist angle (\nÎ±\ni\n\\alpha_{i}\n), link offset (\nd\ni\nd_{i}\n), and joint variable (\nÎ¸\ni\n\\theta_{i}\n) were determined for each joint as summarized in Table\n2\n. These parameters represent the geometric relationships between adjacent links in the robotâ€™s kinematic chain.\nFigure 3\n:\nOverview of the experimental setup used in this research. The configuration includes: (left) a bimanual haptic surgeon interface for controlling both robotic arms with left and right hands and a foot pedal for robot selection; (center) a multibody simulator that integrates robot kinematics, collision detection, and redundancy resolution; and (right) dual Kinova Gen3 collaborative robotic arms performing coordinated manipulation tasks.\n(a)\nJoint rotations\n(b)\nDH frames\nFigure 4\n:\nModeling of the Kinova Gen3 7-DOF robotic arm.\n(a) Rotation of joint angles illustrating the seven revolute joints (\nq\n1\nq_{1}\nto\nq\n7\nq_{7}\n) that define the manipulatorâ€™s full configuration and contribute to the positioning and orientation of the end effector.\n(b) Denavit-Hartenberg (DH) frame definitions showing coordinate frame assignments for each joint according to the DH convention, representing the spatial orientation and relative displacement between adjacent links.\nTable 2\n:\nDenavit-Hartenberg (DH) parameters for Kinova Gen3 7-DOF robotic arm.\ni\ni\nÎ±\ni\n\\alpha_{i}\n(rad)\na\ni\na_{i}\n(m)\nd\ni\nd_{i}\n(m)\nÎ¸\ni\n\\theta_{i}\n(rad)\n0 (base)\nÏ€\n\\pi\n0\n0\n0\n1\nÏ€\n/\n2\n\\pi/2\n0\n-0.2848\nq\n1\nq_{1}\n2\nÏ€\n/\n2\n\\pi/2\n0\n-0.0118\nq\n2\n+\nÏ€\nq_{2}+\\pi\n3\nÏ€\n/\n2\n\\pi/2\n0\n-0.4208\nq\n3\n+\nÏ€\nq_{3}+\\pi\n4\nÏ€\n/\n2\n\\pi/2\n0\n-0.0128\nq\n4\n+\nÏ€\nq_{4}+\\pi\n5\nÏ€\n/\n2\n\\pi/2\n0\n-0.3143\nq\n5\n+\nÏ€\nq_{5}+\\pi\n6\nÏ€\n/\n2\n\\pi/2\n0\n0\nq\n6\n+\nÏ€\nq_{6}+\\pi\n7 (end effector)\nÏ€\n\\pi\n0\n-0.1674\nq\n7\n+\nÏ€\nq_{7}+\\pi\nThe DH parameters were used to construct the forward kinematic model describing the relationship between the robotâ€™s joint angles and end-effector position. The forward kinematics solution is obtained using the Homogeneous Transformation Matrix (HTM) formulation\n[\nwen2014study\n]\n. Combining the\n3\nÃ—\n3\n3\\times 3\nrotation matrix (\nR\nn\n0\nR^{0}_{n}\n) and the\n3\nÃ—\n1\n3\\times 1\ntranslation vector (\np\nn\n0\np^{0}_{n}\n), the transformation matrix from the base to the\nn\nt\nâ€‹\nh\nn^{th}\njoint is defined as:\nH\nn\n0\n=\n(\nR\nn\n0\np\nn\n0\n0\n1\n)\n\\textbf{H}_{n}^{0}=\\begin{pmatrix}\\textbf{R}_{n}^{0}&\\textbf{p}_{n}^{0}\\\\\n\\textbf{0}&1\\end{pmatrix}\n(1)\nfor each joint\nj\n=\n1\n,\nâ€¦\n,\n7\nj=1,\\ldots,7\n, the corresponding transformation matrix between adjacent links is expressed as:\nH\nj\nj\nâˆ’\n1\n=\n(\nc\nâ€‹\n(\nq\nj\n)\nâˆ’\nc\nâ€‹\n(\nÎ±\ni\n)\nâ€‹\ns\nâ€‹\n(\nq\nj\n)\ns\nâ€‹\n(\nÎ±\nj\n)\nâ€‹\ns\nâ€‹\n(\nq\nj\n)\na\ni\nâ€‹\nc\nâ€‹\n(\nq\nj\n)\ns\nâ€‹\n(\nq\nj\n)\nc\nâ€‹\n(\nÎ±\ni\n)\nâ€‹\nc\nâ€‹\n(\nq\nj\n)\nâˆ’\ns\nâ€‹\n(\nÎ±\nj\n)\nâ€‹\nc\nâ€‹\n(\nq\nj\n)\na\ni\nâ€‹\ns\nâ€‹\n(\nq\nj\n)\n0\ns\nâ€‹\n(\nÎ±\nj\n)\nc\nâ€‹\n(\nÎ±\nj\n)\nd\nj\n0\n0\n0\n1\n)\n\\small\\textbf{H}_{j}^{j-1}=\\begin{pmatrix}c(q_{j})&-c(\\alpha_{i})s(q_{j})&s(\\alpha_{j})s(q_{j})&a_{i}c(q_{j})\\\\\ns(q_{j})&c(\\alpha_{i})c(q_{j})&-s(\\alpha_{j})c(q_{j})&a_{i}s(q_{j})\\\\\n0&s(\\alpha_{j})&c(\\alpha_{j})&d_{j}\\\\\n0&0&0&1\\end{pmatrix}\n(2)\nwhere\nq\nj\nq_{j}\ndenotes the joint actuator angles, and\ns\ns\nand\nc\nc\nrepresent the sine and cosine functions, respectively. The parameters\na\ni\na_{i}\n,\nd\ni\nd_{i}\n, and\nÎ±\ni\n\\alpha_{i}\nare the DH constants for each link, defining the spatial relationship between consecutive joints. The transformation matrix for the end effector (EE) is defined as:\nH\n7\nE\nâ€‹\nE\n=\n(\n1\n0\n0\n0\n0\nâˆ’\n1\n0\n0\n0\n0\nâˆ’\n1\nâˆ’\n0.0615\n0\n0\n0\n1\n)\n\\small H_{7}^{EE}=\\begin{pmatrix}1&0&0&0\\\\\n0&-1&0&0\\\\\n0&0&-1&-0.0615\\\\\n0&0&0&1\\end{pmatrix}\n(3)\nthe total transformation from the robot base to the tool tip is calculated by multiplying the individual transformations for all joints:\nH\nt\nâ€‹\ni\nâ€‹\np\n0\n=\nH\n1\n0\nâ€‹\nH\n2\n1\nâ€‹\nH\n3\n2\nâ€‹\nH\n4\n3\nâ€‹\nH\n5\n4\nâ€‹\nH\n6\n5\nâ€‹\nH\n7\n6\nâ€‹\nH\n7\nE\nâ€‹\nE\n\\textbf{H}_{tip}^{0}=\\textbf{H}_{1}^{0}\\textbf{H}_{2}^{1}\\textbf{H}_{3}^{2}\\textbf{H}_{4}^{3}\\textbf{H}_{5}^{4}\\textbf{H}_{6}^{5}\\textbf{H}_{7}^{6}\\textbf{H}_{7}^{EE}\n(4)\nthe relative pose of Robot 2 with respect to Robot 1 is obtained by computing their transformations with respect to the NDI tracking system. The base transformations are defined as:\nH\nb\n1\nNDI\n=\nH\nNDI\nâ€‹\n(\nH\nm\n1\nb\n1\n)\nâˆ’\n1\nH_{b_{1}}^{\\text{NDI}}=H^{\\text{NDI}}\\left(H_{{m_{1}}}^{{b_{1}}}\\right)^{-1}\n(5)\nH\nb\n2\nNDI\n=\nH\nm\n2\nNDI\nâ€‹\n(\nH\nm\n2\nb\n2\n)\nâˆ’\n1\nH_{{b_{2}}}^{\\text{NDI}}=H_{{m_{2}}}^{\\text{NDI}}\\left(H_{{m_{2}}}^{{b_{2}}}\\right)^{-1}\n(6)\nH\nb\n2\nb\n1\n=\n(\nH\nb\n1\nNDI\n)\nâˆ’\n1\nâ€‹\nH\nb\n2\nNDI\nH_{b_{2}}^{b_{1}}=\\left(H_{{b_{1}}}^{\\text{NDI}}\\right)^{-1}H_{{b_{2}}}^{\\text{NDI}}\n(7)\nfrom the resulting matrix\nH\nb\n2\nb\n1\nH_{b_{2}}^{b_{1}}\n, the relative position of Robot 2 with respect to Robot 1 is determined by extracting the three elements of the final column, representing the translational displacement between the two robot bases.\n2.3\nSimulation-based Training Dataset\nAs illustrated in Fig.\n5\n, two Kinova Gen3 robotic arms were modeled and simulated within a virtual environment created in Unity 2022.3.32f1. The simulation aimed to replicate the physical structure, kinematic behavior, and motion interactions of the dual-arm setup used in the real-world experiments. The Unified Robot Description Format (URDF) files of the Kinova robots, including mesh representations of each link and joint, were imported into the Unity workspace to construct accurate 3D models of the manipulators. Within the Unity environment, two GameObjects, RobotÂ 1 and RobotÂ 2, were defined, each representing one of the robotic manipulators. Custom C# scripts were developed to control joint rotations, manage hierarchical link relationships, and define actuator behavior consistent with the robotsâ€™ physical constraints. The joint hierarchies were organized according to a parent-child structure, ensuring that each actuator motion propagated correctly through the kinematic chain. The home and zero configurations were defined using data retrieved from the Kinova Application Programming Interface (API), with the zero position corresponding to a configuration where all joint angles were set to zero. The simulation environment enabled the two robots to move through a wide range of configurations and random poses, allowing for systematic testing of motion coordination, workspace overlap, and collision detection within a realistic 3D setting. This setup served as the foundation for generating synthetic data used to train and validate the learning-based collision detection framework described in Section\n2.1\n.\nFigure 5\n:\nSimulation of two Kinova Gen3 robotic arms in Unity 3D. The environment models the dual-arm setup with accurate link geometries, actuator hierarchies, and kinematic constraints, enabling realistic motion simulation, collision visualization, and data generation for learning-based analysis.\n2.4\nAnalytical Benchmark\nTo detect potential collisions between different configurations of robotic arms, each robotic arm was represented by a series of 1-degree linear BÃ©zier curves. A linear BÃ©zier curve is a parametric representation of a straight line segment defined by its start and end points. This representation simplifies the structural modelling of the robotic arms while maintaining the necessary accuracy for collision detection. Specifically, every link connecting two actuators was represented as a one-dimensional linear BÃ©zier line, where the actuatorsâ€™ positions defined the start and end points of the curve. In this model, each robot link is represented as a straight line segment defined by its start (\nğ©\ni\n\\mathbf{p}_{i}\n) and end (\nğ©\ni\n+\n1\n\\mathbf{p}_{i+1}\n) points. The equations for the links are parameterized using\nÎ·\n\\eta\nand\nÎ³\n\\gamma\n, where\nÎ·\n,\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\n\\eta,\\gamma\\in[0,1]\n. Each link for both RobotÂ 1 (\nğ¥\ni\n1\n{}^{1}\\mathbf{l}_{i}\n) and RobotÂ 2 (\nğ¥\ni\n2\n{}^{2}\\mathbf{l}_{i}\n) can be defined using a linear interpolation equation. For RobotÂ 1, the equation for the\ni\ni\n-th link is expressed as:\nğ¥\ni\n1\nâ€‹\n(\nÎ·\n)\n=\nğ©\ni\n1\n+\nÎ·\nâ‹…\n(\nğ©\ni\n+\n1\n1\nâˆ’\nğ©\ni\n1\n)\n,\nÎ·\nâˆˆ\n[\n0\n,\n1\n]\n,\ni\n=\n1\n,\nâ€¦\n,\n7\n{}^{1}\\mathbf{l}_{i}(\\eta)={}^{1}\\mathbf{p}_{i}+\\eta\\cdot\\left({}^{1}\\mathbf{p}_{i+1}-{}^{1}\\mathbf{p}_{i}\\right),\\quad\\eta\\in[0,1],\\quad i=1,\\ldots,7\n(8)\nwhere\nğ©\ni\n1\n{}^{1}\\mathbf{p}_{i}\nand\nğ©\ni\n+\n1\n1\n{}^{1}\\mathbf{p}_{i+1}\nrepresent the start and end points of the\ni\ni\n-th link, respectively, and\nÎ·\n\\eta\nserves as the interpolation parameter. Similarly, the equation for the\ni\ni\n-th link for RobotÂ 2 is given by:\nğ¥\ni\n2\nâ€‹\n(\nÎ³\n)\n=\nğ©\ni\n2\n+\nÎ³\nâ‹…\n(\nğ©\ni\n+\n1\n2\nâˆ’\nğ©\ni\n2\n)\n,\nÎ³\nâˆˆ\n[\n0\n,\n1\n]\n,\ni\n=\n1\n,\nâ€¦\n,\n7\n{}^{2}\\mathbf{l}_{i}(\\gamma)={}^{2}\\mathbf{p}_{i}+\\gamma\\cdot\\left({}^{2}\\mathbf{p}_{i+1}-{}^{2}\\mathbf{p}_{i}\\right),\\quad\\gamma\\in[0,1],\\quad i=1,\\ldots,7\n(9)\nwhere\nÎ³\n\\gamma\nserves as the interpolation parameter, analogous to\nÎ·\n\\eta\n. The control points\nğ©\ni\n1\n{}^{1}\\mathbf{p}_{i}\nand\nğ©\ni\n2\n{}^{2}\\mathbf{p}_{i}\nare derived from the homogeneous transformation matrices (\nH\nâ€‹\nT\nâ€‹\nM\nâ€‹\ns\nHTMs\n) corresponding to each joint as indicated in Eq.\n1\n. Specifically, for RobotÂ 1, the base of the robot\nğ©\nb\n1\n1\n{}^{1}\\mathbf{p}_{b_{1}}\nis positioned at the center of the coordinate system, i.e.\n(\n0\n0\n0\n)\nT\n\\begin{pmatrix}0&0&0\\end{pmatrix}^{T}\nand the rest of the control points are computed using consecutive multiplication of the HTMs:\nH\ni\n0\n=\nâˆ\nj\n=\n1\ni\nH\nj\nj\nâˆ’\n1\n,\ni\n=\n1\n,\nâ€¦\n,\n7\n\\textbf{H}_{i}^{0}=\\prod_{j=1}^{i}H_{j}^{j-1},\\quad i=1,\\ldots,7\n(10)\nThe position of every point\nğ©\ni\n1\n{}^{1}\\mathbf{p}_{i}\nis extracted from the first three elements of the last column of the resulting\nğ‡\ni\n0\n\\mathbf{H}_{i}^{0}\n:\nğ©\nb\n1\n=\nğ©\n0\n1\n=\n(\n0\n0\n0\n)\n\\displaystyle\\mathbf{p}_{b_{1}}={}^{1}\\mathbf{p}_{0}=\\begin{pmatrix}0\\\\\n0\\\\\n0\\end{pmatrix}\n(11)\nğ©\ni\n1\n=\nH\ni\n0\nâ€‹\nS\n\\displaystyle{}^{1}\\mathbf{p}_{i}=\\textbf{H}_{i}^{0}\\textbf{S}\n(12)\nwhere:\nS\n=\n(\n0\n0\n0\n1\n)\n\\textbf{S}=\\begin{pmatrix}0\\\\\n0\\\\\n0\\\\\n1\\end{pmatrix}\n(13)\nfurthermore, for RobotÂ 2, the base of the robot\nğ©\nb\n2\n2\n{}^{2}\\mathbf{p}_{b_{2}}\nis defined as the first three elements of the last column of\nğ‡\nb\n2\nb\n1\n\\mathbf{H}_{b_{2}}^{b_{1}}\n:\nğ©\nb\n2\n=\nğ©\n0\n2\n=\nğ‡\nb\n2\nb\n1\nâ€‹\nS\n\\displaystyle\\mathbf{p}_{b_{2}}={}^{2}\\mathbf{p}_{0}=\\mathbf{H}_{b_{2}}^{b_{1}}\\textbf{S}\n(14)\nğ©\ni\n2\n=\nH\ni\n0\nâ€‹\nS\n\\displaystyle{}^{2}\\mathbf{p}_{i}=\\textbf{H}_{i}^{0}\\textbf{S}\n(15)\nthis formulation ensures systematic computation of the positions of all points along the robotsâ€™ links, based on their respective kinematic chains and the Denavitâ€“Hartenberg (DH) convention. To detect possible collisions, the distance between the two robotic arms was computed by finding the minimum distance between the BÃ©zier curves representing their respective links. Initially, all values for the possible differences between the link equations were determined as indicated in (\n14\n):\nd\ni\nâ€‹\nj\nâ€‹\n(\nÎ·\n,\nÎ³\n)\n=\nl\ni\n1\nâ€‹\n(\nÎ·\n)\nâˆ’\nl\nj\n2\nâ€‹\n(\nÎ³\n)\n\\textbf{d}_{ij}(\\eta,\\gamma)={}^{1}\\textbf{l}_{i}(\\eta)-{}^{2}\\textbf{l}_{j}(\\gamma)\n(16)\nwhere\nd\ni\nâ€‹\nj\n\\textbf{d}_{ij}\nrepresents the translation vector between an arbitrary point on\nğ¥\ni\n1\nâ€‹\n(\nÎ·\n)\n{}^{1}\\mathbf{l}_{i}(\\eta)\nand a point on\nğ¥\nj\n2\nâ€‹\n(\nÎ³\n)\n{}^{2}\\mathbf{l}_{j}(\\gamma)\n. Thus, the size (norm) of\nd\ni\nâ€‹\nj\n\\textbf{d}_{ij}\nestimates the Euclidean distance (length of the common normal) between the two links of the two robots for a given pair\n(\nÎ·\n,\nÎ³\n)\n(\\eta,\\gamma)\n. To obtain the minimum distance between the two robot links, the squared distance function\nÏˆ\n\\psi\nwas analytically derived, differentiated using the chain differentiation rule, and solved for its roots in terms of\n(\nÎ·\n,\nÎ³\n)\n(\\eta,\\gamma)\n:\nÏˆ\n=\nâ€–\nd\ni\nâ€‹\nj\nâ€–\n2\n=\nd\ni\nâ€‹\nj\nâŠ¤\nâ€‹\nd\ni\nâ€‹\nj\n\\displaystyle\\psi=\\|\\textbf{d}_{ij}\\|^{2}=\\textbf{d}_{ij}^{\\top}\\textbf{d}_{ij}\n(17)\nâˆ‡\nÏˆ\n=\n(\nâˆ‚\nÏˆ\nâˆ‚\nÎ·\nâˆ‚\nÏˆ\nâˆ‚\nÎ³\n)\n=\n(\nâˆ‚\nd\ni\nâ€‹\nj\nâŠ¤\nâˆ‚\nÎ·\nâ€‹\nd\ni\nâ€‹\nj\n+\nd\ni\nâ€‹\nj\nâŠ¤\nâ€‹\nâˆ‚\nd\ni\nâ€‹\nj\nâˆ‚\nÎ·\nâˆ‚\nd\ni\nâ€‹\nj\nâŠ¤\nâˆ‚\nÎ³\nâ€‹\nd\ni\nâ€‹\nj\n+\nd\ni\nâ€‹\nj\nâŠ¤\nâ€‹\nâˆ‚\nd\ni\nâ€‹\nj\nâˆ‚\nÎ³\n)\n=\n0\n\\displaystyle\\nabla\\psi=\\begin{pmatrix}\\frac{\\partial\\psi}{\\partial\\eta}\\\\\n\\frac{\\partial\\psi}{\\partial\\gamma}\\end{pmatrix}=\\begin{pmatrix}\\frac{\\partial\\textbf{d}_{ij}^{\\top}}{\\partial\\eta}\\textbf{d}_{ij}+\\textbf{d}_{ij}^{\\top}\\frac{\\partial\\textbf{d}_{ij}}{\\partial\\eta}\\\\\n\\frac{\\partial\\textbf{d}_{ij}^{\\top}}{\\partial\\gamma}\\textbf{d}_{ij}+\\textbf{d}_{ij}^{\\top}\\frac{\\partial\\textbf{d}_{ij}}{\\partial\\gamma}\\end{pmatrix}=\\textbf{0}\n(18)\nThese derivatives allow optimization of\nÎ·\n\\eta\nand\nÎ³\n\\gamma\nto achieve the minimum distance. This process was repeated for all arm configurations of the robots to identify where and between which links the minimum distance occurs, providing a detailed analysis of the collision-free zones and possible interference regions. The optimal\nÎ·\n\\eta\nand\nÎ³\n\\gamma\nvalues, obtained by solving the derivative equations, were then substituted back into the link equations to calculate the minimum distance. If this minimum distance was found to be less than 0.2Â m, a collision was detected. This hybrid approach offers several advantages. The use of BÃ©zier curves simplifies the collision detection process, avoiding the computational complexity associated with traditional 3D solid geometry models. At the same time, the DH convention ensures precise computation of actuator positions, which is fundamental to accurate collision analysis. Together, these methods provide a robust framework for real-time collision detection in dynamic environments where robotic arms frequently change configurations and operate nearby. Using this approach, the system ensures high efficiency and reliability, making it suitable for complex robotic applications.\n2.5\nSimulation-Based Collision Detection: Dataset Curation\nA custom script was developed within Unity to compute the minimum distance between the simulated robotic arms for each random configuration, enabling real-time collision detection and spatial analysis. To model the individual links of each robotic arm, box colliders were applied to represent the physical geometry of the arm segments, and each collider was assigned to its corresponding link with vertices computed based on their respective positions and orientations in 3D space. The Euclidean distance method was then implemented to determine the shortest distance between the vertices of all possible pairs of box colliders, allowing the identification of the minimum distance between the two robotic arms. The Euclidean distance formula, which calculates the straight-line distance between two points in 3D space, was iteratively applied to all vertex pairs corresponding to the links of the arms, and this process was repeated across all possible configurations to ensure accurate detection of the minimum inter-arm distance for each configuration. This simulation-based approach allowed for continuous and computationally efficient monitoring of spatial proximity, providing a reliable means of validating potential collisions within the virtual environment.\n2.6\nModel Training\n2.6.1\nDataset Preparation\nThe dataset used for training the regressor network was generated through a series of random configurations within the Unity-based simulation. A total of 75,655 configurations were collected by randomly positioning the robotic arms and calculating the minimum distance between them. Each input data point consisted of the following features:\n1.\nThe relative position vector between the two robots:\nğ©\n2\n1\n=\nğ©\nb\n2\n=\n(\np\nx\np\ny\np\nz\n)\n,\n{}^{1}\\mathbf{p}_{2}=\\mathbf{p}_{b_{2}}=\\begin{pmatrix}p_{x}\\\\\np_{y}\\\\\np_{z}\\end{pmatrix},\n(19)\nwhich represents the position of the base of Robotâ€“2 relative to the base of Robotâ€“1. Rotational degrees of freedom for the robot bases were omitted as the bases remained fixed throughout the experiments.\n2.\nSeven joint angles for Robotâ€“1 and seven joint angles for Robotâ€“2, corresponding to the specific configurations of their arms at any given moment.\nThe minimum distance between the two robotic arms was computed in the simulation, and each input data point was formatted as a vector, represented as:\n(\np\nx\np\ny\np\nz\nq\n1\n(\n1\n)\nâ€¦\nq\n7\n(\n1\n)\nq\n1\n(\n2\n)\nâ€¦\nq\n7\n(\n2\n)\n)\nT\n,\n\\begin{pmatrix}p_{x}&p_{y}&p_{z}&q^{(1)}_{1}&\\dots&q^{(1)}_{7}&q^{(2)}_{1}&\\dots&q^{(2)}_{7}\\end{pmatrix}^{T},\n(20)\nwhere\nq\ni\n(\n1\n)\nq^{(1)}_{i}\nand\nq\ni\n(\n2\n)\nq^{(2)}_{i}\ndenote the joint angles for Robotâ€“1 and Robotâ€“2, respectively, and\nd\nm\nâ€‹\ni\nâ€‹\nn\nd_{min}\nrepresents the minimum distance between the two robots.\nAdditionally, the minimum distance values corresponding to the input data were used as the target outputs for this model. Prior to training, several preprocessing steps were applied to prepare the data for input into the NN. First, feature selection was performed by removing the 5th, 6th, 7th, 12th, 13th, and 14th features from the input dataset to reduce noise and improve model performance. Next, normalization was carried out by dividing each feature value by\n2\nâ€‹\nÏ€\n2\\pi\n, ensuring that all input features were standardized within a consistent range. Finally, the dataset was randomly divided into two subsets: 90% of the data was allocated for training and 10% for validation. This partitioning enabled efficient model learning while allowing for a robust evaluation of generalization performance during the validation phase.\n2.6.2\nTraining and Performance\nThe NN used for regressing the minimum distance between the two robotic arms was designed with a feedforward architecture. The model consisted of an input layer, four hidden layers, and an output layer. The input layer received a total of eight features, which were derived from the original fourteen joint actuator angles of both robots. These included three features representing the joint angles for Robotâ€“2 and five features representing the joint angles for Robotâ€“1, following the removal of six features during preprocessing. The output layer comprised a single neuron representing the predicted minimum distance between the robotic arms. The network included four hidden layers with 200, 100, 100, and 25 neurons, respectively, chosen based on experimental trials to balance model complexity and training efficiency. To introduce nonlinearity, the Leaky Rectified Linear Unit (Leaky ReLU) activation function with a negative slope of 0.001 was applied to all hidden layers. Additionally, each hidden layer incorporated a dropout rate of 0.001 to prevent overfitting and enhance the modelâ€™s generalization ability. The training process utilized the AdamW optimizer with a learning rate of 0.05 and a weight decay of 1e-5. To dynamically adjust the learning rate, a cosine annealing scheduler was employed, which modified the learning rate based on the validation loss, starting with an initial learning rate of 0.05. The model was trained for a total of 2500 epochs, with the full dataset used in each iteration. For the loss function, the Charbonnier loss was selected due to its ability to effectively handle outliers, making it particularly suitable for regression tasks. The dataset was partitioned into training (90%) and validation (10%) sets to enable efficient model training and evaluation. Following training, the model was evaluated based on the\nR\n2\nR^{2}\nscore, which was computed on the validation set.\n3\nValidation Studies\n3.1\nSimulation-Based Validation\n3.1.1\nProtocol\nThe designed analytical algorithm was validated by simulating the robot arms in Unity. Initially, a simulation environment was initiated in Unity, and the URDF files for the Kinova 7-DOF robot were used to model the robots. The control logic was implemented via C# within the Unity-VS development pipeline. The robotsâ€™ local rotations were adjusted to control their movement in the Unity environment. To set the relative position and orientation between Robotâ€“1 and Robotâ€“2, the transformation matrices for both robots were computed based on their desired relative position and rotation. Using the code in Unity, Robotâ€“2â€™s position and rotation were determined by applying a transformation matrix that combined Robotâ€“1â€™s transformation with the desired relative position and rotation values. This setup ensured that Robotâ€“2 was placed correctly in relation to Robotâ€“1.\nFor generating random configurations, the joint angles and the base position of Robotâ€“2 were randomized. The random joint angles were generated within the range of motion of Kinova Gen3 robots. For collision detection, BoxColliders were created around each link of the robots. The vertices of the BoxColliders were calculated by transforming the local coordinates of each collider to world coordinates. This method allowed for the precise location of each linkâ€™s vertices, which were used to compute the minimum distance between the robotsâ€™ colliding links. The Euclidean distance method was employed to find the minimum distance between the vertices of the colliding links. This was done by iterating through all the vertices of Robotâ€“1 and Robotâ€“2 and calculating the distance between each pair of vertices. The minimum distance was tracked to identify potential collisions between the robots. The experimental robot configurations were defined later in the paper. These configurations, such as the zero and home positions, were used as reference points to compare real-time robot movements. Future phases of the study will involve physical robots, and the results from the Unity simulation will be verified in real-world conditions, as outlined in later sections of the paper.\n3.1.2\nSetup\nThe simulation setup was created in Unity 2022.3.32f1, with the project managed through Unity Hub 3.8.0. The robot models used in the simulation are based on the Kinova Gen3 7-DOF robotic arm, chosen for its versatility in robotic manipulation tasks. The models were imported into Unity as URDF files via a custom package obtained from the Unity Package Manager, specifically designed for the Kinova Gen3 robot. Two C# scripts were written in Visual Studio 2022 to manage the configuration and control of the robots, as well as to compute the minimum distance between the two robots during their interaction. One script controlled the robotâ€™s joint angles and generated random configurations, while the other handled the collision detection and distance calculation between the robots. Unity was selected as the simulation platform due to its real-time physics engine, visualization tools, and seamless integration with robotics libraries. The URDF format was used for representing the robot models as it allows easy integration with Unity and accurately defines the robotâ€™s kinematics, including joint positions and link relationships.\n3.1.3\nResults and Discussion\nThe minimum distance values between potentially colliding links were compared between the experimental observations and simulation results. The comparison revealed a fair agreement, with a mean absolute error (MAE) of\n73.3\nÂ±\n\\pm\n42.2 mm\n. This demonstrates that the simulation model accurately predicts the minimum distance between the links when the robotic arms are in a collision state, confirming the validity and reliability of the simulation setup. Furthermore, the configurations observed in the simulation closely matched those observed in real-life experiments, further validating the accuracy of the simulated robotic arm behaviours. The observed MAE can be attributed to the simplified representation of the robotic arms in the simulation, where each arm was modelled as a line segment without considering the actual radius of the arm (\n50 mm\n). Despite this simplification, the results indicate that the simulation model is reliable for collision detection and for accurately representing robotic arm configurations, making it a valuable tool for predicting real-world performance.\n3.2\nExperimental Validation\n3.2.1\nProtocol\nTo validate the proposed methodology experimentally, two robotic arms were placed in 10 different random configurations, and one customized passive marker was attached to each base of the robots. Initially, the position values of the base markers (\nT\nx\n,\nT\ny\n,\nT\nz\nT_{x},T_{y},T_{z}\n) and orientation quaternion values (\nQ\n0\n,\nQ\nx\n,\nQ\ny\n,\nQ\nz\nQ_{0},Q_{x},Q_{y},Q_{z}\n) were captured using the NDI Track application and NDI camera system, a high-precision tool for real-time 3D position tracking. Joint actuator values for each configuration were recorded through the Kinova Web Application. For configurations where a collision occurred between the robotic arms (as determined visually), a calibrated probe was used to mark points on each colliding link. The probe was calibrated using pivot calibration, which involves rotating the tracked tool around a stationary point on the link to determine the 3D position of the probeâ€™s tip and minimize measurement error. Two points were selected on each link in the collision state: the highest reachable point on the link and the lowest reachable point. The position of each point was recorded in real-time as (\nT\nx\n,\nT\ny\n,\nT\nz\nT_{x},T_{y},T_{z}\n) using the NDI camera system. To align with the simulation model, where each robotic arm link was simplified as a line segment, the mean value of the highest and lowest points was calculated to represent the central point of the link. This procedure was repeated for the second robotic arm. The Euclidean distance between the mean points of the colliding links was then computed. This approach ensured consistency between the experimental data and the line-segment representation of the robotic arms used in the simulation model.\n3.2.2\nSetup\nThe experimental setup consisted of two Kinova Gen3 robotic arms, each with 7-DoF, an NDI optical tracking system, a passive 4-marker probe, and two custom-designed passive 4-marker rigid bodies, as shown in Fig.\n6\n. The setup was designed to evaluate the proposed collision detection framework under realistic surgical conditions. Both robotic arms were equipped with DaVinciÂ Si surgical instruments mounted on their end-effectors, and the system operated around a laparoscopic surgery phantom that mimicked a constrained workspace environment. For RobotÂ 1, a passive 4-marker rigid body was securely attached to its base, with its 3D solid model designed in SOLIDWORKSÂ 2022 following the NDI Tool Design Guide. The design and physical attachment of this marker are illustrated in Fig.\n7\n(a). Similarly, for RobotÂ 2, a passive 4-marker rigid body was designed and mounted on its base to provide consistent positional reference during motion, as shown in Fig.\n7\n(b). These rigid bodies enabled accurate tracking of the robotsâ€™ base poses in real time using the NDI optical tracking system. The NDI infrared (IR) camera was fixed at an elevated position to maintain an unobstructed view of both robotsâ€™ base markers throughout the experiment. The setup also included dedicated controllers for each robotic arm, allowing independent or coordinated motion control. The joint actuator values for each random configuration were accessed through the Kinova Web Application, while the positional and rotational data for the probe and rigid body markers were recorded using the NDI Track software. The complete experimental validation setup, including the dual-arm configuration, laparoscopic phantom, and tracking system, is shown in Fig.\n8\n.\nFigure 6\n:\nExperimental setup consisting of two Kinova Gen3 7-DoF robotic arms equipped with DaVinciÂ Si surgical instruments operating around a laparoscopic phantom. The NDI IR tracker was positioned to capture both robotsâ€™ base markers for precise motion tracking.\n(a)\nRobotÂ 1 base marker\n(b)\nRobotÂ 2 base marker\nFigure 7\n:\nCustom-designed passive 4-marker rigid bodies used for NDI optical tracking.\n(a) Passive 4-marker rigid body attached to the RobotÂ 1 base, with a rendered SOLIDWORKS model illustrating marker design and placement.\n(b) Passive 4-marker rigid body attached to the RobotÂ 2 base, with its corresponding rendered design.\nThese markers ensured accurate base pose tracking for both robotic arms.\nFigure 8\n:\nExperimental validation setup showing the dual Kinova Gen3 robotic arms, laparoscopic phantom, and NDI tracking system in operation. The setup enabled real-time tracking, synchronized control, and data acquisition for evaluating the collision detection framework.\n3.2.3\nResults and Discussion\nThe joint actuator values for 10 random robot configurations used to test the designed algorithm are indicated in Table\n4\n. To quantify the rate of error between the minimum distance values obtained from the proposed analytical algorithm and the observed values from the experiments for the 10 random configurations, the absolute error was defined as:\nE\nâ€‹\nr\nâ€‹\nr\nâ€‹\no\nâ€‹\nr\n=\n|\nÏˆ\nğğ±ğ©\nâˆ’\nÏˆ\nğ­ğ¡ğğ¨\n|\nError=\\left|\\mathbf{\\sqrt{\\psi_{exp}}}-\\mathbf{\\sqrt{\\psi_{theo}}}\\right|\n(21)\nwhere\nÏˆ\nğğ±ğ©\n\\mathbf{\\sqrt{\\psi_{exp}}}\nrepresents the minimum distance values observed and measured using the probe, while\nÏˆ\nğ­ğ¡ğğ¨\n\\mathbf{\\sqrt{\\psi_{theo}}}\ndenotes the theoretical minimum distance values calculated using the analytical method. The results, including the mean error, are reported in Table\n5\n.\nTable 3\n:\nSet of ten robot configurations used for collision simulation with corresponding minimum distances.\nÏˆ\ne\n\\sqrt{\\psi_{e}}\n: experimental distance measured using the probe;\nÏˆ\nt\n\\sqrt{\\psi_{t}}\n: theoretical distance from the analytical model;\nÏˆ\np\n\\sqrt{\\psi_{p}}\n: predicted distance from the neural network.\nCase\nConfiguration\nğ\nğ’†\n\\sqrt{\\psi_{e}}\n(mm)\nğ\nğ’•\n\\sqrt{\\psi_{t}}\n(mm)\nğ\nğ’‘\n\\sqrt{\\psi_{p}}\n(mm)\n1\n305\n191\n332\n2\n178\n150\n662\n3\n64\n135\n715\n4\n65\n166\n656\n5\n115\n118\n669\n6\n163\n114\n690\n7\n141\n83\n469\n8\n191\n50\n737\n9\n110\n170\n704\n10\n140\n32\n684\nTable 4\n:\nJoint angles for the set of ten robot configurations in collision.\nRobotâ€“1\nRobotâ€“2\nConf.\nq\n1\nq_{1}\nq\n2\nq_{2}\nq\n3\nq_{3}\nq\n4\nq_{4}\nq\n5\nq_{5}\nq\n6\nq_{6}\nq\n7\nq_{7}\nq\n1\nq_{1}\nq\n2\nq_{2}\nq\n3\nq_{3}\nq\n4\nq_{4}\nq\n5\nq_{5}\nq\n6\nq_{6}\nq\n7\nq_{7}\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n(rad)\n1\n4.88\n1.84\n3.36\n0.06\n5.40\n2.10\n3.02\n6.25\n1.04\n3.62\n4.28\n5.48\n2.06\n1.34\n2\n4.64\n1.52\n3.42\n6.25\n5.47\n2.00\n3.36\n0.33\n0.93\n3.80\n4.95\n5.85\n1.37\n1.39\n3\n4.96\n1.50\n3.38\n6.27\n5.43\n1.74\n3.11\n1.42\n1.29\n0.60\n5.07\n5.51\n2.10\n1.67\n4\n4.92\n0.69\n3.13\n4.32\n6.11\n1.08\n1.65\n1.42\n1.29\n3.60\n5.07\n5.51\n2.10\n1.67\n5\n4.92\n0.69\n3.13\n4.32\n6.11\n1.08\n1.65\n1.42\n0.61\n3.50\n5.00\n1.40\n1.76\n0.27\n6\n4.48\n1.43\n2.79\n6.13\n5.10\n0.02\n3.09\n1.42\n0.61\n3.50\n5.00\n1.40\n1.76\n0.27\n7\n4.91\n0.75\n2.30\n0.05\n4.02\n5.61\n4.73\n1.42\n0.61\n3.50\n5.00\n1.40\n1.76\n0.27\n8\n5.44\n0.29\n3.13\n3.99\n0.04\n1.00\n1.54\n1.56\n1.38\n2.85\n6.19\n4.93\n0.74\n3.26\n9\n5.23\n5.60\n2.83\n3.74\n1.15\n0.98\n0.37\n1.56\n1.38\n2.85\n6.19\n4.93\n0.74\n3.26\n10\n1.67\n4.43\n2.97\n5.58\n2.32\n1.71\n3.45\n1.18\n0.98\n2.09\n4.14\n0.12\n1.64\n2.44\nTable 5\n:\nError Analysis of Theoretical and Experimental Minimum Distances.\n#\nÏˆ\ne\nâ€‹\nx\nâ€‹\np\n\\sqrt{\\psi_{exp}}\n(mm)\nÏˆ\nt\nâ€‹\nh\nâ€‹\ne\nâ€‹\no\n\\sqrt{\\psi_{theo}}\n(mm)\nE\nâ€‹\nr\nâ€‹\nr\nâ€‹\no\nâ€‹\nr\nError\n(mm)\n1\n305\n191\n114\n2\n178\n150\n28\n3\n64\n135\n71\n4\n65\n166\n101\n5\n115\n118\n3\n6\n163\n114\n49\n7\n141\n83\n58\n8\n191\n50\n141\n9\n110\n170\n60\n10\n140\n32\n110\nMean\nâ€“\nâ€“\n73.33\n3.3\nValidation on Test Data\n3.3.1\nProtocol\nThe NN regressor was validated using a systematic protocol to ensure its robustness and generalization capability. The dataset, comprising 75,655 configurations generated through random robotic arm positions in a Unity-based simulation, was divided by a 70%:15%:15% split ratio. Preprocessing steps included the removal of six features to reduce noise and improve performance, normalization by scaling input features within a standardized range of\n[\n0\n,\n1\n]\n[0,1]\nthrough division by\n2\nâ€‹\nÏ€\n2\\pi\n, and careful feature selection, resulting in eight input features derived from the joint actuator angles of the robotic arms. The model, designed with a feedforward architecture, consisted of an input layer, four hidden layers with progressively smaller neuron counts (200, 100, 100, 25), and an output layer predicting the minimum distance between the arms. Non-linearity was introduced using the Leaky ReLU activation function, and dropout regularization with a rate of 0.001 was employed to prevent overfitting. Training leveraged the AdamW optimizer with weight decay and a cosine annealing scheduler for learning rate adjustment. The model was evaluated on the validation set using the\nR\n2\nR^{2}\nscore, providing a comprehensive measure of its predictive performance and alignment with the regression task objectives.\n3.3.2\nSetup\nThe training and validation process was conducted in a controlled computational environment to ensure consistent and reproducible results. The NN was implemented using Python and TensorFlow library, utilizing a workstation with an NVIDIA GeForce RTX 4070 Ti GPU, 128GB of RAM for efficient computation. The dataset, consisting of 75,655 configurations of robotic arm positions, was preprocessed and loaded using NumPy and Pandas. The training pipeline incorporated extensive debugging and logging tools to monitor the loss and validation performance during training. Random seeds were set for data splitting and model initialization to ensure reproducibility. The AdamW optimizer and cosine annealing scheduler were implemented to dynamically adjust the learning rate based on validation loss, improving training stability. The model was trained over 2500 epochs, with batch processing employed to iterate over the entire dataset in each epoch. The use of the Charbonnier loss function facilitated robust handling of outliers during optimization, further enhancing the reliability of the validation process.\n3.3.3\nResults and Discussions\nIn this section, we present the results obtained from the trained NN, which was evaluated on a dataset of 73,817 random configurations as previously described. To assess the networkâ€™s performance in the context of the theoretical model, we compared the predicted minimum distances with those calculated using the experimental method. The comparison is summarized in Table\n6\n, where the predicted values (from the NN) were compared to the experimental measurements. As shown in Table\n6\n, the predicted values fairly approximated the experimental measurements, with an average deviation of 282.2 mm. High relative errors occurred when distances were below 100 mm, indicating collisions.\nTable 6\n:\nComparison of predicted minimum distances with experimental values\nConfiguration\nPredicted (mm)\nExperimental (mm)\nRelative Error (%)\n1\n330\n305\n8.20\n2\n662\n178\n271.91\n*\n3\n715\n642\n11.37\n4\n656\n653\n0.46\n5\n669\n115\n481.74\n*\n6\n690\n163\n323.93\n*\n7\n469\n455\n4.08\n8\n737\n749\n1.60\n9\n704\n110\n540.90\n*\n10\n684\n148\n497.20\n*\nMean\n-\n-\n4.94%\n*\n: Experimental distance\n<\n200\n<200\nmm\n3.4\nPerformance\nThe networkâ€™s performance was evaluated using several key metrics, including the MAE, root mean squared error (RMSE), and coefficient of determination (\nR\n2\nR^{2}\n). The performance metrics are summarized in Table\n7\n, where the modelâ€™s predictions were compared with the actual values obtained from the experiments. The results demonstrated that the network accurately predicted the minimum distance between the robotic arms. Specifically, the MAE of 282.2Â mm represented the average deviation between the predicted and observed minimum distances. Given that each robotic arm had a radius of 50Â mm and the robots were modeled as linear lines without accounting for this radius, the model demonstrated predictive performance sufficient for real-time collision warning. Therefore, the networkâ€™s predictions effectively represent the distance between the central axes of the robotic arms rather than the true physical distances between their surfaces. Despite this simplification, the relatively low MAE and RMSE values indicated that the networkâ€™s predictions were closely aligned with the ground truth data. Additionally, the\nR\n2\nR^{2}\nvalue of 0.85 reflected a strong correlation between the predicted and actual minimum distance values, confirming that the model successfully captured the underlying patterns in the data.\nTable 7\n:\nPerformance metrics for the trained NN.\nMetric\nValue (mm)\nMean Absolute Error (MAE)\n282.2\nRoot Mean Squared Error (RMSE)\n382.8\nR-squared (\nR\n2\nR^{2}\n)\n0.85\nAlso identity plot of predicted minimum distance vs ground truth is shown in Fig.\n9\n, where the predicted values on the y-axis and the reference values on the x-axis. The points in the plot closely followed the identity line, indicating that the predictions were nearly identical to the observed measurements. This further corroborated the modelâ€™s success in predicting the minimum distances between the robotic arms.\nFigure 9\n:\nIdentity plot of predicted minimum distance versus ground truth. The dashed red line represents perfect prediction, and the clustering of data points along this line demonstrates strong agreement between the neural network predictions and experimental measurements.\nTo evaluate the learning behaviour of the NN during training, the loss values for both the training and validation datasets were recorded and plotted for each epoch, as shown in Fig.\n10\n. This plot provides insights into the convergence of the model and its ability to generalize to unseen data. It demonstrates that the training loss steadily decreases as the epochs progress, indicating that the model successfully optimizes the objective function. The validation loss follows a similar trend, stabilizing after approximately 15 epochs. This stabilization suggests that the model achieves a balance between fitting the training data and generalizing to the validation data without significant overfitting.\nFurthermore, a filtered error histogram was achieved to illustrate the distribution of prediction errors for the trained NN when estimating the minimum distances between robotic arms as indicated in Fig.\n11\n. The Filtered Error Histogram revealed a clear Gaussian-like distribution, with errors predominantly centered around zero. This indicated that the NNâ€™s predictions closely aligned with the ground truth in most cases. The majority of the errors fell within\nÂ±\n1\n\\pm 1\nstandard deviation (\nÏƒ\n\\sigma\n), demonstrating a high level of prediction accuracy. Larger deviations, beyond\nÂ±\n2\nâ€‹\nÏƒ\n\\pm 2\\sigma\n(marked in orange) and\nÂ±\n3\nâ€‹\nÏƒ\n\\pm 3\\sigma\n(marked in red), occurred infrequently, signifying that significant prediction errors were rare. This overall distribution reflected the robustness and reliability of the trained NN in estimating the minimum distances.\nFigure 10\n:\nTraining and validation loss curves of the NN over 2,500 epochs. The rapid convergence within the first few hundred epochs, followed by stable loss values, indicates effective learning and minimal overfitting, confirming the networkâ€™s robustness and generalization capability.\nFigure 11\n:\nFiltered error histogram showing the distribution of prediction errors for the validation set. Most errors fall within\nÂ±\n1\nâ€‹\nÏƒ\n\\pm 1\\sigma\n, demonstrating that the majority of predicted minimum distances closely match the ground truth, while outliers beyond\nÂ±\n2\nâ€‹\nÏƒ\n\\pm 2\\sigma\nare infrequent, confirming reliable model performance.\nThe trained NN demonstrated high accuracy in predicting the minimum distance between the robotic arms based on a combination of joint angles and relative positions. The networkâ€™s ability to generalize to unseen configurations, as evidenced by the low MAE and high agreement with experimental results, highlighted its robustness. Moreover, the comparison with the theoretical model further validated the network as a promising alternative for real-time predictions in robotic applications, where traditional methods might fall short in handling complex configurations and non-linear relationships. The results confirmed the feasibility of using machine learning techniques, specifically NNs, to enhance the precision and reliability of robotic simulations and real-world applications.\n4\nConclusions\nThis study presented an integrated approach to collision detection for multi-arm robotic systems through analytical modeling, simulation, and learning-from-simulation methods. The proposed BÃ©zier-based analytical framework provided an efficient geometric representation for modeling robotic links and enabled the computation of inter-arm distances with high precision. The Unity-based simulation platform further allowed the exploration of diverse arm configurations and validation of proximity estimation models under realistic kinematic conditions. Moreover, the deep learning model trained on the synthetic dataset demonstrated strong predictive accuracy for estimating minimum distances in real time, supporting safe and coordinated operation of robotic manipulators in shared workspaces. While the framework achieved robust performance, several directions remain for future work. First, incorporating more complex collision geometries and surface contact models would improve the accuracy of proximity estimation, particularly for robots with non-cylindrical or flexible links. Second, extending the analytical formulation to include dynamic modeling and time-varying constraints would enable predictive collision avoidance rather than post-detection estimation. Third, integrating additional sensory feedback such as force or tactile data could enhance situational awareness and support active control strategies for collision avoidance. Finally, deploying the trained neural model on real robotic hardware for closed-loop validation will further verify the frameworkâ€™s generalizability and real-time performance in clinical and industrial environments. Together, these extensions will advance the presented framework toward a complete, adaptive, and learning-based safety system for next-generation surgical and collaborative robotic platforms.\nFunding\nThis work was supported by Montreal General Hospital Foundation and Natural Science and Engineering Research Council (NSERC) of Canada.\nConflict of Interest/Competing Interests\nNone.\nEthics Approval and Consent to Participate\nNot applicable.\nData Availability\nThe data that support the findings of this study are available from the corresponding author upon reasonable request.\nAuthor Contributions\nS. Ghiasi: Investigation; Data curation; Formal analysis; Methodology; Writing (original draft).\nM. Roshanfar: Investigation; Data curation; Formal analysis; Visualization; Writing (original draft).\nJ. Barralet: Conceptualization; Methodology; Writing (review & editing); Supervision.\nL. Feldman: Conceptualization; Writing (review & editing); Supervision.\nA. Hooshiar: Conceptualization; Writing (review & editing); Funding acquisition; Supervision.\nReferences",
    "preview_text": "This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.\n\n1]\n\\orgdiv\nSurgical Performance Enhancement and Robotics (SuPER) Centre, Department of Surgery,\n\\orgname\nMcGill University,\n\\orgaddress\n\\city\nMontreal,\n\\state\nQC,\n\\country\nCanada\n2]\n\\orgdiv\nThe Wilfred and Joyce Posluns Centre for Image Guided Innovation & Therapeutic Intervention (PCIGITI),\n\\orgname\nThe Hospital for Sick Children (SickKids),\n\\orgaddress\n\\city\nToronto,\n\\state\nON,\n\\country\nCanada\nNeural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation\n\\fnm\nSarvin\n\\sur\nGhiasi\n\\fnm\nMajid\n\\sur\nRoshanfar\n\\fnm\nJake\n\\sur\nBarralet\n\\fnm\nLiane S.\n\\sur\nFeldman\n\\fnm\nAmir\n\\sur\nHooshiar\namir.hooshiar@mcgill.ca\n[\n[\nAbstract\nThis study presents ",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "collision detection",
        "minimum distance estimation",
        "robotic arms",
        "laparoscopic surgery",
        "deep neural network",
        "simulation",
        "machine learning"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆåˆ†æå»ºæ¨¡å’Œæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œç”¨äºå¤šè‡‚è…¹è…”é•œæ‰‹æœ¯æœºå™¨äººçš„ç¢°æ’æ£€æµ‹å’Œæœ€å°è·ç¦»ä¼°è®¡ï¼Œä»¥æé«˜å®‰å…¨æ€§å’Œæ“ä½œæ•ˆç‡ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T20:54:35Z",
    "created_at": "2026-01-27T15:53:26.178360",
    "updated_at": "2026-01-27T15:53:26.178366",
    "recommend": 0
}