{
    "id": "2512.01850v1",
    "title": "Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching",
    "authors": [
        "Yue Pan",
        "Tao Sun",
        "Liyuan Zhu",
        "Lucas Nunes",
        "Iro Armeni",
        "Jens Behley",
        "Cyrill Stachniss"
    ],
    "abstract": "ç‚¹äº‘é…å‡†å°†å¤šä¸ªæ— ä½å§¿ä¿¡æ¯çš„ç‚¹äº‘å¯¹é½åˆ°åŒä¸€åæ ‡ç³»ä¸‹ï¼Œæ˜¯ä¸‰ç»´é‡å»ºå’Œæœºå™¨äººå®šä½ä¸­çš„å…³é”®æ­¥éª¤ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†é…å‡†é—®é¢˜è½¬åŒ–ä¸ºæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼šé€šè¿‡å­¦ä¹ ä¸€ä¸ªè¿ç»­çš„ã€é€ç‚¹çš„é€Ÿåº¦åœºï¼Œå°†å¸¦æœ‰å™ªå£°çš„è¾“å…¥ç‚¹é€æ­¥å˜æ¢è‡³å¯¹é½åçš„åœºæ™¯ä¸­ï¼Œä»è€Œæ¢å¤æ¯ä¸ªè§†è§’çš„ä½å§¿ã€‚ä¸ä»¥å¾€æ–¹æ³•å…ˆè¿›è¡Œå¯¹åº”å…³ç³»åŒ¹é…ä¼°è®¡ç‚¹äº‘å¯¹ä¹‹é—´çš„å˜æ¢ï¼Œå†ä¼˜åŒ–å¤šè§†å›¾é—´æˆå¯¹å˜æ¢ä»¥å®ç°å¤šè§†å›¾é…å‡†ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç›´æ¥ç”Ÿæˆå·²å¯¹é½çš„ç‚¹äº‘ã€‚ç»“åˆè½»é‡çº§å±€éƒ¨ç‰¹å¾æå–å™¨ä»¥åŠæ¨ç†é˜¶æ®µçš„åˆšæ€§çº¦æŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç‚¹å¯¹å’Œå¤šè§†å›¾é…å‡†åŸºå‡†æµ‹è¯•ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ä½é‡å ç‡åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½è·¨å°ºåº¦å’Œè·¨ä¼ æ„Ÿå™¨æ¨¡æ€æ³›åŒ–ã€‚è¯¥æ–¹æ³•è¿˜å¯æ”¯æŒé‡å®šä½ã€å¤šæœºå™¨äººSLAMä»¥åŠå¤šæ—¶æ®µåœ°å›¾èåˆç­‰ä¸‹æ¸¸åº”ç”¨ã€‚æºä»£ç è§ï¼šhttps://github.com/PRBonn/RAPã€‚",
    "url": "https://arxiv.org/abs/2512.01850v1",
    "html_url": "https://arxiv.org/html/2512.01850v1",
    "html_content": "Register Any Point:\nScaling 3D Point Cloud Registration by Flow Matching\nYue Pan\n1\nTao Sun\n2\nLiyuan Zhu\n2\nLucas Nunes\n1\nIro Armeni\n2\nJens Behley\n1\nCyrill Stachniss\n1\n1\nUniversity of Bonn\n2\nStanford University\nAbstract\nPoint cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\nIn this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\nUnlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.\nWith a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities.\nIt further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging.\nFigure 1\n:\nOur method for scalable multi-view point cloud registration.\nTo register multiple unposed point clouds, prior work typically first performs correspondence matching and then optimizes a pose graph (top-left).\nIn contrast, we introduce a single-stage model that directly generates the registered point cloud via flow matching in Euclidean space (top-right), bypassing the need for explicit correspondence matching and pose graph optimization.\nOur model generalizes across diverse point cloud data from object-centric, indoor, and outdoor scenarios at scan, sub-map, and map levels (bottom).\n1\nIntroduction\nPoint cloud registration is a cornerstone in 3D vision, robotics, and photogrammetry with broad applications, from merging multiple partial 3D scans into a consistent 3D model to localizing sensors in an existing 3D map for downstream tasks, including simultaneous localization and mapping (SLAM)\n[\nzhang20243d\n]\n, 3D reconstruction\n[\ndong2020jprs-whutls\n]\n, and robotic manipulation\n[\npomerleau2015ftr\n]\n.\nYet, obtaining reliable registration in the wild is a hard problem.\nReal-world data is sparse, noisy, and non-uniform in density; sensors differ in modality and calibration; overlaps between point clouds can be small, and local matches can be ambiguous\n[\nhuang2021cvpr-predator\n,\nsun2025jprs\n,\nan2024pointtr\n,\nliu2024cvpr-eyoc\n]\n.\nPrevailing approaches for multi-view point cloud registration follow a two-stage pipeline: align all overlapping pairs of scans, then solve a global pose graph to enforce consistency\n[\nschneider2012isprs\n,\nhuang2021ral-bundle\n]\n.\nPairwise alignment typically relies on matching local feature correspondences with a robust estimator\n[\nfischler1981cacm\n]\n.\nWhile conceptually appealing, this has two limitations: (i)\nquadratic complexity\n: cost scales quadratically with the number of scans due to exhaustive correspondence search across all pairs;\nand (ii)\nlimited global context\n: the pairwise stage limits capturing global context, hurting performance under low overlap and incomplete observations.\nAlthough specialized modules can improve low-overlap pairwise registration\n[\nhuang2021cvpr-predator\n,\nyao2024iccv-parenet\n]\nand some works conduct hierarchical registration\n[\ndong2018jprs\n]\nor edge selection\n[\nwang2023cvpr-sghr\n]\nto avoid the quadratic cost, these add complexity while remaining tied to iterative pose-graph refinement sensitive to pairwise alignment errors.\nRecent 3D vision research departs from this two-stage pipeline by leveraging feed-forward and generative models.\nIn image-based 3D reconstruction, feed-forward approaches\n[\nwang2024cvpr-dust3r\n]\nencapsulate the entire structure-from-motion process into a single neural network, directly producing globally consistent poses and dense geometry from a set of images.\nVGGT\n[\nwang2025cvpr-vggt\n]\ndemonstrates that a large transformer can infer all key 3D attributes, including camera poses and depth maps, from one or many views in a single pass.\nIn the point cloud domain, Rectified Point FlowÂ (RPF)\n[\nsun2025neurips\n]\npioneered a generative approach to pose estimation by learning a continuous flow field that moves points from random noise to their assembled target positions for multiple object-centric benchmarks.\nThese findings suggest that a single feed-forward model can holistically reason about multiple partial observations and produce a consistent 3D alignment, given sufficient capacity and training data.\nScaling such single-stage models to large-scale, multi-view 3D registration, however, raises another key challenge: the sampling process does not always yield stable, perfectly rigid predictions, especially in cluttered environments where geometry is more diverse than in object-centric settings.\nEven with an explicit projection step of the final prediction onto\nSE\nâ€‹\n(\n3\n)\n\\mathrm{SE}(3)\n, as in RPF\n[\nsun2025neurips\n]\n, the post-hoc correction cannot constrain the entire flow trajectory and, thus, the sampled flows can drift away from the flow distribution on which the model was trained, limiting performance.\nThis motivates our work, a scalable generative model that aligns multiple point clouds in a single stage while explicitly enforcing rigidity.\nInstead of exhaustive pairwise pose estimation, the model learns to transform all input point clouds directly into a canonical coordinate frame, effectively fusing them into a coherent scene.\nTo make generation robust and satisfy rigid constraints, we propose using rigidity both as a guidance signal for flow sampling and as a criterion for selecting generations.\nTo train at scale, we curate over 100K samples from 17 diverse datasets spanning object-centric, indoor, and outdoor settings.\nSupervising in Euclidean space across this mixture of data provides strong scene priors that enable the model to complete partial views and generalize across scales and sensor modalities.\nWe will release our code and models at\nhttps://github.com/PRBonn/RAP\n.\nIn summary, our contributions are four-fold:\nâ€¢\nWe propose a generative flow-matching model that performs multi-view point cloud registration in a single stage, bypassing iterative pose graph optimization.\nâ€¢\nWe introduce a rigidity-forcing sampling and selection strategy that enforces per-scan rigid constraints and improves registration accuracy.\nâ€¢\nWe develop a large-scale training recipe that aggregates over 100K samples from 17 heterogeneous datasets, enabling strong generalization across diverse scenarios, scales, and sensor modalities.\nâ€¢\nWe demonstrate robust performance on both pairwise and multi-view benchmarks, achieving substantial improvements over state-of-the-art methods in challenging large-scale, low-overlap scenarios.\n2\nRelated Work\nPairwise point cloud registration\nhas long relied on local feature matching with robust estimators\n[\nfischler1981cacm\n,\nyang2020tro\n,\nlim2025icra-kissmatcher\n]\n.\nEarly approaches rely on hand-crafted local descriptors\n[\nrusu2009icra\n,\nsalti2014cviu\n]\nto establish correspondences.\nModern methods\n[\nbai2020cvpr-d3feat\n,\nhuang2021cvpr-predator\n,\nqin2022cvpr-geotransformer\n,\nao2021cvpr-spinnet\n,\nwang2022mm-yoho\n,\nliu2024cvpr-eyoc\n,\nseo2025iccv\n,\nlim2025icra-kissmatcher\n]\nreplace or augment these with learned local features.\nBeyond explicit matches, correspondence-free and end-to-end approaches such as RPM-Net\n[\nyew2020cvpr\n]\ndirectly supervise the pose with differentiable assignment and iterative refinement.\nOur approach departs from both families of approaches: rather than seeking correspondences or iteratively refining poses, we learn a conditional velocity field that transports noise to the merged scene, from which rigid transforms are recovered using SVD.\nMulti-view point cloud registration\nis typically handled by first estimating noisy pairwise relative poses and then running pose graph optimization (PGO). In practice, optimization is performed on a factor graph with one node per scan and edges that encode relative pose measurements and their uncertainties under various robust objectives\n[\ntheiler2015jprs\n,\ndong2018jprs\n,\ndong2020jprs-whutls\n,\ngojcic2020cvpr\n,\nwang2023cvpr-sghr\n,\ndellaert2012git\n]\n.\nBy contrast, our single-stage approach aligns an arbitrary number of scans at once, enforcing multi-view consistency by construction. As a result, our method dispenses with the need for a separate PGO stage and avoids the quadratic costs from pairwise pose estimation.\nNonetheless, our predictions can still serve as strong and time-efficient initializations for downstream PGO solvers that incorporate additional signals (e.g., from gravity, IMU, or GNSS) or task-specific constraints.\nGenerative modeling approaches for 3D data\nleverage diffusion- and flow-based models to generate geometric structures\n[\nnunes2024cvpr\n,\nsun2025neurips\n,\nsanghi2023cvpr\n,\nwang2025iclr-puzzlefusionplusplus\n,\nxu2023cvpr-dzts\n,\nzeng2024cvpr-ppaw\n,\nbian2025iclr\n,\nguo2025neurips\n,\nzhang2025iccv\n,\ndu2025cvpr\n,\nli2025iccv-garf\n,\nren2024cvpr\n,\nnunes2025arxiv\n,\njiang2023neurips-se3diffusion\n]\n. Both model generation as stochastic transport from source to target distributions: diffusion via iterative denoising, while flow-matching predicts velocity fields to iteratively transform data. These models have been applied to text-to-shape generation\n[\nsanghi2023cvpr\n,\nxu2023cvpr-dzts\n,\nzeng2024cvpr-ppaw\n]\n, 3D scene completion\n[\nnunes2024cvpr\n,\nzhang2025iccv\n,\ndu2025cvpr\n]\n, and annotated data generation\n[\nren2024cvpr\n,\nnunes2025arxiv\n,\nbian2025iclr\n,\nguo2025neurips\n]\n.\nMore recent works leverage diffusion and flow-matching models to achieve point cloud registration\n[\njiang2023neurips-se3diffusion\n,\nsun2025neurips\n]\nas a way to overcome limitations of standard approaches. In DiffusionReg\n[\njiang2023neurips-se3diffusion\n]\n, point cloud registration is formulated as a diffusion process on the SE(3) manifold, generating the corresponding rigid-body transformation between the source and target point clouds.\nCloser to our approach, RPF\n[\nsun2025neurips\n]\nuses a conditional flow-matching model to unify pairwise registration and multi-part shape assembly tasks.\nOur method builds on the point cloud flow matching ideas but differs from RPF in two ways: (i) we do not rely on a specialized overlap prediction network; instead, we represent each point cloud by a sparse set of learned features, enabling scaling to large scenes; and (ii) we rigidify flows at each step to guarantee per-view rigid motion. Together, these choices enable strong generalizations to unseen datasets and robustness at extremely low overlap.\n3\nGenerative Point Cloud Registration\nFigure 2\n:\nOverview of our approach to multi-view point cloud registration. Starting with unposed point clouds\nğ’«\n\\mathcal{P}\n, we sample points\nğ’¬\n\\mathcal{Q}\nwith corresponding local features\nâ„±\n\\mathcal{F}\n. We use a diffusion transformer with alternating-attention blocks for conditional flow matching that generates the aggregated point cloud\nğ—\n^\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}(0)\nfrom Gaussian noise\nğ—\nâ€‹\n(\n1\n)\n\\mathbf{X}(1)\n. Finally, we recover the individual transformations\nğ“\n^\ni\n\\hat{\\mathbf{T}}_{i}\nusing SVD from the aggregated point cloud and apply them to the original unposed point clouds to get the registered point clouds\nğ\n^\nr\n\\hat{\\mathbf{P}}^{r}\n.\nIn this section, we present our generative approach to multi-view point cloud registration. We formulate registration as conditional flow matching, where a transformer-based model learns to directly generate the aggregated registered point cloud from unposed inputs.\nWe then describe our model architecture and training procedure, and how we enforce generation rigidity during inference.\n3.1\nProblem Definition\nWe consider the general setting of the multi-view point cloud registration problem. The input is a set of\nN\nN\n(\nN\nâ‰¥\n2\nN\\geq 2\n) unordered point clouds\nğ’«\n:=\n{\nğ\ni\nâˆˆ\nâ„\n3\nÃ—\nM\ni\nâˆ£\ni\n=\n1\n,\nâ€¦\n,\nN\n}\n,\n\\mathcal{P}:=\\left\\{\\mathbf{P}_{i}\\in\\mathbb{R}^{3\\times M_{i}}\\mid i=1,\\ldots,N\\right\\},\n(1)\nwhere\nM\ni\nM_{i}\nis the point count of the\ni\ni\n-th point cloud.\nThese point clouds may come from individual LiDAR or depth-camera scans, or from accumulated point cloud maps built by a SLAM system.\nWe do not assume any initial guess of the transformation from the coordinate frame of each point cloud to a global frame, but we assume that all point clouds are observations of the same scene and can be registered into a single connected point cloud.\nThe goal is to estimate registered point clouds\nğ’«\nr\n:=\n{\nğ\ni\nr\nâˆˆ\nâ„\n3\nÃ—\nM\ni\nâˆ£\ni\n=\n1\n,\nâ€¦\n,\nN\n}\n\\mathcal{P}^{r}:=\\{\\mathbf{P}_{i}^{r}\\in\\mathbb{R}^{3\\times M_{i}}\\mid i=1,\\ldots,N\\}\n(2)\nin a global coordinate frame. In practice, there may be non-rigid deformations caused by effects such as motion distortion, dynamic objects, or SLAM drift. When these non-rigid effects are negligible, the registered point clouds can be modeled as rigid transformations of the inputs:\nğ’¯\n=\n{\nğ“\ni\nâˆˆ\nSE\nâ€‹\n(\n3\n)\nâˆ£\ni\n=\n1\n,\nâ€¦\n,\nN\n}\n\\mathcal{T}=\\{\\mathbf{T}_{i}\\in\\mathrm{SE}(3)\\mid i=1,\\ldots,N\\}\n, which can be recovered via the Kabsch algorithm using SVD between\nğ’«\n\\mathcal{P}\nand\nğ’«\nr\n\\mathcal{P}^{r}\n.\n3.2\nFlow Matching for Multi-view Registration\nFollowing RPF\n[\nsun2025neurips\n]\n, we formulate multi-view registration as a conditional generation problem and directly generate the registered point cloud\nğ’«\nr\n\\mathcal{P}^{r}\ngiven the unposed input\nğ’«\n\\mathcal{P}\n. The rigid transformations\nğ’¯\n\\mathcal{T}\nare then recovered as a by-product.\nWe apply flow matching\n[\nliu2023iclr\n]\ndirectly to the 3D Euclidean coordinates of point clouds. The model learns to transport a 3D noised point cloud\nğ—\nâ€‹\n(\n1\n)\nâˆˆ\nâ„\n3\nÃ—\nM\n\\mathbf{X}(1)\\in\\mathbb{R}^{3\\times M}\nsampled from a Gaussian\nğ’©\nâ€‹\n(\nğŸ\n,\nğˆ\n)\n\\mathcal{N}(\\mathbf{0},\\mathbf{I})\nto a target point cloud\nğ—\nâ€‹\n(\n0\n)\nâˆˆ\nâ„\n3\nÃ—\nM\n\\mathbf{X}(0)\\in\\mathbb{R}^{3\\times M}\nby learning a time-dependent velocity field\nâˆ‡\nt\nğ—\nâ€‹\n(\nt\n)\n\\nabla_{t}\\mathbf{X}(t)\n, parameterized by a neural network\nğ•\nÎ¸\nâ€‹\n(\nt\n,\nğ—\nâ€‹\n(\nt\n)\nâˆ£\nğ‚\n)\n\\mathbf{V}_{\\theta}(t,\\mathbf{X}(t)\\mid\\mathbf{C})\nconditioned on\nğ‚\n\\mathbf{C}\n, which will be detailed in Sec.\n3.4\n.\nThe forward process is a linear interpolation in 3D space between noise and the target,\ni.e\n.,\nğ—\nâ€‹\n(\nt\n)\n=\n(\n1\nâˆ’\nt\n)\nâ€‹\nğ—\nâ€‹\n(\n0\n)\n+\nt\nâ€‹\nğ—\nâ€‹\n(\n1\n)\n,\nt\nâˆˆ\n[\n0\n,\n1\n]\n.\n\\mathbf{X}(t)=(1-t)\\mathbf{X}(0)+t\\mathbf{X}(1),\\quad t\\in[0,1].\n(3)\nThe flow model\nğ•\nÎ¸\n\\mathbf{V}_{\\theta}\nis trained using the conditional flow matching loss\n[\nlipman2023iclr\n]\n.\nFor our registration task, the target\nğ—\nâ€‹\n(\n0\n)\n\\mathbf{X}(0)\nis the aggregated registered point cloud\nğ\nr\n=\nâ‹ƒ\ni\n=\n1\nN\nğ\ni\nr\n\\mathbf{P}^{r}=\\bigcup_{i=1}^{N}\\mathbf{P}_{i}^{r}\n.\nAt inference time, we reconstruct the registered point cloud by numerically integrating the predicted velocity field\nğ•\nÎ¸\nâ€‹\n(\nt\n,\nğ—\nâ€‹\n(\nt\n)\nâˆ£\nğ‚\n)\n\\mathbf{V}_{\\theta}(t,\\mathbf{X}(t)\\mid\\mathbf{C})\nfrom\nt\n=\n1\nt=1\nto\nt\n=\n0\nt=0\n. In practice, we use\nÎº\n=\n10\n\\kappa=10\nuniform Euler steps:\nğ—\n^\nâ€‹\n(\nt\nâˆ’\nÎ”\nâ€‹\nt\n)\n=\nğ—\n^\nâ€‹\n(\nt\n)\nâˆ’\nğ•\nÎ¸\nâ€‹\n(\nt\n,\nğ—\n^\nâ€‹\n(\nt\n)\nâˆ£\nğ‚\n)\nâ€‹\nÎ”\nâ€‹\nt\n,\n\\hat{\\mathbf{X}}(t-\\Delta t)=\\hat{\\mathbf{X}}(t)-\\mathbf{V}_{\\theta}(t,\\hat{\\mathbf{X}}(t)\\mid\\mathbf{C})\\Delta t,\n(4)\nwith\nÎ”\nâ€‹\nt\n=\n1\n/\nÎº\n\\Delta t=1/\\kappa\n. After integration, the resulting\nğ—\n^\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}(0)\napproximates the registered point cloud.\nWe then partition\nğ—\n^\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}(0)\ninto per-view subsets and estimate the corresponding poses\nğ“\n^\ni\n\\hat{\\mathbf{T}}_{i}\nvia the Kabsch algorithm.\n3.3\nRigidity-forcing Inference\nThe flow model alone does not guarantee per-view rigidity. While allowing non-rigid point motions increases the modelâ€™s expressiveness, it can also drive sampling trajectories away from the training distribution defined in Eq.Â (\n9\n). We therefore exploit the Euclidean nature of our flow formulation and introduce a\nrigidity-forcing\nEuler integration that projects intermediate predictions onto per-view\nSE\nâ€‹\n(\n3\n)\n\\mathrm{SE}(3)\norbits at each step. In addition, we empirically find that the resulting rigidity error during this procedure provides an effective criterion for selecting among multiple generations.\nEuler integration with rigidity-forcing.\nTo illustrate, we define the per-view projection operator\nÎ \n\\Pi\nof an estimate\nğ—\n^\ni\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}_{i}(0)\nonto the rigid orbit of the input\nğ\ni\n\\mathbf{P}_{i}\n, as\nÎ \nğ\ni\nâ€‹\n(\nğ—\n^\ni\nâ€‹\n(\n0\n)\n)\n:=\nğ‘\n^\ni\nâ€‹\nğ\ni\n+\nğ­\n^\ni\n,\n\\Pi_{{\\mathbf{P}}_{i}}\\left(\\hat{\\mathbf{X}}_{i}(0)\\right):=\\hat{\\mathbf{R}}_{i}\\,{\\mathbf{P}}_{i}+\\hat{\\mathbf{t}}_{i},\n(5)\nwhere\n(\nğ‘\n^\ni\n,\nğ­\n^\ni\n)\n(\\hat{\\mathbf{R}}_{i},\\hat{\\mathbf{t}}_{i})\nis the optimal rigid transformation between\nğ\ni\n\\mathbf{P}_{i}\nand\nğ—\n^\ni\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}_{i}(0)\n, computed via the Kabsch algorithm.\nGiven the current state\nğ—\nâ€‹\n(\nt\n)\n\\mathbf{X}(t)\nand the velocity\nğ•\nÎ¸\nâ€‹\n(\nt\n,\nğ—\nâ€‹\n(\nt\n)\nâˆ£\nğ‚\n)\n\\mathbf{V}_{\\theta}(t,\\mathbf{X}(t)\\!\\mid\\!\\mathbf{C})\n,\nwe extrapolate the registered point cloud estimate, as\nğ˜\n^\nâ€‹\n(\n0\n)\n:=\nğ—\nâ€‹\n(\nt\n)\nâˆ’\nt\nâ€‹\nğ•\nÎ¸\nâ€‹\n(\nt\n,\nğ—\nâ€‹\n(\nt\n)\nâˆ£\nğ‚\n)\n.\n\\hat{\\mathbf{Y}}(0):=\\mathbf{X}(t)-t\\,\\mathbf{V}_{\\theta}(t,\\mathbf{X}(t)\\!\\mid\\!\\mathbf{C}).\n(6)\nWe now rigidify it to obtain the rigid projection of all views\nÎ \nğ’«\n\\Pi_{{\\mathcal{P}}}\nfollowing Eq. (\n5\n)\nand compute the flow at the next step\nt\nâ€²\nâ†\nt\nâˆ’\nÎ”\nâ€‹\nt\nt^{\\prime}\\leftarrow t-\\Delta t\nas\nğ—\nâ€‹\n(\nt\nâ€²\n)\n:=\n(\n1\nâˆ’\nt\nâ€²\n)\nâ€‹\nÎ \nğ’«\nâ€‹\n(\nğ˜\n^\nâ€‹\n(\n0\n)\n)\n+\nt\nâ€²\nâ€‹\nğ—\nâ€‹\n(\n1\n)\n.\n\\mathbf{X}(t^{\\prime}):=(1-t^{\\prime})\\,\\Pi_{{\\mathcal{P}}}\\left(\\hat{\\mathbf{Y}}(0)\\right)+t^{\\prime}\\,\\mathbf{X}(1).\n(7)\nWe repeat the above sampling step until\nt\nt\nreaches\n0\n.\nRigidity-based sample selection.\nIn addition to sampling, rigidity also provides a geometric criterion for selecting among multiple generative predictions of the same input.\nGiven a generated estimate\nğ—\n^\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}(0)\nand its rigidified result\nÎ \nğ’«\nâ€‹\n(\nğ—\n^\nâ€‹\n(\n0\n)\n)\n\\Pi_{{\\mathcal{P}}}\\!\\left(\\hat{\\mathbf{X}}(0)\\right)\n, we quantify how well the sample respects per-view rigidity by measuring the average rigidity residual between the original prediction and its rigid projection.\nFor view\ni\ni\n, let\nğ—\n^\ni\nâ€‹\n(\n0\n)\n=\n{\nğ±\n^\ni\n,\nj\n}\nj\n=\n1\nM\ni\n\\hat{\\mathbf{X}}_{i}(0)=\\{\\hat{\\mathbf{x}}_{i,j}\\}_{j=1}^{M_{i}}\nand\nğ—\n~\ni\nâ€‹\n(\n0\n)\n=\nÎ \nğ\ni\nâ€‹\n(\nğ—\n^\ni\nâ€‹\n(\n0\n)\n)\n=\n{\nğ±\n~\ni\n,\nj\n}\nj\n=\n1\nM\ni\n\\tilde{\\mathbf{X}}_{i}(0)=\\Pi_{{\\mathbf{P}}_{i}}\\!\\left(\\hat{\\mathbf{X}}_{i}(0)\\right)=\\{\\tilde{\\mathbf{x}}_{i,j}\\}_{j=1}^{M_{i}}\ndenote the rigidified points.\nWe define the rigidity residual as\nâ„°\nrigid\nâ€‹\n(\nğ—\n^\nâ€‹\n(\n0\n)\n)\n:=\n(\n1\nM\nâ€‹\nâˆ‘\ni\nâˆ‘\nj\n=\n1\nM\ni\nâ€–\nğ±\n^\ni\n,\nj\nâˆ’\nğ±\n~\ni\n,\nj\nâ€–\n2\n2\n)\n1\n2\n.\n\\mathcal{E}_{\\mathrm{rigid}}(\\hat{\\mathbf{X}}(0)):=\\left(\\frac{1}{M}\\sum_{i}\\sum_{j=1}^{M_{i}}\\bigl\\|\\hat{\\mathbf{x}}_{i,j}-\\tilde{\\mathbf{x}}_{i,j}\\bigr\\|_{2}^{2}\\right)^{\\!\\frac{1}{2}}.\n(8)\nAt inference, we draw\nS\nS\nsamples\n{\nğ—\n^\n(\ns\n)\nâ€‹\n(\n0\n)\n}\ns\n=\n1\nS\n\\{\\hat{\\mathbf{X}}^{(s)}(0)\\}_{s=1}^{S}\nfrom the flow, compute their corresponding\nrigidity errors\nâ„°\nrigid\n(\ns\n)\n:=\nâ„°\nrigid\nâ€‹\n(\nğ—\n^\n(\ns\n)\nâ€‹\n(\n0\n)\n)\n\\mathcal{E}^{(s)}_{\\mathrm{rigid}}:=\\mathcal{E}_{\\mathrm{rigid}}(\\hat{\\mathbf{X}}^{(s)}(0))\n, and select the generation with the lowest rigidity error.\nThis rigidity-based selection is purely geometric and adds negligible overhead to the sampling procedure as it reuses the Kabsch projections already computed during rigidity forcing.\n3.4\nCanonicalized Registration Pipeline\nOur method is designed to scale across scenes with diverse point densities, arbitrary coordinate frames, and a metric scale ranging from object-level scans to large outdoor environments. For this, we introduce a\ncanonicalized keypoint-based\nregistration pipeline for our flow model.\nThe pipeline has four steps:\n(i)\nsampling a compact keypoint representation with local descriptors,\n(ii)\ncanonicalizing all views into a shared similarity-invariant frame,\n(iii)\nconditioning a flow network on this representation to generate a canonical registered point cloud, and\n(iv)\nlifting the canonical prediction back to the original dense point clouds.\nKeypoint selection with local descriptors.\nDirectly generating millions of points is computationally inefficient and unstable. Instead, we construct a compact, geometry-aware representation by sampling keypoints in each view and attaching local descriptors.\nFor each input point cloud\nğ\ni\n\\mathbf{P}_{i}\n, we first perform voxel downsampling with voxel size\nv\nd\nv_{d}\n, obtaining a reduced cloud\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\n. We then apply the farthest point sampling (FPS) to\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nto select\nK\ni\nK_{i}\nkeypoints,\nforming\nğ\ni\nâˆˆ\nâ„\n3\nÃ—\nK\ni\n\\mathbf{Q}_{i}\\in\\mathbb{R}^{3\\times K_{i}}\n, with the total keypoint count\nK\n:=\nâˆ‘\ni\nK\ni\nK:=\\sum_{i}K_{i}\nacross all views. To obtain uniform coverage over the scene, we choose\nK\ni\nK_{i}\nin proportion to the metric scale of\nğ\ni\n\\mathbf{P}_{i}\n.\nTo encode local geometry, for each sampled point in\nğ\ni\n\\mathbf{Q}_{i}\nwe define a local patch by a ball query of radius\nr\ns\n=\n20\nâ€‹\nv\nd\nr_{s}=20v_{d}\nin the reduced point cloud\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nand extract a local descriptor from the normalized points within this patch. We use the lightweight, rotation-invariant MiniSpinNet\n[\nao2023cvpr-buffer\n,\nseo2025iccv\n]\npretrained on 3DMatch\n[\nzeng2017cvpr\n]\n, yielding\nğ…\ni\nâˆˆ\nâ„\n32\nÃ—\nK\ni\n\\mathbf{F}_{i}\\in\\mathbb{R}^{32\\times K_{i}}\n.\nConcatenating all views, we obtain sampled points\nğ’¬\n=\n{\nğ\ni\nâˆˆ\nâ„\n3\nÃ—\nK\ni\nâˆ£\ni\n=\n1\n,\nâ€¦\n,\nN\n}\n\\mathcal{Q}=\\{\\mathbf{Q}_{i}\\in\\mathbb{R}^{3\\times K_{i}}\\mid i=1,\\ldots,N\\}\nand their local features\nâ„±\n=\n{\nğ…\ni\nâˆˆ\nâ„\nD\nÃ—\nK\ni\nâˆ£\ni\n=\n1\n,\nâ€¦\n,\nN\n}\n\\mathcal{F}=\\{\\mathbf{F}_{i}\\in\\mathbb{R}^{D\\times K_{i}}\\mid i=1,\\ldots,N\\}\n, which serve as a compressed yet informative representation of the dense input clouds\nğ’«\n\\mathcal{P}\n.\nCanonicalization of inputs and targets.\nTo make training invariant to global pose and metric scale, we canonicalize both the conditioning representation and the flow target in a shared frame.\nFor each view\ni\ni\n, we first translate the sampled points\nğ\ni\n\\mathbf{Q}_{i}\nso that its center of mass is at the origin. We then compute a global scale factor\ns\ns\nas the longest edge length of the bounding box of the view with the most points, and scale all centered point sets by\n1\n/\ns\n1/s\n, so that the entire scene fits in a unit cube. Finally, we apply a random 3D rotation to each centered, scaled cloud. This yields normalized unposed keypoints\nğ’¬\nÂ¯\n=\n{\nğ\nÂ¯\ni\nâˆˆ\nâ„\n3\nÃ—\nK\ni\nâˆ£\ni\n=\n1\n,\nâ€¦\n,\nN\n}\n\\bar{\\mathcal{Q}}=\\{\\bar{\\mathbf{Q}}_{i}\\in\\mathbb{R}^{3\\times K_{i}}\\mid i=1,\\dots,N\\}\nand corresponding similarity transforms\nğ“\nÂ¯\ni\nâˆˆ\nSIM\nâ€‹\n(\n3\n)\n\\bar{\\mathbf{T}}_{i}\\in\\mathrm{SIM}(3)\nthat map the original\nğ\ni\n\\mathbf{Q}_{i}\nto\nğ\nÂ¯\ni\n\\bar{\\mathbf{Q}}_{i}\n.\nThe training target is defined in the same canonical frame. We first transform the keypoints by the ground-truth poses\nğ’¯\n\\mathcal{T}\nto obtain registered keypoints\nğ’¬\nr\n=\n{\nğ\ni\nr\n}\ni\n=\n1\nN\n\\mathcal{Q}^{r}=\\{\\mathbf{Q}_{i}^{r}\\}_{i=1}^{N}\nand merge them into\nğ\nr\n=\nâ‹ƒ\ni\n=\n1\nN\nğ\ni\nr\n\\mathbf{Q}^{r}=\\bigcup_{i=1}^{N}\\mathbf{Q}_{i}^{r}\n.\nWe then (i) recenter\nğ\nr\n\\mathbf{Q}^{r}\nat the origin, (ii) apply the same random rotation used for the view with the most points to fix a reference orientation, and (iii) scale by the global factor\ns\ns\n, resulting in the normalized registered point cloud\nğ\nÂ¯\nr\n\\bar{\\mathbf{Q}}^{r}\n, which is set as the target\nğ—\nâ€‹\n(\n0\n)\n\\mathbf{X}(0)\n.\nConditional flow model.\nWe adopt the Diffusion Transformer\n[\npeebles2023iccv\n]\nfor\nğ•\nÎ¸\n\\mathbf{V}_{\\theta}\n, and, following VGGT\n[\nwang2025cvpr-vggt\n]\n, employ a transformer with alternating-attention blocks (Fig.\n2\n). Specifically, we alternate per-view self-attention within each point cloud, which consolidates view-specific structure, with global attention over all point tokens to fuse information across views. Our model comprises\nL\n=\n10\nL=10\nalternating-attention blocks with hidden dimension\nd\n=\n512\nd=512\nand\nh\n=\n8\nh=8\nattention heads, totaling 73 million parameters.\nThe modelâ€™s condition\nğ‚\n=\nf\nemb\nâ€‹\n(\nğ’¬\nÂ¯\n,\nâ„±\n,\ns\n)\n\\mathbf{C}=f_{\\text{emb}}(\\bar{\\mathcal{Q}},\\mathcal{F},s)\nis obtained via a linear feature embedder\nf\nemb\nf_{\\text{emb}}\nwith the concatenated local descriptors\nâ„±\n\\mathcal{F}\n, positional encodings of the normalized coordinates\nğ’¬\nÂ¯\n\\bar{\\mathcal{Q}}\n, and a global embedding of the scene scale\ns\ns\n.\nThe flow network\nğ•\nÎ¸\n\\mathbf{V}_{\\theta}\ntakes\nğ—\nâ€‹\n(\nt\n)\n\\mathbf{X}(t)\nand\nğ‚\n\\mathbf{C}\nas input and is trained with the conditional flow matching loss\n[\nlipman2023iclr\n]\n.\nLifting to dense registered point clouds.\nAt inference, the model generates a canonical registered keypoint cloud\nğ—\n^\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}(0)\n.\nUsing the rigidity-forcing procedure in Sec.\n3.3\n, we recover per-view rigid transformations\nğ“\n^\ni\n\\hat{\\mathbf{T}}_{i}\nthat align\nğ\nÂ¯\ni\n\\bar{\\mathbf{Q}}_{i}\nto the corresponding subsets of\nğ—\n^\nâ€‹\n(\n0\n)\n\\hat{\\mathbf{X}}(0)\n.\nThe overall transformation from the original input frame of view\ni\ni\nto the final registered frame is\nğ“\ni\n=\nğ“\n^\ni\nâ€‹\nğ“\nÂ¯\ni\n\\mathbf{T}_{i}=\\hat{\\mathbf{T}}_{i}\\bar{\\mathbf{T}}_{i}\n,\nwhich we apply to all points in the dense cloud\nğ\ni\n\\mathbf{P}_{i}\n.\nFinally, we undo the global scaling by\ns\ns\nto obtain the registered point clouds at the metric scale.\n4\nExperimental Evaluation\nThe main focus of this work is a scalable generative model that aligns multiple point clouds using flow matching while enforcing rigidity constraints.\nWe present our experiments on pairwise and multi-view point cloud registration to show the capabilities of our method.\n4.1\nExperimental Setup\nImplementation details.\nWe train our model in two phases. In phase one, we train for four days using training samples with point cloud view count\nN\nN\nfrom two to six. In phase two, we do not limit the maximum view count and keep training for three more days. We use 8 NVIDIA A100 GPUs with 80GB VRAM each.\nWe use Muon\n[\nliu2025arxiv-muon\n]\nas the optimizer with an initial learning rate of\n2\nâ‹…\n10\nâˆ’\n3\n2\\cdot 10^{-3}\nfor matrix-like parameters and\n2\nâ‹…\n10\nâˆ’\n4\n2\\cdot 10^{-4}\nfor vector-like parameters.\nUnlike VGGT\n[\nwang2025cvpr-vggt\n]\nor RPF\n[\nsun2025neurips\n]\n, our training samples have varying numbers of views\nN\nN\nand point-token counts\nK\nK\n. To efficiently train under this irregular data setup, we devise a dynamic batching strategy that allocates samples to each GPU with a suitable batch size. We set the maximum token count per batch on one GPU to 100,000 in phase one and 80,000 in phase two to fill the GPU memory.\nTable 1\n:\nDatasets used for training our model.\nDataset\n#Scenes\n#Scans\nReal/Synth.\n#Train\nKITTI\n[\ngeiger2012cvpr\n]\n22\n16,453\nReal\n19\nKITTI360\n[\nliao2022pami\n]\n9\n21,255\nReal\n7\nApollo\n[\nhuang2018cvprws\n]\n11\n25,036\nReal\n9\nnuScenes\n[\ncaesar2020cvpr\n]\n642\n24,320\nReal\n642\nMulRAN\n[\nkim2020icra\n]\n4\n4,477\nReal\n3\nBoreas\n[\nburnett2023ijrr-boreas\n]\n2\n3,613\nReal\n2\nOxford Spires\n[\ntao2025ijrr-oxfordspires\n]\n6\n5,781\nReal\n5\nVBR\n[\nbrizi2024icra-vbr\n]\n5\n8,647\nReal\n4\nUrbanNav\n[\nhsu2023navi-urbannav\n]\n4\n4,228\nReal\n3\nWildPlace\n[\nknights2023icra-wildplaces\n]\n2\n2,613\nReal\n2\nHeLiPR\n[\njung2024ijrr\n]\n3\n10,882\nReal\n3\nKITTI-Carla\n[\ndeschaud2021arxiv-kitticarla\n]\n7\n7,729\nSynth.\n5\n3DMatch\n[\nzeng2017cvpr\n]\n82\n29,314\nReal\n48\nNSS\n[\nsun2025jprs\n]\n6\n54,396\nReal\n3\nScanNet\n[\ndai2017cvpr\n]\n661\n75,299\nReal\n511\nScanNet++\n[\nyeshwanth2023iccv-scannetpp\n]\n220\n88,783\nReal\n194\nModelNet\n[\nwu2015cvpr\n]\n12,308\n24,616\nSynth.\n9,800\nTraining data.\nOur training requires only a set of point clouds under the same reference frame without any annotations for keypoints or correspondences. This makes it straightforward to scale up training as any dataset providing point clouds and accurate sensor poses can be used.\nFor our model, we curate over 100k samples from 17 diverse datasets spanning outdoor, indoor, and object-centric settings as summarized in\nTab.\n1\n. Each sample consists of\nN\nN\npoint clouds, where\n2\nâ‰¤\nN\nâ‰¤\n24\n2\\leq N\\leq 24\n. We split the samples into training and validation sets with an approximate ratio of 9:1 while we also exclude sequences used for testing in common registration benchmarks from the training set.\nThe datasets span diverse scenes across multiple continents, captured by LiDAR and depth cameras with varying resolutions and fields of view.\nWe curate both single-frame and sequence-accumulated submap samples, ensuring sufficient overlap between point clouds in each sample.\nTesting data.\nFor pairwise registration evaluation, we build our benchmark based on the established generalizability benchmark\n[\nseo2025iccv\n]\n. The benchmark includes 10 datasets: 3DMatch\n[\nzeng2017cvpr\n]\n, 3DLoMatch\n[\nhuang2021cvpr-predator\n]\n, NSS\n[\nsun2025jprs\n]\n, TIERS\n[\nsier2023rs-tiers\n]\n, KITTI\n[\ngeiger2012cvpr\n]\n, Waymo\n[\nsun2020cvpr\n]\n, KAIST\n[\njung2024ijrr\n]\n, MIT\n[\ntian2023iros-kimeramultidata\n]\n, ETH\n[\npomerleau2012ijrr\n]\n, NCD\n[\nramezani2020iros\n]\nwith different scales and sensor modalities, where TIERS, Waymo, KAIST, MIT, and NCD are zero-shot testing datasets for our model.\nFor multi-view registration evaluation, we follow prior work\n[\ngojcic2020cvpr\n,\nwang2023cvpr-sghr\n]\nand use the common multi-view registration benchmarks on 3DMatch\n[\nzeng2017cvpr\n]\nand ScanNet\n[\ndai2017cvpr\n]\n. As these datasets are both indoor depth camera datasets, we also add two zero-shot outdoor LiDAR datasets: Waymo\n[\nsun2020cvpr\n]\nfor scan-level registration and KAIST\n[\njung2024ijrr\n]\nfor map-level multi-view registration evaluation.\nTable 2\n:\nQuantitative comparison of the pairwise point cloud registration performance in terms of success rate (%) on 10 diverse datasets. Best results are shown in\nbold\n, second best are\nunderscored\n.\nEnvironment\nIndoor\nOutdoor\nDataset\n3DMatch\n3DLoMatch\nNSS\nTIERS\nKITTI\nWaymo\nKAIST\nMIT\nETH\nNCD\nConven-\ntional\nFPFH\n[\nrusu2009icra\n]\n+ FGR\n[\nzhou2016eccv\n]\n62.53\n15.42\n30.82\n80.60\n98.74\n100.00\n89.80\n74.78\n91.87\n99.00\nFPFH\n[\nrusu2009icra\n]\n+ Quatro\n[\nlim2022icra\n]\n8.22\n1.74\n-\n86.57\n99.10\n100.00\n91.46\n79.57\n51.05\n91.03\nFPFH\n[\nrusu2009icra\n]\n+ TEASER\n[\nyang2020tro\n]\n52.00\n13.25\n-\n73.13\n98.92\n100.00\n89.20\n71.30\n93.69\n99.34\nDeep learning-based\nFCGF\n[\nchoy2019iccv\n]\n88.18\n40.09\n42.86\n80.11\n98.92\n97.69\n93.55\n93.04\n55.53\n95.68\nPredator\n[\nhuang2021cvpr-predator\n]\n90.60\n62.40\n92.99\n75.74\n99.82\n86.92\n87.09\n79.56\n54.42\n93.68\nGeoTransformer\n[\nqin2022cvpr-geotransformer\n]\n92.00\n75.00\n55.59\n92.99\n99.82\n89.23\n91.86\n95.65\n71.53\n97.01\nBUFFER\n[\nao2023cvpr-buffer\n]\n92.90\n71.80\n-\n88.96\n99.64\n100.00\n97.24\n95.65\n99.30\n99.00\nPARENet\n[\nyao2024iccv-parenet\n]\n95.00\n80.50\n-\n75.06\n99.82\n92.31\n86.44\n84.78\n69.42\n93.36\nBUFFER-X\n[\nseo2025iccv\n]\n95.58\n74.18\n-\n93.45\n99.82\n100.00\n99.15\n97.39\n99.72\n99.67\nOurs (\nS\n=\n5\nS=5\nselected)\n95.64\n78.22\n96.18\n95.09\n100.00\n99.47\n99.15\n99.13\n98.04\n99.34\nOurs (\nS\n=\n1\nS=1\naverage)\n95.27\n77.43\n96.39\n94.75\n100.00\n98.51\n99.05\n98.96\n98.30\n99.34\nFigure 3\n:\nComparison of the pairwise point cloud registration:\nRegistration success rate with increasing spatial distance between the two point clouds (thus decreasing overlap ratio) on KITTI and Waymo datasets.\nEvaluation metrics.\nWe follow previous works on point cloud registration to evaluate the performance of our model using the registration success rateÂ (%) calculated with thresholds on correspondence RMSE for 3DMatch and on translation and rotation error for the other datasets. We use threshold settings of previous works\n[\nzeng2017cvpr\n,\nsun2025jprs\n,\nseo2025iccv\n]\n, detailed in the supplementary material.\nFor multi-view registration, we follow prior work\n[\ngojcic2020cvpr\n,\nwang2023cvpr-sghr\n]\nreporting the mean rotation errorÂ (RE) and translation errorÂ (TE). We additionally use the Chamfer distanceÂ (CD) to evaluate the quality of the registered point cloud by comparing it with the ground truth aggregated point cloud.\n4.2\nResults\nPairwise registration evaluation.\nTab.\n2\nshows that our model (with and without rigidity-based generation selection) achieves on par or better performance compared to state-of-the-art methods\n[\nseo2025iccv\n,\nqin2022cvpr-geotransformer\n,\nyao2024iccv-parenet\n]\nfor pairwise registration on standard benchmarks.\nTo further demonstrate our modelâ€™s robustness to low overlap between point clouds, we follow EYOC\n[\nliu2024cvpr-eyoc\n]\nto curate testing data on KITTI and Waymo with increasing spatial distance between point clouds (thus decreasing overlap). As shown in\nFig.\n3\n, our model shows superior performance over state-of-the-art methods with increasing scan distance.\nMulti-view registration evaluation.\nWe evaluate on sparse-view subsets (with\n3\nâ‰¤\nN\nâ‰¤\n12\n3\\leq N\\leq 12\n) of 3DMatch and ScanNet, with a minimum overlap ratio of\n0.2\n0.2\n. We evaluate the generalization ability of our model in outdoor scenarios with Waymo for scan-level registration and KAIST for map-level multi-view registration, also with the number of input views set to\n3\nâ‰¤\nN\nâ‰¤\n12\n3\\leq N\\leq 12\n. Comparisons with the baseline methods are shown in\nTab.\n3\n, demonstrating superior performance in these diverse scenarios and strong generalization capabilities of our proposed method. The sparse-view setting makes the pairwise registration more challenging, thus most of the two-stage multi-view registration baselines fail to achieve good performance. Note that the two learnable multi-view registration baselines\n[\ngojcic2020cvpr\n,\nwang2023cvpr-sghr\n]\ndo not work properly with the outdoor LiDAR datasets.\nTable 3\n:\nQuantitative comparison of multi-view point cloud registration. For each dataset, we report the mean rotation error (RE) in degrees, mean translation error (TE) in meters, and Chamfer distance (CD) in meters. Best results are shown in\nbold\n.\n3DMatch\nScanNet\nWaymo Scan\nKAIST Map\nMethod\nRE\nâ†“\n\\downarrow\nTE\nâ†“\n\\downarrow\nCD\nâ†“\n\\downarrow\nRE\nâ†“\n\\downarrow\nTE\nâ†“\n\\downarrow\nCD\nâ†“\n\\downarrow\nRE\nâ†“\n\\downarrow\nTE\nâ†“\n\\downarrow\nCD\nâ†“\n\\downarrow\nRE\nâ†“\n\\downarrow\nTE\nâ†“\n\\downarrow\nCD\nâ†“\n\\downarrow\nLMVR\n[\ngojcic2020cvpr\n]\n15.46\n0.44\n0.16\n46.10\n0.87\n0.50\n-\n-\n-\n-\n-\n-\nSGHR\n[\nwang2023cvpr-sghr\n]\n50.28\n0.78\n0.53\n23.59\n0.64\n0.34\n-\n-\n-\n-\n-\n-\nFGR\n[\nzhou2016eccv\n]\n+ PGO\n52.81\n0.71\n0.49\n68.80\n1.43\n0.76\n54.60\n25.81\n3.55\n90.07\n105.94\n41.07\nBUFFER-X\n[\nseo2025iccv\n]\n+ PGO\n48.16\n0.74\n0.48\n47.63\n1.31\n0.52\n38.37\n24.32\n2.90\n99.47\n148.88\n39.17\nOurs\n7.30\n0.22\n0.10\n13.66\n0.35\n0.12\n1.10\n1.56\n0.23\n5.52\n9.36\n2.97\nFurthermore, we showcase the zero-shot generalization ability of our model on the WHU-TLS dataset\n[\ndong2020jprs-whutls\n]\nin\nFig.\n4\nand on the ETH-TLS dataset\n[\ntheiler2015jprs\n]\nin\nFig.\n1\n. Note that our model was never trained on any terrestrial laser scannerÂ (TLS) point clouds, but learned strong geometric priors from the rich outdoor LiDAR data.\nFigure 4\n:\nMulti-view registration results of our model on WHU-TLS dataset\n[\ndong2020jprs-whutls\n]\n. Note that our model is never trained on a terrestrial laser scanner point cloud dataset and generalizes zero-shot to this dataset. Left: heritage building scene; Right: park scene. Different colors represent different point clouds. CD refers to Chamfer distance between the registered point cloud and the ground truth point cloud.\nDownstream robotics applications.\nWe also showcase in\nFig.\n5\n, the potential of our model for downstream robotics applications, including relocalization, multi-robot collaborative mapping, and multi-session map merging.\nThe mix of training data for scan-to-scan, scan-to-map and map-to-map registration allows our model to handle various types of point cloud data collected by robotic platforms.\nFigure 5\n:\nOur model has the potential to support multiple robotics downstream tasks, including (a) Relocalization: we show the pairwise registration result of two submap point clouds with low overlap in a challenging forest scene from WildPlace\n[\nknights2023icra-wildplaces\n]\ndataset; (b) Multi-robot collaborative mapping: we show the multi-view registration result of six LiDAR scans in a campus scene from Oxford Spires\n[\ntao2025ijrr-oxfordspires\n]\ndataset; (c) Multi-session map merging: we show the multi-view registration result of three map-level point clouds of the same place collected from different sessions from KAIST\n[\njung2024ijrr\n]\ndataset. Different colors represent different point clouds.\n4.3\nAblation Studies\nWe evaluate our model under different inference and architectural configurations on four testing datasets: 3DMatch and 3DLoMatch for pairwise registration, and ScanNet and map-level KAIST for multi-view registration. The results are summarized in Table\n4\n.\nAt inference time, we draw\nS\n=\n5\nS=5\nrandom generations and select the one with the smallest rigidity error. Under this default setting [A], our model achieves the highest success rates in three of the four datasets. Compared with [A], the setting without selection [C] generally performs worse, demonstrating the effectiveness of our rigidity-based selection strategy. By comparing [B] and [C], we further observe that enforcing rigidity during sampling consistently improves performance over using the plain Euler integrator. Setting [D] shows that reducing the number of flow integration steps\nÎº\n\\kappa\nfrom 10 to 1 slightly degrades performance but significantly cuts the inference time, as shown in Fig.\n6\n.\nThe settings [E]-[G] study the impact of model design. Setting [E] uses only the sampled keypoint coordinates as conditions without local descriptors. Its performance drops sharply compared to [A], highlighting the importance of local geometric features. The settings [F] and [G] evaluate the influence of smaller transformer models with blocks\nL\nL\ndecreasing from 10 to 8 and 6, respectively. As expected, reducing model capacity induces a clear performance trade-off.\nFor additional ablations and results on more datasets, we refer the reader to the supplementary material.\nTable 4\n:\nAblation studies. We report the registration success rate (%) on datasets for both pairwise and multi-view registration tasks. Best results are shown in\nbold\n, second best are\nunderscored\n.\nSetting\nPairwise\nMulti-view\n3DMatch\n3DLoMatch\nScanNet\nKAIST\n[A] Ours (S = 5)\n95.64\n78.22\n64.04\n87.40\nInference\n[B] w/o rigidity forcing\n94.65\n74.63\n59.76\n83.69\n[C] w/o rigidity-based selection\n95.27\n77.43\n63.83\n88.12\n[D] w/ 1-step generation\n90.35\n61.05\n48.33\n52.95\nModel\n[E] w/o local feature extraction\n83.28\n50.05\n21.82\n47.20\n[F] w/\nL\n=\n6\nL=6\ntransformer blocks\n90.90\n57.59\n45.71\n73.77\n[G] w/\nL\n=\n8\nL=8\ntransformer blocks\n93.24\n71.63\n56.24\n85.14\n4.4\nRuntime\nWe further evaluate the inference time of our model on a local workstation with a single NVIDIA A5000 GPU of 24GB VRAM. As shown in Figure\n6\n, the computation time of our model is mainly twofold: the preprocessing time for selecting keypoints and extracting local descriptors and the sampling time of the flow matching. The preprocessing time is shown to increase linearly with the number of input point clouds. In addition, we can sacrifice the accuracy of the registration to reduce the computation time by reducing the number of generation steps\nÎº\n\\kappa\n, as expected.\nFigure 6\n:\nInference time of our model with increasing number of input point clouds\nN\nN\non a single A5000 GPU. We show the preprocessing time for keypoint selection and local descriptors as well as the sampling time of our flow model on the multi-view 3DMatch dataset.\n5\nConclusion\nWe presented a generative approach to multi-view point cloud registration that directly generates registered point clouds in a forward pass, bypassing traditional two-stage pipelines. Our method casts registration as conditional generation using rectified point flow matching, where a transformer-based model predicts a continuous velocity field transporting points to registered positions. Holistic reasoning through alternating-attention transformers enables robust registration under low-overlap conditions and effective generalization across diverse scenarios, scales, and sensor modalities.\nTrained on over 100k samples from 17 diverse datasets, our model achieves state-of-the-art performance on pairwise and multi-view registration benchmarks, with particularly strong results at low overlap ratios. A rigidity-enforcing sampler combined with generation selection further improves test-time performance. Our work advances toward a foundation model for 3D point cloud registration, applicable to SLAM, 3D reconstruction, and robotic manipulation.\nLimitations and future work\nOur approach assumes scans are recorded in the same scene, like other feed-forward reconstruction methods. Besides, current generation times preclude real-time operation at automotive LiDAR frame rates. By modeling flow in Euclidean space rather than the transformation group, our method could potentially handle non-rigid transformations, though this remains unexplored. Future work may extend to scene flow estimation and merging point maps from photogrammetry and feed-forward 3D reconstruction.\nAppendix\nIn the appendix, we provide the following:\nâ€¢\nImplementation details including point cloud sampling and feature extraction, flow model training, training data curation, testing data details, and evaluation metrics (sections\nA\n)\nâ€¢\nDetails on the selected baseline methods used in our experiments (section\nB\n)\nâ€¢\nAdditional experimental results including comprehensive results on all validation and testing datasets as well as extended ablation studies (sections\nC\n)\nâ€¢\nAdditional qualitative results and failure cases (section\nD\n)\nAppendix A\nImplementation Details\nPoint cloud sampling and feature extraction\nFor each point cloud\nğ\ni\n\\mathbf{P}_{i}\n, we first apply voxel downsampling with voxel size\nv\nd\nv_{d}\nto obtain\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\n. We then apply statistical outlier removal to\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nto remove outliers. To ensure uniform sampling density across input point clouds, we determine the sample count\nK\ni\nK_{i}\nproportionally to the spatial voxel coverage: we voxelize\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nwith voxel size\nv\nc\nv_{c}\nand let\nV\ni\nV_{i}\nbe the number of remaining points, then set\nK\ni\n=\nâŒŠ\nÎ±\ns\nâ€‹\nV\ni\nâŒ‹\nK_{i}=\\lfloor\\alpha_{s}V_{i}\\rfloor\n, where\nÎ±\ns\n\\alpha_{s}\nis a hyperparameter controlling the FPS sampling density. We apply farthest point sampling (FPS) on\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nto sample\nK\ni\nK_{i}\npoints as the feature points\nğ\ni\n=\n{\nğª\ni\n,\nk\n}\nk\n=\n1\nK\ni\nâˆˆ\nâ„\n3\nÃ—\nK\ni\n\\mathbf{Q}_{i}=\\{\\mathbf{q}_{i,k}\\}_{k=1}^{K_{i}}\\in\\mathbb{R}^{3\\times K_{i}}\n. Typically, we got\nK\ni\nâˆˆ\n[\n200\n,\n2000\n]\nK_{i}\\in[200,2000]\nfor our training data.\nFor each feature point\nğª\ni\n,\nk\nâˆˆ\nğ\ni\n\\mathbf{q}_{i,k}\\in\\mathbf{Q}_{i}\n, we extract a local patch by a ball query of radius\nr\ns\n=\n20\nâ€‹\nv\nd\nr_{s}=20\\,v_{d}\nin\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nand normalize the patch points. We then use the lightweight MiniSpinNet\n[\nao2023cvpr-buffer\n,\nseo2025iccv\n]\npretrained on 3DMatch\n[\nzeng2017cvpr\n]\nas our feature extractor\nâ„±\n\\mathcal{F}\nto compute a descriptor for each patch with maximum\n512\n512\npoints and stack them into local features\nğ…\ni\nâˆˆ\nâ„\nD\nÃ—\nK\ni\n\\mathbf{F}_{i}\\in\\mathbb{R}^{D\\times K_{i}}\n, where\nD\n=\n32\nD=32\n.\nThe pseudocode for this step is shown in\nAlgorithm\n1\n.\nInput:\nSet of input point clouds\n{\nğ\ni\n}\n\\{\\mathbf{P}_{i}\\}\n;\nvoxel sizes\nv\nd\nv_{d}\n(downsampling) and\nv\nc\nv_{c}\n(coverage);\nFPS sampling ratio\nÎ±\ns\n\\alpha_{s}\n; patch radius\nr\ns\nr_{s}\n;\nminiSpinNet feature extractor\nâ„±\n\\mathcal{F}\n.\nOutput:\nSampled points\n{\nğ\ni\n}\n\\{\\mathbf{Q}_{i}\\}\nand features\n{\nğ…\ni\n}\n\\{\\mathbf{F}_{i}\\}\n.\n1\n2\nforeach\ninput point cloud\nğ\ni\n\\mathbf{P}_{i}\ndo\n3\nVoxel-downsample\nğ\ni\n\\mathbf{P}_{i}\nwith voxel size\nv\nd\nv_{d}\nto obtain\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\n;\n4\nApply statistical outlier removal to\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nto remove outliers;\n5\nVoxelize\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nwith voxel size\nv\nc\nv_{c}\nand let\nV\ni\nV_{i}\nbe the number of remaining points;\n6\nSet\nK\ni\nâ†\nâŒŠ\nÎ±\ns\nâ€‹\nV\ni\nâŒ‹\nK_{i}\\leftarrow\\lfloor\\alpha_{s}V_{i}\\rfloor\n;\n7\nApply FPS on\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nto sample\nK\ni\nK_{i}\npoints\nas feature points\nğ\ni\n=\n{\nğª\ni\n,\nk\n}\nk\n=\n1\nK\ni\n\\mathbf{Q}_{i}=\\{\\mathbf{q}_{i,k}\\}_{k=1}^{K_{i}}\n;\n8\n9\nFor each\nğª\ni\n,\nk\nâˆˆ\nğ\ni\n\\mathbf{q}_{i,k}\\in\\mathbf{Q}_{i}\n, extract a local patch by\na ball query of radius\nr\ns\nr_{s}\nin\nğ\ni\nv\n\\mathbf{P}_{i}^{v}\nand normalize the\npatch points;\n10\nUse\nâ„±\n\\mathcal{F}\n(miniSpinNet) to compute a descriptor\nfor each patch and stack them into\nğ…\ni\nâˆˆ\nâ„\nD\nÃ—\nK\ni\n\\mathbf{F}_{i}\\in\\mathbb{R}^{D\\times K_{i}}\n;\n11\n12\n13\nreturn\n{\nğ\ni\n}\n\\{\\mathbf{Q}_{i}\\}\nand\n{\nğ…\ni\n}\n\\{\\mathbf{F}_{i}\\}\n;\nAlgorithmÂ 1\nPoint cloud sampling and miniSpinNet feature extraction\nTo improve efficiency and reduce memory usage, we adopt the lightweight patch-wise network MiniSpinNet from BUFFER\n[\nao2023cvpr-buffer\n]\nas our local feature descriptor.\nMiniSpinNet is a compact SpinNet-style\n[\nao2021cvpr-spinnet\n]\nnetwork that encodes each input patch into a 32-dimensional feature descriptor by decreasing the voxelization hyperparameters and simplifying the 3D cylindrical convolution (3DCC) layers, making it nearly nine times faster than the vanilla SpinNet.\nCurrently we follow BUFFER-X\n[\nseo2025iccv\n]\nand use MiniSpinNet as our feature extractor. However, we argue that we can also use other feature extractors to encode local geometry, such as FCGF\n[\nchoy2019iccv\n]\n, YOHO\n[\nwang2022mm-yoho\n]\n, and Sonata\n[\nwu2025cvpr-sonata\n]\n.\nFlow matching model architecture\nWe adopt the same flow matching model architecture as in RPF\n[\nsun2025neurips\n]\nand use a diffusion transformer\n[\npeebles2023iccv\n]\nwith alternating-attention blocks\n[\nwang2025cvpr-vggt\n]\nfor conditional flow matching.\nOur transformer architecture comprises\nL\n=\n10\nL=10\nalternating-attention blocks with hidden dimension\nd\n=\n512\nd=512\nand\nh\n=\n8\nh=8\nattention heads, totaling 73 million parameters. The alternating-attention mechanism alternates between two types of attention layers: (i) per-view self-attention that operates within each point cloud to consolidate view-specific structure, and (ii) global attention over all point tokens across all views to fuse information and enable cross-view reasoning. This design allows the model to simultaneously capture local geometric structure within each view and global relationships across multiple views.\nThe modelâ€™s input consists of the noisy point cloud\nğ—\nâ€‹\n(\nt\n)\n\\mathbf{X}(t)\nat time step\nt\nt\nand a conditioning signal\nğ‚\n=\nf\nemb\nâ€‹\n(\nğ’¬\nÂ¯\n,\nâ„±\n,\ns\n)\n\\mathbf{C}=f_{\\text{emb}}(\\bar{\\mathcal{Q}},\\mathcal{F},s)\nobtained via a linear feature embedder\nf\nemb\nf_{\\text{emb}}\n. The conditioning\nğ‚\n\\mathbf{C}\nconcatenates three components: (i) local geometric descriptors\nâ„±\n\\mathcal{F}\nextracted by MiniSpinNet for each sampled keypoint, (ii) positional encodings of the normalized point coordinates\nğ’¬\nÂ¯\n\\bar{\\mathcal{Q}}\nusing a multi-frequency Fourier feature mapping\n[\nmildenhall2020eccv\n]\n, and (iii) a global embedding of the scene scale\ns\ns\nto maintain scale awareness, also lifted by the same Fourier feature mapping. The flow matching network\nğ•\nÎ¸\n\\mathbf{V}_{\\theta}\ntakes\nğ—\nâ€‹\n(\nt\n)\n\\mathbf{X}(t)\nand\nğ‚\n\\mathbf{C}\nas input and predicts the velocity field\nâˆ‡\nt\nğ—\nâ€‹\n(\nt\n)\n\\nabla_{t}\\mathbf{X}(t)\nthat transports the noisy points toward the target registered configuration.\nUnlike RPF\n[\nsun2025neurips\n]\n, we do not take the point-wise normals and the scalar view index as additional conditioning signals. We also do not rely on a PTv3-based encoder\n[\nwu2024cvpr-ptv3\n]\npretrained for the overlapping prediction task. These design choices make our model simpler and can be applied to various training datasets.\nFlow matching model training\nAs mentioned in the main paper, our flow matching model\nğ•\nÎ¸\n\\mathbf{V}_{\\theta}\nis trained by minimizing the following conditional flow matching loss\n[\nlipman2023iclr\n]\n, which minimizes the difference between the predicted velocity field and the true velocity field:\nâ„’\nFM\n=\nğ”¼\nt\n,\nğ—\n[\nâˆ¥\nğ•\nÎ¸\n(\nt\n,\nğ—\n(\nt\n)\nâˆ£\nğ‚\n)\nâˆ’\nâˆ‡\nt\nğ—\n(\nt\n)\nâˆ¥\n2\n]\n.\n\\mathcal{L}_{\\text{FM}}=\\mathbb{E}_{t,\\mathbf{X}}\\left[\\left\\|\\mathbf{V}_{\\theta}(t,\\mathbf{X}(t)\\mid\\mathbf{C})-\\nabla_{t}\\mathbf{X}(t)\\right\\|^{2}\\right].\n(9)\nDuring the flow matching model training, we sample the time steps from a U-shaped distribution\n[\nlee2024neurips\n]\n.\nWe train our model using the Muon Optimizer\n[\nliu2025arxiv-muon\n]\n, with an initial learning rate of\n2\nÃ—\n10\nâˆ’\n3\n2\\times 10^{-3}\nfor matrix-like parameters and\n2\nÃ—\n10\nâˆ’\n4\n2\\times 10^{-4}\nfor vector-like parameters. From our experiments we find that using Muon instead of AdamW\n[\nkingma2015iclr\n]\ncan achieve faster convergence and better performance. For the learning rate schedule, we halve the learning rate after 400, 550, 700, 850, and 1000 epochs.\nWe train the model with 8 NVIDIA A100 GPUs with 80GB VRAM each. We train the model in two stages. In the first stage, we train for four days (about 900 epochs) using training samples with point cloud view count\nN\nâˆˆ\n[\n2\n,\n6\n]\nN\\in[2,6]\n. In the second stage, we train with more views (\nN\nâˆˆ\n[\n2\n,\n24\n]\nN\\in[2,24]\n) and keep training for three more days (till about 1500 epochs).\nTraining data curation\nWe curate both the scan-level and submap-level training samples using the same script with different settings.\nFor each dataset, given the per-frame poses, we first select keyframes based on temporal and spatial thresholds, which removes redundant frames when the sensor is stationary or moving slowly.\nFor datasets lacking accurate and globally consistent reference poses, we use a state-of-the-art SLAM system\n[\npan2024tro\n]\nto estimate the poses for data curation.\nFor most LiDAR-based datasets (e.g. KITTI is already deskewed), we additionally apply deskewing (motion undistortion) to the keyframe point clouds when point-wise timestamps are available. For each sequence with\nM\nM\nkeyframes, we aim to generate\nN\ntarget\n=\nÎ²\nâ€‹\nM\nN_{\\text{target}}=\\beta M\ntraining samples, where\nÎ²\n\\beta\ncontrols the number of samples per keyframe. For every sample, we randomly select\nN\nâˆˆ\n[\nN\nmin\n,\nN\nmax\n]\nN\\in[N_{\\text{min}},N_{\\text{max}}]\npoint clouds. Each point cloud is constructed by accumulating points from\nF\nâˆˆ\n[\nF\nmin\n,\nF\nmax\n]\nF\\in[F_{\\text{min}},F_{\\text{max}}]\nconsecutive keyframes, and the resulting point clouds do not share frames. We then transform the point clouds to the world frame using the corresponding keyframe poses. For each sample, we allow at most\nT\nmax\nT_{\\max}\nattempts to find a valid configuration. A sample is considered valid only if (i) all point clouds are spatially close to each other, i.e. the pairwise distances between their centers are below a threshold\nd\nmax\nd_{\\text{max}}\n, and (ii) the point clouds are not isolated from each other, i.e. they form a connected graph under a minimum overlap-ratio threshold\nÏµ\noverlap\n\\epsilon_{\\text{overlap}}\n.\nThe overlap ratio between two point clouds is computed as the ratio of the number of occupied voxels in their intersection to the number of occupied voxels in their union, evaluated on a voxel grid with an adaptively set voxel size\nv\noverlap\nv_{\\text{overlap}}\n. We set a very small overlapping ratio threshold\nÏµ\noverlap\n\\epsilon_{\\text{overlap}}\n(0.5%-2%) to add some hard samples that allow the model to learn to register low-overlapped point clouds.\nWhenever we find a valid set of point clouds in an attempt, we save it as a training sample. We set\nN\nmin\n=\n2\nN_{\\text{min}}=2\nand\nF\nmin\n=\n1\nF_{\\text{min}}=1\nfor all datasets. For scan-based samples, we set\nF\nmax\n=\n1\nF_{\\text{max}}=1\n, and for submap-based samples, we use\nF\nmax\n>\n1\nF_{\\text{max}}>1\n.\nThe pseudocode for generating the training samples is shown in\nAlgorithm\n2\n.\nInput:\nPer-frame poses\n{\nğ“\ni\n}\n\\{\\mathbf{T}_{i}\\}\n, point cloud scans\n{\nğ’\ni\n}\n\\{\\mathbf{S}_{i}\\}\n;\nKeyframe thresholds\n(\nÏ„\ntime\n,\nÏ„\nspace\n)\n(\\tau_{\\text{time}},\\tau_{\\text{space}})\n;\nSampling parameters:\nÎ²\n\\beta\n,\nT\nmax\nT_{\\max}\n,\nN\nmin\n,\nN\nmax\nN_{\\min},N_{\\max}\n,\nF\nmin\n,\nF\nmax\nF_{\\min},F_{\\max}\n;\nSpatial and overlap thresholds:\nd\nmax\nd_{\\max}\n,\nÏµ\noverlap\n\\epsilon_{\\text{overlap}}\n,\nv\noverlap\nv_{\\text{overlap}}\n;\nOutput:\nGenerated training samples for training.\n1\n2\nSelect keyframe indices\nğ’¦\nâ†\nSelectKeyframes\nâ€‹\n(\n{\nğ“\ni\n}\n,\nÏ„\ntime\n,\nÏ„\nspace\n)\n\\mathcal{K}\\leftarrow\\textsc{SelectKeyframes}(\\{\\mathbf{T}_{i}\\},\\tau_{\\text{time}},\\tau_{\\text{space}})\n;\n3\nif\ndeskewing enabled\nthen\n4\nforeach\nk\nâˆˆ\nğ’¦\nk\\in\\mathcal{K}\ndo\n5\nApply\nDeskew\nto\nğ’\nk\n\\mathbf{S}_{k}\nif pointwise timestamps are available;\n6\n7\n8\n9\nLet\nM\nâ†\n|\nğ’¦\n|\nM\\leftarrow|\\mathcal{K}|\n,\nN\ntarget\nâ†\nÎ²\nâ€‹\nM\nN_{\\text{target}}\\leftarrow\\beta M\n;\n10\n11\nfor\nn\n=\n1\nn=1\nto\nN\ntarget\nN_{\\text{target}}\ndo\n12\nfor\nt\n=\n1\nt=1\nto\nT\nmax\nT_{\\max}\ndo\n13\nSample\nN\nâˆ¼\nğ’°\nâ€‹\n{\nN\nmin\n,\nN\nmax\n}\nN\\sim\\mathcal{U}\\{N_{\\min},N_{\\max}\\}\n;\n14\nSample\nN\nN\ndisjoint keyframe intervals\n{\nI\nj\n}\nj\n=\n1\nN\n\\{I_{j}\\}_{j=1}^{N}\nfrom\nğ’¦\n\\mathcal{K}\nwith lengths\nF\nj\nâˆ¼\nğ’°\nâ€‹\n{\nF\nmin\n,\nF\nmax\n}\nF_{j}\\sim\\mathcal{U}\\{F_{\\min},F_{\\max}\\}\n;\n15\n16\nInitialize\nâ„³\nâ†\n[\n]\n\\mathcal{M}\\leftarrow[\\,]\n,\nğ’\nâ†\n[\n]\n\\mathcal{C}\\leftarrow[\\,]\n;\n17\nfor\nj\n=\n1\nj=1\nto\nN\nN\ndo\n18\nAccumulate a point cloud\nğŒ\nj\nâ†\nAccumulateFrames\nâ€‹\n(\n{\nğ’\ni\n}\ni\nâˆˆ\nI\nj\n)\n\\mathbf{M}_{j}\\leftarrow\\textsc{AccumulateFrames}(\\{\\mathbf{S}_{i}\\}_{i\\in I_{j}})\n;\n19\nTransform\nğŒ\nj\n\\mathbf{M}_{j}\nto the world frame using\n{\nğ“\ni\n}\ni\nâˆˆ\nI\nj\n\\{\\mathbf{T}_{i}\\}_{i\\in I_{j}}\n;\n20\nCompute point cloud center\nğœ\nj\n\\mathbf{c}_{j}\nfrom\nğŒ\nj\n\\mathbf{M}_{j}\nand append to\nğ’\n\\mathcal{C}\n;\n21\nAppend\nğŒ\nj\n\\mathbf{M}_{j}\nto\nâ„³\n\\mathcal{M}\n;\n22\n23\n24\nif\nâˆƒ\n(\na\n,\nb\n)\n\\exists(a,b)\nsuch that\nâ€–\nğœ\na\nâˆ’\nğœ\nb\nâ€–\n2\n>\nd\nmax\n\\|\\mathbf{c}_{a}-\\mathbf{c}_{b}\\|_{2}>d_{\\max}\nthen\n25\ncontinue\nto next attempt;\n26\n27\n28\nBuild a graph\nG\nG\nover nodes\n{\n1\n,\nâ€¦\n,\nN\n}\n\\{1,\\dots,N\\}\nwith edge\n(\na\n,\nb\n)\n(a,b)\nif\nOverlapRatio\nâ€‹\n(\nğŒ\na\n,\nğŒ\nb\n,\nv\noverlap\n)\nâ‰¥\nÏµ\noverlap\n\\textsc{OverlapRatio}(\\mathbf{M}_{a},\\mathbf{M}_{b},v_{\\text{overlap}})\\geq\\epsilon_{\\text{overlap}}\n;\n29\nif\nG\nG\nis connected\nthen\n30\nSaveTrainingSample\n(\nâ„³\n\\mathcal{M}\n);\n31\nbreak\n;\n32\n33\n34\nAlgorithmÂ 2\nGenerate training samples from a sequence\nTable 5\n:\nSummary of the training datasets with parameter settings for data curation. Sampling parameters:\nN\nmax\nN_{\\max}\n,\nF\nmax\nF_{\\max}\n; Spatial and overlap thresholds:\nd\nmax\nd_{\\max}\n,\nÏµ\noverlap\n\\epsilon_{\\text{overlap}}\n; Point cloud preprocessing parameters:\nÎ±\ns\n\\alpha_{s}\n,\nv\nd\nv_{d}\n.\nDataset\nScenario\nSensor\n# Scenes\nType\n# Samples\n# P. Clouds\nSampling Parameters\nSpatial Thre.\nPreprocess\nN\nmax\nN_{\\max}\nF\nmax\nF_{\\max}\nd\nmax\nd_{\\max}\nÏµ\noverlap\n\\epsilon_{\\text{overlap}}\nÎ±\ns\n\\alpha_{s}\nv\nd\nv_{d}\nOutdoor LiDAR\nKITTI\n[\ngeiger2012cvpr\n]\nGermany; urban & highway\nVelodyne-64\n22\nScan\n1,226\n2,852\n8\n1\n100.0\n1%\n0.2\n0.25\nSubmap\n3,810\n16,453\n10\n600\n400.0\n0.5%\n0.05\n0.25\nKITTI360\n[\nliao2022pami\n]\nGermany; urban\nVelodyne-64\n9\nScan\n3,002\n6,223\n8\n1\n100.0\n1%\n0.2\n0.25\nSubmap\n7,530\n21,255\n10\n600\n400.0\n0.5%\n0.05\n0.25\nApollo\n[\nhuang2018cvprws\n]\nUSA; urban & highway\nVelodyne-64\n11\nScan\n3,343\n7,874\n8\n1\n100.0\n1%\n0.2\n0.25\nSubmap\n6,660\n25,036\n10\n600\n400.0\n0.5%\n0.05\n0.25\nMulRAN\n[\nkim2020icra\n]\nSouth Korea; urban & campus\nOuster-64\n4\nScan\n898\n1900\n8\n1\n100.0\n2%\n0.2\n0.25\nSubmap\n1,388\n4,477\n10\n600\n400.0\n0.5%\n0.05\n0.25\nOxford Spires\n[\ntao2025ijrr-oxfordspires\n]\nUK; campus\nHesai-64\n6\nScan\n1,356\n5,781\n10\n1\n60.0\n2%\n0.2\n0.25\nSubmap\n541\n2,763\n10\n200\n150.0\n1%\n0.1\n0.25\nVBR\n[\nbrizi2024icra-vbr\n]\nItaly; urban & campus\nOuster-64\n5\nScan\n3,371\n8,647\n8\n1\n80.0\n1%\n0.2\n0.25\nSubmap\n1,906\n8,511\n10\n500\n300.0\n0.5%\n0.05\n0.25\nUrbanNav\n[\nhsu2023navi-urbannav\n]\nChina; urban\nVelodyne-32\n4\nScan\n1,912\n4,228\n8\n1\n100.0\n1%\n0.2\n0.25\nSubmap\n979\n3,540\n10\n600\n400.0\n0.5%\n0.05\n0.25\nHeLiPR\n[\njung2024ijrr\n]\nSouth Korea; urban\nOuster-128, Avia, Aeva\n3\nScan\n1,808\n3,691\n8\n1\n100.0\n1%\n0.2\n0.25\nSubmap\n3,624\n10,882\n10\n600\n400.0\n0.5%\n0.05\n0.25\nBoreas\n[\nburnett2023ijrr-boreas\n]\nCanada; urban\nVelodyne-128\n2\nScan\n1,131\n2,311\n5\n1\n100.0\n1%\n0.2\n0.25\nSubmap\n1,429\n3,613\n10\n600\n400.0\n0.5%\n0.05\n0.25\nWildPlace\n[\nknights2023icra-wildplaces\n]\nAustralia; forest\nVelodyne-16\n2\nSubmap\n1,167\n2,613\n5\n600\n300.0\n1%\n0.1\n0.25\nNuScenes\n[\ncaesar2020cvpr\n]\nUSA & Singapore; urban\nVelodyne-32\n642\nScan\n12,160\n24,320\n2\n1\n80.0\n2%\n0.5\n0.25\nKITTI-Carla\n[\ndeschaud2021arxiv-kitticarla\n]\nSynthetic; urban\nSimulated-64\n7\nSubmap\n1,733\n7,729\n10\n600\n400.0\n0.5%\n0.05\n0.25\nIndoor Depth Camera\n3DMatch\n[\nzeng2017cvpr\n]\nUSA; office & apartment\nKinect, RealSense, etc.\n82\nScan\n7,044\n14,088\n2\n1\n10.0\n2%\n1.0\n0.02\nSubmap\n3,904\n29,314\n16\n10\n10.0\n1%\n0.5\n0.02\nScanNet\n[\ndai2017cvpr\n]\nUSA; office & apartment\nStructure sensor\n661\nSubmap\n10,840\n75,299\n24\n50\n15.0\n1%\n0.2\n0.02\nScanNet++\n[\nyeshwanth2023iccv-scannetpp\n]\nGermany; office & apartment\nFaro, DSLR, iPhone\n220\nScan\n9,306\n101,743\n24\n1\n15.0\n1%\n0.5\n0.02\nNSS\n[\nsun2025jprs\n]\nUSA; office & construction site\nMatterport camera\n6\nScan\n17,275\n72,866\n20\n1\n30.0\n0.1%\n0.2\n0.05\nObject-centric\nModelNet-40\n[\nwu2015cvpr\n]\nSynthetic; CAD\n-\n(12,308)\nScan\n24,616\n49,232\n2\n1\n-\n-\n-\n0.01\nTraining Set\n1,621\n118,143\n457,195\nTotal\n1,685\n141,002\n520,315\nThe details of the training datasets and used parameters for data curation (\nN\nmax\nN_{\\max}\n,\nF\nmax\nF_{\\max}\n,\nd\nmax\nd_{\\max}\n,\nÏµ\noverlap\n\\epsilon_{\\text{overlap}}\n) and point cloud preprocessing (\nÎ±\ns\n\\alpha_{s}\n,\nv\nd\nv_{d}\n) are shown in\nTab.\n5\n. In total, we curate 141k samples containing 520k point clouds, resulting in more than 10 billion points. The data covers a diverse range of scenes across 9 countries on 4 continents, captured by 9 types of LiDAR and 6 types of depth cameras with varying resolutions and scales.\nWe split the data into training and validation sets. To ensure fair evaluation, we exclude sequences used for testing in commonly used benchmarks from the training set and designate them as validation sequences. For example, sequences 08â€“10 from the KITTI dataset are excluded from training. For some datasets not used in the testing benchmark (such as Nuscenes, Boreas, and WildPlace), we use all the sequences for training but keep 10% randomly selected samples for the in-sequence validation.\nTesting data details\nThe details of the testing datasets and the evaluation success criteria are shown in\nTab.\n6\n.\nFor pairwise registration, we follow the generalizability benchmark proposed by BUFFER-X\n[\nseo2025iccv\n]\n. Since BUFFER-X does not directly provide the testing data for ScanNet++ dataset due to the dataset sharing policies, we exclude ScanNet++ from our benchmark. To have enough datasets for indoor depth camera-based point cloud registration evaluation, we additionally add the nothing stands still (NSS) dataset\n[\nsun2025jprs\n]\n. NSS contains Matterport RGB-D scans of six large indoor areas (offices and construction sites) captured repeatedly over several months as the buildings evolve, leading to large geometric, topological, and appearance changes between fragments\n[\nsun2025jprs\n]\n. Since our training data do not explicitly consider the temporal change, we adopt the same-stage split in NSS for the evaluation.\nFor multi-view registration evaluation, except for the commonly used 3DMatch\n[\nzeng2017cvpr\n]\nand ScanNet\n[\ndai2017cvpr\n]\ndatasets, we select two outdoor LiDAR datasets: Waymo\n[\nsun2020cvpr\n]\nand KAIST\n[\njung2024ijrr\n]\nfor the zero-shot multi-view registration evaluation, at both scan- and submap-level.\nWe additionally select two terrestrial laser scanner (TLS) datasets: WHU-TLS\n[\ndong2020jprs-whutls\n]\nand ETH-TLS\n[\ntheiler2015jprs\n]\nfor the zero-shot multi-view registration evaluation. They cover various scenarios such as classroom, park, campus, tunnel, historical building, forest, and mountain, with the scale ranging from 10â€‰m to 1000â€‰m. We only show the qualitative results in the main paper and here we provide the quantitative results in\nTab.\n9\n.\nTable 6\n:\nSummary of the testing datasets used for evaluation. Zero-shot datasets are those not included in the training.\nDataset\nScenario\nSensor\nType\n# Samples\nScale [m]\nSuccess Criteria\nZero-shot\nPairwise Registration\n3DMatch\n[\nzeng2017cvpr\n]\nUSA; office & apartment\nKinect, RealSense, etc.\nScan\n1,623\n5\nPointwise RMSE 0.2m\nNo\n3DLoMatch\n[\nhuang2021cvpr-predator\n]\nUSA; office & apartment\nKinect, RealSense, etc.\nScan\n1,781\n5\nPointwise RMSE 0.2m\nNo\nNSS\n[\nsun2025jprs\n]\nUSA; office & construction site\nMatterport camera\nScan\n1,125\n10\nTE 0.2m, RE 10\nâˆ˜\nNo\nTIERS\n[\nsier2023rs-tiers\n]\nFinland; office & campus\nVelodyne-16, Ouster-64/128\nScan\n870\n120\nTE 2m, RE 5\nâˆ˜\nYes\nKITTI\n[\ngeiger2012cvpr\n]\nGermany; urban & highway\nVelodyne-64\nScan\n555\n160\nTE 2m, RE 5\nâˆ˜\nNo\nWaymo\n[\nsun2020cvpr\n]\nUSA; urban & highway\nLaser Bear Honeycomb\nScan\n130\n200\nTE 2m, RE 5\nâˆ˜\nYes\nKAIST\n[\njung2024ijrr\n]\nSouth Korea; campus\nOuster-128, Avia, Aeva\nScan\n1,991\n200\nTE 2m, RE 5\nâˆ˜\nYes\nMIT\n[\ntian2023iros-kimeramultidata\n]\nUSA; campus\nVelodyne-16\nScan\n230\n160\nTE 2m, RE 5\nâˆ˜\nYes\nETH\n[\npomerleau2012ijrr\n]\nSwitzerland; park\nHokuyo\nScan\n713\n100\nTE 2m, RE 5\nâˆ˜\nYes\nNCD\n[\nramezani2020iros\n]\nUK; campus\nOuster-64\nScan\n301\n200\nTE 2m, RE 5\nâˆ˜\nYes\nMulti-view Registration\n3DMatch\n[\nzeng2017cvpr\n]\nUSA; office & apartment\nKinect, RealSense, etc.\nScan\n214\n10\nTE 0.3m, RE 15\nâˆ˜\nNo\nScanNet\n[\ndai2017cvpr\n]\nUSA; office & apartment\nStructure sensor\nScan\n155\n10\nTE 0.3m, RE 15\nâˆ˜\nNo\nWaymo\n[\nsun2020cvpr\n]\nUSA; urban & highway\nLaser Bear Honeycomb\nScan\n77\n300\nTE 2m, RE 5\nâˆ˜\nYes\nKAIST\n[\njung2024ijrr\n]\nSouth Korea; campus\nOuster-128, Avia, Aeva\nSubmap\n134\n1000\nTE 2m, RE 5\nâˆ˜\nYes\nWHU-TLS\n[\ndong2020jprs-whutls\n]\nChina; park, campus, mountain, etc.\nRiegl, Faro, Leica TLS\nScan\n11\n600\nTE 2m, RE 5\nâˆ˜\nYes\nETH-TLS\n[\ntheiler2015jprs\n]\nSwitzerland; courtyard, forest, etc.\nZ+F, Faro TLS\nScan\n5\n300\nTE 2m, RE 5\nâˆ˜\nYes\nEvaluation metrics\nWe follow previous works on point cloud registration to evaluate the performance of our model using the registration success rateÂ (%) calculated with thresholds on correspondence RMSE for 3DMatch and on translation and rotation error for the other datasets, summarized in\nTab.\n6\n. We use threshold settings of previous works\n[\nzeng2017cvpr\n,\nsun2025jprs\n,\nseo2025iccv\n]\n.\nGiven the ground truth transformation\nğ“\ngt\n=\n[\nğ‘\ngt\n|\nğ­\ngt\n]\n\\mathbf{T}_{\\text{gt}}=[\\mathbf{R}_{\\text{gt}}|\\mathbf{t}_{\\text{gt}}]\nand the estimated transformation\nğ“\nest\n=\n[\nğ‘\nest\n|\nğ­\nest\n]\n\\mathbf{T}_{\\text{est}}=[\\mathbf{R}_{\\text{est}}|\\mathbf{t}_{\\text{est}}]\n, we compute the translation error (TE) and rotation error (RE) as:\nTE\n=\nâ€–\nğ­\ngt\nâˆ’\nğ­\nest\nâ€–\n2\n,\n\\displaystyle=\\|\\mathbf{t}_{\\text{gt}}-\\mathbf{t}_{\\text{est}}\\|_{2},\n(10)\nRE\n=\narccos\nâ¡\n(\ntr\nâ€‹\n(\nğ‘\ngt\nâŠ¤\nâ€‹\nğ‘\nest\n)\nâˆ’\n1\n2\n)\nâ‹…\n180\nÏ€\n,\n\\displaystyle=\\arccos\\left(\\frac{\\text{tr}(\\mathbf{R}_{\\text{gt}}^{\\top}\\mathbf{R}_{\\text{est}})-1}{2}\\right)\\cdot\\frac{180}{\\pi},\n(11)\nwhere TE is measured in meters and RE is measured in degrees. For multi-view registration, we report the mean translation error and mean rotation error across all point clouds.\nFor multi-view registration, we also evaluate the quality of the registered point cloud using the Chamfer Distance (CD), which measures the bi-directional root mean squared distance between the registered point cloud and the ground truth aggregated point cloud. Given the registered point cloud\nğ\nreg\n\\mathbf{P}_{\\text{reg}}\nand the ground truth point cloud\nğ\ngt\n\\mathbf{P}_{\\text{gt}}\n, the Chamfer Distance is computed as:\nCD\n=\n1\n2\nâ€‹\n(\n1\n|\nğ\nreg\n|\nâ€‹\nâˆ‘\nğ©\nâˆˆ\nğ\nreg\nmin\nğª\nâˆˆ\nğ\ngt\nâ¡\nâ€–\nğ©\nâˆ’\nğª\nâ€–\n2\n2\n+\n1\n|\nğ\ngt\n|\nâ€‹\nâˆ‘\nğª\nâˆˆ\nğ\ngt\nmin\nğ©\nâˆˆ\nğ\nreg\nâ¡\nâ€–\nğª\nâˆ’\nğ©\nâ€–\n2\n2\n)\n,\n\\displaystyle\\text{CD}=\\sqrt{\\frac{1}{2}\\left(\\frac{1}{|\\mathbf{P}_{\\text{reg}}|}\\sum_{\\mathbf{p}\\in\\mathbf{P}_{\\text{reg}}}\\min_{\\mathbf{q}\\in\\mathbf{P}_{\\text{gt}}}\\|\\mathbf{p}-\\mathbf{q}\\|_{2}^{2}+\\frac{1}{|\\mathbf{P}_{\\text{gt}}|}\\sum_{\\mathbf{q}\\in\\mathbf{P}_{\\text{gt}}}\\min_{\\mathbf{p}\\in\\mathbf{P}_{\\text{reg}}}\\|\\mathbf{q}-\\mathbf{p}\\|_{2}^{2}\\right)},\n(12)\nwhere CD is measured in meters.\nAppendix B\nDetails on the Selected Baselines\nIn the main paper, we did not include detailed descriptions of the baseline methods due to space limitations. Here we provide comprehensive descriptions of all baselines used in our experiments in\nTab.\n7\n.\nAll pairwise registration baselines follow the standard correspondence matching and transformation estimation pipeline. Similarly, all multi-view registration baselines follow a two-stage pipeline: first performing pairwise registration, then applying pose graph optimization to enforce global consistency. In contrast, our method is a single-stage multi-view registration approach that directly generates the registered point cloud using flow matching, eliminating the need for exhaustive pairwise correspondence matching, pairwise transformation estimation, and pose graph optimization.\nTable 7\n:\nDescription of baseline methods used in our experiments, organized by pairwise and multi-view registration tasks.\nMethod\nCategory\nVenue\nDescription\nPairwise Registration Baselines (all follow the correspondence matching and transformation estimation pipeline)\nFPFH\n[\nrusu2009icra\n]\n+ FGR\n[\nzhou2016eccv\n]\nConventional\nECCVâ€™16\nHandcrafted Fast Point Feature Histogram (FPFH) descriptor extracts local geometric features for correspondence matching, combined with Fast Global Registration (FGR) that formulates registration as a global optimization problem using a robust cost function to handle outliers effectively\nFPFH\n[\nrusu2009icra\n]\n+ TEASER\n[\nyang2020tro\n]\nConventional\nTROâ€™20\nHandcrafted FPFH descriptor for correspondence matching combined with TEASER (Truncated least squares Estimation And SEmidefinite Relaxation), which uses truncated least squares and semidefinite relaxation to provide certifiably optimal solutions robust to extreme outlier rates\nFPFH\n[\nrusu2009icra\n]\n+ Quatro\n[\nlim2022icra\n]\nConventional\nICRAâ€™22\nHandcrafted FPFH descriptor for correspondence matching combined with Quatro, a Quasi-SE(3) registration method that extends TEASER to handle degenerate cases in urban environments by requiring only a single correspondence\nFCGF\n[\nchoy2019iccv\n]\nDeep learning\nICCVâ€™19\nFully Convolutional Geometric Features uses a sparse 3D convolutional network to extract dense geometric features from point clouds, enabling dense correspondence matching for robust registration\nPredator\n[\nhuang2021cvpr-predator\n]\nDeep learning\nCVPRâ€™21\nOverlap-aware registration network with attention mechanism that learns to focus on overlapping regions between point clouds, particularly effective for low-overlap scenarios by predicting overlap scores and using attention-weighted features for correspondence matching and transformation estimation\nGeoTransformer\n[\nqin2022cvpr-geotransformer\n]\nDeep learning\nCVPRâ€™22\nKeypoint-free registration method that matches superpoints based on patch overlap. It encodes pair-wise distances and triplet-wise angles to learn transformation-invariant geometric features through self-attention, enabling robust superpoint matching in low-overlap scenarios. Dense correspondences are obtained via optimal transport, achieving high inlier ratios without requiring RANSAC\nBUFFER\n[\nao2023cvpr-buffer\n]\nDeep learning\nCVPRâ€™23\nRegistration method that balances accuracy, efficiency, and generalizability through three key components: a Point-wise Learner that predicts keypoints and estimates point orientations, a Patch-wise Embedder that extracts efficient and general patch features, and an Inliers Generator that identifies inlier correspondences for robust transformation estimation\nEYOC\n[\nliu2024cvpr-eyoc\n]\nDeep learning\nCVPRâ€™24\nUnsupervised method for distant point cloud registration that progressively trains a feature extractor using near point cloud pairs to self-supervise on farther pairs, eliminating the need for global pose labels through progressive distance extension, works well for outdoor low-overlap scenarios\nPARENet\n[\nyao2024iccv-parenet\n]\nDeep learning\nECCVâ€™24\nPosition-Aware Rotation-Equivariant Network designed for robust registration in low-overlap scenarios by learning rotation-equivariant features and position-aware representations that handle partial point clouds effectively\nBUFFER-X\n[\nseo2025iccv\n]\nDeep learning\nICCVâ€™25\nExtended BUFFER method with improved zero-shot generalization capabilities, adaptively determining voxel sizes and search radii, bypassing learned keypoint detectors using farthest point sampling, and employing patch-wise scale normalization for consistent performance across diverse scenes\nMulti-view Registration Baselines (all follow the pairwise registration and pose graph optimization two-stage pipeline)\nFGR\n[\nzhou2016eccv\n]\n+ PGO\nConventional\nECCVâ€™16\nFast Global Registration performs pairwise alignment between all point cloud pairs. Then we apply grow-based algorithm to get the initial guess of the scan poses, followed by pose graph optimization (PGO) implemented by Open3D to enforce global consistency across all views\nBUFFER-X\n[\nseo2025iccv\n]\n+ PGO\nDeep learning\nICCVâ€™25\nWe use BUFFER-X for pairwise registration and apply grow-based algorithm to get the initial guess of the scan poses. We then apply PGO implemented by Open3D to achieve globally consistent multi-view alignment\nLMVR\n[\ngojcic2020cvpr\n]\nDeep learning\nCVPRâ€™20\nLearned Multi-View Registration end-to-end network that jointly estimates transformations for multiple point clouds by learning to predict the exhaustive pairwise transformations and optimizing them simultaneously with a transformation synchronization step in a unified framework\nSGHR\n[\nwang2023cvpr-sghr\n]\nDeep learning\nCVPRâ€™23\nUses NetVLAD-based global features to estimate overlaps and construct a sparse pose graph, reducing the number of pairwise registrations. Applies history reweighting in IRLS that considers previous iterationsâ€™ poses to prevent outlier domination and ensure stable convergence\nAppendix C\nAdditional Experimental Results\nComprehensive results on all validation and testing datasets\nTo provide a holistic view of our modelâ€™s performance across all validation datasets used during training, we summarize the multi-view registration metricsâ€”registration success rateÂ (SR), Chamfer DistanceÂ (CD), mean rotation errorÂ (RE), and mean translation errorÂ (TE)â€”for both average (\nS\n=\n1\nS=1\n) and rigidity-selected (\nS\n=\n3\nS=3\n) inference settings on both scan- and submap-level data in\nTab.\n8\n. As mentioned in the main paper,\nS\nS\nis the number of random generations for inference. We show both the results for validation samples in both the seen sequences and the unseen sequences. Overall, our model has better performance on the scan-level samples than the submap-level samples, and better performance on the seen sequences than on the unseen sequences. Besides, the rigidity-selected inference setting generally outperforms the average inference setting, demonstrating the effectiveness of our rigidity-based selection strategy.\nWe also provide the comprehensive experimental results on the testing datasets (including those TLS datasets) in\nTab.\n9\n.\nTable 8\n:\nComprehensive multi-view registration performance on the validation sets of all datasets.\nWe report registration success rateÂ (SR, inÂ %), Chamfer DistanceÂ (CD, in meters), mean rotation errorÂ (RE, in degrees), and mean translation errorÂ (TE, in meters) for both average (\nS\n=\n1\nS=1\n) and rigidity-selected (\nS\n=\n3\nS=3\n) inference settings on both scan- and submap-level data where applicable. We show the results of validation samples on both the seen and unseen sequences.\nAverage (\nS\n=\n1\nS=1\n)\nRigidity Selected (\nS\n=\n3\nS=3\n)\nDataset\n# Views\nUnseen Sequence\nSR [%]\nâ†‘\n\\uparrow\nCD [m]\nâ†“\n\\downarrow\nRE [\nâˆ˜\n]\nâ†“\n\\downarrow\nTE [m]\nâ†“\n\\downarrow\nSR [%]\nâ†‘\n\\uparrow\nCD [m]\nâ†“\n\\downarrow\nRE [\nâˆ˜\n]\nâ†“\n\\downarrow\nTE [m]\nâ†“\n\\downarrow\nOutdoor LiDAR\nKITTI Scan\n[2,8]\nYes\n82.00\n1.46\n15.84\n8.65\n83.11\n1.38\n14.41\n8.09\nNo\n91.60\n0.73\n3.72\n3.22\n91.87\n0.82\n4.29\n3.81\nKITTI Submap\n[2,10]\nYes\n32.89\n12.77\n35.18\n74.45\n33.85\n12.67\n34.69\n73.93\nNo\n51.56\n3.22\n5.22\n11.59\n54.52\n2.89\n4.66\n9.81\nKITTI360 Scan\n[2,8]\nYes\n85.04\n0.76\n4.57\n7.17\n86.28\n0.72\n4.28\n7.05\nNo\n96.25\n0.33\n0.75\n0.81\n97.35\n0.31\n0.69\n0.75\nKITTI360 Submap\n[2,10]\nYes\n39.56\n10.31\n34.68\n57.36\n41.53\n10.19\n34.03\n56.77\nNo\n72.90\n2.27\n5.15\n10.64\n74.27\n2.21\n5.16\n10.43\nApollo Scan\n[2,8]\nYes\n73.23\n1.13\n8.78\n14.00\n73.36\n1.13\n8.73\n13.96\nNo\n90.26\n0.64\n2.50\n3.42\n91.65\n0.52\n2.12\n2.84\nApollo Submap\n[2,10]\nYes\n91.88\n1.57\n3.45\n9.84\n92.15\n1.65\n3.51\n10.19\nNo\n94.12\n0.66\n1.42\n2.30\n95.13\n0.61\n1.42\n1.97\nMulRAN Scan\n[2,5]\nNo\n86.30\n0.97\n6.41\n4.74\n88.89\n0.86\n5.78\n3.67\nMulRAN Submap\n[2,10]\nYes\n69.20\n7.08\n12.71\n38.36\n69.93\n6.71\n12.74\n37.61\nNo\n66.43\n2.45\n5.30\n10.85\n70.51\n2.19\n5.12\n10.25\nOxford Spires Scan\n[2,10]\nYes\n27.80\n2.49\n26.18\n10.47\n29.18\n2.51\n25.28\n10.36\nNo\n73.77\n0.82\n4.07\n1.59\n75.00\n0.77\n3.63\n1.41\nOxford Spires Submap\n[2,10]\nYes\n85.09\n0.74\n3.80\n1.39\n89.47\n0.66\n2.86\n1.20\nNo\n82.42\n1.27\n4.63\n2.37\n81.82\n1.23\n4.94\n2.31\nVBR Scan\n[2,8]\nYes\n40.28\n2.53\n20.43\n16.88\n40.74\n2.56\n20.94\n16.93\nNo\n81.76\n0.67\n2.47\n2.25\n83.43\n0.64\n2.27\n2.09\nVBR Submap\n[2,10]\nYes\n15.91\n10.95\n39.43\n48.23\n16.82\n10.86\n38.81\n47.77\nNo\n51.83\n2.83\n7.27\n10.28\n53.92\n2.65\n6.78\n9.75\nUrbanNav Scan\n[2,8]\nYes\n60.00\n3.39\n27.32\n14.42\n59.43\n3.36\n27.88\n15.04\nNo\n80.91\n1.10\n5.17\n6.29\n81.26\n1.01\n5.51\n6.02\nUrbanNav Submap\n[2,10]\nYes\n32.84\n6.42\n31.12\n33.49\n32.11\n6.37\n32.05\n35.10\nNo\n80.28\n1.58\n4.91\n5.74\n77.56\n1.60\n4.63\n5.85\nHeLiPR Scan\n[2,8]\nYes\n88.89\n0.55\n2.38\n4.93\n88.62\n0.54\n2.49\n4.78\nHeLiPR Submap\n[2,10]\nNo\n92.31\n0.86\n1.85\n2.18\n92.31\n0.94\n1.80\n2.44\nBoreas Scan\n[2,5]\nNo\n97.68\n0.32\n2.55\n2.05\n97.39\n0.25\n2.40\n1.79\nBoreas Submap\n[2,8]\nNo\n96.04\n0.81\n1.59\n4.94\n96.52\n0.88\n1.50\n4.37\nWildPlace Submap\n[2,5]\nNo\n86.64\n0.60\n3.87\n4.41\n86.35\n0.62\n4.36\n4.83\nNuScenes Pairwise Scan\n[2,2]\nNo\n97.62\n0.28\n1.47\n0.79\n97.86\n0.27\n1.42\n0.73\nKITTI-Carla Submap\n[2,10]\nNo\n56.49\n2.69\n5.20\n9.13\n59.08\n2.56\n4.66\n9.81\nIndoor Depth Camera\n3DMatch Scan\n[2,2]\nYes\n76.17\n0.09\n16.34\n0.46\n76.57\n0.09\n16.05\n0.45\nNo\n88.33\n0.05\n6.02\n0.16\n88.66\n0.05\n5.53\n0.16\n3DMatch Submap\n[2,16]\nYes\n30.61\n0.34\n38.12\n1.46\n31.64\n0.34\n38.96\n1.46\nNo\n42.36\n0.27\n25.13\n1.15\n43.25\n0.27\n24.61\n1.15\nScanNet Submap\n[2,16]\nYes\n66.22\n0.13\n10.90\n0.35\n71.10\n0.12\n9.59\n0.33\nNo\n80.27\n0.09\n11.00\n0.31\n81.87\n0.08\n10.53\n0.29\nScanNet++ Scan\n[2,12]\nYes\n74.53\n0.10\n14.28\n0.34\n74.83\n0.10\n14.35\n0.33\nNo\n83.18\n0.08\n5.11\n0.23\n84.02\n0.07\n5.30\n0.23\nNSS Pairwise Scan\n[2,2]\nNo\n96.21\n0.03\n2.78\n0.12\n96.35\n0.02\n2.61\n0.11\nNSS Multi-view Scan\n[2,20]\nYes\n53.98\n0.30\n19.33\n1.41\n54.14\n0.30\n19.24\n1.41\nObject-centric\nModelNet-40\n[2,2]\nYes\n97.18\n0.006\n4.81\n0.016\n97.33\n0.006\n4.65\n0.015\nTable 9\n:\nComprehensive multi-view registration performance on the testing datasets.\nWe report registration success rateÂ (SR, inÂ %), Chamfer DistanceÂ (CD, in meters), mean rotation errorÂ (RE, in degrees), and mean translation errorÂ (TE, in meters) for both average (\nS\n=\n1\nS=1\n) and rigidity-selected (\nS\n=\n5\nS=5\n) inference settings on both scan- and submap-level data where applicable.\nAverage (\nS\n=\n1\nS=1\n)\nRigidity Selected (\nS\n=\n5\nS=5\n)\nDataset\n# Views\nType\nZero-shot\nSR [%]\nâ†‘\n\\uparrow\nCD [m]\nâ†“\n\\downarrow\nRE [\nâˆ˜\n]\nâ†“\n\\downarrow\nTE [m]\nâ†“\n\\downarrow\nSR [%]\nâ†‘\n\\uparrow\nCD [m]\nâ†“\n\\downarrow\nRE [\nâˆ˜\n]\nâ†“\n\\downarrow\nTE [m]\nâ†“\n\\downarrow\nPairwise Registration\n3DMatch\n[\nzeng2017cvpr\n]\n[2,2]\nScan\nNo\n95.27\n0.02\n3.76\n0.08\n95.64\n0.02\n3.71\n0.07\n3DLoMatch\n[\nhuang2021cvpr-predator\n]\n[2,2]\nScan\nNo\n77.43\n0.08\n13.46\n0.36\n78.22\n0.08\n13.22\n0.34\nNSS\n[\nsun2025jprs\n]\n[2,2]\nScan\nNo\n96.39\n0.03\n2.83\n0.15\n96.18\n0.03\n2.78\n0.15\nTIERS\n[\nsier2023rs-tiers\n]\n[2,2]\nScan\nYes\n94.75\n0.13\n3.87\n0.37\n95.09\n0.12\n3.53\n0.34\nKITTI\n[\ngeiger2012cvpr\n]\n[2,2]\nScan\nNo\n100.00\n0.14\n0.20\n0.09\n100.00\n0.13\n0.20\n0.09\nWaymo\n[\nsun2020cvpr\n]\n[2,2]\nScan\nYes\n98.51\n0.22\n2.69\n0.69\n99.47\n0.18\n2.33\n0.45\nKAIST\n[\njung2024ijrr\n]\n[2,2]\nScan\nYes\n99.05\n0.19\n1.45\n0.44\n99.15\n0.18\n1.48\n0.46\nMIT\n[\ntian2023iros-kimeramultidata\n]\n[2,2]\nScan\nYes\n98.96\n0.28\n1.98\n0.43\n99.13\n0.24\n1.93\n0.26\nETH\n[\npomerleau2012ijrr\n]\n[2,2]\nScan\nYes\n98.30\n0.07\n2.07\n0.15\n98.04\n0.06\n1.97\n0.13\nNCD\n[\nramezani2020iros\n]\n[2,2]\nScan\nYes\n99.34\n0.32\n2.14\n0.96\n99.34\n0.24\n1.65\n0.55\nMulti-view Registration\n3DMatch\n[\nzeng2017cvpr\n]\n[3,12]\nScan\nNo\n75.29\n0.11\n7.46\n0.23\n76.50\n0.10\n7.30\n0.22\nScanNet\n[\ndai2017cvpr\n]\n[3,12]\nScan\nNo\n63.83\n0.13\n16.54\n0.42\n64.04\n0.12\n13.66\n0.35\nWaymo\n[\nsun2020cvpr\n]\n[3,12]\nScan\nYes\n79.17\n0.51\n1.38\n2.12\n87.50\n0.23\n1.10\n1.56\nKAIST\n[\njung2024ijrr\n]\n[3,8]\nSubmap\nYes\n88.12\n3.06\n5.37\n9.14\n87.40\n2.97\n5.52\n9.36\nWHU-TLS\n[\ndong2020jprs-whutls\n]\n[5,10]\nScan\nYes\n34.34\n6.34\n24.55\n46.50\n34.31\n6.08\n23.05\n44.95\nETH-TLS\n[\ntheiler2015jprs\n]\n[5,8]\nScan\nYes\n60.00\n0.86\n6.34\n3.48\n75.00\n0.71\n4.37\n2.39\nAdditional ablation studies\nFor additional ablation studies, we provide detailed results in\nTab.\n10\n, as an extension of the ablation studies in Table 4 from the main paper.\nIt is shown that with more generation steps, the performance of our model generally improves. However, the improvement becomes marginal beyond 20 steps. Besides, using only indoor training data [K] also degrades the performance, even on the indoor datasets.\nWe additionally provide the results for the best-of-3, best-of-5, and best-of-10 generation in\nTab.\n10\nto show the upper bound performance of our model at different stochastic budget.\nTable 10\n:\nExtended ablation studies. We report the registration success rate (%) on datasets for both pairwise and multi-view registration tasks. Best results are shown in\nbold\n, second best are\nunderscored\n.\nSetting\nPairwise\nMulti-view\n3DMatch\n3DLoMatch\nScanNet\nKAIST\n[A] Ours (S = 5)\n95.64\n78.22\n64.04\n87.40\nInference\n[B] w/o rigidity forcing\n94.65\n74.63\n59.76\n83.69\n[C] w/o rigidity-based selection\n95.27\n77.43\n63.83\n88.12\n[D] w/ 1-step generation\n90.35\n61.05\n48.33\n52.95\n[E] w/ 5-step generation\n95.52\n76.83\n58.91\n87.91\n[F] w/ 20-step generation\n96.07\n79.30\n66.61\n91.04\n[G] w/ 50-step generation\n95.88\n79.45\n65.37\n91.46\nModel\n[H] w/o local feature extraction\n83.28\n50.05\n21.82\n47.20\n[I] w/\nL\n=\n6\nL=6\ntransformer blocks\n90.90\n57.59\n45.71\n73.77\n[J] w/\nL\n=\n8\nL=8\ntransformer blocks\n93.24\n71.63\n56.24\n85.14\n[K] w/ only indoor training data\n92.19\n70.02\n57.40\n12.25\nStochastic\n[L] Best-of-3 generation\n95.45\n79.24\n65.96\n91.12\n[M] Best-of-5 generation\n95.88\n81.09\n69.81\n92.59\n[N] Best-of-10 generation\n96.31\n81.91\n72.38\n93.34\nAppendix D\nAdditional Qualitative Results\nWe provide additional qualitative results of our model on the pairwise and multi-view point cloud registration tasks on various datasets in this section, as shown in\nFig.\n7\n,\nFig.\n8\n,\nFig.\n9\n, and\nFig.\n10\n.\nWe also provide some failure cases of our model in\nFig.\n11\n. It is shown that our model sometimes fails to correctly register the point clouds in the cases of (i) very low overlap ratio or even no overlap due to the thickness of walls (NSS, ScanNet++); (ii) complex scenes unseen during training (TLS tunnel, TLS forest). From the examples shown for NSS, it is shown that our model can still guess the reasonable global layout of the scene for the ambiguous failed cases.\nFigure 7\n:\nQualitative results of our model on the pairwise registration benchmarks with large scan interval (Waymo, KITTI, KAIST, NCD, MIT, TIERS) or low overlap ratio (NSS, 3DLoMatch).\nFigure 8\n:\nQualitative results of our model on multi-view point cloud registration.\nFigure 9\n:\nQualitative results of our model on multi-view point cloud registration (continued).\nFigure 10\n:\nQualitative results of our model on ModelNet for object-centric pairwise registration.\nFigure 11\n:\nFailure cases of our model. We provide the prediction of our model and the ground truth for comparison.",
    "preview_text": "Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.\n\nRegister Any Point:\nScaling 3D Point Cloud Registration by Flow Matching\nYue Pan\n1\nTao Sun\n2\nLiyuan Zhu\n2\nLucas Nunes\n1\nIro Armeni\n2\nJens Behley\n1\nCyrill Stachniss\n1\n1\nUniversity of Bonn\n2\nStanford University\nAbstract\nPoint cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization.\nIn this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered.\nUnlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud.\nWith a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art re",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "flow matching",
        "conditional generation",
        "point cloud registration",
        "3D reconstruction",
        "multi-view registration"
    ],
    "one_line_summary": "è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„ç‚¹äº‘é…å‡†æ–¹æ³•ï¼Œå°†é…å‡†é—®é¢˜è½¬åŒ–ä¸ºæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œç”¨äº3Dé‡å»ºå’Œæœºå™¨äººå®šä½ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "published_date": "2025-12-01T16:36:51Z",
    "created_at": "2026-01-09T11:30:47.272320",
    "updated_at": "2026-01-09T11:30:47.272330",
    "flag": true
}