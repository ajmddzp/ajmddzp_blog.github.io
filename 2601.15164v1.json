{
    "id": "2601.15164v1",
    "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks",
    "authors": [
        "Yaru Liu",
        "Ao-bo Wang",
        "Nanyang Ye"
    ],
    "abstract": "从合成数据中学习长时程具身行为仍然面临挑战，因为生成的场景往往物理上不真实，语言驱动程序经常在未满足任务语义的情况下“成功”，而高层级指令需要被具象化为可执行的动作序列。为解决这些局限性，我们提出了V-CAGE——一个用于大规模生成鲁棒且语义对齐的操作数据集的闭环框架。首先，我们提出了一种上下文感知的实例化机制，在场景合成过程中强制保持几何一致性。通过动态维护物体放置时的禁止空间区域地图，我们的系统能防止物体相互穿透，并确保在杂乱环境中生成可达且无冲突的配置。其次，为弥合抽象意图与底层控制之间的鸿沟，我们采用了分层指令分解模块。该模块将高层级目标（例如“准备工作”）分解为组合式动作基元，从而促进连贯的长时程规划。关键的是，我们通过基于视觉语言模型的验证循环来强制语义正确性。该视觉语言模型作为视觉评判器，在每个子任务后执行严格的拒绝采样，过滤掉那些代码执行成功但未能达成视觉目标的“静默失败”。实验表明，与未经验证的基线方法相比，V-CAGE生成的数据集具有更优的物理和语义保真度，能显著提升下游策略的成功率与泛化能力。",
    "url": "https://arxiv.org/abs/2601.15164v1",
    "html_url": "https://arxiv.org/html/2601.15164v1",
    "html_content": "V-CAGE: Context-Aware Generation and Verification\nfor Scalable Long-Horizon Embodied Tasks\nYaru Liu\nAobo Wang\nNanyang Ye\nAbstract\nLearning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently “succeed” without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction module. This decomposes high-level goals (e.g., ”get ready for work”) into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out ”silent failures” where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.\n1\nIntroduction\nRecent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities in reasoning and planning\n(Driess et al.,\n2023\n; Ahn et al.,\n2022b\n)\n. However, translating these high-level semantic capabilities into robust embodied agents capable of executing long-horizon manipulation tasks remains a formidable challenge. While the ”Scaling Laws” have proven effective in NLP and Vision, applying them to robotics is hindered by the lack of high-quality, large-scale demonstration data. Collecting real-world trajectories is expensive, unsafe, and difficult to scale\n(Brohan et al.,\n2023\n)\n. Therefore, learning from synthetic data has emerged as a promising paradigm\n(Mandlekar et al.,\n2023\n; Wang et al.,\n2024c\n)\n.\nDespite its promise, naively generated synthetic data often suffers from a significant ”quality gap” that severely limits downstream policy performance. This gap is particularly pronounced in long-horizon tasks, for example ”prepare the desk for work”, where success depends on the precise execution of sequential subtasks. We identify two fundamental failure modes in existing data generation pipelines.\nFirst, Geometric Inconsistency: Standard procedural generation often places objects without considering the dynamic evolution of the workspace. As the scene becomes cluttered, objects are instantiated in conflicting poses or unstable configurations, leading to physics simulator crashes or unrealistic interpenetrations.\nSecond, Semantic Misalignment: Language-conditioned code generation can be brittle. A generated script might execute without runtime errors, but fail to achieve the intended semantic goal (e.g., the switch was not actually toggled, or the object was placed off the pad). Training on such ”false positive” data introduces noise that catastrophic degrades the agent’s ability to reason about cause and effect\n(de Haan et al.,\n2019\n)\n.\nTo address these challenges, we present V-CAGE (VLM-Guided Context-Aware Generation for Embodied Planning), a closed-loop framework designed to synthesize long-horizon manipulation trajectories. Unlike open-loop generation methods that blindly execute LLM-produced plans, V-CAGE integrates geometric constraints and visual verification directly into the generation pipeline.\nOur core insight treats data synthesis as a rigorous optimization process, aggressively pruning invalid trajectories to ensure only high-fidelity data reaches the training buffer.\nV-CAGE operates on three hierarchical levels. At the planning level, we leverage an LLM, for example Pangu\n(Chen et al.,\n2025a\n)\n, to ground abstract user instructions into executable action sequences. At the geometric level, we propose a context-aware instantiation mechanism. By maintaining a dynamic map of ”prohibited volumes” that updates after each object placement, we ensure that new objects are instantiated only in feasible, collision-free regions, effectively solving the packing problem in cluttered scenes. Crucially, at the verification level, we employ a VLM, Gemini3\n(Gemini Team, Google,\n2025\n)\n, as a semantic critic. Treating data generation as a rejection sampling problem, the VLM evaluates the visual outcome of each subtask. Trajectories that fail to meet visual success criteria are rejected and regenerated, ensuring that the final dataset consists exclusively of physically plausible and semantically correct demonstrations.\nOur main contributions are summarized as follows:\n(1) We introduce V-CAGE, a scalable, closed-loop pipeline for synthesizing high-fidelity trajectory data for long-horizon tasks, integrating geometric constraints with visual verification.\n(2) We propose a context-aware instantiation mechanism that maintains an evolving map of prohibited volumes during scene generation, ensuring valid object placement in cluttered environments.\n(3) We formulate data validation as a VLM-guided rejection sampling process, using visual critics to filter out ”silent failures” where code executes without satisfying task semantics.\n(4) We demonstrate that policies trained on V-CAGE data significantly outperform baselines in terms of success rate and generalization across diverse, cluttered scenarios.\n2\nRelated work\n2.1\nLLM-Generated Robotic Simulation Tasks and Training Data\nA growing line of work leverages LLMs to automatically construct robotic simulation tasks and training data, aiming to reduce the substantial human effort required for task design and data collection.\nWang et al. (\n2024a\n)\nintroduce an LLM-driven code generation pipeline that writes simulation environments, task specifications, and expert policies, enabling the automatic creation of rich task libraries for manipulation in synthetic environments. By combining goal-directed and exploratory task generation modes, GenSim scales from hand-designed benchmarks to a much larger set of diverse tasks and shows improved task-level generalization and sim-to-real transfer for learned policies.\nBuilding on this idea,\nHua et al. (\n2024\n)\npropose a more scalable framework that exploits coding LLMs with multi-modal and reasoning capabilities to create complex and realistic simulation tasks, including long-horizon manipulation with articulated objects. The framework first uses an LLM to propose tasks and generate executable task code, then employs planning and reinforcement learning solvers to automatically produce demonstrations at scale, and finally trains a language-conditioned policy architecture on the generated dataset. This pipeline can generate data for up to hundreds of articulated-object tasks and achieves strong zero-shot sim-to-real transfer and performance gains when co-trained with limited real-world data\n(Hua et al.,\n2024\n)\n.\nIn parallel,\nWang et al. (\n2024c\n)\ntreat foundation and generative models as generative simulators rather than direct controllers. They define a self-guided robotic agent that autonomously proposes new tasks, generates corresponding environments, and acquires diverse skills via generative simulation. The system continuously expands a training corpus that covers over a hundred tasks and demonstrates that such procedurally generated, model-driven simulations can rival or outperform human-authored datasets for multi-task robot learning.\nFor bimanual manipulation and digital-twin scenarios,\nMu et al. (\n2025\n)\nand\nChen et al. (\n2025b\n)\nintegrate LLMs into a generative digital-twin pipeline. Starting from single RGB images, RoboTwin\n(Mu et al.,\n2025\n)\nuses 3D generative models to create diverse object instances and employs LLM agents to synthesize task programs for dual-arm manipulation, yielding an aligned synthetic–real benchmark and scalable expert data generation. RoboTwin 2.0\n(Chen et al.,\n2025b\n)\nfurther extends this framework into a large-scale data generator and benchmark with over fifty tasks and hundreds of object categories, incorporating multimodal LLM-based program synthesis and extensive domain randomization to improve robustness of learned bimanual policies.\nBeyond fully LLM-coded pipelines, benchmark-focused efforts such as RoboCAS\n(Zheng et al.,\n2024\n)\nfocuses on complex object arrangement scenarios in robotic manipulation. RoboCAS defines a benchmark for long-horizon manipulation in cluttered arrangement scenes and uses flexible scripted policies to collect diverse demonstrations across scattered, ordered, and stacked configurations. It highlights the importance of compositional scene and task complexity for evaluating generalist manipulation policies and foundation models.\nOverall, existing work shows that LLMs can significantly reduce human effort in designing simulation tasks and generating demonstrations, while improving task diversity, long-horizon complexity, and sim-to-real transfer.\n2.2\nLLMs for Long-Horizon Robotic Task Planning\nA complementary line of research employs LLMs as high-level planners for long-horizon robotic tasks. PaLM-SayCan\n(Ahn et al.,\n2022a\n)\ngrounds a large language model in a discrete set of pre-trained skills, combining the LLM’s estimate of which skill is useful with value functions that predict whether a skill will succeed in the current state. This affordance-grounded formulation enables a mobile manipulator to execute abstract multi-step instructions in real environments while maintaining feasibility and safety. DELTA\n(Liu et al.,\n2024b\n)\nuses 3D scene graphs as structured environment representations and leverages LLMs to (i) generate formal planning domain and problem specifications and (ii) decompose long-term goals into a sequence of sub-goals, which are then solved autoregressively by a classical task planner. By coupling scene-graph reasoning with LLM-based decomposition, DELTA achieves higher success rates and lower planning time on large-scale, long-term navigation and manipulation tasks.\nMore recently, RoboHorizon\n(Chen et al.,\n2025c\n)\nproposes an LLM-assisted multi-view world model for long-horizon manipulation, instantiated within a Recognize–Sense–Plan–Act pipeline. In RoboHorizon, an LLM generates dense reward structures over multi-stage sub-tasks from language instructions, and keyframe discovery is integrated into a visual world model to better capture critical steps in long-horizon processes, leading to substantial improvements over prior model-based RL baselines on RLBench and FurnitureBench. Other works explore LLMs for translating natural-language task descriptions into formal constraints or planning specifications for task and motion planning (TAMP)\n(Guo et al.,\n2025\n)\n, further extending LLM-based planning to larger action spaces and more complex constraints.\nBeyond robotics,\nMeyerson et al. (\n2025\n)\npropose MAKER, an instance of a massively decomposed agentic process (MDAP), which solves a Towers-of-Hanoi task with over one million LLM steps and zero errors by combining extreme subtask decomposition, subtask-level voting, and red-flag-based error filtering. Their results demonstrate that scaling long-horizon LLM execution can come not only from stronger base models, but also from systematic decomposition and fine-grained error correction at the agentic level.\nThese approaches largely position LLMs as high-level planners that output symbolic sub-goals, skill sequences, or formal planning problems to be executed by separate low-level controllers.\n2.3\nVLM-Driven Manipulation and Planning\nRecent works have significantly advanced the integration of LLMs and VLMs into robotic decision-making. VILA\n(Lin et al.,\n2024\n)\ndemonstrates the efficacy of pre-training VLMs on massive video-text data to unlock strong reasoning capabilities for long-horizon planning tasks. Similarly, OmniManip\n(Team Octo et al.,\n2023\n)\nleverages large-scale datasets to train generalist policies capable of performing diverse manipulation skills across varied environments. To address the grounding problem in these high-level plans, MOKA\n(Liu et al.,\n2024a\n)\nintroduces a mark-based visual prompting framework, utilizing VLMs to generate affine transformations and grasp poses by “marking” keypoints on target objects.\nHowever, these approaches primarily focus on inference-time reasoning or policy learning assuming the existence of data. They do not explicitly address the scalability bottleneck of acquiring high-quality, long-horizon demonstration data from scratch. While VILA and MOKA improve how a robot acts, V-CAGE focuses on synthesizing the training data itself, using VLMs not just as planners, but as rigorous automated verifiers.\n2.4\nScalable Synthetic Data Generation\nTo overcome the scarcity of real-world data, generative simulation has emerged as a promising paradigm. GenManip\n(Wang et al.,\n2024b\n)\npioneered the use of LLMs to automatically generate simulation environments and task codes, scaling data collection beyond manual engineering. These pipelines typically follow an open-loop “generate-and-execute” protocol.\nIn contrast, V-CAGE identifies a critical limitation in prior generative works: the lack of closed-loop verification and spatial awareness. Naive generation often results in “silent failures”—where generated code executes without errors but fails to achieve the semantic goal—or geometric conflicts in cluttered scenes. V-CAGE advances this domain by introducing a context-aware instantiation mechanism to handle dynamic spatial constraints and a VLM-guided rejection sampling loop. This ensures that every trajectory added to the dataset is not only generated but strictly validated for physical and semantic correctness.\nFigure 1:\nOverview of the V-CAGE framework. A high-level instruction is decomposed by an LLM into subtasks. During simulation, the VLM Gemini3\n(Gemini Team, Google,\n2025\n)\nacts as a visual critic after each step, verifying semantic success based on the post-execution image. If a failure is detected, the trajectory generation is aborted to ensure high-fidelity data.\n3\nMethod\nWe introduce V-CAGE (VLM-Guided Context-Aware Generation for Embodied Planning), a closed-loop framework for synthesizing high-fidelity manipulation datasets. The pipeline consists of three hierarchical modules: (1) Semantic Instruction Grounding via LLMs, (2) Context-Aware Scene Instantiation to resolve geometric constraints, and (3) VLM-Guided Rejection Sampling to ensure causal task success.\n3.1\nProblem Formulation\nOur goal is to generate a dataset of successful long-horizon trajectories\n𝒟\n=\n{\nτ\ni\n}\ni\n=\n1\nN\n\\mathcal{D}=\\{\\tau_{i}\\}_{i=1}^{N}\nconditioned on high-level user instructions\nI\nu\n​\ns\n​\ne\n​\nr\nI_{user}\n. A trajectory\nτ\n\\tau\nconsists of a sequence of state-action pairs satisfying a sequence of subgoals.\nThe core challenge is that the joint probability of a successful trajectory\nP\n​\n(\nτ\n|\nI\nu\n​\ns\n​\ne\n​\nr\n)\nP(\\tau|I_{user})\nfactorizes into both physical feasibility\nP\np\n​\nh\n​\ny\n​\ns\n​\n(\nτ\n)\nP_{phys}(\\tau)\nand semantic correctness\nP\ns\n​\ne\n​\nm\n​\n(\nτ\n)\nP_{sem}(\\tau)\n. Naive generation maximizes neither; V-CAGE explicitly optimizes both via geometric and visual constraints.\n3.2\nSemantic Instruction Grounding\nGiven an abstract instruction\nI\nu\n​\ns\n​\ne\n​\nr\nI_{user}\n(e.g., “Get ready for work”), we employ a Language Model, Pangu\n(Chen et al.,\n2025a\n)\n, as a high-level planner to decompose the intent into a sequence of executable subtasks\n𝒯\n=\n{\nT\n1\n,\nT\n2\n,\n…\n,\nT\nk\n}\n\\mathcal{T}=\\{T_{1},T_{2},\\dots,T_{k}\\}\n.\nEach subtask\nT\ni\nT_{i}\nis mapped to a parameterized motion primitive (e.g.,\nturn_switch\n,\nplace_object\n). This decomposition transforms the long-horizon problem into a series of short-horizon grounding problems.\n3.3\nVLM-Guided Rejection Sampling\nEven with valid geometry, execution artifacts (e.g., gripper slip, switch not toggled) can lead to semantic failures. To filter these “silent failures,” we model data generation as a Rejection Sampling process using a Vision-Language Model, Gemini3\n(Gemini Team, Google,\n2025\n)\n, as the density estimator.\nLet\nϕ\nV\n​\nL\n​\nM\n​\n(\nI\ni\n​\nm\n​\ng\n,\nT\ni\n)\n→\n{\n0\n,\n1\n}\n\\phi_{VLM}(I_{img},T_{i})\\to\\{0,1\\}\nbe the verification function, where\nI\ni\n​\nm\n​\ng\nI_{img}\nis the post-execution image and\nT\ni\nT_{i}\nis the subtask description. The VLM acts as a visual critic. For a sequence of tasks\nT\n1\n:\nk\nT_{1:k}\n, a trajectory is accepted into the dataset only if:\n∏\ni\n=\n1\nk\nϕ\nV\n​\nL\n​\nM\n​\n(\nI\ni\n​\nm\n​\ng\n(\ni\n)\n,\nT\ni\n)\n=\n1\n\\prod_{i=1}^{k}\\phi_{VLM}(I_{img}^{(i)},T_{i})=1\n(1)\nIf\nϕ\nV\n​\nL\n​\nM\n​\n(\n⋅\n)\n=\n0\n\\phi_{VLM}(\\cdot)=0\nat any step, the generation episode is aborted, and the scene is reset (or re-sampled). This rigorous filtering ensures that the final dataset\n𝒟\n\\mathcal{D}\ncontains only trajectories that are effectively causally valid, bridging the gap between code execution and visual reality.\n3.4\nGenerative Scene and Task Synthesis Pipeline\nFigure 2:\nThe proposed pipeline for autonomous scene generation and task verification. It bridges high-level semantic planning with low-level physical simulation through visual rearrangement and iterative refinement.\nTo bridge the gap between abstract natural language instructions and executable robotic skills, we propose a five-stage generative pipeline. This framework synthesizes physically valid, semantically rich scenes and validates corresponding manipulation subtasks within a simulation environment.\nThe process initiates with\nLanguage-Guided Asset Retrieval and Scene Planning\n. Given a high-level user prompt (e.g., “organize the breakfast table”), a Large Language Model (LLM) acts as a semantic planner. It parses the instruction to retrieve a set of compatible object assets\n𝒪\n=\n{\no\n1\n,\n…\n,\no\nn\n}\n\\mathcal{O}=\\{o_{1},\\ldots,o_{n}\\}\nfrom our annotated database, utilizing metadata such as object semantics and grasp affordances. Simultaneously, the LLM generates a detailed natural language scene description\n𝒟\ns\n​\nc\n​\ne\n​\nn\n​\ne\n\\mathcal{D}_{scene}\nthat explicitly specifies spatial constraints and layout logic (e.g., “place the bread inside the wooden box”).\nTo realize this description visually, we employ a\nVisual-Conditioned Rearrangement\nstrategy. The selected objects are first instantiated in the simulation via a randomized collision-free scattering to produce an initial top-down orthographic view\nI\ni\n​\nn\n​\ni\n​\nt\nI_{init}\n. This image, along with the generated description\n𝒟\ns\n​\nc\n​\ne\n​\nn\n​\ne\n\\mathcal{D}_{scene}\n, serves as input to an image-inpainting model (NanoBanana-Pro). The model hallucinates a goal configuration\nI\ng\n​\no\n​\na\n​\nl\n=\nℳ\n​\n(\nI\ni\n​\nn\n​\ni\n​\nt\n,\n𝒟\ns\n​\nc\n​\ne\n​\nn\n​\ne\n)\nI_{goal}=\\mathcal{M}(I_{init},\\mathcal{D}_{scene})\nthat respects the semantic spatial relations described in the text while maintaining visual consistency with the assets.\nSubsequently, we perform\nScene Refinement and Grounding\n. We extract the 2D pixel coordinates of objects from\nI\ng\n​\no\n​\na\n​\nl\nI_{goal}\nvia template matching against\nI\ni\n​\nn\n​\ni\n​\nt\nI_{init}\nand project them back into the 3D simulation space. To ensure physical plausibility, these raw coordinates are passed to a Refine Agent. The agent optimizes the final pose configuration\n𝐏\n∗\n\\mathbf{P}^{*}\nby minimizing a cost function that penalizes object collisions and deviations from the semantic description:\n𝐏\n∗\n=\narg\n⁡\nmin\n𝐏\n(\nℒ\nc\n​\no\n​\nl\n​\nl\n​\ni\n​\ns\n​\ni\n​\no\n​\nn\n​\n(\n𝐏\n)\n+\nλ\n​\nℒ\ns\n​\ne\n​\nm\n​\na\n​\nn\n​\nt\n​\ni\n​\nc\n​\n(\n𝐏\n,\n𝒟\ns\n​\nc\n​\ne\n​\nn\n​\ne\n)\n)\n\\mathbf{P}^{*}=\\mathop{\\arg\\min}_{\\mathbf{P}}\\left(\\mathcal{L}_{collision}(\\mathbf{P})+\\lambda\\mathcal{L}_{semantic}(\\mathbf{P},\\mathcal{D}_{scene})\\right)\n(2)\nwhere\n𝐏\n\\mathbf{P}\nrepresents the set of 6-DoF poses for all objects. This step corrects artifacts introduced by the image generation process, ensuring the scene is physically stable.\nUpon stabilizing the scene, the system proceeds to\nSubtask Code Generation\n. We define a library of parameterized skill templates (e.g.,\npick_and_place(obj, container)\n). The LLM identifies actionable relationships within the scene—specifically pairing objects with valid grasp annotations to containers with placement annotations—and instantiates executable Python code for each potential subtask\nT\ni\nT_{i}\n.\nFinally, we enforce a\nSimulation-Based Verification\nmechanism. Each generated subtask\nT\ni\nT_{i}\nis executed in the physics simulator. We measure the success rate\nS\n​\nR\n​\n(\nT\ni\n)\nSR(T_{i})\nover multiple trials. A subtask is deemed valid and committed to the subtask database only if it satisfies a robustness threshold\nτ\n\\tau\n:\nStatus\n​\n(\nT\ni\n)\n=\n{\nAccept\nif\n​\nS\n​\nR\n​\n(\nT\ni\n)\n>\n0.5\nReject\notherwise\n\\text{Status}(T_{i})=\\begin{cases}\\text{Accept}&\\text{if }SR(T_{i})>0.5\\\\\n\\text{Reject}&\\text{otherwise}\\end{cases}\n(3)\nThis closed-loop verification ensures that the generated data is not only visually plausible but functionally actionable for downstream policy learning.\nFigure 3:\nExample scenes generated by V-CAGE\n4\nExperiments\nIn this section, we empirically evaluate the effectiveness of the V-CAGE framework. Our experiments are designed to answer two primary questions: (1) Does incorporating context-aware geometric constraints and VLM-based verification improve the quality of synthetic training data? (2) How does this data quality translate to the performance of downstream policies in diverse, long-horizon manipulation tasks?\n4.1\nExperimental Setup\nWe evaluate our method on a suite of 35 diverse manipulation tasks derived from the RoboTwin benchmark\n(Mu et al.,\n2025\n)\n. These tasks range from simple object rearrangement to complex, multi-stage activities such as table setting and semantic sorting in cluttered environments.\nWe compare our full pipeline (\nV-CAGE\n) against a strong baseline (\nVanilla\n), which employs the same LLM planner and code generation module but lacks the closed-loop VLM verification and dynamic context-aware instantiation. In the Vanilla setting, success is determined solely by the absence of execution errors (e.g., Python exceptions or collision crashes) during simulation, a standard proxy used in prior open-loop generation works.\n4.2\nMain Results\nWe trained diffusion-based manipulation policies using datasets generated by both V-CAGE and the Vanilla baseline. We evaluate the policy performance using three metrics:\nAverage Success Rate\nacross all tasks, and the\nTop-K Success Rate\n(for\nK\n=\n5\nK=5\nand\n10\n10\n), which measures the reliability of the best-performing checkpoints.\nThe quantitative results are summarized in Table\n1\n. V-CAGE achieves a significant improvement over the baseline across all metrics. Notably, the\nAverage Success Rate\nincreases from 46.86% to 64.58%, a substantial gain of +17.72%. This performance gap highlights the superior quality of the data synthesized by our framework.\nTable 1:\nPerformance Comparison on Downstream Manipulation Tasks.\nWe report the success rates averaged over 35 long-horizon tasks from the RoboTwin benchmark. The ”Vanilla” baseline relies on open-loop generation, while ”V-CAGE” (Ours) integrates VLM-based semantic verification. The gap in Top-10 performance highlights the stability provided by our method.\nMetric\nV-CAGE (Ours)\nVanilla\nAverage Success Rate\n64.58%\n46.86%\nTop 5 Success Rate\n100.00%\n92.00%\nTop 10 Success Rate\n100.00%\n77.00%\n4.3\nThe Critical Role of VLM Verification\nThe most revealing insight from Table\n1\nlies in the disparity between the Top-K metrics.\nWhile the Vanilla baseline achieves a respectable 92% in the Top-5 metric, its performance drops precipitously to 77% for the Top-10. In stark contrast, V-CAGE maintains a\n100% success rate\nfor both Top-5 and Top-10.\nThis stability attests to the critical role of the VLM as a semantic filter. In the Vanilla pipeline, trajectories are labeled as ”successful” as long as the generated code executes without throwing system exceptions. However, this introduces a high volume of\nsilent failures\n—instances where the robot moves and the code terminates gracefully, but the semantic goal is not achieved (e.g., an object slips from the gripper, or a switch is missed by millimeters). These false positives introduce severe label noise into the training dataset, causing the downstream policy to learn incorrect associations between actions and outcomes.\nBy integrating the VLM verification loop, V-CAGE effectively performs rejection sampling on the\nsemantic\noutcome rather than just the\nprogrammatic\noutcome. The VLM acts as a rigorous visual critic, pruning these silent failures before they enter the training buffer. The fact that our Top-10 performance remains perfect (100%) indicates that the VLM ensures a much higher ”purity” of valid demonstrations, allowing the policy to learn robust, generalizable behaviors without being confused by noisy supervision. Consequently, VLM-based verification is not merely an auxiliary feature but a fundamental prerequisite for scaling up synthetic data generation for long-horizon tasks.\n5\nConclusion\nIn this work, we presented V-CAGE, a closed-loop framework for synthesizing high-fidelity, long-horizon manipulation datasets. Our context-aware instantiation mechanism eliminates physical conflicts in complex, multi-object scenes, while our VLM-guided rejection sampling pipeline rigorously filters out execution failures that plague unverified synthetic data.\nExperiments demonstrate the effectiveness of V-CAGE in improving policy robustness to cluttered environments, enabling generalization to unseen tasks. These findings underscore the critical importance of data quality over mere quantity in the era of generative robotics.\nLimitations and Future Work.\nWhile V-CAGE ensures high data fidelity, the rejection sampling process can be computationally expensive for highly complex tasks with low inherent success probabilities. Future work could explore using the VLM critic to provide dense reward signals for correcting failed trajectories in-the-loop, rather than discarding them, thereby improving generation efficiency. Additionally, extending V-CAGE to dynamic interactions involving deformable objects or fluids remains an exciting direction for future research.\nReferences\nAhn et al. (2022a)\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., et al.\nDo as i can, not as i say: Grounding language in robotic affordances.\narXiv preprint arXiv:2204.01691\n, 2022a.\nAhn et al. (2022b)\nAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., et al.\nDo as i can, not as i say: Grounding language in robotic affordances.\nIn\nConference on Robot Learning (CoRL)\n, 2022b.\nBrohan et al. (2023)\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., et al.\nRt-1: Robotics transformer for real-world control at scale.\nIn\nRobotics: Science and Systems (RSS)\n, 2023.\nChen et al. (2025a)\nChen, H., Wang, Y., Han, K., Li, D., Li, L., Bi, Z., Li, J., Wang, H., Mi, F., Zhu, M., Wang, B., Song, K., Nie, Y., Wu, X., He, W., Hu, H., Tang, Y., Tao, D., Chen, X., and Wang, Y.\nPangu embedded: An efficient dual-system llm reasoner with metacognition.\narXiv preprint arXiv:2505.22375\n, 2025a.\nURL\nhttps://arxiv.org/abs/2505.22375\n.\nChen et al. (2025b)\nChen, T., Chen, Z., Chen, B., Cai, Z., Liu, Y., Li, Z., Liang, Q., Lin, X., Ge, Y., Gu, Z., Guo, Y., Nian, T., Xie, X., Chen, Q., Su, K., Xu, T., Liu, G., Hu, M., Gao, H., Wang, K., Liang, Z., Qin, Y., Yang, X., Luo, P., and Mu, Y.\nRoboTwin 2.0: A scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation.\narXiv preprint arXiv:2506.18088\n, 2025b.\nChen et al. (2025c)\nChen, Z., Huo, J., Chen, Y., and Gao, Y.\nRobohorizon: An LLM-assisted multi-view world model for long-horizon robotic manipulation.\narXiv preprint arXiv:2501.06605\n, 2025c.\nde Haan et al. (2019)\nde Haan, P., Jayaraman, D., and Levine, S.\nCausal confusion in imitation learning.\nIn\nAdvances in Neural Information Processing Systems (NeurIPS)\n, 2019.\nDriess et al. (2023)\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., et al.\nPalm-e: An embodied multimodal language model.\nIn\nInternational Conference on Machine Learning (ICML)\n, 2023.\nGemini Team, Google (2025)\nGemini Team, Google.\nGemini 3: Frontier multimodal intelligence.\narXiv preprint\n, 2025.\nURL\nhttps://deepmind.google/technologies/gemini/\n.\nTechnical Report.\nGuo et al. (2025)\nGuo, W., Kingston, Z., and Kavraki, L. E.\nCaStL: Constraints as specifications through LLM translation for long-horizon task and motion planning.\nIn\nProceedings of the IEEE International Conference on Robotics and Automation (ICRA)\n, pp.  11957–11964, 2025.\ndoi:\n10.1109/ICRA55743.2025.11127555\n.\nHua et al. (2024)\nHua, P., Liu, M., Macaluso, A., Lin, Y., Zhang, W., Xu, H., and Wang, L.\nGensim2: Scaling robot data generation with multi-modal and reasoning LLMs.\nIn\nProceedings of the Conference on Robot Learning (CoRL)\n, 2024.\nCoRL 2024.\nLangley (2000)\nLangley, P.\nCrafting papers on machine learning.\nIn Langley, P. (ed.),\nProceedings of the 17th International Conference on Machine Learning (ICML 2000)\n, pp.  1207–1216, Stanford, CA, 2000. Morgan Kaufmann.\nLin et al. (2024)\nLin, J., Yin, H., Ping, W., Lu, Y., Molchanov, Pavlo andlob, A., Han, S., and Alvarez, J. M.\nVila: On pre-training for visual language models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, 2024.\nLiu et al. (2024a)\nLiu, F., Lin, K., Yan, H., Yi, L., Abbeel, P., and Gao, Y.\nMoka: Open-vocabulary robotic manipulation through mark-based visual prompting.\narXiv preprint arXiv:2403.03174\n, 2024a.\nLiu et al. (2024b)\nLiu, Y., Palmieri, L., Koch, S., Georgievski, I., and Aiello, M.\nDELTA: Decomposed efficient long-term robot task planning using large language models.\narXiv preprint arXiv:2404.03275\n, 2024b.\nMandlekar et al. (2023)\nMandlekar, A., Nasiriany, S., Wen, B., Akinola, I., and Zhu, Y.\nMimicgen: A data generation system for scalable robot learning using human demonstrations.\nIn\nConference on Robot Learning (CoRL)\n, 2023.\nMeyerson et al. (2025)\nMeyerson, E., Paolo, G., Dailey, R., Shahrzad, H., Francon, O., Hayes, C. F., Qiu, X., Hodjat, B., and Miikkulainen, R.\nSolving a million-step LLM task with zero errors.\narXiv preprint arXiv:2511.09030\n, 2025.\nMu et al. (2025)\nMu, Y., Chen, T., Chen, Z., Peng, S., Lan, Z., Gao, Z., Liang, Z., Yu, Q., Zou, Y., Xu, M., Lin, L., Xie, Z., Ding, M., and Luo, P.\nRoboTwin: Dual-arm robot benchmark with generative digital twins.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference (CVPR)\n, pp.  27649–27660, June 2025.\nTeam Octo et al. (2023)\nTeam Octo, Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Quureshi, S., He, T., et al.\nOcto: An open-source generalist robot policy.\narXiv preprint arXiv:2305.10455\n, 2023.\nWang et al. (2024a)\nWang, L., Ling, Y., Yuan, Z., Shridhar, M., Bao, C., Qin, Y., Wang, B., Xu, H., and Wang, X.\nGensim: Generating robotic simulation tasks via large language models.\nIn\nProceedings of the International Conference on Learning Representations (ICLR)\n, 2024a.\nICLR 2024.\nWang et al. (2024b)\nWang, L., Zhao, Y., Xu, Y., Feng, T., Jiang, Y., Xu, D., Wong, J., and Lim, J. J.\nGensim: Generating robotic simulation tasks via large language models.\nIn\nInternational Conference on Learning Representations (ICLR)\n, 2024b.\nWang et al. (2024c)\nWang, Y., Xian, Z., Chen, F., Wang, T.-H., Wang, Y., Fragkiadaki, K., Erickson, Z., Held, D., and Gan, C.\nRobogen: Towards unleashing infinite data for automated robot learning via generative simulation.\nIn\nProceedings of the 41st International Conference on Machine Learning (ICML)\n, 2024c.\nURL\nhttps://arxiv.org/abs/2311.01455\n.\nZheng et al. (2024)\nZheng, L., Yan, F., Liu, F., Feng, C., Kang, Z., and Ma, L.\nRoboCAS: A benchmark for robotic manipulation in complex object arrangement scenarios.\nIn\nNeurIPS Datasets and Benchmarks Track\n, 2024.\nAppendix A\nAppendix A: Pangu-7B Supervised Fine-Tuning for Motion Planning\nIn this section, we detail the Supervised Fine-Tuning (SFT) process of the Pangu-7B model, which is designed to synthesize motion planning code for sub-tasks within the V-CAGE task chain. Our goal is to enable the model to generate correct and robust low-level control policies based on high-level instructions.\nA.1\nData Collection and Augmentation\nTo construct a high-quality SFT dataset, we leveraged expert data from the RoboTwin benchmark. Due to the limited number of original expert demonstrations, we employed a data augmentation strategy using the DeepSeek model.\nSpecifically, we started with 50 unique expert motion planning scripts from RoboTwin. We prompted DeepSeek to rewrite these scripts, generating functionally equivalent code with diverse logic structures and variable naming conventions while ensuring correctness. This data augmentation process resulted in a curated dataset of approximately 900 instruction-code pairs, which served as the foundation for our SFT stage.\nA.2\nFine-Tuning and Evaluation\nWe fine-tuned the Pangu-7B base model using the generated dataset to adapt it to the specific API calls and physical constraints of the simulation environment.\nTo verify the effectiveness of our approach, we evaluated the fine-tuned model (Ours-Pangu-SFT) against the original pre-trained model (Pangu-Base) on a suite of RoboTwin validation tasks. For each task, we conducted 10 independent trials. We report two metrics: the Average Success Rate (Avg SR) across all trials, and the Top-3 Success Rate (Top3 SR), which reflects the model’s peak performance capability.\nA.3\nResults\nThe quantitative results are presented in Table\n2\n.\nThe Pangu-Base model failed to complete any of the tasks (0.0% success rate), highlighting the difficulty of the motion planning format without specific adaptation. In contrast, the Pangu-SFT model demonstrated significant improvements, achieving high success rates on various complex manipulation tasks such as\nplace_mouse_pad\n(100% Avg SR) and\ngrab_roller\n(90% Avg SR). The results confirm that fine-tuning on the DeepSeek-augmented expert data successfully enables Pangu-7B to master the intricate motion planning logic required for the V-CAGE tasks.\nTable 2:\nComparison of Success Rates between Pangu-SFT and Pangu-Base on RoboTwin Tasks. ’Avg SR’ denotes the average success rate over 10 runs, and ’Top3 SR’ denotes the average of the top 3 runs.\nTask Name\nPangu-SFT\nPangu-Base\nAvg SR\nTop3 SR\nAvg SR\nTop3 SR\nadjust_bottle\n74.0%\n96.7%\n0.0%\n0.0%\nbeat_block_hammer\n59.0%\n100.0%\n0.0%\n0.0%\nblocks_ranking_rgb\n60.0%\n100.0%\n0.0%\n0.0%\nblocks_ranking_size\n12.0%\n40.0%\n0.0%\n0.0%\nclick_alarmclock\n56.0%\n90.0%\n0.0%\n0.0%\nclick_bell\n10.0%\n33.3%\n0.0%\n0.0%\ngrab_roller\n90.0%\n100.0%\n0.0%\n0.0%\nhanging_mug\n6.0%\n20.0%\n0.0%\n0.0%\nlift_pot\n10.0%\n33.3%\n0.0%\n0.0%\nmove_and_press_stapler\n9.0%\n30.0%\n0.0%\n0.0%\nmove_can_pot\n19.0%\n60.0%\n0.0%\n0.0%\nmove_pillbottle_pad\n7.0%\n23.3%\n0.0%\n0.0%\nmove_playingcard_away\n59.0%\n100.0%\n0.0%\n0.0%\nmove_stapler_pad\n65.0%\n83.3%\n0.0%\n0.0%\nplace_a2b_right\n45.0%\n80.0%\n0.0%\n0.0%\nplace_bread_basket\n3.0%\n10.0%\n0.0%\n0.0%\nplace_dual_shoes\n27.0%\n73.3%\n0.0%\n0.0%\nplace_empty_cup\n10.0%\n33.3%\n0.0%\n0.0%\nplace_fan\n78.0%\n90.0%\n0.0%\n0.0%\nplace_mouse_pad\n100.0%\n100.0%\n0.0%\n0.0%\nplace_object_stand\n27.0%\n90.0%\n0.0%\n0.0%\nplace_phone_stand\n67.0%\n96.7%\n0.0%\n0.0%\npress_stapler\n63.0%\n90.0%\n0.0%\n0.0%\nrotate_qrcode\n85.0%\n90.0%\n0.0%\n0.0%\nstack_blocks_two\n30.0%\n100.0%\n0.0%\n0.0%\nstack_bowls_three\n18.0%\n46.7%\n0.0%\n0.0%\nstamp_seal\n27.0%\n60.0%\n0.0%\n0.0%",
    "preview_text": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.\n\nV-CAGE: Context-Aware Generation and Verification\nfor Scalable Long-Horizon Embodied Tasks\nYaru Liu\nAobo Wang\nNanyang Ye\nAbstract\nLearning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently “succeed” without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduc",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLM",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "V-CAGE是一个用于生成和验证长时程具身任务数据集的闭环框架，通过上下文感知的场景合成和基于VLM的语义验证来提高物理和语义保真度。",
    "detailed_summary": "## 论文《V-CAGE：面向可扩展长视野具身任务的情境感知生成与验证》详细摘要\n\n**1. 研究背景和动机**\n在具身人工智能领域，从合成数据中学习长视野的机器人操作行为面临重大挑战。现有方法生成的场景常存在物理不合理性，语言驱动的程序可能在未满足任务语义的情况下“成功”执行，且高层指令难以精确落地为可执行动作序列。这些“质量鸿沟”导致下游策略性能受限。因此，亟需一种能够大规模生成高质量、高保真度操作轨迹数据的方法。\n\n**2. 核心方法和技术创新**\n本文提出了 **V-CAGE**，一个用于合成鲁棒、语义对齐操作数据集的闭环框架。其核心创新在于：\n- **情境感知实例化机制**：在场景合成过程中，通过动态维护一个“禁止空间区域”地图，确保物体放置的几何一致性，防止穿透并保证在杂乱环境中的可达、无冲突配置。\n- **分层指令模块**：利用大语言模型将高层目标分解为组合式动作基元，实现连贯的长视野规划。\n- **基于视觉语言模型的验证循环**：将数据生成建模为**拒绝采样**过程，使用VLM作为视觉评判员，在每个子任务执行后严格验证视觉结果，过滤掉代码执行成功但语义目标未达成的“静默失败”。\n\n**3. 主要实验结果**\n在基于RoboTwin基准的35个多样化长视野操作任务上进行了评估。与缺乏闭环验证的基线方法相比，V-CAGE取得了显著提升：\n- **平均成功率**从46.86%提升至**64.58%**。\n- **Top-10成功率**达到**100%**，而基线仅为77%，证明了V-CAGE生成的数据具有极高的纯净度和稳定性，有效消除了训练数据中的标签噪声。\n\n**4. 研究意义和价值**\nV-CAGE为解决合成数据生成中的物理一致性与语义对齐难题提供了系统性的闭环解决方案。它强调了在生成式机器人学中，**数据质量优先于单纯数量**的核心原则。该框架能够高效生成高保真度演示数据，显著提升下游策略的成功率与泛化能力，为大规模、可扩展的机器人技能学习奠定了坚实基础。未来工作可探索利用VLM提供密集奖励以提升生成效率，并扩展至涉及可变形物体等更复杂的动态交互任务。",
    "qa_pairs": [
        {
            "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
            "answer": "## V-CAGE 论文分析\n\n### **核心问题**\n论文旨在解决**从合成数据中学习长时程具身行为**的关键瓶颈。具体表现为三个核心挑战：\n1.  **几何不一致性**：传统程序化生成在放置物体时，不考虑场景的动态演变，导致物体相互穿透、姿态冲突或不稳定，产生物理上不可信的合成场景。\n2.  **语义错位**：语言驱动的代码生成可能产生“静默失败”——代码能无错执行，但未能实现语义目标（如开关未真正拨动、物体未放到指定位置）。用这种“假阳性”数据训练会引入噪声，严重损害智能体对因果关系的理解。\n3.  **高层指令到可执行动作的鸿沟**：抽象的用户指令（如“准备工作”）需要被有效地分解和落地为一系列可执行的低层动作。\n\n### **核心创新点**\n论文提出了 **V-CAGE** 框架，这是一个**闭环的、集成了几何约束与视觉验证**的合成数据生成框架。其创新性体现在三个层次的系统性整合：\n\n1.  **上下文感知的场景实例化机制**\n    - **创新**：提出动态维护“禁止体积”地图。在场景生成过程中，每当一个物体被放置后，系统会实时更新这张地图，标记出已被占据或不可达的空间区域。\n    - **解决方式**：后续物体的放置必须避开这些“禁止体积”，从而在杂乱场景中自动解决“装箱问题”，确保所有物体配置都是**无碰撞、可达且物理稳定**的。这从根本上解决了几何不一致性问题。\n\n2.  **基于VLM的拒绝采样验证循环**\n    - **创新**：将数据生成建模为一个**拒绝采样**问题，并引入**视觉语言模型作为语义评判官**。这超越了仅检查代码是否无错执行的“开环”验证。\n    - **解决方式**：在每个子任务执行后，VLM会分析执行后的场景图像，判断语义目标是否达成（`φ_VLM(I_img, T_i) → {0,1}`）。只有当所有子任务都通过VLM验证，整条轨迹才会被接受进入数据集。这有效过滤了“静默失败”，确保了数据的**语义保真度**。\n\n3.  **分层指令模块与闭环生成流程**\n    - **创新**：构建了一个将**高层规划（LLM）、几何约束（上下文感知实例化）和语义验证（VLM）** 紧密耦合的层级化、闭环流水线。\n    - **解决方式**：\n        - **规划层**：使用LLM（如Pangu）将抽象指令分解为可执行的子任务序列。\n        - **几何层**：应用上述上下文感知机制生成物理可行的场景。\n        - **验证层**：VLM对每个步骤进行视觉验收。任何步骤失败，则整个生成过程回滚或重新采样。\n    - 这种设计将数据合成视为一个**严格的优化过程**，主动剪除无效轨迹，确保只有高质量数据进入训练缓冲区。\n\n### **实际价值与技术贡献**\n- **数据质量 > 数据数量**：论文实证表明，通过严格的几何和语义验证生成的高保真数据，能**显著提升下游策略的成功率和泛化能力**（平均成功率从46.86%提升至64.58%）。\n- **解决规模化瓶颈**：为获取大规模、高质量的长时程机器人演示数据提供了一种可扩展的自动化方案，减少了对昂贵、不安全且难以扩展的真实世界数据收集的依赖。\n- **方法论贡献**：提出了一个将大模型（LLM/VLM）用作**数据生成流程中的“构建者”和“质检员”** 的范式，而不仅仅是推理时的“规划者”。这为生成式机器人学习领域提供了新的技术路径。\n\n**总结**：V-CAGE的核心创新在于**通过动态空间约束和视觉语义验证构建了一个闭环、自校正的数据合成系统**，旨在系统性解决合成数据在物理真实性和语义对齐上的缺陷，从而为训练鲁棒的长时程具身智能体生成可靠的大规模数据。",
            "timestamp": "2026-01-27T16:01:35.407230",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
            "answer": "## 论文《V-CAGE》及其前作发展脉络分析\n\n本文提出的V-CAGE框架是**可扩展合成数据生成**与**长视野具身任务规划**交叉领域的最新进展。其核心在于通过**闭环验证**（VLM作为视觉评判者）和**上下文感知的几何约束**，解决合成数据中的“物理不一致性”和“语义失准”两大顽疾。以下梳理该方向的发展脉络，着重于几个关键的技术演进分支。\n\n### 发展脉络总览\n该领域的发展主线是从**开环、手工设计**的数据/任务生成，逐步走向**闭环、自动化、高保真**的合成管道，并深度融合LLM/VLM的语义理解与规划能力。\n\n---\n\n### 分支一：LLM驱动的仿真任务与数据生成（从开环到闭环）\n\n这一分支关注如何利用LLM自动生成仿真环境、任务代码和专家演示，以替代昂贵的人工设计。\n\n- **早期开环生成 (GenSim, Wang et al., 2024a/b)**：\n    - **核心改进**：首次系统性地利用LLM作为“代码生成器”，自动编写模拟环境、任务定义和脚本化策略，极大地扩展了任务库的规模和多样性。\n    - **局限性**：遵循“生成-执行”的开环模式。缺乏对生成结果的**物理可行性**和**语义正确性**的验证，导致数据集中混杂大量“无声失败”（代码无错但任务未完成）的轨迹。\n\n- **增强复杂性与规模 (GenSim2, Hua et al., 2024; RoboTwin系列, Mu et al., 2025; Chen et al., 2025b)**：\n    - **核心改进**：\n        1.  **任务复杂度提升**：处理包含**关节物体**的长视野操作、**双手操作**等更复杂的任务。\n        2.  **数据规模扩大**：通过结合规划与强化学习求解器，自动生成数百个任务的演示数据。\n        3.  **引入数字孪生**：从单张RGB图像生成3D资产和任务程序，构建对齐的仿真-现实基准。\n    - **局限性**：虽然规模与复杂度提升，但**验证机制依然薄弱**。成功标准通常仅是“代码无异常运行”，未能从根本上解决语义对齐问题。\n\n- **V-CAGE的突破性改进 (本文)**：\n    - **核心改进**：引入了**严格的闭环验证机制**，将数据生成视为**拒绝采样**问题。\n        1.  **VLM作为语义评判者**：在每个子任务执行后，使用VLM（如Gemini3）根据视觉结果判断语义成功与否（`φ_VLM(I_img, T_i)`）。失败则整条轨迹被丢弃。\n        2.  **确保数据“纯度”**：此举直接过滤掉“无声失败”，使下游策略学习到的动作-结果关联是干净、因果正确的。实验表明，这带来了**下游策略成功率的显著提升（+17.72%）** 和**性能稳定性（Top-10成功率100%）**。\n\n### 分支二：LLM用于长视野机器人任务规划（从符号规划到具身验证）\n\n这一分支关注如何利用LLM进行高层任务分解和规划，但早期工作与低层执行和验证脱节。\n\n- **技能 grounding (PaLM-SayCan, Ahn et al., 2022a)**：\n    - **核心改进**：将LLM的规划能力与预定义技能的可执行性（价值函数）相结合，使机器人能执行抽象的多步指令。\n    - **局限性**：依赖于一组有限的、预训练的技能，规划在符号层面进行，缺乏对执行过程中**动态物理交互**和**视觉结果**的实时评估。\n\n- **结构化环境表征与规划 (DELTA, Liu et al., 2024b; RoboHorizon, Chen et al., 2025c)**：\n    - **核心改进**：\n        1.  利用**3D场景图**等结构化表征，使LLM能进行更精确的空间推理和形式化问题描述。\n        2.  将LLM生成的子目标交给经典任务规划器求解，或用于构建密集奖励。\n    - **局限性**：仍然是**开环的前馈式规划**。规划基于初始状态假设，一旦执行中出现偏差（如物体滑落），无法及时检测和纠正。\n\n- **V-CAGE的突破性改进 (本文)**：\n    - **核心改进**：将**规划、执行、验证紧密耦合在一个闭环中**。\n        1.  **规划即生成**：LLM（如Pangu）将高级指令分解为可执行的子任务代码。\n        2.  **执行即验证**：每个子任务执行后立即进行VLM视觉验证。这相当于为**整个长视野规划链**增加了**实时的事实检查点**。\n        3.  **从“规划器”到“数据生成管道的核心组件”**：V-CAGE中的LLM/VLM不仅用于单次任务的在线规划，更是用于**批量生成高质量训练数据**的关键模块，其验证功能确保了数据的可靠性。\n\n### 分支三：可扩展合成数据生成（从忽略物理到上下文感知）\n\n这一分支关注合成数据生成本身的**可扩展性瓶颈**，即如何自动生成大量且**物理合理**的场景和轨迹。\n\n- **早期生成方法**：\n    - **问题**：标准程序化生成在放置物体时忽略场景的动态演变，在杂乱环境中容易产生物体穿透、姿态冲突、不可达配置等问题，导致模拟器崩溃或不真实数据。\n\n- **V-CAGE的突破性改进 (本文)**：\n    - **核心改进**：提出了**上下文感知实例化机制**。\n        1.  **动态禁止体积图**：在场景生成过程中，随着每个物体的放置，动态维护一张“禁止空间”地图。\n        2.  **解决杂乱场景下的“打包问题”**：确保新物体只被放置在**无碰撞、可达、符合语义**的可行区域。这从根源上提升了生成场景的**物理保真度**，与VLM的语义验证形成互补（一个管物理，一个管语义）。\n\n### 总结：V-CAGE的核心贡献与方向演进\n\n| 发展方向 | 前作的核心进展 | V-CAGE 的关键改进 |\n| :--- | :--- | :--- |\n| **数据生成范式** | **开环生成**：LLM生成代码并执行，以“无运行错误”为成功标准。 | **闭环验证**：引入VLM作为视觉评判者进行**拒绝采样**，确保数据语义正确性。 |\n| **规划与执行** | **脱节规划**：LLM进行高层符号规划，与低层执行和物理验证脱钩。 | **紧耦合闭环**：规划（LLM）、执行（模拟器）、验证（VLM）形成闭环，实时纠正语义偏差。 |\n| **物理合理性** | **忽视或简单处理**：物体放置常导致物理冲突。 | **上下文感知**：通过动态禁止体积图，强制执行几何一致性，解决杂乱场景下的物体放置问题。 |\n| **实际价值** | 扩大了任务和数据规模，但**数据质量噪声大**，限制了下游策略性能上限。 | **显著提升数据质量**，从而直接、大幅提升了下游策略的**成功率**和**泛化稳定性**，证明了“质量优于数量”在机器人学习中的重要性。 |\n\n**结论**：V-CAGE代表了该领域从追求**数据规模**向追求**数据质量**和**生成可靠性**的关键转变。它通过**VLM驱动的语义验证环**和**上下文感知的几何约束**这两个核心创新，系统性地解决了长期困扰合成数据生成的“无声失败”和物理不合理问题，为训练鲁棒、可泛化的长视野具身智能体提供了更可靠的数据基础。未来的工作可能围绕提高验证效率（如用VLM提供修正信号而非简单丢弃）和扩展到更复杂的物理交互（如可变形物体）展开。",
            "timestamp": "2026-01-27T16:02:38.808245",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
            "answer": "## V-CAGE 的前作分析\n\nV-CAGE 的研究建立在作者团队（特别是 Wang, Chen, Mu 等人）在 **LLM驱动的机器人仿真与数据生成** 这一系列工作的基础上。以下是其核心前作的详细梳理，以及 V-CAGE 与它们的区别和改进。\n\n### 前作概览表格\n\n| 论文名称 | 核心贡献 / 方法 | 与 V-CAGE 的关键区别 | V-CAGE 的主要改进 |\n| :--- | :--- | :--- | :--- |\n| **GenSim** (Wang et al., ICLR 2024) | **开环数据生成**：利用LLM自动生成仿真环境、任务代码和专家策略，创建大规模任务库。遵循“生成-执行”范式。 | 1. **开环 vs. 闭环**：GenSim是开环生成，缺乏执行后的验证。<br>2. **验证缺失**：成功标准仅为代码无运行时错误，无法检测“静默失败”。<br>3. **几何一致性**：未显式处理动态场景下的物体放置冲突。 | 1. **引入闭环验证**：增加了VLM引导的拒绝采样循环，对每个子任务的**视觉结果**进行语义验证，确保数据语义正确性。<br>2. **增加空间感知**：提出了**上下文感知实例化机制**，动态维护“禁止体积”地图，从根本上解决杂乱场景中的几何冲突问题。 |\n| **GenSim2** (Hua et al., CoRL 2024) | **扩展与规模化**：利用多模态和推理能力更强的编码LLM，生成包含铰接物体的复杂长视程任务。集成规划与RL求解器自动生成演示。 | 1. **验证粒度**：虽然可能使用求解器生成演示，但缺乏**细粒度、基于视觉的逐步骤语义验证**。<br>2. **失败处理**：侧重于生成和求解，对“代码执行成功但任务语义失败”的噪声数据过滤不足。 | 1. **精细化验证**：V-CAGE将验证提升到**子任务级别**，使用VLM作为“视觉评论家”，严格过滤语义未对齐的轨迹。<br>2. **数据纯度优先**：明确将数据合成视为**优化问题**，以牺牲部分生成功率为代价，追求最终数据集的**高保真度**。 |\n| **RoboTwin** (Mu et al., CVPR 2025) | **数字孪生与双手机器人**：从单张RGB图像出发，使用3D生成模型创建物体，LLM智能体合成双臂操作任务程序。构建了对齐的仿真-现实基准。 | 1. **生成焦点**：RoboTwin侧重于从图像到3D场景和任务的**生成流程**，是V-CAGE使用的**基准和数据来源之一**。<br>2. **验证环节**：其流程更关注生成对齐的仿真环境，对生成的动作序列本身的**物理和语义可靠性验证**不是核心。 | 1. **增强生成流程**：V-CAGE **复用并增强了RoboTwin的生成起点**，在其后增加了**强约束的实例化和验证模块**。<br>2. **解决“静默失败”**：RoboTwin生成的任务代码可能执行但失败，V-CAGE专门针对此问题设计了VLM验证环。 |\n| **RoboTwin 2.0** (Chen et al., 2025b) | **大规模数据生成与基准**：扩展为大规模数据生成器，包含超50个任务和数百个物体类别。引入多模态LLM程序合成和广泛的**领域随机化**以提高鲁棒性。 | 1. **方法论互补**：RoboTwin 2.0通过**领域随机化**增加数据多样性以提升鲁棒性，属于“广度”和“覆盖性”的扩展。<br>2. **数据质量保证**：其质量保证仍依赖于程序合成和随机化，缺乏对单个轨迹**语义正确性**的主动、确定性验证。 | 1. **质量保证深化**：V-CAGE通过**主动验证**来保证数据质量，属于“深度”和“精确性”的强化。两者可视为互补：RoboTwin 2.0生成大量候选数据，V-CAGE可对其执行严格过滤。<br>2. **验证驱动**：V-CAGE的核心创新在于将**VLM定位为验证器**，而非仅仅是规划器或生成器。 |\n\n---\n\n### 总结：V-CAGE 相对于前作的核心改动与改善\n\nV-CAGE 并非否定前作，而是针对已识别出的**两大根本性缺陷**进行系统性补强：\n\n1.  **从开环生成到闭环验证**\n    *   **前作模式**：LLM生成任务代码 -> 在仿真中执行 -> 若无崩溃或异常则视为成功数据。\n    *   **V-CAGE改动**：在上述流程中嵌入 **VLM验证环**。每个子任务执行后，对仿真画面进行VLM问答（如“开关是否已打开？”）。仅当所有步骤的视觉验证均通过，该轨迹才被收录。\n    *   **主要改善**：**彻底解决“静默失败”问题**。实验数据（Top-10成功率100% vs. 77%）强有力地证明，此验证能极大提升训练数据的“纯度”，使下游策略学习到真实的因果关联。\n\n2.  **从几何无意识到上下文感知**\n    *   **前作局限**：在杂乱场景中顺序放置物体时，容易产生穿透、不可达或不稳定配置。\n    *   **V-CAGE改动**：提出**上下文感知实例化机制**。系统动态维护一个“禁止体积”地图，每当一个物体被放置，其占据的空间就被标记为禁止。后续物体只在剩余可行空间内采样位置。\n    *   **主要改善**：**保证了生成场景的物理合理性**。这不仅避免了仿真器崩溃，更重要的是生成了符合真实物理约束（如可操作空间）的数据，使训练出的策略更能适应现实世界的 clutter。\n\n3.  **研究重心转移：从“生成能力”到“数据质量”**\n    *   前作系列（GenSim, RoboTwin）的核心贡献是证明了**LLM能够自动化、规模化地生成仿真任务和数据**，解决了“有无”问题。\n    *   V-CAGE 的核心贡献则是指出并解决了规模化生成中的**“质劣”问题**。它强调在机器人学习中，**数据的保真度与因果一致性比单纯的数量更重要**。这是方法论上的一个重要演进：将数据生成视为一个需要**严格优化和过滤**的过程。\n\n**总而言之，V-CAGE 是作者所在研究脉络中一个关键的“质量提升”版本。它继承了前作利用LLM/VLM进行自动化生成的框架，但创新性地引入了几何约束引擎和视觉语义验证器，形成了一个**闭环、自校正的高保真数据合成系统**，旨在为学习长视程具身智能提供更可靠的数据基础。**",
            "timestamp": "2026-01-27T16:03:36.219871",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
            "answer": "## V-CAGE关键技术方法分析\n\n该论文为解决长视野具身任务中合成数据的物理不一致性和语义错位问题，提出了一个闭环框架V-CAGE。其核心技术创新在于将**几何约束**与**视觉验证**深度集成到数据生成流程中，确保生成轨迹的高保真度。以下是其关键技术方法的详细说明：\n\n| 方法模块 | 核心目标 | 技术细节与数学原理 | 具体参数/模型 |\n| :--- | :--- | :--- | :--- |\n| **1. 上下文感知实例化机制** | 在场景合成中**强制几何一致性**，防止物体穿透，确保在杂乱环境中的可达、无冲突配置。 | **动态禁止空间地图**：系统在放置物体时，动态维护一个“禁止体积”地图。该地图记录了已被占据或不可达的空间区域。当需要实例化新物体时，其候选位姿 \\( P_{candidate} \\) 必须满足与所有禁止体积 \\( V_{prohibited} \\) 无交集的条件：<br> \\( P_{candidate} \\cap \\bigcup V_{prohibited} = \\varnothing \\) <br> 这本质上是在线求解一个**动态三维装箱问题**，通过实时更新空间约束来保证场景的物理合理性。 | 未在正文明确给出地图分辨率、膨胀半径等具体参数，但其机制依赖于物理仿真引擎（如PyBullet, MuJoCo）的碰撞检测功能。 |\n| **2. 分层指令模块（语义指令落地）** | 将高级目标（如“准备工作”）**分解为可组合的动作基元**，桥接抽象意图与低级控制。 | 使用大语言模型（LLM）作为高级规划器。给定用户指令 \\( I_{user} \\)，LLM将其分解为一系列可执行的子任务序列 \\( \\mathcal{T} = \\{T_1, T_2, \\dots, T_k\\} \\)。<br> 每个子任务 \\( T_i \\) 被映射到一个参数化的运动基元（如 `pick(obj)`, `place(obj, loc)`）。这实现了概率分解：<br> \\( P(\\tau \\| I_{user}) \\approx \\prod_{i=1}^{k} P(T_i \\| I_{user}, T_{<i}) \\) <br> 其中 \\( \\tau \\) 是轨迹，\\( T_{<i} \\) 是历史子任务。 | **LLM模型**：Pangu（例如Pangu-7B，经过监督微调）。<br> **动作基元库**：预定义的参数化技能模板。 |\n| **3. VLM引导的拒绝采样（核心验证循环）** | 充当**视觉评论家**，过滤“静默失败”（代码执行但未达成视觉目标），确保**语义正确性**。 | 将数据生成建模为一个**拒绝采样过程**。定义验证函数 \\( \\phi_{VLM}(I_{img}, T_i) \\to \\{0, 1\\} \\)，其中 \\( I_{img} \\) 是子任务 \\( T_i \\) 执行后的图像。<br> **接受准则**：一条轨迹 \\( \\tau \\) 仅当所有子任务均通过视觉验证时才被接受进入数据集 \\( \\mathcal{D} \\)：<br> \\( \\prod_{i=1}^{k} \\phi_{VLM}(I_{img}^{(i)}, T_i) = 1 \\) <br> 若任一 \\( \\phi_{VLM}(\\cdot) = 0 \\)，则生成过程被中止，场景重置。这保证了数据集的“因果有效性”，即 \\( P_{sem}(\\tau) \\approx 1 \\)。 | **VLM模型**：Gemini 3。<br> **验证触发点**：每个子任务执行后立即进行。 |\n| **4. 生成式场景与任务合成管道（整合流程）** | 自主生成物理有效、语义丰富的场景，并验证其中的操作子任务。 | 这是一个五阶段管道：<br> 1. **语言引导的资源检索与场景规划**：LLM根据指令生成场景描述 \\( \\mathcal{D}_{scene} \\) 并检索物体集 \\( \\mathcal{O} \\)。<br> 2. **视觉条件重排**：使用图像修复模型（如NanoBanana-Pro）根据 \\( I_{init} \\) 和 \\( \\mathcal{D}_{scene} \\) 生成目标图像 \\( I_{goal} = \\mathcal{M}(I_{init}, \\mathcal{D}_{scene}) \\)。<br> 3. **场景细化与落地**：将 \\( I_{goal} \\) 中的2D坐标反投影到3D空间，通过**优化位姿 \\( \\mathbf{P} \\)** 来最小化成本函数：<br> \\( \\mathbf{P}^* = \\arg\\min_{\\mathbf{P}} \\left( \\mathcal{L}_{collision}(\\mathbf{P}) + \\lambda \\mathcal{L}_{semantic}(\\mathbf{P}, \\mathcal{D}_{scene}) \\right) \\) <br> 其中 \\( \\mathcal{L}_{collision} \\) 惩罚碰撞，\\( \\mathcal{L}_{semantic} \\) 惩罚与语义描述的偏差，\\( \\lambda \\) 是权衡系数。<br> 4. **子任务代码生成**：LLM根据场景中的物体关系实例化可执行的技能代码。<br> 5. **基于仿真的验证**：执行子任务 \\( T_i \\)，计算成功率 \\( SR(T_i) \\)。仅当 \\( SR(T_i) > \\tau \\)（例如 \\( \\tau = 0.5 \\)）时才接受该子任务。 | **图像修复模型**：NanoBanana-Pro。<br> **优化器**：用于求解 \\( \\mathbf{P}^* \\) 的优化算法（如梯度下降）。<br> **成功率阈值**：\\( \\tau = 0.5 \\)。<br> **仿真试验次数**：未明确，但需多次以评估 \\( SR(T_i) \\)。 |\n| **5. Pangu-7B的监督微调（附录A）** | 使LLM能够为V-CAGE任务链中的子任务**合成正确的运动规划代码**。 | **数据构建**：使用DeepSeek模型对50个专家脚本进行数据增强，生成约900个指令-代码对，作为SFT数据集。<br> **微调目标**：最小化标准语言建模损失，使模型适应仿真环境的特定API和物理约束：<br> \\( \\mathcal{L}_{SFT} = -\\sum_{t} \\log P(w_t \\| w_{<t}, \\mathcal{I}) \\) <br> 其中 \\( w_t \\) 是代码令牌，\\( \\mathcal{I} \\) 是任务指令。 | **基础模型**：Pangu-7B。<br> **数据增强模型**：DeepSeek。<br> **评估指标**：平均成功率（Avg SR）、Top-3成功率（Top3 SR）。 |\n\n### **技术核心与价值总结**\n1.  **闭环验证范式**：V-CAGE的核心突破是将开环的“生成-执行”模式，转变为“**生成-执行-验证-重试**”的闭环。这通过VLM的拒绝采样实现，从根本上提升了数据的语义保真度。\n2.  **动态空间建模**：通过“禁止体积地图”将场景生成从一个静态过程变为一个**增量式、受约束的优化过程**，解决了杂乱环境中几何不一致的根本问题。\n3.  **分层解耦设计**：将**高层规划（LLM）、中层几何推理（禁止体积地图）、低层验证（VLM）** 清晰分离又紧密协作，使系统兼具语义理解、物理合理性和执行可靠性。\n4.  **实际价值**：实验表明，基于V-CAGE数据训练的策略，其**平均成功率从46.86%提升至64.58%**，且**Top-10成功率稳定在100%**（基线仅为77%）。这证明了高质量合成数据对于提升下游策略性能和泛化能力的决定性作用，为大规模机器人学习提供了可扩展的高质量数据源。",
            "timestamp": "2026-01-27T16:04:38.032599",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
            "answer": "## 论文分析：V-CAGE的评价指标与数据集\n\n基于对论文内容的分析，以下是V-CAGE框架所使用的**评价指标**和**数据集**的详细说明。\n\n### 一、 评价指标 (Evaluation Metrics)\n\n论文在实验部分主要使用了以下两类指标来评估**下游策略的性能**和**数据生成的有效性**：\n\n#### 1. 下游策略性能指标 (用于评估训练出的机器人策略)\n这些指标在 **4.2 Main Results** 部分提出，用于比较使用V-CAGE生成的数据和基线（Vanilla）数据训练出的策略。\n\n| 指标名称 | 计算方式与细节 | 目的与解读 |\n| :--- | :--- | :--- |\n| **平均成功率 (Average Success Rate)** | 在35个长视野任务上，每个任务进行多次试验，计算所有试验的平均成功率。 | **衡量策略的整体性能和鲁棒性**。值越高，说明策略在多样任务上的平均表现越好。V-CAGE将此指标从基线的46.86%提升至64.58%。 |\n| **Top-K 成功率 (Top-K Success Rate)** | 对于每个任务，取K次试验中**成功率最高的K次结果**，然后计算这些“最佳表现”在所有任务上的平均成功率。论文中报告了 **K=5** 和 **K=10** 的情况。 | **衡量策略的峰值性能和稳定性**。<br>- **Top-5**: 评估策略在少量尝试中的最佳潜力。<br>- **Top-10**: 评估策略在更多尝试中的**稳定性和一致性**。V-CAGE的Top-5和Top-10均为100%，而基线从92%降至77%，这**凸显了V-CAGE数据在消除“静默失败”和提升训练稳定性方面的关键作用**。 |\n\n#### 2. 运动规划模型微调评估指标 (用于附录A中的Pangu模型微调)\n这些指标在 **Appendix A** 中用于评估经过监督微调(SFT)的Pangu-7B模型在生成运动规划代码方面的能力。\n\n| 指标名称 | 计算方式与细节 | 目的与解读 |\n| :--- | :--- | :--- |\n| **平均成功率 (Avg SR)** | 对每个验证任务，运行**10次独立试验**，计算成功的次数比例。 | 衡量微调后模型生成代码的**平均可靠性和泛化能力**。 |\n| **Top-3 成功率 (Top3 SR)** | 对每个任务，取10次试验中**成功率最高的3次结果**，计算其平均成功率。 | 衡量模型在理想情况下的**峰值性能上限**，反映其解决任务的理论能力。 |\n\n### 二、 使用的数据集 (Datasets)\n\n论文没有使用一个单一的、固定的外部数据集进行训练，而是**自主生成合成数据**，并在此基础上进行策略训练和评估。其数据流程和基准如下：\n\n#### 1. 数据生成来源与流程\n- **核心方法**：V-CAGE本身是一个**数据生成框架**。它从零开始合成高质量的机器人操作轨迹数据。\n- **输入**：接收**高级用户指令**（如“get ready for work”, “organize the breakfast table”）。\n- **流程**：通过LLM（Pangu）进行任务分解、场景规划（从资产库中检索物体），利用图像修复模型进行视觉重排，再通过优化和物理模拟生成动作序列，并最终由VLM（Gemini3）进行逐步骤的语义验证。\n- **输出**：一个由**经过物理和语义双重验证的成功轨迹**组成的合成数据集 `𝒟`。\n\n#### 2. 评估基准与任务集\n为了定量评估生成数据的质量及其对下游策略的影响，论文在特定的基准任务集上进行了测试：\n\n| 基准/任务集名称 | 来源与细节 | 在论文中的作用 |\n| :--- | :--- | :--- |\n| **RoboTwin 基准** | 引自论文 `(Mu et al., 2025)`。这是一个专注于**双臂操作**的基准，包含从单张RGB图像生成多样化数字孪生场景和任务。 | **主要评估平台**。论文从RoboTwin基准中**衍生出35个多样化的长视野操作任务**，用于训练和评估扩散策略。这些任务涵盖了从简单重排到复杂的多阶段活动（如布置餐桌、语义分类）。 |\n| **专家演示数据 (用于SFT)** | 在附录A中，为了微调Pangu模型以生成低级运动规划代码，作者从RoboTwin基准中收集了**50个独特的专家运动规划脚本**作为种子数据。 | **模型微调的基础**。这些少量专家数据经过DeepSeek模型进行**数据增强**（重写逻辑和变量名），生成了约900个指令-代码对，用于对Pangu-7B进行监督微调，使其适应仿真环境的API和物理约束。 |\n\n### 总结表格\n\n| 类别 | 名称 | 关键细节 | 论文中的主要发现/用途 |\n| :--- | :--- | :--- | :--- |\n| **评价指标** | 平均成功率 | 所有任务试验的平均成功比例 | V-CAGE (64.58%) vs. Vanilla (46.86%) |\n| **评价指标** | Top-5/10 成功率 | 最佳K次试验的平均成功比例 | V-CAGE稳定性(100%)远超Vanilla(Top-10: 77%)，证明VLM验证有效过滤了噪声数据。 |\n| **评价指标** | Avg SR / Top3 SR (附录) | 用于评估代码生成模型，10次试验平均 vs. 最佳3次平均 | 微调后的Pangu-SFT模型在多项任务上达到高成功率，而基础模型为0%。 |\n| **数据集/基准** | V-CAGE 合成数据 | 自主生成的、经过闭环验证的高质量轨迹数据 | 用于训练下游扩散策略，其质量直接决定了策略性能的提升。 |\n| **数据集/基准** | RoboTwin 任务集 (35个任务) | 从RoboTwin基准衍生的长视野操作任务集合 | 作为统一的测试平台，用于公平比较不同数据生成方法训练出的策略性能。 |\n| **数据集/基准** | RoboTwin 专家脚本 (50个) | 用于运动规划代码生成的种子数据 | 经过增强后，用于对Pangu-7B进行监督微调，使其能生成可执行的底层控制代码。 |\n\n**核心结论**：V-CAGE的创新之处在于其**评价指标紧密服务于其核心主张**——即通过几何一致性和VLM语义验证来提升合成数据质量。Top-K成功率，尤其是Top-10的显著差距，强有力地证明了其VLM验证环在消除“静默失败”和提升数据“纯度”方面的**实际价值**。所有实验均在其自主生成的、基于RoboTwin任务结构的数据上进行，形成了一个从数据生成到策略评估的完整闭环验证体系。",
            "timestamp": "2026-01-27T16:05:31.819212",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
            "answer": "## 论文实验数据集与评估结果分析\n\n### 一、实验数据集\n论文主要在以下数据集/基准上进行了实验：\n- **RoboTwin Benchmark**：这是论文核心实验所使用的基准。该基准源自 **RoboTwin** 和 **RoboTwin 2.0** 的工作，专注于双手机器人操作任务。论文从中选取了 **35个多样化的长时程操作任务**，涵盖了从简单物体重新摆放到复杂的多阶段活动（如布置餐桌、语义分类等）。\n- **内部构建的合成数据集**：通过 V-CAGE 框架自身生成的轨迹数据，用于训练下游策略模型。\n\n### 二、主要评估指标\n论文采用了以下三个关键指标来评估下游策略的性能：\n1.  **平均成功率**：在所有35个任务上计算的平均成功执行率。\n2.  **Top-K 成功率**：衡量最佳性能检查点的可靠性。\n    - **Top-5 成功率**：表现最好的5个检查点的平均成功率。\n    - **Top-10 成功率**：表现最好的10个检查点的平均成功率。\n\n### 三、性能提升与主要结果\n论文将 **V-CAGE**（完整框架）与 **Vanilla** 基线（仅使用LLM规划和代码生成，**无**VLM验证和上下文感知实例化）进行了对比。\n\n**核心性能数据如下表所示：**\n\n| 评估指标 | V-CAGE (Ours) | Vanilla 基线 | **性能提升** |\n| :--- | :--- | :--- | :--- |\n| **平均成功率** | **64.58%** | 46.86% | **+17.72个百分点** |\n| **Top-5 成功率** | **100.00%** | 92.00% | +8.00个百分点 |\n| **Top-10 成功率** | **100.00%** | 77.00% | **+23.00个百分点** |\n\n### 四、结果深度分析\n1.  **显著的总体提升**：V-CAGE 在平均成功率上实现了 **17.72%** 的绝对提升，这直接证明了其生成的**高质量数据**能有效提升下游策略的泛化能力和鲁棒性。\n2.  **VLM验证的关键作用**：Top-K 指标的对比揭示了最深刻的洞见：\n    - **Vanilla基线**的Top-10成功率（77%）远低于其Top-5成功率（92%），表明其策略性能**不稳定**，存在较大方差。\n    - **V-CAGE**在Top-5和Top-10上均保持了 **100%的成功率**，表现出**极致的稳定性**。\n    - **原因分析**：这归功于VLM作为“视觉评论家”的**拒绝采样**机制。它过滤掉了“静默失败”（代码执行无报错但语义目标未达成）的轨迹，极大**净化了训练数据**，消除了标签噪声，使策略能学习到正确的因果关联。\n\n**结论**：实验结果表明，V-CAGE框架通过整合**几何一致性约束**和**VLM语义验证**，生成了物理可信、语义对齐的高质量合成数据。这不仅显著提高了策略在已知任务上的成功率，更重要的是通过提供更纯净的监督信号，增强了策略在复杂、长时程任务中的**泛化能力和鲁棒性**。数据质量而非单纯数量，是提升具身智能性能的关键。",
            "timestamp": "2026-01-27T16:05:58.503442",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
            "answer": "## 论文主要局限性分析\n\n根据论文结论部分（Section 5: Limitations and Future Work）及全文内容，V-CAGE框架的主要局限性可归纳如下：\n\n- **计算效率与成本**：V-CAGE的核心验证机制依赖于**VLM引导的拒绝采样**。对于成功率本身就很低的复杂长时程任务，反复生成并验证轨迹会导致计算开销显著增加。这个过程可能比开环生成方法（如Vanilla基线）慢得多。\n\n- **任务范围限制**：当前框架主要针对**刚体物体的静态操作和重排任务**进行验证（如整理桌面、放置物体）。论文明确指出，尚未扩展到涉及**可变形物体**（如布料、绳索）或**流体**的动态交互场景。这些场景的物理仿真和语义验证更为复杂。\n\n- **验证依赖性与VLM的可靠性**：整个数据质量保障严重依赖于**VLM（Gemini3）作为视觉评判者**的准确性。如果VLM自身对某些视觉状态或细微变化（如开关的微小位移、物体的轻微滑动）的判断出现错误，可能会错误地接受失败轨迹或拒绝成功轨迹，从而影响生成数据的质量。论文未深入探讨VLM的误判率及其影响。\n\n- **场景生成的复杂性**：虽然**上下文感知实例化机制**解决了物体放置时的几何冲突，但对于极其杂乱、动态变化的环境，维护“禁止体积”地图的算法可能面临挑战。论文未讨论在物体被频繁移动或场景结构发生重大变化时，该机制的实时更新效率和鲁棒性。\n\n## 未来可能的改进方向\n\n论文在局限性部分也相应提出了未来的研究思路：\n\n- **从拒绝到修正**：当前框架对失败的轨迹采取**直接丢弃**的策略。未来可以探索利用VLM批评者提供的**密集奖励信号或反馈**，对失败轨迹进行**在线修正或重规划**，而不是完全从头开始。这可以显著提高数据生成效率，将“拒绝采样”转变为“迭代优化”过程。\n\n- **扩展任务与物理范畴**：一个明确的方向是将V-CAGE框架扩展到**更复杂的物理交互领域**，例如：\n    - 操作可变形物体（折叠衣物、操纵绳索）\n    - 与流体互动（倒水、混合液体）\n    - 涉及关节物体或软体机器人的任务\n\n- **提升验证机制的效率与鲁棒性**：\n    - 研究更轻量级或专用的视觉验证模型，以降低计算成本。\n    - 开发多模态验证策略，结合**物理引擎的原始状态数据**（如物体位置、接触力）与VLM的视觉判断，提高验证的可靠性和效率。\n\n- **闭环训练与数据生成的融合**：未来工作可以探索将V-CAGE的生成-验证管道与**策略训练过程更紧密地结合**。例如，可以根据当前策略的弱点，主动生成具有挑战性的训练数据，实现课程学习或对抗性数据生成。\n\n- **sim-to-real的深化**：虽然论文展示了在仿真中提升策略性能，但未来可以进一步研究如何利用V-CAGE生成的高保真数据，更好地**弥合仿真与现实之间的差距**，例如通过集成更广泛的领域随机化或学习对物理参数不确定性的鲁棒性。\n\n**总结**：V-CAGE的核心贡献在于通过几何约束和视觉验证确保了合成数据的质量，但其局限性主要体现在计算效率、任务范围以及对VLM的依赖上。未来的改进将围绕**提高生成效率、扩展物理复杂性以及构建更鲁棒的验证体系**展开。",
            "timestamp": "2026-01-27T16:06:27.701954",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T16:41:51Z",
    "created_at": "2026-01-27T15:53:22.645372",
    "updated_at": "2026-01-27T15:53:22.645378",
    "recommend": 0
}