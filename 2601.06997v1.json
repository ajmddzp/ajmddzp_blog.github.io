{
    "id": "2601.06997v1",
    "title": "ObjSplat: Geometry-Aware Gaussian Surfels for Active Object Reconstruction",
    "authors": [
        "Yuetao Li",
        "Zhizhou Jia",
        "Yu Zhang",
        "Qun Hao",
        "Shaohui Zhang"
    ],
    "abstract": "è‡ªä¸»é«˜ä¿çœŸç‰©ä½“é‡å»ºæ˜¯åˆ›å»ºæ•°å­—èµ„äº§å’Œå¼¥åˆæœºå™¨äººå­¦ä¸­ä»¿çœŸä¸ç°å®å·®è·çš„åŸºç¡€ã€‚æˆ‘ä»¬æå‡ºäº†ObjSplatï¼Œä¸€ç§ä¸»åŠ¨é‡å»ºæ¡†æ¶ï¼Œåˆ©ç”¨é«˜æ–¯é¢å…ƒä½œä¸ºç»Ÿä¸€è¡¨ç¤ºï¼Œé€æ­¥é‡å»ºå…·æœ‰é€¼çœŸå¤–è§‚å’Œç²¾ç¡®å‡ ä½•å½¢çŠ¶çš„æœªçŸ¥ç‰©ä½“ã€‚é’ˆå¯¹ä¼ ç»ŸåŸºäºä¸é€æ˜åº¦æˆ–æ·±åº¦çº¿ç´¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„è§†ç‚¹è¯„ä¼°æµç¨‹ï¼Œæ˜¾å¼å»ºæ¨¡èƒŒé¢å¯è§æ€§å’Œé®æŒ¡æ„ŸçŸ¥çš„å¤šè§†è§’å…±è§†æ€§ï¼Œå³ä½¿åœ¨å‡ ä½•å¤æ‚çš„ç‰©ä½“ä¸Šä¹Ÿèƒ½å¯é è¯†åˆ«é‡å»ºä¸è¶³çš„åŒºåŸŸã€‚æ­¤å¤–ï¼Œä¸ºå…‹æœè´ªå©ªè§„åˆ’ç­–ç•¥çš„å±€é™ï¼ŒObjSplaté‡‡ç”¨äº†ä¸€ç§åŸºäºåŠ¨æ€æ„å»ºç©ºé—´å›¾è¿›è¡Œå¤šæ­¥å‰ç»çš„\"æœ€ä¼˜ä¸‹ä¸€è·¯å¾„\"è§„åˆ’å™¨ã€‚é€šè¿‡è”åˆä¼˜åŒ–ä¿¡æ¯å¢ç›Šä¸ç§»åŠ¨æˆæœ¬ï¼Œè¯¥è§„åˆ’å™¨èƒ½ç”Ÿæˆå…¨å±€é«˜æ•ˆçš„è½¨è¿¹ã€‚åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œæ–‡åŒ–é—äº§ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒObjSplatå¯åœ¨æ•°åˆ†é’Ÿå†…ç”Ÿæˆç‰©ç†ä¸€è‡´çš„æ¨¡å‹ï¼Œåœ¨æ˜¾è‘—å‡å°‘æ‰«ææ—¶é—´å’Œè·¯å¾„é•¿åº¦çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„é‡å»ºä¿çœŸåº¦ä¸è¡¨é¢å®Œæ•´æ€§ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://li-yuetao.github.io/ObjSplat-page/ã€‚",
    "url": "https://arxiv.org/abs/2601.06997v1",
    "html_url": "https://arxiv.org/html/2601.06997v1",
    "html_content": "ObjSplat: Geometry-Aware Gaussian Surfels for\nActive Object Reconstruction\nYuetao Li, Zhizhou Jia, Yu Zhang, Qun Hao, Shaohui Zhang\nThe corresponding author:\nzhangshaohui@bit.edu.cn,\nSchool of Optics and Photonics, Beijing Institute of Technology, China\nAbstract\nAutonomous high-fidelity object reconstruction is fundamental for creating digital assets and bridging the simulation-to-reality gap in robotics. We present ObjSplat, an active reconstruction framework that leverages Gaussian surfels as a unified representation to progressively reconstruct unknown objects with both photorealistic appearance and accurate geometry. Addressing the limitations of conventional opacity or depth-based cues, we introduce a geometry-aware viewpoint evaluation pipeline that explicitly models back-face visibility and occlusion-aware multi-view covisibility, reliably identifying under-reconstructed regions even on geometrically complex objects. Furthermore, to overcome the limitations of greedy planning strategies, ObjSplat employs a next-best-path (NBP) planner that performs multi-step lookahead on a dynamically constructed spatial graph. By jointly optimizing information gain and movement cost, this planner generates globally efficient trajectories. Extensive experiments in simulation and on real-world cultural artifacts demonstrate that ObjSplat produces physically consistent models within minutes, achieving superior reconstruction fidelity and surface completeness while significantly reducing scan time and path length compared to state-of-the-art approaches. Project page:\nhttps://li-yuetao.github.io/ObjSplat-page/\n.\nI\nIntroduction\nHigh-quality digitization of physical objects, characterized by both accurate geometry and photorealistic appearance, has been a long-standing pursuit in robotics and computer vision\n[\n54\n]\n. Such fine-grained digital models are fundamental to creating immersive extended reality (XR) experiences and ensuring the long-term preservation of digital cultural heritage\n[\n8\n]\n. Notably, in the robotics domain, these models hold the potential to bridge the simulation-to-reality (sim-to-real) gap, empowering the robust development and validation of perception, planning, and control algorithms within physics-based simulated environments\n[\n7\n,\n42\n,\n38\n,\n55\n,\n10\n]\n.\nFigure 1:\nObjSplat autonomously plans viewpoints and progressively reconstructs an unknown object into a high-fidelity Gaussian model and water-tight mesh, enabling direct use in physics simulations.\nRecent advances in radiance fields, represented by neural radiance fields (NeRF)\n[\n32\n]\nand 3D Gaussian splatting (3DGS)\n[\n24\n]\n, have demonstrated remarkable promise in high-quality reconstruction and novel view synthesis. However, achieving these results typically relies on dense, passive data acquisition and complex post-processing pipelines involving structure-from-motion (SfM)\n[\n40\n]\ninitialization and computationally intensive offline optimization. This often requires substantial manual intervention, fundamentally limiting their scalable application in object-level digital asset creation.\nTo mitigate this issue, active object reconstruction (AOR) has emerged as a promising solution, aiming to automate the reconstruction process with robotic systems. The core of this task lies in accurately identifying under-reconstructed regions (e.g., incomplete or poor-quality areas) and planning a sequence of viewpoints for objects with diverse geometries and appearances, aiming to achieve complete reconstruction efficiently. Many well-established approaches formulate this task as a sequential next-best-view (NBV) planning problem\n[\n26\n,\n37\n,\n50\n,\n21\n,\n3\n,\n52\n,\n20\n,\n49\n,\n5\n,\n47\n,\n19\n,\n25\n]\n, where methods leverage cues such as frontier information\n[\n3\n,\n19\n]\n, color and density distributions\n[\n37\n]\n, or visibility fields\n[\n49\n]\nto quantify the uncertainty or information gain of candidate views. While effective in identifying informative views, this inherently greedy strategy is confined to single-step optimization, ignoring the robotâ€™s kinematic constraints and movement costs, which often results in lengthy and suboptimal scanning trajectories. While some works mitigate this issue by introducing one-shot view planners\n[\n34\n,\n33\n,\n35\n,\n36\n]\n, a fundamental challenge persists:\nhow to reliably assess reconstruction completeness and quality, and critically, how to strike an effective balance between efficient exploration and high-quality reconstruction\n.\nIn this work, we introduce\nObjSplat\n, an active object reconstruction system leveraging 2D Gaussian surfels as a unified representation for both model optimization and exploration guidance. Our system is designed to autonomously and progressively achieve high-fidelity geometric and textural reconstruction from RGB-D data, producing physically consistent assets ready for downstream tasks (see Fig.\n1\n). The core of our approach lies in tightly coupling high-quality reconstruction with efficient exploration within a unified framework, enabling both accuracy and efficiency in object-level reconstruction. Specifically, we leverage surface normals to guide the incremental update of the Gaussian model and facilitate joint geometryâ€“texture optimization. To guide the active reconstruction of unknown objects, we design a geometry-aware viewpoint evaluation strategy. Unlike existing methods that primarily rely on opacity or depth residuals to detect under-reconstructed regions\n[\n31\n,\n28\n]\n, our strategy specifically addresses object-centric viewpoints and non-closed surfaces, enabling more accurate detection of incomplete and unobserved back-face regions as well as robust quantification of true multi-view covisibility, which is essential for handling challenging hollow, self-occluded, and thin structures. On the exploration side, we move beyond the traditional greedy NBV paradigm. Instead, we propose a next-best-path (NBP) planning strategy, which performs multi-step lookahead planning on a dynamically constructed spatial topology, jointly accounting for information gain and movement cost to generate globally superior scanning trajectories while significantly reducing redundant movements and online scan time. Both simulation and real-world experiments demonstrate that our system adaptively achieves complete, high-fidelity reconstruction of unknown objects (with varying geometric and textural complexities) with fewer movements, consistently outperforming state-of-the-art baselines in both efficiency and reconstruction quality.\nIn summary, our work advances the state of the art in active object reconstruction through the following contributions:\nâ€¢\nA geometry-aware view evaluation pipeline that accurately quantifies reconstruction quality and completeness for non-closed surfaces, by explicitly modeling back-face visibility and true multi-view covisibility, providing reliable guidance for both exploration and refinement.\nâ€¢\nA next-best-path (NBP) planning strategy that performs multi-step lookahead on a dynamically constructed spatial graph, jointly optimizing information gain and movement cost to generate efficient scanning trajectories, significantly reducing total path length and scan time compared to conventional greedy NBV approaches.\nâ€¢\nA unified active object reconstruction framework, ObjSplat, which leverages surface normals for the incremental update of Gaussian surfels and joint geometryâ€“texture optimization. This tight coupling enables the efficient, autonomous production of physically consistent, high-fidelity digital assets ready for downstream tasks.\nâ€¢\nExtensive experiments on objects with diverse geometric and textural complexities demonstrate that ObjSplat consistently outperforms existing methods in both reconstruction quality and exploration efficiency, while real-world experiments on four cultural heritage artifacts further validate its robustness and practical applicability.\nII\nRelated Work\nOur work builds upon two primary research areas: high-quality reconstruction and active view planning for robotics. In this section, we review the key advancements in both fields.\nII-A\nHigh-Quality Reconstruction\nHigh-quality reconstruction aims to achieve both accurate geometry and photorealistic appearance. Classical pipelines primarily rely on SfM/MVS\n[\n41\n]\n, or the fusion of high-precision depth measurements (e.g., structured light) to estimate dense point clouds. These raw representations are subsequently converted into explicit surface meshes via TSDF fusion or Poisson reconstruction\n[\n23\n]\n, followed by texture mapping\n[\n12\n,\n56\n]\n. While these explicit methods offer metrically consistent surfaces and benefit from mature engineering ecosystems, they often require dense, well-conditioned observations and tend to degrade on texture-poor or severely occluded objects, with limited capability in reproducing high-fidelity view-dependent appearance. Subsequently, NeRF and 3DGS have demonstrated the powerful capability of radiance field representation in synthesizing high-fidelity novel views. To further improve geometric quality, subsequent methods have integrated signed distance functions (SDFs)\n[\n44\n]\nand multi-resolution 3D hash grids\n[\n29\n]\n, and other techniques\n[\n58\n,\n14\n]\nto generate more accurate surface reconstructions. However, NeRF-based methods often suffer from high computational costs and rendering inefficiencies due to their implicit nature. In contrast, recent GS-based follow-up works enhance geometric fidelity while reducing computational overhead by incorporating explicit geometry representations and normal priors\n[\n18\n,\n4\n,\n6\n,\n59\n]\n, as well as applying depth regularization\n[\n30\n,\n17\n]\n, making it more efficient for real-time applications. More recently, mesh-based Gaussian splatting methods\n[\n13\n,\n16\n,\n15\n]\nhave emerged, which directly bind Gaussians to mesh surfaces or utilize restricted Delaunay triangulation to enforce surface consistency, significantly bridging the gap between volumetric rendering and explicit surface reconstruction. In addition, to address the challenges of data scarcity in sparse-view and object-centric scenarios, recent works\n[\n51\n,\n11\n,\n2\n]\nleverage strong external priorsâ€”ranging from diffusion-based generative models\n[\n51\n,\n2\n]\nto geometric foundation models\n[\n11\n]\nâ€”to infer missing geometry and resolve spatial ambiguities. While these approaches achieve impressive visual fidelity by hallucinating plausible details from minimal inputs, they remain fundamentally passive and may struggle with hallucination artifacts or over-smoothed details on unseen complex objects.\nTo address this, our approach resolves the information deficiency through active observation, building on a Gaussian-surfel representation, automating the data collection process via a robotic arm and turntable setup. Notably, unlike traditional disjointed multi-view reconstruction pipelines, our system seamlessly integrates online scanning and offline refinement within a unified optimization framework, ensuring consistent model evolution. By introducing a joint geometryâ€“texture optimization strategy, we effectively enhance both global consistency and geometric precision, enabling the autonomous creation of high-fidelity digital twins.\nII-B\nActive Object Reconstruction\nActive object reconstruction has emerged as a task due to the limited field of view (FoV) of vision sensors and the inability of predefined scanning trajectories to meet the needs of a wide range of object reconstruction tasks, including those with varying degrees of complex textures (e.g., repetitive or sparse patterns) and complex geometry (e.g., self-occlusion, non-convexity). Active object reconstruction, as the object-level reconstruction task within active vision tasks, aims to autonomously plan a series of viewpoints to construct a complete model of an unknown object efficiently. Existing strategies can be broadly categorized into dominant paradigm iterative NBV planning\n[\n26\n,\n37\n,\n50\n,\n21\n,\n3\n,\n52\n,\n20\n,\n49\n,\n5\n,\n47\n,\n19\n,\n25\n]\nand global one-shot planning\n[\n34\n,\n33\n,\n35\n,\n36\n]\n.\nThe iterative NBV planning methods sequentially select a single next viewpoint to maximize an information gain metric. Early approaches focused on geometric completeness, using frontier-based methods\n[\n3\n,\n19\n]\nto gradually expand the boundaries toward a complete reconstruction. PB-NBV\n[\n19\n]\nleverages GMMs to fit voxels of varying visibility and employs a projection-based approach to accelerate viewpoint evaluation. Sample-based methods, on the other hand, often require elaborate heuristic sampling strategies (uniform\n[\n48\n]\n, random\n[\n22\n]\n, or sampling at topology nodes\n[\n28\n]\n), and inherent candidate viewpoints are not adaptive for unknown objects of varying geometrical morphology. PVP-Recon\n[\n52\n]\nemploys a progressive approach, where view selection is guided by warping consistency under sparse view surface reconstruction. With the emergence of radiance fields, recent methods have shifted towards quantifying visual or model uncertainty. For example, ActiveNeRF\n[\n37\n]\nmodels color distributions, while information-theoretic methods such as FisherRF\n[\n20\n]\nand GauSS-MI\n[\n47\n]\nleverage Fisher information and Shannon mutual information, respectively, to guide view selection towards regions of high model uncertainty. While these methods are effective in identifying informative views, they often struggle with complex geometries like self-occlusions due to a lack of explicit geometric awareness. Moreover, their greedy, single-step optimization ignores movement costs, typically resulting in inefficient scanning trajectories. While some approaches balance efficiency and cost\n[\n48\n,\n28\n,\n27\n]\n, they are designed for scene-level tasks and fail to address the specific geometric challenges of object reconstruction.\nTo mitigate the inefficiencies of iterative planning, one-shot methods\n[\n34\n,\n33\n]\nframe one-shot viewpoint planning as a set covering optimization problem, aiming to obtain the minimum set of viewpoints for dense coverage and solving the traveling salesman problem (TSP) to generate a complete scan trajectory in a single step. With the introduction of diffusion models, powerful geometric priors have been leveraged to enable RGB-based active object reconstruction. For example, DM-OSVP\n[\n35\n]\nand its improved version DM-OSVP++\n[\n36\n]\nuse 3D diffusion priors to generate geometry from RGB data, enabling the selection of a compact, information-rich set of views. DM-OSVP++ further introduces geometric/texture complexity conditioning, which is beneficial for RGB-only planning. However, these methods struggle with unknown scales and severe occlusions due to their reliance on coarse, early-stage priors. Furthermore, the substantial inference latency of diffusion models hinders real-time performance, and their â€one-shotâ€ nature lacks the adaptability required to progressively refine reconstruction for complex, unseen objects.\nOur work, ObjSplat, effectively bridges the gap between short-sighted NBV planning and one-shot methods. We propose an NBP strategy that performs multi-step lookahead on a spatial topology, optimizing the trade-off between information gain and movement cost. This approach combines the adaptability of progressive reconstruction with the foresight of global planning. Consequently, ObjSplat achieves a superior trade-off, balancing reconstruction quality and exploration efficiency in a unified, progressive planning framework.\nFigure 2:\nSystem Overview.\nObjSplat progressively reconstructs unknown objects from RGB-D frames using Gaussian surfels as a unified representation. (\nTop left\n) Incoming frames are fused into the global model, where geometryâ€“texture joint optimization, enforcing both photometric and geometric consistency. (\nRight\n) A geometry-aware view evaluation pipeline renders an uncertainty map by integrating occlusion-aware covisibility, surfel-wise confidence, and back-face detection to quantify surface quality and completeness. (\nBottom left\n) Guided by this dense uncertainty, the next-best-path planner performs multi-step lookahead on a spatial topology, generating globally efficient trajectories that balance information gain and movement cost for active reconstruction.\nIII\nMethodology\nWe propose ObjSplat, a closed-loop framework that integrates Reconstruction, Perception, and Planning into a unified system for high-fidelity active object reconstruction. As illustrated in Fig.\n2\n, the system takes RGB-D inputs and progressively updates a Gaussian surfel-based object representation while assessing reconstruction completeness and quality to guide active view planning. ObjSplat consists of three main components: (i) Gaussian surfel-based reconstruction (see\nSec.\nIII-A\n), which incrementally updates the object model and jointly optimizes geometry and appearance by using surface normals; (ii) Geometry-aware viewpoint evaluation (see\nSec.\nIII-B\n), a perception module that explicitly quantifies reconstruction quality, identifying under-reconstructed and back-facing regions via occlusion-aware visibility analysis; and (iii) next-best-path planning (see\nSec.\nIII-C\n), which utilizes these fine-grained quality metrics to perform multi-step lookahead on a spatial topology, generating globally efficient trajectories that balance information gain and movement cost.\nIII-A\nGaussian Surfel-based Reconstruction\nIII-A\n1\nGaussian Surfels for Object Representation\nWe represent the target object using an unstructured set of 2D Gaussian surfels (GSurfels)\n[\n6\n]\n, where the spatial distribution of each surfel approximates a disk. Each surfel is parameterized by a center\nğ\nâˆˆ\nâ„\n3\n\\boldsymbol{\\mu}\\in\\mathbb{R}^{3}\n, a covariance matrix\nğšº\n\\boldsymbol{\\Sigma}\n, an opacity\no\nâˆˆ\n[\n0\n,\n1\n]\no\\in[0,1]\n, and view-dependent color information\nğ’„\nâˆˆ\nâ„\nK\nÃ—\n3\n\\boldsymbol{c}\\in\\mathbb{R}^{K\\times 3}\nutilizing spherical harmonics (SH) coefficients of degree\nL\nD\nL_{D}\n, where\nK\n=\n(\nL\nD\n+\n1\n)\n2\nK=(L_{D}+1)^{2}\n. To facilitate optimization, the covariance matrix\nğšº\n\\boldsymbol{\\Sigma}\nis decomposed into a rotation matrix\nğ‘¹\n\\boldsymbol{R}\n(derived from a quaternion\nğ’’\nâˆˆ\nSO\nâ€‹\n(\n3\n)\n\\boldsymbol{q}\\in\\mathrm{SO}(3)\n) and a scaling matrix\nğ‘º\n=\ndiag\nâ€‹\n(\ns\nx\n,\ns\ny\n,\n0\n)\n\\boldsymbol{S}=\\text{diag}(s^{x},s^{y},0)\n, such that\nğšº\n=\nğ‘¹\nâ€‹\nğ‘º\nâ€‹\nğ‘º\nT\nâ€‹\nğ‘¹\nT\n\\boldsymbol{\\Sigma}=\\boldsymbol{R}\\boldsymbol{S}\\boldsymbol{S}^{T}\\boldsymbol{R}^{T}\n. Notably, the zero\nz\nz\n-scale constraint enforces a local planar structure, allowing the surface normal\nğ’\n\\boldsymbol{n}\nto be directly derived from\nğ’’\n\\boldsymbol{q}\n. This anisotropic formulation enables each surfel to capture local surface geometry more effectively.\nDuring rendering, GSurfels are transformed into the camera coordinate system via pose\nğ‘»\nc\nw\nâˆˆ\nSE\nâ€‹\n(\n3\n)\n\\boldsymbol{T}_{c}^{w}\\in\\mathrm{SE}(3)\nand projected onto the image plane using differentiable splatting\n[\n24\n]\n. For a given pixel\nğ’–\n\\boldsymbol{u}\n, intersected surfels are sorted by depth along each viewing ray, and the pixel color is computed through\nÎ±\n\\alpha\n-blending as follows:\nC\n^\n=\nâˆ‘\ni\n=\n1\nn\nT\ni\nâ€‹\nÎ±\ni\nâ€‹\nğœ\ni\n,\nO\n^\n=\nâˆ‘\ni\n=\n1\nn\nT\ni\nâ€‹\nÎ±\ni\n,\nT\ni\n=\nâˆ\nj\n=\n1\ni\nâˆ’\n1\n(\n1\nâˆ’\nÎ±\nj\n)\n,\n\\hat{C}=\\sum_{i=1}^{n}T_{i}\\alpha_{i}\\mathbf{c}_{i},\\quad\\hat{O}=\\sum_{i=1}^{n}T_{i}\\alpha_{i},\\quad T_{i}=\\prod_{j=1}^{i-1}(1-\\alpha_{j}),\n(1)\nwhere\nÎ±\ni\n=\no\ni\nâ€‹\nğ’©\nâ€‹\n(\nğ’–\n;\nğ\ni\n2\nâ€‹\nD\n,\nğšº\ni\n2\nâ€‹\nD\n)\n\\alpha_{i}=o_{i}\\mathcal{N}(\\boldsymbol{u};\\boldsymbol{\\mu}^{2D}_{i},\\boldsymbol{\\Sigma}^{2D}_{i})\nis the opacity modulated by the 2D Gaussian probability, and\nğœ\ni\n\\mathbf{c}_{i}\ndenote the SH-encoded color of the\ni\ni\n-th surfel. The term\nT\ni\nT_{i}\nrepresents the accumulated transmittance. Similarly, depth\nD\n^\n\\hat{D}\nand normals\nN\n^\n\\hat{N}\nare rendered with\nÎ±\n\\alpha\n-blending and normalized to account for partial opacity:\nD\n^\n=\n1\n1\nâˆ’\nT\nn\n+\n1\nâ€‹\nâˆ‘\ni\n=\n1\nn\nT\ni\nâ€‹\nÎ±\ni\nâ€‹\nğ\ni\n,\nN\n^\n=\n1\n1\nâˆ’\nT\nn\n+\n1\nâ€‹\nâˆ‘\ni\n=\n1\nn\nT\ni\nâ€‹\nÎ±\ni\nâ€‹\nğ‘¹\ni\nz\n,\n\\hat{D}=\\frac{1}{1-T_{n+1}}\\sum_{i=1}^{n}T_{i}\\alpha_{i}\\mathbf{d}_{i},\\quad\\hat{N}=\\frac{1}{1-T_{n+1}}\\sum_{i=1}^{n}T_{i}\\alpha_{i}\\boldsymbol{R}^{z}_{i},\n(2)\nwhere\nğ\ni\n\\mathbf{d}_{i}\nis the unbiased depth computed by intersecting the\ni\ni\n-th surfel with the viewing ray, and\nR\ni\nz\nR^{z}_{i}\nrepresents the surface normal of the\ni\ni\n-th surfel in the camera frame.\nIII-A\n2\nGeometryâ€“Texture Joint Optimization\nTo achieve accurate and consistent reconstruction, we jointly optimize geometric and photometric parameters of all GSurfels using a combination of loss terms. Given an input RGB-D frame\nI\n=\n{\nC\n,\nD\n}\nI=\\{C,D\\}\n, which includes an RGB image\nC\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nC\\in\\mathbb{R}^{H\\times W\\times 3}\nand its corresponding depth map\nD\nâˆˆ\nâ„\nH\nÃ—\nW\nD\\in\\mathbb{R}^{H\\times W}\n, we additionally extract an object-centric binary mask\nM\nâˆˆ\n{\n0\n,\n1\n}\nH\nÃ—\nW\nM\\in\\{0,1\\}^{H\\times W}\nusing pre-trained visual foundation model (VFM)\n[\n39\n]\n. We apply a photometric loss\nâ„’\np\n\\mathcal{L}_{p}\nto supervise the color appearance, combining an\nL\n1\nL_{1}\nterm and a differentiable structural similarity index measure (D-SSIM) term\n[\n45\n]\n. Meanwhile, depth supervision is enforced via an\nL\n1\nL_{1}\nloss on the predicted depth:\nâ„’\np\n=\nÎ»\n1\nâ€‹\n|\nC\nâˆ’\nC\n^\n|\n+\nÎ»\n2\nâ€‹\n(\n1\nâˆ’\nSSIM\nâ€‹\n(\nC\n,\nC\n^\n)\n)\n,\nâ„’\nd\n=\n|\nD\nâˆ’\nD\n^\n|\n,\n\\mathcal{L}_{p}=\\lambda_{1}\\left|C-\\hat{C}\\right|+\\lambda_{2}\\left(1-\\text{SSIM}(C,\\hat{C})\\right),\\mathcal{L}_{d}=\\left|D-\\hat{D}\\right|,\n(3)\nwhere\nÎ»\n1\n=\n0.8\n,\nÎ»\n2\n=\n0.2\n\\lambda_{1}=0.8,\\lambda_{2}=0.2\n,\nC\n^\n\\hat{C}\nand\nD\n^\n\\hat{D}\nare the rendered results. To further enhance surface quality and alignment, we incorporate normal supervision\nâ„’\nn\n\\mathcal{L}_{n}\nand geometric consistency loss\nâ„’\nc\n\\mathcal{L}_{c}\nbetween the rendered normal and the one derived from depth:\nâ„’\nn\n=\n|\nN\nobs\nâˆ’\nN\n^\n|\n,\nâ„’\nc\n=\n|\n1\nâˆ’\nN\nd\nâŠ¤\nâ€‹\nN\n^\n|\n,\n\\quad\\mathcal{L}_{n}=\\left|N_{\\mathrm{obs}}-\\hat{N}\\right|,\\mathcal{L}_{c}=\\left|1-N_{d}^{\\top}\\hat{N}\\right|,\n(4)\nwhere\nN\nobs\nN_{\\mathrm{obs}}\nand\nN\nd\nN_{d}\nrepresent surface normals estimated via finite differences from the observed and rendered depth maps, respectively. To prevent semi-transparent blending artifacts and enforce sharp surface boundaries, we regularize the opacity of each surfel to favor either fully opaque or fully transparent states:\nâ„’\no\n=\nexp\nâ€‹\n(\nâˆ’\n(\nÎ±\ni\nâˆ’\n0.5\n)\n2\n/\n0.05\n)\n.\n\\mathcal{L}_{\\text{o}}=\\text{exp}\\left(-(\\alpha_{i}-0.5)^{2}/0.05\\right).\n(5)\nAdditionally, we employ a binary cross-entropy (BCE) loss\nâ„’\nm\n\\mathcal{L}_{m}\nto supervise the reconstruction of the object mask. The overall objective function combines photometric, depth, normal, opacity, and mask supervision terms:\nâ„’\n=\nâ„’\np\n+\nÎ»\nd\nâ€‹\nâ„’\nd\n+\nÎ»\nn\nâ€‹\n(\nâ„’\nn\n+\nâ„’\nc\n)\n+\nÎ»\nm\nâ€‹\nâ„’\nm\n+\nÎ»\no\nâ€‹\nâ„’\no\n,\n\\mathcal{L}=\\mathcal{L}_{p}+\\lambda_{d}\\mathcal{L}_{d}+\\lambda_{n}(\\mathcal{L}_{n}+\\mathcal{L}_{c})+\\lambda_{m}\\mathcal{L}_{m}+\\lambda_{o}\\mathcal{L}_{o},\n(6)\nwhere\nÎ»\nd\n=\n0.8\n,\nÎ»\nn\n=\nÎ»\nm\n=\n0.1\n\\lambda_{d}=0.8,\\lambda_{n}=\\lambda_{m}=0.1\n, and\nÎ»\no\n=\n0.01\n\\lambda_{o}=0.01\nare weighting coefficients that balance the contributions of each term.\nIII-A\n3\nProgressive Update of Gaussian Surfels\nDuring progressive object reconstruction through autonomous scanning, each newly captured frame provides additional observations that significantly overlap with previously reconstructed regions. To efficiently fuse new observations while minimizing redundant primitives, it is essential to identify surface areas that are either insufficiently reconstructed or uncovered. New Gaussian surfels are then selectively initialized in these regions to refine both geometry and appearance progressively. We observe that such under-reconstructed regions typically fall into four categories (see Fig.\n3\n): (i) Low opacity (\nm\nop\nm_{\\mathrm{op}}\n): Empty regions where the accumulated rendered opacity\nO\n^\n\\hat{O}\nfalls below a threshold\nÏ„\nO\n=\n0.5\n\\tau_{O}=0.5\n; (ii) photometric discrepancy (\nm\ntex\nm_{\\mathrm{tex}}\n): Pixels where the rendered color deviates from the observed color with a mean squared error (\nMSE\n) exceeding\nÏ„\nC\n=\n0.25\n\\tau_{C}=0.25\n; (iii) geometric deviation (\nm\ngeo\nm_{\\mathrm{geo}}\n): Regions where the rendered depth lies behind the observed depth, exceeding the mean depth error (\nMDE\n) by a factor of\nÎ»\n=\n2\n\\lambda=2\n; (iv) Back-facing surfaces (\nm\nback\nm_{\\mathrm{back}}\n): Regions where the rendered normal forms an acute angle with the camera optical axis\nğ’›\n\\boldsymbol{z}\n(i.e.,\nN\n^\nâ‹…\nğ’›\n>\n0\n\\hat{N}\\cdot\\boldsymbol{z}>0\n). This condition implies the camera is observing the â€backâ€ of an existing surface, yet the sensor detects valid geometry, indicating a need for refinement (e.g., for thin structures). We define the criterion-specific masks as follows:\nm\nop\nâ€‹\n(\nğ’–\n)\n\\displaystyle m_{\\mathrm{op}}(\\boldsymbol{u})\n:=\n[\nO\n^\nâ€‹\n(\nğ’–\n)\n<\nÏ„\nO\n]\n,\n\\displaystyle:=[\\,\\hat{O}(\\boldsymbol{u})<\\tau_{O}\\,],\n(7a)\nm\ntex\nâ€‹\n(\nğ’–\n)\n\\displaystyle m_{\\mathrm{tex}}(\\boldsymbol{u})\n:=\n[\nâ€–\nC\n^\nâ€‹\n(\nğ’–\n)\nâˆ’\nC\nâ€‹\n(\nğ’–\n)\nâ€–\n2\n2\n>\nÏ„\nC\n]\n,\n\\displaystyle:=[\\,\\|\\hat{C}(\\boldsymbol{u})-C(\\boldsymbol{u})\\|_{2}^{2}>\\tau_{C}\\,],\n(7b)\nm\ngeo\nâ€‹\n(\nğ’–\n)\n\\displaystyle m_{\\mathrm{geo}}(\\boldsymbol{u})\n:=\n[\nD\n^\nâ€‹\n(\nğ’–\n)\nâˆ’\nD\nâ€‹\n(\nğ’–\n)\n>\nÎ»\nâ‹…\nMDE\n]\n,\n\\displaystyle:=[\\,\\hat{D}(\\boldsymbol{u})-D(\\boldsymbol{u})>\\lambda\\cdot\\text{MDE}\\,],\n(7c)\nm\nback\nâ€‹\n(\nğ’–\n)\n\\displaystyle m_{\\mathrm{back}}(\\boldsymbol{u})\n:=\n[\nN\n^\nâ€‹\n(\nğ’–\n)\nâ‹…\nğ’›\n>\n0\n]\n,\nğ’–\nâˆˆ\nÎ©\n.\n\\displaystyle:=[\\,\\hat{N}(\\boldsymbol{u})\\cdot\\boldsymbol{z}>0\\,],\\qquad\\boldsymbol{u}\\in\\Omega.\n(7d)\nAccordingly, the unified update mask\nM\nM\nis the logical disjunction of these conditions:\nM\n=\n{\nğ’–\nâˆˆ\nÎ©\nâˆ£\nm\nop\nâˆ¨\nm\ntex\nâˆ¨\nm\ngeo\nâˆ¨\nm\nback\n}\nM=\\{\\boldsymbol{u}\\in\\Omega\\mid m_{\\mathrm{op}}\\lor m_{\\mathrm{tex}}\\lor m_{\\mathrm{geo}}\\lor m_{\\mathrm{back}}\\}\n, where\nÎ©\n\\Omega\ndenotes the set of pixels with valid sensor depth.\nFigure 3:\nIdentification of insufficiently reconstructed or uncovered regions.\nNew Gaussian surfels are selectively added to regions exhibiting insufficient opacity, significant photometric discrepancy, geometric deviation, or back-facing surfaces.\nFormally, the update mask\nM\nM\nhighlights candidate areas for new surfel insertion, allowing the system to progressively refine geometry and texture while avoiding redundant updates in well-reconstructed regions. For all pixels\nğ’–\nâˆˆ\nM\n\\boldsymbol{u}\\in M\n, we initialize new Gaussian surfels to fill the information gap. The center position\nğ\nnew\n\\boldsymbol{\\mu}_{\\text{new}}\nis recovered via back-projection using the current pose\nğ‘»\nc\nw\n\\boldsymbol{T}_{c}^{w}\n:\nğ\nnew\n=\nğ‘»\nc\nw\nâ‹…\n(\nD\nâ€‹\n(\nğ’–\n)\nâ€‹\nÎ \nâˆ’\n1\nâ€‹\n(\nğ’–\n)\n)\n,\n\\boldsymbol{\\mu}_{\\text{new}}=\\boldsymbol{T}_{c}^{w}\\cdot\\left(D(\\boldsymbol{u})\\Pi^{-1}(\\boldsymbol{u})\\right),\n(8)\nwhere\nÎ \nâˆ’\n1\n\\Pi^{-1}\nis the inverse projection operator. The color\nğ’„\n\\boldsymbol{c}\nis assigned from the image observation\nC\nâ€‹\n(\nğ’–\n)\nC(\\boldsymbol{u})\n, and the orientation quaternion\nğ’’\n\\boldsymbol{q}\nis aligned with the sensor-derived surface normal to get well geometric initialization. Finally, the scaling vector\nğ’”\n\\boldsymbol{s}\nis initialized adaptively based on the local point density, approximated by the mean distance to the\nk\nk\n-nearest neighbors in the back-projected point cloud, ensuring that newly added surfels maintain a consistent spatial distribution suitable for optimization.\nIII-B\nGeometry-Aware Viewpoint Evaluation\nIII-B\n1\nOcclusion-Aware Co-visibility Analysis\nAccurate co-visibility estimation is a prerequisite for both reliable historical keyframe selection in optimization (ref.\nIII-B\n2\n) and informative viewpoint evaluation (ref.\nIII-B\n3\n). Traditional approaches in scene-level mapping typically rely on depth-based heuristics\n[\n28\n]\n, which simply count the number of reprojection points falling within the keyframeâ€™s view frustum or determine visibility by rendering cumulative opacity\n[\n31\n]\n. While effective for large-scale scene mapping, these methods are often insufficient for object-level reconstruction, where the camera observes the target from outside, leading to frequent self-occlusions and non-closed surfaces. As illustrated in Fig.\n4\n, covisibility metrics that ignore occlusion tend to overestimate shared visibility, resulting in false correspondences and unreliable covisibility estimation.\nFigure 4:\nCo-visible views comparison.\nWhile conventional frustum-based metrics (\nLeft\n) fail to account for self-occlusion, leading to false correspondences, our rendering-based check (\nRight\n) explicitly detects occlusion and back-facing surfaces, thus correctly identifying\nvalid\nand\ninvalid\nco-visible points, ensuring valid geometric constraints and confidence updating.\nTo overcome these limitations, we introduce an\nocclusion-aware covisibility metric\nthat explicitly models geometric visibility between views using differentiable rendering. Specifically, given a source view\nI\ni\n=\n{\nD\ni\n,\nT\ni\nw\n}\nI_{i}=\\{D_{i},T_{i}^{w}\\}\nand a target view\nI\nj\n=\n{\nT\nj\nw\n}\nI_{j}=\\{T_{j}^{w}\\}\n, we randomly sample a subset of valid pixels\nğ’®\ni\nâŠ‚\nÎ©\ni\n\\mathcal{S}_{i}\\subset\\Omega_{i}\n(default\n|\nğ’®\ni\n|\n=\n1600\n|\\mathcal{S}_{i}|=1600\n). Each pixel\nğ’–\ni\nâˆˆ\nğ’®\ni\n\\boldsymbol{u}_{i}\\in\\mathcal{S}_{i}\nis back-projected to world coordinates using\nD\ni\nD_{i}\nand\nT\ni\nw\nT_{i}^{w}\n, then transformed to the coordinate frame of\nI\nj\nI_{j}\nand reprojected as:\nğ’–\nj\n=\nÎ \nâ€‹\n(\n(\nT\nj\nw\n)\nâˆ’\n1\nâ€‹\nT\ni\nw\nâ€‹\nÎ \nâˆ’\n1\nâ€‹\n(\nğ’–\ni\n)\n)\n,\n\\boldsymbol{u}_{j}=\\Pi\\!\\left((T_{j}^{w})^{-1}T_{i}^{w}\\Pi^{-1}(\\boldsymbol{u}_{i})\\right),\n(9)\nwhere\nÎ \n\\Pi\nand\nÎ \nâˆ’\n1\n\\Pi^{-1}\ndenote projection and back-projection functions parameterized by camera intrinsics. We then render the candidate view\nI\nj\nI_{j}\nfrom the current GSurfels model to obtain its depth\nD\n^\nj\n\\hat{D}_{j}\nand normal map\nN\n^\nj\n\\hat{N}_{j}\n. For each valid correspondence\n(\nğ’–\ni\n,\nğ’–\nj\n)\n(\\boldsymbol{u}_{i},\\boldsymbol{u}_{j})\n, let\nD\ni\nâ†’\nj\nproj\nâ€‹\n(\nğ’–\nj\n)\nD^{\\mathrm{proj}}_{i\\!\\to\\!j}(\\boldsymbol{u}_{j})\nbe the depth of the reprojected 3D point in\nI\nj\nI_{j}\n. A correspondence is marked as\ninvalid\nif the reprojected point is occluded by the rendered surface or falls on a back-facing region (ref Eq.\n7d\n). Formally, the invalid covisibility mask is defined as:\nm\ninv\nâ€‹\n(\nğ’–\nj\n)\n=\n[\nD\ni\nâ†’\nj\nproj\nâ€‹\n(\nğ’–\nj\n)\nâˆ’\nD\n^\nj\nâ€‹\n(\nğ’–\nj\n)\n>\nÏ„\nd\n]\nâˆ¨\nm\nback\nâ€‹\n(\nğ’–\nj\n)\n,\nm_{\\mathrm{inv}}(\\boldsymbol{u}_{j})=\\big[\\,D^{\\mathrm{proj}}_{i\\!\\to\\!j}(\\boldsymbol{u}_{j})-\\hat{D}_{j}(\\boldsymbol{u}_{j})>\\tau_{d}\\,\\big]\\,\\lor\\,m_{\\mathrm{back}}(\\boldsymbol{u}_{j}),\n(10)\nwhere\nÏ„\nd\n\\tau_{d}\nis a depth tolerance threshold. The covisibility ratio\nÏ\ni\n,\nj\n\\rho_{i,j}\nis further defined as the proportion of sampled pixels\nğ’®\ni\n\\mathcal{S}_{i}\nthat maintain valid covisibility in the target view\nI\nj\nI_{j}\n:\nÏ\ni\n,\nj\n=\n|\nğ’®\ni\nâ†’\nj\n|\nâˆ’\nâˆ‘\nğ’–\nj\nâˆˆ\nğ’®\ni\nâ†’\nj\nğ•€\nâ€‹\n[\nm\ninv\nâ€‹\n(\nğ’–\nj\n)\n]\n|\nğ’®\ni\n|\n=\n|\nÎ©\ni\n,\nj\n|\n|\nğ’®\ni\n|\n,\n\\rho_{i,j}=\\frac{|\\mathcal{S}_{i\\!\\to\\!j}|-\\sum_{\\boldsymbol{u}_{j}\\in\\mathcal{S}_{i\\!\\to\\!j}}\\mathbb{I}\\!\\left[m_{\\mathrm{inv}}(\\boldsymbol{u}_{j})\\right]}{|\\mathcal{S}_{i}|}=\\frac{|\\Omega_{i,j}|}{|\\mathcal{S}_{i}|},\n(11)\nwhere\nğ’®\ni\nâ†’\nj\nâŠ†\nğ’®\ni\n\\mathcal{S}_{i\\!\\to\\!j}\\!\\subseteq\\!\\mathcal{S}_{i}\nrepresents the subset of points whose reprojections fall within the field of view of\nI\nj\nI_{j}\n, and\nÎ©\ni\n,\nj\n\\Omega_{i,j}\ndenotes the subset of\nğ’®\ni\nâ†’\nj\n\\mathcal{S}_{i\\!\\to\\!j}\nthat remains valid after excluding occluded and back-facing points. This metric provides a robust measure of geometric overlap, essential for maintaining consistency. If no valid projection exists (\n|\nğ’®\ni\nâ†’\nj\n|\n=\n0\n|\\mathcal{S}_{i\\!\\to\\!j}|=0\n), we set\nÏ\ni\n,\nj\n=\n0\n\\rho_{i,j}=0\n.\nIII-B\n2\nGlobalâ€“Local View Selection\nDuring online reconstruction, Gaussian surfel attributes are updated through differentiable rendering and gradient-based optimization. Unlike the offline refinement stage, jointly optimizing all historical frames is computationally inefficient and unnecessary for incremental reconstruction. Instead, we employ a globalâ€“local view selection strategy inspired by GS-based SLAM\n[\n31\n,\n46\n]\nto balance local adaptability with long-term global consistency.\nLocal views\n: To refine newly observed surface regions through multi-view constraints, we maintain a local window\nğ’²\nlocal\n\\mathcal{W}_{\\text{local}}\n. Given the current view\nI\nt\nI_{t}\n, we select the top-\nk\nk\nviews with the highest covisibility ratio\nÏ\nt\n,\nj\n\\rho_{t,j}\n(Eq.\n11\n). These views jointly observe the newly captured regions, providing dense geometric and photometric constraints for incremental optimization of the corresponding GSurfels.\nGlobal views\n: To mitigate the overfitting of newly observed regions caused by relying solely on local views and to ensure global consistency, we additionally maintain a sparse set of global views\nğ’²\nglobal\n\\mathcal{W}_{\\text{global}}\n, sampled from earlier keyframes outside the local window. To ensure temporal balance, we follow the reweighting scheme in\n[\n46\n]\n, increasing the sampling probability of early or under-optimized keyframes.\nThis hybrid selection strategy prioritizes covisible keyframes for accurate local updates while preserving long-range consistency across the reconstruction.\nIII-B\n3\nGeometry-Aware Uncertainty Qualification\nAccurate online evaluation of reconstruction quality and completeness is crucial for active reconstruction, as it enables the robot to identify under-reconstructed regions and plan informative subsequent viewpoints. However, conventional uncertainty metrics\n[\n20\n,\n28\n,\n22\n]\noften overlook self-occlusion and back-face visibility, leading to unreliable viewpoint evaluation, particularly in object-level reconstruction, where the camera observes the target from an external perspective and the surface is often open or partially occluded. To address this, we propose a\ngeometry-aware uncertainty map\nthat jointly encodes surfel confidence, back-face observation, and visibility-based completeness. This map provides a dense, view-dependent quality estimate for any candidate viewpoint, serving as a guiding signal for subsequent path planning. As illustrated in Fig.\n2\n, it consists of three components: (1) surfel-wise confidence modeling, (2) back-face observation detection, and (3) visibility-based completeness estimation.\nFigure 5:\nUncertainty qualification comparison on the\nBunny Racer\nobject. Our method enables fine-grained uncertainty evaluation with millisecond-level latency, effectively addressing complex geometric challenges such as self-occlusions and thin structures.\nSurfel-wise Confidence Modeling\nWe assign a confidence attribute\nÎº\ni\nâˆˆ\n[\n0\n,\n1\n]\n\\kappa_{i}\\in[0,1]\nto each Gaussian surfel\ng\ni\ng_{i}\nto quantify the reliability of its historical observations. Extending the formulation in\n[\n22\n]\nto the object-centric setting, we explicitly incorporate (i) front-view weighting to prioritize near-orthogonal observations and (ii) occlusion-aware validity checks to discard back-facing or occluded surfels. Upon acquiring a new view\nI\nt\nI_{t}\n, we identify the set of effectively observed surfels\nğ’¢\nvalid\nâ€‹\n(\nI\nt\n)\n\\mathcal{G}^{\\text{valid}}(I_{t})\nusing the validity check in Eq.\n10\n. For each\ng\ni\nâˆˆ\nğ’¢\nvalid\nâ€‹\n(\nI\nt\n)\ng_{i}\\in\\mathcal{G}^{\\text{valid}}(I_{t})\n, we update its confidence by aggregating contributions from its set of valid historical views\nS\nvalid\nâ€‹\n(\ng\ni\n)\nS^{\\text{valid}}(g_{i})\n(including the current view\nI\nt\nI_{t}\n). Specifically, for each view\nj\nâˆˆ\nS\nvalid\nâ€‹\n(\ng\ni\n)\nj\\in S^{\\text{valid}}(g_{i})\n, we compute a geometric weight\nw\ni\nâ€‹\nj\n=\nw\ni\nâ€‹\nj\ndist\nâ€‹\nw\ni\nâ€‹\nj\nfront\nw_{ij}=w^{\\text{dist}}_{ij}w^{\\text{front}}_{ij}\nto penalize distant or oblique observations:\nw\ni\nâ€‹\nj\ndist\n\\displaystyle w^{\\text{dist}}_{ij}\n=\nmax\nâ¡\n(\n0\n,\n1\nâˆ’\nd\ni\nâ€‹\nj\nd\nmax\n)\n,\n\\displaystyle=\\max\\!\\left(0,1-\\frac{d_{ij}}{d_{\\max}}\\right),\n(12a)\nw\ni\nâ€‹\nj\nfront\n\\displaystyle w^{\\text{front}}_{ij}\n=\nÏƒ\nâ€‹\n(\nğ§\ni\nâŠ¤\nâ€‹\nğ¯\ni\nâ€‹\nj\nâˆ’\nc\n0\nÏ„\n)\n,\n\\displaystyle=\\sigma\\!\\left(\\frac{\\mathbf{n}_{i}^{\\top}\\mathbf{v}_{ij}-c_{0}}{\\tau}\\right),\n(12b)\nwhere\nğ¯\ni\nâ€‹\nj\n\\mathbf{v}_{ij}\ndenotes the normalized viewing direction from the surfel center\nğ\ni\n\\boldsymbol{\\mu}_{i}\nto the camera center\nğ±\nj\np\n\\mathbf{x}^{p}_{j}\n,\nd\ni\nâ€‹\nj\nd_{ij}\nis the distance, and the\nÏƒ\nâ€‹\n(\nâ‹…\n)\n\\sigma(\\cdot)\nis a sigmoid function prioritizing near-frontal views, configured with threshold\nc\n0\n=\n0.5\nc_{0}=0.5\n(approx.\n60\nâˆ˜\n60^{\\circ}\n) and temperature\nÏ„\n=\n0.1\n\\tau=0.1\n. Furthermore, to encourage multi-view coverage, we incorporate angular diversity\nÎ²\ni\n=\n1\nâˆ’\nâ€–\nğ\ni\nâ€–\n\\beta_{i}=1-\\|\\boldsymbol{\\mu}_{i}\\|\nbased on the mean viewing direction of\nS\nvalid\nâ€‹\n(\ng\ni\n)\nS^{\\text{valid}}(g_{i})\n. The final confidence\nÎº\ni\n\\kappa_{i}\nis derived by combining the accumulated geometric weights with the angular diversity factor:\nÎ³\ni\n=\nâˆ‘\nj\nâˆˆ\nS\nvalid\nâ€‹\n(\ng\ni\n)\nw\ni\nâ€‹\nj\nâ€‹\nmax\nâ¡\n(\n0\n,\nğ§\ni\nâŠ¤\nâ€‹\nğ¯\ni\nâ€‹\nj\n)\n,\nÎº\ni\nâ†\nÎ³\ni\nâ€‹\nexp\nâ¡\n(\nÎ²\ni\n)\n.\n\\gamma_{i}=\\sum_{j\\in S^{\\text{valid}}(g_{i})}w_{ij}\\,\\max(0,\\mathbf{n}_{i}^{\\top}\\mathbf{v}_{ij}),\\quad\\kappa_{i}\\leftarrow\\gamma_{i}\\exp(\\beta_{i}).\n(13)\nThis update rule ensures that high confidence is assigned only when surfels are observed from diverse, non-occluded, and near-orthogonal viewpoints, whereas those seen from sparse or oblique views receive lower scores.\nBack-Face Observation\nWe detect back-facing regions where the rendered normal\nN\n^\n\\hat{N}\nforms an acute angle with the cameraâ€™s optical axis\nğ³\n\\mathbf{z}\n. These areas are marked as potentially incomplete, as they are viewed from the â€wrongâ€ side. This encourages exploration from opposing directions. The back-face score for a pixel\nu\nu\nis defined as:\nB\nâ€‹\n(\nğ’–\n)\n=\nmax\nâ¡\n(\n0\n,\nN\n^\nâ€‹\n(\nğ’–\n)\nâŠ¤\nâ€‹\nğ’›\n)\n.\nB(\\boldsymbol{u})=\\max(0,\\hat{N}(\\boldsymbol{u})^{\\top}\\boldsymbol{z}).\n(14)\nVisibility-based Completeness\nWe use the rendered opacity\nO\n^\nâ€‹\n(\nu\n)\n\\hat{O}(u)\nfrom current GSurfels as a direct measure of geometric coverage. Regions with low opacity indicate insufficient surfel density or unobserved space, providing a dense metric for reconstruction completeness. We define the visibility-based uncertainty score as:\nV\nâ€‹\n(\nu\n)\n=\n1\nâˆ’\nO\n^\nâ€‹\n(\nğ’–\n)\n.\nV(u)=1-\\hat{O}(\\boldsymbol{u}).\n(15)\nCombining these components, the overall geometry-aware uncertainty map\nU\n^\nâ€‹\n(\nğ’–\n)\n\\hat{U}(\\boldsymbol{u})\nfor a candidate view is formulated as a weighted sum:\nU\n^\nâ€‹\n(\nğ’–\n)\n=\nÎ»\nk\nâ€‹\n(\n1\nâˆ’\nK\nâ€‹\n(\nğ’–\n)\n)\n+\nÎ»\nb\nâ€‹\nB\nâ€‹\n(\nğ’–\n)\n+\nÎ»\nv\nâ€‹\nV\nâ€‹\n(\nğ’–\n)\n,\n\\hat{U}(\\boldsymbol{u})=\\lambda_{k}(1-K(\\boldsymbol{u}))+\\lambda_{b}B(\\boldsymbol{u})+\\lambda_{v}V(\\boldsymbol{u}),\n(16)\nwhere\nK\nâ€‹\n(\nğ’–\n)\nK(\\boldsymbol{u})\nis the rendered confidence map derived from surfel attributes\nÎº\ni\n\\kappa_{i}\n. The coefficients\n(\nÎ»\nk\n,\nÎ»\nb\n,\nÎ»\nv\n)\n(\\lambda_{k},\\lambda_{b},\\lambda_{v})\ndynamically balance the relative importance of confidence, back-face, and visibility terms throughout the reconstruction process, as detailed in\nSec.\nIII-C\n3\n. Benefiting from the CUDA-accelerated rasterization in Gaussian splatting, our system renders this map in about 2 ms, enabling real-time evaluation for path planning.\nIII-C\nNext-Best-Path Planning\nTo achieve both efficient and thorough reconstruction, the robot must plan a globally-aware scanning trajectory that optimizes the trade-off between exploration gain and movement cost. Moving beyond the greedy, single-step nature of traditional NBV planning, we propose a Next-Best-Path (NBP) planning strategy that performs multi-step lookahead on dynamically constructed spatial topology, jointly considering the accumulated information gainâ€”derived from our geometry-aware uncertainty metric (Eq.\n16\n)â€”and traversal cost. The planning process consists of two main stages: candidate viewpoint generation and path planning.\nIII-C\n1\nCandidate Viewpoint Generation\nWe generate candidate viewpoints on a hemispherical shell defined around the objectâ€™s current bounding box, which is updated online as the Gaussian surfel model expands. This ensures the entire object remains within the cameraâ€™s field of view. Specifically, we define a sphere with a radius equal to the sum of the boxâ€™s half-diagonal and an optimal standoff distance (related to focal length). To ensure uniform coverage, we distribute candidate positions on the sphere using a Vogel spiral pattern\n[\n43\n]\n. To prevent redundant exploration, previously visited viewpoints are pruned from the candidate set. Each candidate viewpoint is oriented to face the center of the bounding box, ensuring object-centric observation. This uniform sampling strategy generates a sparse yet comprehensive set of candidates that adaptively adjusts to the objectâ€™s perceived size and position.\nIII-C\n2\nNext-Best-Path Strategy\nWith a set of candidate viewpoints\nğ’±\ncand\n\\mathcal{V}_{\\text{cand}}\nand their corresponding uncertainty scores\n{\nU\n(\nv\n)\nâˆ£\nv\nâˆˆ\nğ’±\ncand\n\\{U(v)\\mid v\\in\\mathcal{V}_{\\text{cand}}\n, our goal is to determine a trajectory that maximizes total information gain while minimizing travel distance. We formulate this as a variant of the prize-collecting traveling salesman problem (PC-TSP)\n[\n1\n]\n. First, we identify a long-term goal\nv\ng\nâˆˆ\nğ’±\ncand\nv_{g}\\in\\mathcal{V}_{\\text{cand}}\n, defined as the viewpoint with the highest uncertainty score. The problem then reduces to finding an optimal path from the current pose\nv\nt\nv_{t}\nto\nv\ng\nv_{g}\nthat collects the most â€prizesâ€ (information gain) from intermediate viewpoints at a minimal travel cost.\nWe solve this by constructing a spatial topology graph\nG\n=\n(\nV\n,\nE\n)\nG=(V,E)\n, where the node set\nV\n=\nğ’±\ncand\nâˆª\n{\nv\nt\n}\nV=\\mathcal{V}_{\\text{cand}}\\cup\\{v_{t}\\}\n. Edges are formed by connecting each viewpoint to its\nk\nk\n-nearest neighbors (\nk\nk\n-NN) in Euclidean space. The current view\nv\nt\nv_{t}\nis also integrated into the graph by connecting it to its own\nk\nk\nnearest candidate viewpoints. The graph structure provides a sparse yet effective representation of the feasible motion pathways. To guide the path search, we assign an uncertainty-aware weight to each edge\n(\nv\ni\n,\nv\nj\n)\nâˆˆ\nE\n(v_{i},v_{j})\\in E\nthat balances the motion distance with the potential information gain:\nw\nâ€‹\n(\nv\ni\n,\nv\nj\n)\n=\nd\ni\nâ€‹\nj\nÎ±\n+\nÎ²\nâ€‹\n(\nU\nâ€‹\n(\nv\ni\n)\n+\nU\nâ€‹\n(\nv\nj\n)\n)\n,\nw(v_{i},v_{j})=\\frac{d_{ij}}{\\alpha+\\beta\\left(U(v_{i})+U(v_{j})\\right)},\n(17)\nwhere\nd\ni\nâ€‹\nj\nd_{ij}\nis the Euclidean distance between viewpoints\nv\ni\nv_{i}\nand\nv\nj\nv_{j}\n, and\nÎ±\n,\nÎ²\n>\n0\n\\alpha,\\beta>0\nare constants preventing division by zero and scaling the reward. This formulation ensures that edges connecting high-uncertainty viewpoints have lower weights, making them more attractive to a shortest-path algorithm. In here, finding the single optimal path is a multi-objective optimization problem. Instead of greedily selecting a single shortest path, we search for a set of candidate paths. We first find the top-\nM\nM\nshortest simple paths\n[\n53\n]\nfrom\nv\nt\nv_{t}\nto\nv\ng\nv_{g}\non the weighted graph\nG\nG\n. Each candidate path\nÏ€\n=\n(\nv\nt\n,\nn\n1\n,\nâ€¦\n,\nn\nL\n=\nv\ng\n)\n\\pi=(v_{t},n_{1},\\dots,n_{L}{=}v_{g})\nis then evaluated by explicitly trading off its total collected reward against its path length:\nJ\nâ€‹\n(\nÏ€\n)\n=\nÎ»\nâ€‹\nâˆ‘\nv\nâˆˆ\nÏ€\nU\nÂ¯\nâ€‹\n(\nv\n)\nâˆ’\n(\n1\nâˆ’\nÎ»\n)\nâ€‹\nâˆ‘\nk\n=\n0\nL\nâˆ’\n1\nd\nÂ¯\nâ€‹\n(\nn\nk\n,\nn\nk\n+\n1\n)\n,\nJ(\\pi)=\\lambda\\sum_{v\\in\\pi}\\bar{U}(v)-(1-\\lambda)\\sum_{k=0}^{L-1}\\bar{d}(n_{k},n_{k+1}),\n(18)\nwhere\n(\nâ‹…\n)\nÂ¯\n\\bar{(\\cdot)}\nis the normalization operator, and\nÎ»\nâˆˆ\n[\n0\n,\n1\n]\n\\lambda\\in[0,1]\nis a weighting factor for exploration vs. efficiency. The path\nÏ€\nâ‹†\n\\pi^{\\star}\nwith the maximum score is selected as the next-best-path\nÏ€\nâˆ—\n=\narg\nâ¡\nmax\nÏ€\nâ¡\nJ\nâ€‹\n(\nÏ€\n)\n\\pi^{*}=\\arg\\max_{\\pi}J(\\pi)\nto execute. This strategy allows the system to perform multi-step lookahead, escaping local optima often encountered in greedy NBV planning, resulting in smoother and more globally efficient trajectories.\nAlgorithm 1\nNext-Best-Path (NBP) Planning\n1:\nInput:\nCurrent pose\nv\nt\nv_{t}\n, Constructed Model\nğ’¢\nt\n\\mathcal{G}_{t}\n, Uncertainty map renderer\nU\n^\n\\hat{U}\n2:\nOutput:\nAn optimal path of viewpoints\nÏ€\nâ‹†\n\\pi^{\\star}\n3:\nğ’±\ncand\nâ†\nGenerateCandidates\nâ€‹\n(\nğ’¢\nt\n)\n\\mathcal{V}_{\\text{cand}}\\leftarrow\\text{GenerateCandidates}(\\mathcal{G}_{t})\nâŠ³\n\\triangleright\nSec.\nIII-C\n1\n4:\nğ’±\ncand\nâ†\nPruneVisited\nâ€‹\n(\nğ’±\ncand\n)\n\\mathcal{V}_{\\text{cand}}\\leftarrow\\text{PruneVisited}(\\mathcal{V}_{\\text{cand}})\n5:\nv\ng\nâ†\narg\nâ¡\nmax\nv\nâˆˆ\nğ’±\ncand\nâ¡\nU\n^\nâ€‹\n(\nv\n)\nv_{g}\\leftarrow\\arg\\max_{v\\in\\mathcal{V}_{\\text{cand}}}\\hat{U}(v)\nâŠ³\n\\triangleright\nIdentify Long-term goal\n6:\nG\nâ†\nBuildKNNGraph\nâ€‹\n(\nğ’±\ncand\nâˆª\n{\nv\nt\n}\n)\nG\\leftarrow\\text{BuildKNNGraph}(\\mathcal{V}_{\\text{cand}}\\cup\\{v_{t}\\})\n7:\nAssign edge weights in\nG\nG\nusing Eq.\n17\n8:\nÎ \nâ†\nFindTopKShortestPaths\nâ€‹\n(\nG\n,\nv\nt\n,\nv\ng\n)\n\\Pi\\leftarrow\\text{FindTopKShortestPaths}(G,v_{t},v_{g})\n9:\nÏ€\nâ‹†\nâ†\narg\nâ¡\nmax\nÏ€\nâˆˆ\nÎ \nâ¡\nJ\nâ€‹\n(\nÏ€\n)\n\\pi^{\\star}\\leftarrow\\arg\\max_{\\pi\\in\\Pi}J(\\pi)\nâŠ³\n\\triangleright\nSelect Optimal Path\n10:\nreturn\nÏ€\nâ‹†\n\\pi^{\\star}\nAs summarized in Algorithm\n1\n, the NBP planner generates an optimal path\nÏ€\nâ‹†\n\\pi^{\\star}\nfrom the current pose to a sub-goal\nv\ng\nv_{g}\n. The robot sequentially visits viewpoints in the optimal path\nÏ€\nâ‹†\n\\pi^{\\star}\n, capturing new sensor data to incrementally update the object model at each step. Upon reaching the sub-goal\nv\ng\nv_{g}\n, the system re-evaluates the global uncertainty and triggers a new NBP planning cycle. This iterative replanning strategy ensures continuous adaptation to the evolving reconstruction while reducing computational overhead compared to per-step planning. The process terminates when the average reconstruction quality uncertainty\nU\n^\nq\nâ€‹\n(\nğ’–\n)\n=\nÎ»\nb\nâ€‹\nB\nâ€‹\n(\nğ’–\n)\n+\nÎ»\nv\nâ€‹\nV\nâ€‹\n(\nğ’–\n)\n\\hat{U}_{q}(\\boldsymbol{u})=\\lambda_{b}B(\\boldsymbol{u})+\\lambda_{v}V(\\boldsymbol{u})\nfalls below a predefined threshold\nÏ„\nstop\n\\tau_{\\text{stop}}\n).\nIII-C\n3\nDynamic Uncertainty Weighting\nTo balance exploration and exploitation effectively throughout the reconstruction process, we employ a dynamic weighting strategy for the components of the uncertainty map from Eq.\n16\n.\nIn the early stages of reconstruction, when object information is Insufficient, the system prioritizes exploration (\nÎ»\nk\n=\nÎ»\nb\n=\nÎ»\nv\n=\n1\n\\lambda_{k}=\\lambda_{b}=\\lambda_{v}=1\n) to rapidly cover the objectâ€™s surface, focusing on low-confidence, back-facing, and poor visibility (low-opacity) regions. As the reconstruction progresses and more of the object becomes observed, the focus shifts to exploitationâ€”refining the quality of the reconstructed model. Once visibility-based uncertainty stabilizes, we de-prioritize the visibility term by setting\nÎ»\nv\n=\n0\n\\lambda_{v}=0\n, concentrating planning efforts on resolving subtle defects like back-facing surfaces and low-confidence areas, thereby ensuring high-fidelity final quality.\nIII-D\nImplementation Details\nTo expand the robotâ€™s observation space, our experimental platform comprises a robotic arm equipped with an eye-in-hand RGB-D camera and a motorized turntable positioned 0.7â€‰m in front of the robot base (see Fig.\n6\n) to handle small to medium-sized unknown objects. The object is placed on the turntable, which is treated as the origin of the world coordinate system. By pre-calibrating the eye-in-hand transformation\nğ‘»\nc\nb\n\\boldsymbol{T}^{b}_{c}\n(camera to robot base) and the turntable-to-base transform\nğ‘»\nt\nb\n\\boldsymbol{T}^{b}_{t}\n, we derive the camera pose in the object frame as\nğ‘»\nc\n=\n(\nğ‘»\nt\nb\n)\nâˆ’\n1\nâ€‹\nğ‘»\nc\nb\n\\boldsymbol{T}_{c}=(\\boldsymbol{T}^{b}_{t})^{-1}\\boldsymbol{T}^{b}_{c}\n. Consequently, all view planning is conducted within this object-centric frame. To handle kinematic constraints, we implement a coordinated motion strategy: if a target viewpoint is unreachable by the arm, the turntable rotates to bring it into the feasible workspace, effectively extending the systemâ€™s operational range.\nIII-D\n1\nObject-Centric Pose Tracking\nAccurate and robust camera pose tracking is critical for coherent fusion. In real-world scenarios, inherent motion noise in the robotic arm and minor calibration inaccuracies can lead to pose drift over time. We mitigate this via a robust point-cloud registration-based object-centric tracking pipeline. For each incoming RGB-D frame, we first employ the segment model\n[\n39\n]\nto extract the object of interest from the background, yielding a clean, object-level point cloud. Subsequently, we perform pairwise point-to-point registration between the current cloud and the accumulated global map from the previous keyframe. This step effectively compensates for slight pose inaccuracies from the robotâ€™s odometry, ensuring globally consistent alignment.\nIII-D\n2\nOnline Mapping and Offline Refinement\nLeveraging the consistent parameter space of Gaussian splatting-based approaches, we decompose the reconstruction process into two stages:\nonline mapping\nand\noffline refinement\n. This allows for post-processing of all observations collected during the active scanning, thereby enhancing reconstruction fidelity. In the\nonline mapping\nstage, efficiency is prioritized for real-time performance. We simplify the GSurfel representation by setting the SH degree to\nL\nD\n=\n0\nL_{D}\\!=\\!0\n, effectively reducing memory usage and computational overhead. Each new frame undergoes 10 optimization iterations, while the local optimization window is restricted to the top-\nk\n=\n9\nk\\!=\\!9\nmost covisible keyframes. Once data acquisition is complete, the\noffline refinement\nstage focuses on improving global consistency and visual quality. We conduct 7,000 optimization iterations while restoring higher-order SH coefficients (\nL\nD\n=\n3\nL_{D}\\!=\\!3\n) to achieve photorealistic rendering. Both camera poses and GSurfel parameters are jointly optimized. To mitigate photometric inconsistencies caused by illumination variation during scanning, we additionally optimize per-frame exposure compensation parameters, yielding globally consistent color appearance\n[\n57\n]\n. Finally, to generate physically consistent assets for downstream tasks, we extract surface meshes via TSDF fusion or Poisson reconstruction\n[\n23\n]\nusing depth and normal maps rendered from the refined model.\nIV\nExperiments\nTo comprehensively evaluate ObjSplat, we conduct extensive experiments in both high-fidelity simulation and real-world scenarios. We aim to validate the systemâ€™s reconstruction quality, exploration efficiency, and robustness against diverse geometric and textural complexities. All experiments are performed on a desktop PC equipped with an Intel Core i9-13900K CPU and a single NVIDIA RTX 4090 GPU, running Ubuntu 20.04 LTS with ROS Noetic for inter-module communication. The experimental setup is depicted in Fig.\n6\n.\nFigure 6:\nExperimental Platforms.\n(\nLeft\n) The\nGazebo\nsimulation environment provides realistic rendering and physics-based robot models for quantitative evaluation. (\nRight\n) The real-world platform comprises an Elfin-05 robotic arm equipped with a calibrated industrial 3D camera. The system architecture is built on modular ROS nodes for Gaussian mapping, NBP planning, and visualization across both domains.\nIV-A\nExperimental Setup\nIV-A\n1\nBaselines\nWe compare ObjSplat against a comprehensive set of state-of-the-art (SOTA) active object reconstruction methods, spanning geometry-based planning to information-theoretic radiance field exploration: (i) SEE\n[\n3\n]\n: A frontier-based method that operates directly on point clouds to explore visible surface edges, serving as a strong baseline for geometric completeness; (ii) PB-NBV\n[\n19\n]\n: A projection-based greedy NBV planner that maximizes voxel-based information gain, representing the single-step planning paradigm; (iii) MA-SCVP\n[\n33\n]\n: A point-cloud-based\none-shot view planning\nframework that solves a set covering optimization problem, representing global planning paradigm within geometric reconstruction approaches; (iv) FisherRF\n[\n20\n]\n: An information-theoretic approach that leverages the Fisher Information to quantify the parameter uncertainty of the underlying radiance field, guiding view selection to maximize expected information gain; (v) GauSS-MI\n[\n47\n]\n: A recent information-theoretic approach that leverages the Gaussian splatting Shannon mutual information to quantify the expected reduction in visual uncertainty.\nIV-A\n2\nDatasets and Scenarios\nSimulation\nWe utilize 16 diverse models from the Google Scanned Objects (GSO) dataset\n[\n9\n]\n, which offers high-quality ground-truth models, selected for their challenging attributes such as thin structures, hollow geometries, and fine-grained textures (see Fig.\n6\n). Each object is normalized to fit within a bounding box diagonal of [0.1, 0.35]Â m. Following prior work\n[\n33\n]\n, a simulated RGB-D camera with a resolution of\n1280\nÃ—\n720\n1280\\times 720\n, a field of view of\n90\nâˆ˜\nÃ—\n65\nâˆ˜\n90^{\\circ}\\times 65^{\\circ}\n, and a depth sensing range of [0.1, 3.0]â€‰m is mounted on the robot end-effector. All methods are initialized from an identical pose to ensure fair comparison.\nReal-world\nWe select four challenging objects to assess robustness: (1) a\nSika Deer\nsculpture, featuring complex geometry and fine details; (2) a\nFu Hao Owl Zun\nreplica, characterized by complex geometric topology yet exhibiting a relatively uniform texture; and (3)\nSancai Horse\nand\nPottery Figure\nreplicas, both distinguished by their intricate patterns and rich, high-frequency textural details.\nIV-B\nEvaluation Metircs\nFollowing\n[\n52\n]\nand\n[\n33\n]\n, we evaluate both reconstruction quality and process efficiency. All quality metrics are computed within object masks to avoid background influence.\nIV-B\n1\nReconstruction Quality\nFor appearance, we report the peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS)\n[\n24\n]\non rendered images from both training and novel views (sampled from unvisited candidate views). For geometry, we first assess multi-view shape accuracy by reporting the depth L1 error (cm) on rendered depth maps. We then extract a mesh from the reconstructed Gaussian model, align it to the ground-truth mesh using ICP, and uniformly sample 200k points from both meshes. Geometric fidelity is quantified using the Chamfer Distance (CD) and F-Score@0.005m\n[\n52\n]\n. CD measures the average bidirectional nearest-neighbor distance, capturing fine geometric deviations, while F-Score reflects both local precision and surface completeness (recall).\nIV-B\n2\nCompleteness and Efficiency\nTo evaluate autonomous reconstruction performance, we use: (1) Completion ratio (CR,\n%\n\\%\n) and completion (cm), quantifying the proportion of the target scene successfully scanned; (2) Movement cost (m), the total distance between all viewpoints; (3) Required views, the number of viewpoints; and (4) Time cost (s), including both the entire online autonomous scanning/reconstruction time and the offline refinement time.\nIV-C\nReconstruction Performance Analysis\nIV-C\n1\nQualitative Analysis of Reconstruction Quality\nFig.\n7\npresents a qualitative comparison of six representative objects. All methods underwent a standardized offline refinement phase consisting of 7,000 iterations for fair comparison. Baselines (GauSS-MI\n[\n47\n]\nand FisherRF\n[\n20\n]\n) use their default color refinement scheme from MonoGS\n[\n31\n]\n.\nGauSS-MI, relying on vanilla 3DGS, exhibits noticeable degradation under sparse observations. Lacking explicit multi-view geometric constraints, it tends to produce blurry textures and significant floating artifacts around the object, as unconstrained Gaussians overfit to sparse input views. In contrast, ObjSplat reconstructs complete, photorealistic models with sharp details. While PSNR values are comparable in certain cases, our method achieves significant improvements in perceptual metrics (i.e., LPIPS), reflecting superior visual fidelity, particularly on objects with fine-grained patterns as the\nBackpack\nand\nCup\n. We attribute this performance to the synergy between our active planning strategy and robust representation: (1) Informative Data Acquisition: Our geometry-aware uncertainty module targets geometric defects (e.g., back-facing surfaces, low opacity) and under-observed regions. This guides the robot to acquire high-information observations that efficiently resolve geometric ambiguities, ensuring comprehensive surface coverage within a limited view budget; and (2) Robust Regularization: The adoption of Gaussian surfels, coupled with geometric consistency constraints, introduces a strong inductive bias towards surface manifolds. This effectively regularizes optimization in sparse-view settings, suppressing floating artifacts and enhancing novel-view synthesis fidelity.\nFigure 7:\nQualitative comparison of reconstruction results on six representative objects from the GSO dataset. ObjSplat recovers sharp textures and clean geometry, whereas baselines suffer from blurriness and floating artifacts due to overfitting to sparse views. The bottom row of each picture shows the average novel PSNR (dB) and LPIPS.\nIV-C\n2\nQuantitative Evaluation of Reconstruction Process\nTables\nI\nand\nII\nprovide a detailed quantitative breakdown of the reconstruction process for different methods across three distinct phases: initialization (2 views), exploration (10 views), and convergence (30 views).\nRobust Initialization:\nIn the early stage (\nN\n=\n2\nN=2\n), both our variants (NBV and NBP) significantly outperform baselines (FisherRF, GauSS-MI) in completeness (CR\nâˆ¼\n\\sim\n40% vs.\nâˆ¼\n\\sim\n18%) and geometric quality (F-Score\nâˆ¼\n\\sim\n0.65 vs.\nâˆ¼\n\\sim\n0.32). The quantitative gap validates that the Gaussian surfel representation provides strong geometric priors, enabling robust reconstruction even with extremely sparse observations. In contrast, baselines struggle to infer correct geometry from limited views, resulting in higher depth errors (D-L1).\nThe Efficiency-Completeness Trade-off:\nAt the exploration stage (\nN\n=\n10\nN=10\n), we observe a distinct behavioral divergence between the greedy NBV and our NBP planner. Greedy NBV strategies generally exhibit strong novel-view synthesis capabilities (Test PSNR\n>\n27\n>27\ndB) early on. Specifically, Ours-NBV variant achieves high completeness (\n>\n90\n%\n>90\\%\n) and rendering quality (Â¿30dB) by aggressively transiting to distant, high-information viewpoints. This confirms that our geometry-aware uncertainty evaluation effectively identifies under-reconstructed regions.\nIn contrast, the proposed NBP method adopts a more conservative yet efficient strategy, prioritizing path efficiency and local continuity. While achieving moderate coverage (74.41%) at this stage, it does so with minimal movement (1.17 m). This indicates that the robot explores the local neighborhood thoroughly before transitioning to distant regions, avoiding the inefficient back-and-forth behavior typical of greedy planners.\nGlobal Optimality at Convergence:\nUpon convergence (\nN\n=\n30\nN=30\n), the advantages of the next-best-path planner become decisive. Both our variants achieve final coverage (\n>\n91\n%\n>91\\%\n), comparable to traditional point-cloud-based baselines, while significantly outperforming other GS-based baselines in completion ratio. Crucially, Ours-NBP achieves a final coverage and geometric accuracy comparable to the greedy NBV variant, but with a drastically reduced total path length (3.96 m vs. 18.02 m). This represents a 4\nÃ—\n\\times\nimprovement in motion efficiency. Compared to the point-cloud-based PB-NBV\n[\n19\n]\n(18.84 m), our method is nearly 4.7\nÃ—\n\\times\nmore efficient, confirming that NBV strategies universally incur high motion overheads. Regarding reconstruction quality, Ours-NBP achieves the best or near-best performance across all metrics in the convergence stage. Notably, despite the conservative exploration strategy, NBP eventually surpasses greedy NBV in visual quality. This suggests that the smooth, continuous trajectory generated by NBP facilitates more consistent optimization of Gaussian surfels compared to the erratic viewpoints of greedy strategies, ultimately yielding a more coherent and photorealistic digital twin.\nSystem Efficiency:\nFurthermore, our system demonstrates superior online efficiency. Benefiting from the localized updates of Gaussian surfels and the low movement cost inherent to progressive scanning, Ours-NBP requires only 121.96s for the entire online processâ€”2.3\nÃ—\n\\times\nfaster than GauSS-MI and 6\nÃ—\n\\times\nfaster than FisherRF. Although some baselines may have lower offline refinement costs, our framework maintains a significant advantage in total mission time, capable of completing both online scanning and offline refinement within approximately 5 minutes. This confirms that ObjSplat is highly suitable for time-sensitive robotic applications. For a more detailed performance analysis, please refer to\nSec.\nIV-D\n.\nTABLE I:\nQuantitative Evaluation of Reconstruction Quality.\nWe report visual quality (PSNR, SSIM, LPIPS) and geometric accuracy (Depth L1, Chamfer Distance, F-Score) on both training and novel test views.\nMethod\nSplit\nInitialization (2 Views)\nExploration (10 Views)\nConvergence (30 Views)\nVisual Quality\nGeometry Quality\nVisual Quality\nGeometry Quality\nVisual Quality\nGeometry Quality\nPSNR\nâ†‘\n\\uparrow\nSSIM\nâ†‘\n\\uparrow\nLPIPS\nâ†“\n\\downarrow\nD-L1\nâ†“\n\\downarrow\nCD\nâ†“\n\\downarrow\nF-Score\nâ†‘\n\\uparrow\nPSNR\nâ†‘\n\\uparrow\nSSIM\nâ†‘\n\\uparrow\nLPIPS\nâ†“\n\\downarrow\nD-L1\nâ†“\n\\downarrow\nCD\nâ†“\n\\downarrow\nF-Score\nâ†‘\n\\uparrow\nPSNR\nâ†‘\n\\uparrow\nSSIM\nâ†‘\n\\uparrow\nLPIPS\nâ†“\n\\downarrow\nD-L1\nâ†“\n\\downarrow\nCD\nâ†“\n\\downarrow\nF-Score\nâ†‘\n\\uparrow\nFisherRF\n[\n20\n]\nTrain\n42.05\n0.991\n0.015\n5.118\n2.177\n0.314\n35.73\n0.965\n0.067\n1.939\n2.196\n0.531\n34.86\n0.962\n0.070\n1.374\n2.149\n0.549\nTest\n25.43\n0.921\n0.101\n7.821\n28.02\n0.941\n0.086\n3.255\n28.53\n0.947\n0.084\n3.341\nGauSS-MI\n[\n47\n]\nTrain\n42.64\n0.991\n0.014\n3.842\n2.169\n0.322\n35.75\n0.966\n0.067\n2.366\n2.261\n0.525\n35.08\n0.964\n0.069\n1.514\n2.191\n0.590\nTest\n24.13\n0.910\n0.112\n8.730\n28.61\n0.943\n0.083\n3.181\n30.83\n0.954\n0.075\n2.398\nOurs-NBV\nTrain\n43.59\n0.995\n0.009\n0.741\n1.660\n0.636\n39.09\n0.989\n0.022\n0.167\n0.695\n0.958\n36.66\n0.984\n0.028\n0.153\n0.609\n0.973\nTest\n22.39\n0.909\n0.095\n2.856\n30.50\n0.951\n0.048\n0.237\n32.20\n0.965\n0.040\n0.191\nOurs-NBP\nTrain\n43.65\n0.996\n0.006\n0.746\n1.835\n0.653\n38.83\n0.989\n0.020\n0.212\n0.945\n0.875\n36.76\n0.985\n0.028\n0.165\n0.611\n0.970\nTest\n19.74\n0.892\n0.113\n4.919\n24.65\n0.925\n0.080\n1.604\n32.35\n0.966\n0.039\n0.189\nComparison of visual (PSNR, SSIM, LPIPS) and geometric (Depth L1, Chamfer Distance, F-Score) quality across different view counts. Best values are shown in\nbold\n, and second-best values are\nunderlined\n.\nTABLE II:\nQuantitative Analysis of Reconstruction Completeness and Efficiency.\nWe report reconstruction completion ratio (CR), completion (CE), and efficiency metrics (MC, online/offline time) at three distinct phases.\nMethod\nInitialization (2 Views)\nExploration (10 Views)\nConvergence (30 Views)\nCR[%]\nâ†‘\n\\uparrow\nCE[mm]\nâ†“\n\\downarrow\nMC[m]\nâ†“\n\\downarrow\nOnline[s]\nâ†“\n\\downarrow\nOffline[s]\nâ†“\n\\downarrow\nCR[%]\nâ†‘\n\\uparrow\nCE[mm]\nâ†“\n\\downarrow\nMC[m]\nâ†“\n\\downarrow\nOnline[s]\nâ†“\n\\downarrow\nOffline[s]\nâ†“\n\\downarrow\nCR[%]\nâ†‘\n\\uparrow\nCE[mm]\nâ†“\n\\downarrow\nMC[m]\nâ†“\n\\downarrow\nOnline[s]\nâ†“\n\\downarrow\nOffline[s]\nâ†“\n\\downarrow\nSEE\n[\n3\n]\n57.04\n8.88\n0.53\n20.03\n-\n72.47\n4.88\n1.65\n172.91\n-\n89.14\n1.71\n4.32\n420.80\n-\nPB-NBV\n[\n19\n]\n57.88\n9.88\n0.37\n18.66\n-\n87.27\n2.63\n5.65\n82.22\n-\n90.69\n2.04\n18.84\n223.44\n-\nMA-SCVP\n[\n33\n]\n49.92\n13.92\n0.39\n27.67\n-\n90.82\n1.91\n3.86\n117.23\n-\n92.24\n1.68\n5.59\n171.77\n-\nFisherRF\n[\n20\n]\n19.45\n32.17\n0.15\n36.66\n86.49\n63.98\n6.22\n3.49\n246.95\n97.54\n69.72\n5.05\n11.74\n732.10\n101.07\nGauSS-MI\n[\n47\n]\n18.57\n32.93\n0.15\n21.07\n64.16\n66.53\n4.97\n6.92\n98.35\n86.27\n72.66\n4.39\n24.78\n281.44\n91.51\nOurs-NBV\n46.16\n11.75\n0.59\n19.93\n136.96\n90.04\n2.47\n6.42\n70.55\n145.26\n91.89\n2.04\n18.02\n195.16\n148.43\nOurs-NBP\n41.35\n15.99\n0.13\n9.81\n144.70\n74.41\n5.06\n1.17\n50.53\n149.13\n91.42\n2.11\n3.96\n121.96\n146.21\nOurs-NBP achieves comparable quality to the greedy NBV baseline but with significantly reduced motion and time costs. Best values are shown in\nbold\n, and second-best values are\nunderlined\n.\nIV-C\n3\nReconstruction Process Analysis by Views vs. Path Length\nTo further investigate the reconstruction dynamics, Fig.\n8\nplots the evolution of metrics against both the number of views and the accumulated movement cost.\nConvergence over Views (Top Row):\nIn terms of view efficiency, Ours-NBV (orange) exhibits the fastest initial growth in PSNR and Completion Ratio, aligning with its greedy nature of prioritizing global information gain. However, Ours-NBP (green) demonstrates steady and robust convergence, catching up to NBV in all metrics by the end of the session (30 views). Furthermore, both variants demonstrate rapid geometric convergence: Chamfer Distance drops below 1.0 mm, and F1-Score exceeds 0.85 within the first 10 views. In contrast, baseline methods (GauSS-MI and FisherRF) plateau at a significantly higher error level (\n>\n2.0\n>2.0\nmm). This disparity underscores the geometric advantage of our Gaussian surfel representation in inferring accurate surface geometry from sparse observations.\nDominance in Path Efficiency (Bottom Row):\nThe advantages of our NBP planner become irrefutable when metrics are plotted against movement cost. The Ours-NBP curves (green) exhibit a near-vertical ascent in the early phase, saturating at high quality levels (e.g., PSNR\n>\n32\n>32\ndB, Completion\n>\n90\n%\n>90\\%\n) within a minimal movement budget (about\n5\nâ€‹\nm\n5m\n). In contrast, greedy Ours-NBV and baseline methods require trajectories extending beyond 15 meters to achieve comparable performance. This highlights that greedy strategies often incur high travel costs by jumping between distant informative viewpoints. Our NBP planner mitigates such redundant motions by thoroughly exploring local regions before transitioning, thereby maximizing information acquisition per unit of movement cost.\nFurthermore, regarding geometric fidelity, both our variants maintain a significant lead over baselines throughout the entire process. Baseline curves remain relatively flat, indicating that simply adding views without geometric constraints fails to correct the underlying shape errors caused by overfitting. Additionally, our method exhibits smaller variance in novel view and reconstruction completeness across different objects, indicating superior adaptability and robustness to varying geometric and textural characteristics.\nFigure 8:\nQuantitative comparison of reconstruction progress over the number of views (top row) and path length (bottom row). We report novel view PSNR, Chamfer Distance (CD), F1-Score, and Completion Ratio across 16 objects. Shaded areas indicate standard deviation.\nIV-C\n4\nQualitative Analysis of Reconstruction Process\nFig.\n9\nprovides a qualitative breakdown of the reconstruction results on the\nMario\nobject, illustrating the trade-off between exploration efficiency and reconstruction quality. We visualize the online Gaussian models overlaid with camera trajectories (Row 1), uncertainty maps at the final selected viewpoint (Row 2), and the final extracted meshes (Row 3).\nTrajectory and Efficiency Analysis:\nAs observed in the top row, baseline methods struggle to balance exploration with movement costs. GauSS-MI exhibits a highly chaotic trajectory (26.21m), oscillating between distant viewpoints due to the lack of global path constraints. FisherRF, while moving less, fails to explore the object comprehensively. Comparing our variants, Ours-NBV achieves high coverage but incurs a significant movement cost (19.93m), attributed to its greedy selection of spatially distant informative views. Conversely, Ours-NBP leverages the spatial topology graph to perform multi-step lookahead planning. This results in a smooth, enveloping trajectory that efficiently covers the object, achieving the highest coverage (96.14%) with a drastically reduced path length (4.05m)â€”an improvement of nearly\n5\nÃ—\n\\times\nin motion efficiency compared to the greedy NBV baseline. This reduction is driven by the occlusion and back-face awareness enabled by our surfel representation. Unlike prior approaches that rely on identifying unconverged areas to reduce uncertainty, our planner utilizes explicit normal information to infer unobserved rear surfaces, generating purposeful trajectories that distinguish necessary exploration from redundant movement.\nReconstruction Quality and Uncertainty:\nFurthermore, baselines suffer from severe floating artifacts in the GS-Model and noise in the TSDF mesh, primarily caused by overfitting to sparse, ill-distributed views and a lack of object-centric foreground optimization. Their uncertainty maps (Row 2) often exhibit coarse-grained noise, failing to guide the robot effectively.\nIn contrast, Ours-NBP produces a clean, water-tight mesh with sharp features (e.g., the hat emblem and face texture). The uncertainty visualization confirms that our geometry-aware metric correctly identifies fine-grained reconstruction quality and completeness cuesâ€”typically concentrated at object edges and occluded regionsâ€”thereby enabling targeted refinement. Consequently, Ours-NBP achieves the lowest Chamfer Distance (0.648mm) and the highest F-Score (0.982), demonstrating superior fidelity. Overall, our method generates a globally consistent, smooth trajectory, achieving the highest completeness and geometric accuracy with minimum movement cost.\nFigure 9:\nVisual comparison of reconstruction completeness and exploration efficiency on the\nMario\nobject. Top row: online Gaussian models with camera trajectories and frustums. Metrics denote Surface Coverage (\nâ†‘\n\\uparrow\n)\n|\n|\nPath Length (\nâ†“\n\\downarrow\n). Middle row: the uncertainty map at the final selected viewpoint. Bottom row: final meshes extracted via TSDF fusion with zoomed-in details. Metrics denote Chamfer Distance (\nâ†“\n\\downarrow\n)\n|\n|\nF-Score (\nâ†‘\n\\uparrow\n).\nIV-D\nAblation Study and System Performance\nTo validate the design choices of ObjSplat, we conduct comprehensive ablation studies to assess the individual contributions of its core components.\nIV-D\n1\nAblation on Viewpoint Evaluation\nWe first analyze the impact of different uncertainty quantification strategies on reconstruction results. We establish two baselines representing common heuristics: Opacity-only, which uses rendered opacity as a proxy for visibility, and Confidence-only, which employs raw surfel confidence without occlusion or back-face awareness. Subsequently, we evaluated our method by selectively removing key components: Occlusion-Awareness (w/o O.A.), Back-Face check (w/o B.F.), and Dynamic Scheduling (w/o Dyn.). All variants employ the NBP planner to ensure a fair comparison. As shown in Table\nIII\n, baseline heuristics (Opacity and Confidence) incur lower movement costs but suffer from significantly inferior completeness and quality. This indicates that in object-centric reconstruction, generic metrics are easily biased by the large proportion of empty background space, leading the system to prioritize trivial coverage over intricate surface details. As a result, these methods fail to detect critical geometric defects, such as self-occlusions or back-facing surfaces, resulting in premature termination. In contrast, our full approach incurs a slightly higher movement cost (3.96m). However, this additional motion is deliberate and goal-oriented, explicitly guided by our geometry-aware metric to refine under-reconstructed regions.\nFurther analysis reveals that removing any of the individual components causes varying degrees of degradation in reconstruction completeness and quality. Removing the occlusion-aware check causes erroneous co-visibility estimates due to self-occlusions, which results in overestimated confidence. Removing back-face observation hampers the identification of under-reconstructed surfaces in object-level tasks, where back-facing areas are non-closed surfaces. We note that the performance drops when removing the dynamic scheduling strategy, as this leads to degradation across both quality and completeness metrics. This demonstrates that, during the early stages of object reconstructionâ€”when a large portion remains unconstructedâ€”the system benefits from prioritizing exploration. As the model matures, the system shifts focus to refine finer under-reconstructed areas. By integrating these components, our method accurately identifies truly under-reconstructed regions, achieving an optimal balance between exploration and exploitation.\nTABLE III:\nAblation Study on Viewpoint Evaluation.\nWe evaluate the impact of each component on reconstruction quality and efficiency using the GSO dataset.\nMethod\nPSNR[dB]\nâ†‘\n\\uparrow\nCD[mm]\nâ†“\n\\downarrow\nCR[%]\nâ†‘\n\\uparrow\nMC[m]\nâ†“\n\\downarrow\nOpacity only\n28.75\n0.701\n86.02\n3.84\nConfidence only\n29.99\n0.693\n86.95\n3.85\nOurs w/o O.A.\n32.22\n0.622\n91.07\n4.09\nOurs w/o B.F.\n31.97\n0.625\n90.64\n3.86\nOurs w/o Dyn.\n30.54\n0.680\n87.86\n4.02\nOurs\n32.35\n0.611\n91.42\n3.96\nO.A.\n: Occlusion-Awareness;\nB.F.\n: Back-Face Observation;\nDyn.\n: Dynamic Scheduling Strategy. Best results are\nbold\n, second best are\nunderlined\n.\nTABLE IV:\nAblation Study on View Planning Strategies.\nWe compare different planning paradigms in terms of reconstruction quality and efficiency.\nStrategy\nPSNR\nâ†‘\n\\uparrow\nCD\nâ†“\n\\downarrow\nCR\nâ†‘\n\\uparrow\nMC\nâ†“\n\\downarrow\nOnline\nâ†“\n\\downarrow\n[dB]\n[mm]\n[%]\n[m]\n[s]\nRandom\n31.45\n0.648\n90.50\n16.63\n200.71\nCircle (Fixed)\n31.12\n0.655\n86.21\n3.03\n98.30\nNBV (Greedy)\n32.20\n0.609\n91.89\n18.02\n195.16\nNBV-1 + NBP\n31.62\n0.652\n89.12\n4.51\n130.18\nNBP-R (Step-wise)\n31.16\n0.668\n87.79\n4.01\n150.92\nOurs (NBP-P)\n32.35\n0.611\n91.42\n3.96\n121.96\nCircle\n: efficient but blind;\nNBV\n: accurate but costly. Best results are\nbold\n, second best are\nunderlined\n.\nIV-D\n2\nAblation on View Planning Strategy\nTable\nIV\nevaluates distinct planning paradigms. The Random strategy, which randomly samples viewpoints distributed uniformly across the hemisphere, is effective for covering most of the objectâ€™s surface. This resilience is attributed to our Gaussian surfel representation and local visibility-based optimization in handling non-sequential data. However, the fixed-trajectory Circle baseline, while minimizing movement and time, yields poor reconstruction fidelity. This confirms that dynamic, scene-responsive active perception is essential for capturing complex geometric details missed by fixed paths. Conversely, the greedy NBV strategy achieves high completeness and geometric accuracy but incurs excessive movement costs, rendering it impractical for time-sensitive autonomous tasks.\nA key finding is the superiority of executing the entire planned path (Ours / NBP-P) over the receding horizon approach (NBP-R), where only the first step is executed before replanning. NBP-R performs significantly worse, likely due to its overly conservative nature, which hinders extensive exploration within a limited view budget. Furthermore, frequent replanning increases computational overhead and disrupts global scanning objectives. We also evaluated a hybrid strategy (NBV-1 + NBP) that initializes with a single greedy view. However, the initial large displacement disrupts trajectory smoothness, leading to suboptimal results.\nUltimately, Ours (NBP-P) achieves the best trade-off: it matches the high reconstruction fidelity of the computationally expensive NBV strategy while maintaining a low movement cost comparable to fixed trajectories. Moreover, it achieves the lowest online processing time among active methods, as the reduced planning frequency allows more computational resources to be allocated to real-time mapping and optimization.\nTABLE V:\nAblation Study on Geometry-Texture Optimization.\nWe evaluate the contribution of each component by removing individual loss terms from the optimization process.\nMethod\nPSNR[dB]\nâ†‘\n\\uparrow\nSSIM\nâ†‘\n\\uparrow\nLPIPS\nâ†“\n\\downarrow\nCD[mm]\nâ†“\n\\downarrow\nCR[%]\nâ†‘\n\\uparrow\nOurs w/o\nâ„’\nn\n\\mathcal{L}_{n}\n32.01\n0.961\n0.046\n0.750\n88.23\nOurs w/o\nâ„’\nc\n\\mathcal{L}_{c}\n32.31\n0.963\n0.040\n0.613\n91.52\nOurs w/o\nâ„’\nm\n\\mathcal{L}_{m}\n32.33\n0.963\n0.041\n0.612\n90.64\nOurs w/o\nâ„’\no\n\\mathcal{L}_{o}\n32.25\n0.965\n0.042\n0.608\n90.98\nOurs\n32.35\n0.966\n0.039\n0.611\n91.42\nFigure 10:\nReconstruction quality comparison on the\nBunny Racer\nobject.\nIV-D\n3\nAblation on Geometry-Texture Joint Optimization\nWe selectively remove different loss terms during the optimization process to perform a quantitative analysis (see Table\nV\n). The normal consistency loss helps reduce geometric fluctuations caused by overfitting to texture edges (see Fig.\n10\n), which even leads to inflated completeness. Interestingly, removing opacity regularization leads to an improvement in CD. We hypothesize that the strict binary opacity constraint may hinder the accurate modeling of complex self-occluded regions, whereas relaxing this constraint allows for better geometric fitting at the cost of visual quality. By balancing these trade-offs, we achieve a favorable compromise between geometric accuracy and visual fidelity.\nFigure 11:\nReal-World Progressive Reconstruction Process Results.\nWe show four snapshots of the NBP planning execution. In each step, the robot executes a path to a sub-goal with high estimated uncertainty (visualized in the inset). The final column shows the complete trajectory, total path length, and the final PSNR (dB) of the training views.\nFigure 12:\nFinal reconstruction results of real-world artifacts.\nWe display the rendered images from the final Gaussian surfels (left) and the corresponding extracted surface meshes (right). Our method recovers high-fidelity texture and geometry even for objects with complex topology and surface patterns.\nTABLE VI:\nImpact of the objectâ€™s initial pose. (30 views and 5 trials)\nObject\nPSNR[dB]\nâ†‘\n\\uparrow\nCD[\nm\nâ€‹\nm\nmm\n]\nâ†“\n\\downarrow\nCR[%]\nâ†‘\n\\uparrow\nMC[\nm\nm\n]\nâ†“\n\\downarrow\nBunny\n32.68\nÂ±\n\\pm\n1.02\n0.718\nÂ±\n\\pm\n0.016\n87.16\nÂ±\n\\pm\n0.29\n3.89\nÂ±\n\\pm\n0.13\nChicken\n32.14\nÂ±\n\\pm\n0.61\n0.598\nÂ±\n\\pm\n0.010\n86.85\nÂ±\n\\pm\n0.46\n4.06\nÂ±\n\\pm\n0.44\nTurtle\n31.26\nÂ±\n\\pm\n1.74\n0.747\nÂ±\n\\pm\n0.006\n84.28\nÂ±\n\\pm\n0.39\n3.94\nÂ±\n\\pm\n0.16\nElephant\n31.98\nÂ±\n\\pm\n0.87\n0.730\nÂ±\n\\pm\n0.019\n92.30\nÂ±\n\\pm\n0.37\n4.24\nÂ±\n\\pm\n0.19\nOrtho\n33.48\nÂ±\n\\pm\n0.37\n0.642\nÂ±\n\\pm\n0.019\n80.83\nÂ±\n\\pm\n1.04\n4.08\nÂ±\n\\pm\n0.10\nHorse\n33.92\nÂ±\n\\pm\n0.74\n0.477\nÂ±\n\\pm\n0.025\n99.54\nÂ±\n\\pm\n0.28\n4.05\nÂ±\n\\pm\n0.27\nEagle\n29.76\nÂ±\n\\pm\n1.32\n0.610\nÂ±\n\\pm\n0.010\n96.51\nÂ±\n\\pm\n1.19\n4.20\nÂ±\n\\pm\n0.24\nDragon\n31.02\nÂ±\n\\pm\n0.52\n0.746\nÂ±\n\\pm\n0.033\n95.77\nÂ±\n\\pm\n0.53\n4.13\nÂ±\n\\pm\n0.19\nMario\n32.68\nÂ±\n\\pm\n0.82\n0.650\nÂ±\n\\pm\n0.041\n96.19\nÂ±\n\\pm\n0.25\n4.25\nÂ±\n\\pm\n0.30\nYoshi\n32.48\nÂ±\n\\pm\n0.48\n0.503\nÂ±\n\\pm\n0.012\n96.40\nÂ±\n\\pm\n0.34\n3.88\nÂ±\n\\pm\n0.13\nDino\n32.25\nÂ±\n\\pm\n0.60\n0.683\nÂ±\n\\pm\n0.029\n96.94\nÂ±\n\\pm\n0.82\n4.26\nÂ±\n\\pm\n0.21\nBackpack\n29.49\nÂ±\n\\pm\n0.85\n0.966\nÂ±\n\\pm\n0.037\n80.83\nÂ±\n\\pm\n0.62\n4.31\nÂ±\n\\pm\n0.29\nShoe\n31.51\nÂ±\n\\pm\n1.21\n0.555\nÂ±\n\\pm\n0.010\n82.77\nÂ±\n\\pm\n0.10\n3.66\nÂ±\n\\pm\n0.26\nCandy-box\n34.12\nÂ±\n\\pm\n1.07\n0.378\nÂ±\n\\pm\n0.079\n94.80\nÂ±\n\\pm\n0.05\n3.75\nÂ±\n\\pm\n0.27\nCup\n32.84\nÂ±\n\\pm\n1.16\n0.485\nÂ±\n\\pm\n0.018\n96.51\nÂ±\n\\pm\n0.16\n4.01\nÂ±\n\\pm\n0.16\nStacking\n34.74\nÂ±\n\\pm\n1.10\n0.449\nÂ±\n\\pm\n0.005\n87.15\nÂ±\n\\pm\n0.15\n4.01\nÂ±\n\\pm\n0.08\nAvg.\n32.27\nÂ±\n\\pm\n1.45\n0.621\nÂ±\n\\pm\n0.149\n90.93\nÂ±\n\\pm\n6.45\n4.05\nÂ±\n\\pm\n0.18\nIV-D\n4\nRobustness to Initial Pose\nTo evaluate robustness to the initial object orientation, we randomly perturb the object pose around the z-axis. For each of the 16 GSO objects, we sample 5 random initial yaw angles and evaluate the final reconstruction after 30 views. As shown in Table\nVI\n, the system demonstrates consistent robustness, with low standard deviations across metrics. This confirms that our NBP planner and geometry-aware uncertainty effectively guide the robot to explore critical regions regardless of the starting configuration, ensuring reliable high-fidelity reconstruction.\nIV-D\n5\nRuntime Analysis\nTable\nVII\nreports the average processing time per step for different modules. The system is capable of completing a full perception-planning-action loop in approximately 4 seconds. Benefiting from the sparse decision-making within the spatial topology and efficient rendering-based view evaluation enabled by the unified Gaussian surfel representation, the computational overhead for view evaluation and path planning remains minimal, constituting only a small fraction of the total processing load.\nTABLE VII:\nAverage Processing time per step\nMapper\nCandidate views\nGet uncertainty\nPath planning\nVisualizer (optional)\n326.40ms\n2.71ms\n164.78ms\n21.91ms\n94.28ms\nSegmentation\nExtract Obj-PCD\nMove turntable\nMove arm\n361.68ms\n51.25ms\n1399.40ms\n1461.62ms\nIV-E\nReal-world Experiments\nTo validate the effectiveness of ObjSplat in real-world scenarios, we deploy the system on the robotic platform depicted in Fig.\n6\n. To ensure coherent model fusion amidst mechanical vibration and calibration noise, we activated the object-centric pose tracking pipeline (described in Sec.\nIII-D\n1\n) throughout all experiments, maintaining global coordinate consistency. Fig.\n11\nvisualizes the autonomous reconstruction process for four physical objects, displaying four snapshots of the NBP planning process. In each slice, the uncertainty map rendered from a selected sub-goal viewpoint is shown alongside the executed trajectory. The system actively identifies high-uncertainty regions (red/bright areas) and generates collision-free paths for exploration. As reconstruction progresses, the global surface uncertainty diminishes, eventually confining residual uncertainty to deeply occluded regions or complex undersides that are difficult to observe. Quantitatively, our system completed the reconstruction of these complex objects with an average trajectory length of less than 4.0 m, confirming that the NBP planner effectively translates its â€lookaheadâ€ capability into highly efficient, non-redundant scanning trajectories.\nFig.\n12\npresents the final reconstruction results, displaying both optimized Gaussian surfel models and extracted meshes. The results demonstrate that ObjSplat achieves accurate geometry and photorealistic texture fidelity across objects with diverse characteristics. Specifically, it faithfully recovers fine-grained details, such as the skin texture of the\nSika Deer\n, intricate geometric carvings on the\nFu Hao Owl Zun\n, and rich patterns on the\nPottery Figure\n. The explicit mesh extraction further validates the geometric precision, exhibiting smooth surfaces with sharp features. Crucially, these high-fidelity digital assets are directly applicable to downstream physical simulation tasks (see Fig.\n1\n). For a complete visualization of the scanning process and\n360\nâˆ˜\n360^{\\circ}\nrenderings, please refer to the supplementary video.\nV\nConclusion and Future Work\nIn this paper, we presented ObjSplat, a unified active object reconstruction framework that bridges the gap between high-fidelity digitization and efficient robotic exploration. By adopting 2D Gaussian surfels as a shared representation, our system tightly couples incremental model updates with geometry-aware view planning. We introduce a geometry-aware view evaluation pipeline that explicitly accounts for back-face visibility and multi-view covisibility, providing reliable guidance for targeted refinement beyond simple opacity cues. To overcome the short-sightedness of conventional greedy strategies, our next-best-path planner performs multi-step lookahead on a dynamic spatial topology, effectively optimizing the trade-off between information gain and movement cost. Extensive evaluations in both simulation and real-world setups demonstrate that ObjSplat consistently outperforms state-of-the-art methods, producing photorealistic, watertight digital assets with significantly reduced autonomous scanning time.\nDespite these advancements, our system has limitations that open avenues for future research. First, the current framework is tailored for the reconstruction of single, static, rigid objects. Handling complex optical properties, such as extreme lighting variations, transparency, or high specularity, remains challenging for the current rendering model. Future work will focus on integrating robust material estimation techniques and exploring dynamic object representations, broadening the scope of autonomous digitization to interactive and uncontrolled real-world environments.\nReferences\n[1]\nE. Balas\n(1989)\nThe prize collecting traveling salesman problem\n.\nNetworks\n19\n(\n6\n),\npp.Â 621â€“636\n.\nCited by:\nÂ§\nIII-C\n2\n.\n[2]\nC. Bao, X. Zhang, Z. Yu, J. Shi, G. Zhang, S. Peng, and Z. Cui\n(2025)\nFree360: layered gaussian splatting for unbounded 360-degree view synthesis from extremely sparse and unposed views\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 16377â€“16387\n.\nCited by:\nÂ§\nII-A\n.\n[3]\nR. Border and J. D. Gammell\n(2024)\nThe surface edge explorer (see): a measurement-direct approach to next best view planning\n.\nIntl. J. of Robotics Research\n43\n(\n10\n),\npp.Â 1506â€“1532\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIV-A\n1\n,\nTABLE II\n.\n[4]\nD. Chen, H. Li, W. Ye, Y. Wang, W. Xie, S. Zhai, N. Wang, H. Liu, H. Bao, and G. Zhang\n(2024)\nPgsr: planar-based gaussian splatting for efficient and high-fidelity surface reconstruction\n.\nIEEE Trans. Visualization and Comp. Graphics\n.\nCited by:\nÂ§\nII-A\n.\n[5]\nX. Chen, Q. Li, T. Wang, T. Xue, and J. Pang\n(2024)\nGennbv: generalizable next-best-view policy for active 3d reconstruction\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 16436â€“16445\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[6]\nP. Dai, J. Xu, W. Xie, X. Liu, H. Wang, and W. Xu\n(2024)\nHigh-quality surface reconstruction using gaussian surfels\n.\nIn\nSIGGRAPH\n,\npp.Â 1â€“11\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-A\n1\n.\n[7]\nT. Deng, Y. Pan, S. Yuan, D. Li, C. Wang, M. Li, L. Chen, L. Xie, D. Wang, J. Wang,\net al.\n(2025)\nWhat is the best 3d scene representation for robotics? from geometric to foundation models\n.\narXiv preprint arXiv:2512.03422\n.\nCited by:\nÂ§I\n.\n[8]\nZ. Dong, K. Chen, Z. Lv, H. Yu, Y. Zhang, C. Zhang, Y. Zhu, S. Tian, Z. Li, G. Moffatt,\net al.\n(2025)\nDigital twin catalog: a large-scale photorealistic 3d object digital twin dataset\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 753â€“763\n.\nCited by:\nÂ§I\n.\n[9]\nL. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke\n(2022)\nGoogle scanned objects: a high-quality dataset of 3d scanned household items\n.\nIn\nIEEE Intl. Conf. on Robotics and Automation (ICRA)\n,\npp.Â 2553â€“2560\n.\nCited by:\nÂ§\nIV-A\n2\n.\n[10]\nA. Escontrela, J. Kerr, A. Allshire, J. Frey, R. Duan, C. Sferrazza, and P. Abbeel\n(2025)\nGaussGym: an open-source real-to-sim framework for learning locomotion from pixels\n.\narXiv preprint arXiv:2510.15352\n.\nCited by:\nÂ§I\n.\n[11]\nZ. Fan, W. Cong, K. Wen, K. Wang, J. Zhang, X. Ding, D. Xu, B. Ivanovic, M. Pavone, G. Pavlakos,\net al.\n(2024)\nInstantsplat: unbounded sparse-view pose-free gaussian splatting in 40 seconds\n.\narXiv\n.\nCited by:\nÂ§\nII-A\n.\n[12]\nY. Fu, Q. Yan, J. Liao, H. Zhou, J. Tang, and C. Xiao\n(2021)\nSeamless texture optimization for rgb-d reconstruction\n.\nIEEE Trans. Visualization and Comp. Graphics\n29\n(\n3\n),\npp.Â 1845â€“1859\n.\nCited by:\nÂ§\nII-A\n.\n[13]\nA. GuÃ©don, D. Gomez, N. Maruani, B. Gong, G. Drettakis, and M. Ovsjanikov\n(2025)\nMilo: mesh-in-the-loop gaussian splatting for detailed and efficient surface reconstruction\n.\nACM Trans. Graphics\n44\n(\n6\n),\npp.Â 1â€“15\n.\nCited by:\nÂ§\nII-A\n.\n[14]\nL. Han, X. Zhang, H. Song, K. Shi, Y. Liu, and Z. Han\n(2025)\nSparseRecon: neural implicit surface reconstruction from sparse views with feature and depth consistencies\n.\nIn\nIntl. Conf. on Computer Vision (ICCV)\n,\npp.Â 28514â€“28524\n.\nCited by:\nÂ§\nII-A\n.\n[15]\nJ. Held, S. Son, R. Vandeghen, D. Rebain, M. Gadelha, Y. Zhou, A. Cioppa, M. C. G Lin, M. Van Droogenbroeck, and A. Tagliasacchi\n(2025)\nMeshSplatting: differentiable rendering with opaque meshes\n.\narXiv\n.\nCited by:\nÂ§\nII-A\n.\n[16]\nJ. Held, R. Vandeghen, A. Deliege, A. Hamdi, A. Cioppa, S. Giancola, A. Vedaldi, B. Ghanem, A. Tagliasacchi, and M. Van Droogenbroeck\n(2025)\nTriangle splatting for real-time radiance field rendering\n.\narXiv\n.\nCited by:\nÂ§\nII-A\n.\n[17]\nS. Hong, C. Zheng, Y. Shen, C. Li, F. Zhang, T. Qin, and S. Shen\n(2025)\nGS-livo: real-time lidar, inertial, and visual multi-sensor fused odometry with gaussian mapping\n.\nIEEE Trans. Robotics\n.\nCited by:\nÂ§\nII-A\n.\n[18]\nB. Huang, Z. Yu, A. Chen, A. Geiger, and S. Gao\n(2024)\n2d gaussian splatting for geometrically accurate radiance fields\n.\nIn\nSIGGRAPH\n,\npp.Â 1â€“11\n.\nCited by:\nÂ§\nII-A\n.\n[19]\nZ. Jia, Y. Li, Q. Hao, and S. Zhang\n(2025)\nPB-nbv: efficient projection-based next-best-view planning framework for reconstruction of unknown objects\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIV-A\n1\n,\nÂ§\nIV-C\n2\n,\nTABLE II\n.\n[20]\nW. Jiang, B. Lei, and K. Daniilidis\n(2024)\nFisherrf: active view selection and mapping with radiance fields using fisher information\n.\nIn\nEuropean Conf. on Computer Vision (ECCV)\n,\npp.Â 422â€“440\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIII-B\n3\n,\nÂ§\nIV-A\n1\n,\nÂ§\nIV-C\n1\n,\nTABLE I\n,\nTABLE II\n.\n[21]\nL. Jin, X. Chen, J. RÃ¼ckin, and M. PopoviÄ‡\n(2023)\nNeu-nbv: next best view planning using uncertainty estimation in image-based neural rendering\n.\nIn\nIEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)\n,\npp.Â 11305â€“11312\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[22]\nL. Jin, X. Zhong, Y. Pan, J. Behley, C. Stachniss, and M. PopoviÄ‡\n(2025)\nActivegs: active scene reconstruction using gaussian splatting\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§\nII-B\n,\nÂ§\nIII-B\n3\n,\nÂ§\nIII-B\n3\n.\n[23]\nM. Kazhdan and H. Hoppe\n(2013)\nScreened poisson surface reconstruction\n.\nACM Trans. Graphics\n32\n(\n3\n),\npp.Â 1â€“13\n.\nCited by:\nÂ§\nII-A\n,\nÂ§\nIII-D\n2\n.\n[24]\nB. Kerbl, G. Kopanas, T. LeimkÃ¼hler, and G. Drettakis\n(2023)\n3d gaussian splatting for real-time radiance field rendering\n.\nACM Trans. Graphics\n42\n(\n4\n),\npp.Â 1â€“14\n.\nCited by:\nÂ§I\n,\nÂ§\nIII-A\n1\n,\nÂ§\nIV-B\n1\n.\n[25]\nS. Lee, K. Kang, S. Ha, and H. Yu\n(2025)\nBayesian nerf: quantifying uncertainty with volume density for neural implicit fields\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[26]\nS. Lee, L. Chen, J. Wang, A. Liniger, S. Kumar, and F. Yu\n(2022)\nUncertainty guided policy for active robotic 3d reconstruction using neural radiance fields\n.\nIEEE Robotics and Automation Letters\n7\n(\n4\n),\npp.Â 12070â€“12077\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[27]\nS. Li, A. GuÃ©don, C. Boittiaux, S. Chen, and V. Lepetit\n(2025)\nNextbestpath: efficient 3d mapping of unseen environments\n.\narXiv preprint arXiv:2502.05378\n.\nCited by:\nÂ§\nII-B\n.\n[28]\nY. Li, Z. Kuang, T. Li, Q. Hao, Z. Yan, G. Zhou, and S. Zhang\n(2025)\nActivesplat: high-fidelity scene reconstruction through active gaussian splatting\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nIII-B\n1\n,\nÂ§\nIII-B\n3\n.\n[29]\nZ. Li, T. MÃ¼ller, A. Evans, R. H. Taylor, M. Unberath, M. Liu, and C. Lin\n(2023)\nNeuralangelo: high-fidelity neural surface reconstruction\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 8456â€“8465\n.\nCited by:\nÂ§\nII-A\n.\n[30]\nJ. Liu, Y. Wan, B. Wang, C. Zheng, J. Lin, and F. Zhang\n(2025)\nGS-sdf: lidar-augmented gaussian splatting and neural sdf for geometrically consistent rendering and reconstruction\n.\nIn\nIEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)\n,\npp.Â 19391â€“19398\n.\nCited by:\nÂ§\nII-A\n.\n[31]\nH. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison\n(2024)\nGaussian splatting slam\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 18039â€“18048\n.\nCited by:\nÂ§I\n,\nÂ§\nIII-B\n1\n,\nÂ§\nIII-B\n2\n,\nÂ§\nIV-C\n1\n.\n[32]\nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng\n(2021)\nNerf: representing scenes as neural radiance fields for view synthesis\n.\nCommun. ACM\n65\n(\n1\n),\npp.Â 99â€“106\n.\nCited by:\nÂ§I\n.\n[33]\nS. Pan, H. Hu, H. Wei, N. Dengler, T. Zaenker, M. Dawood, and M. Bennewitz\n(2024)\nIntegrating one-shot view planning with a single next-best view via long-tail multiview sampling\n.\nIEEE Trans. Robotics\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIV-A\n1\n,\nÂ§\nIV-A\n2\n,\nÂ§\nIV-B\n,\nTABLE II\n.\n[34]\nS. Pan, H. Hu, and H. Wei\n(2022)\nScvp: learning one-shot view planning via set covering for unknown object reconstruction\n.\nIEEE Robotics and Automation Letters\n7\n(\n2\n),\npp.Â 1463â€“1470\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[35]\nS. Pan, L. Jin, X. Huang, C. Stachniss, M. PopoviÄ‡, and M. Bennewitz\n(2024)\nExploiting priors from 3d diffusion models for rgb-based one-shot view planning\n.\nIn\nIEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)\n,\npp.Â 13341â€“13348\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[36]\nS. Pan, L. Jin, X. Huang, C. Stachniss, M. PopoviÄ‡, and M. Bennewitz\n(2025)\nDM-osvp++: one-shot view planning using 3d diffusion models for active rgb-based object reconstruction\n.\narXiv\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[37]\nX. Pan, Z. Lai, S. Song, and G. Huang\n(2022)\nActivenerf: learning where to see with uncertainty estimation\n.\nIn\nEuropean Conf. on Computer Vision (ECCV)\n,\npp.Â 230â€“246\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n.\n[38]\nN. Pfaff, E. Fu, J. Binagia, P. Isola, and R. Tedrake\n(2025)\nScalable real2sim: physics-aware asset generation via robotic pick-and-place setups\n.\narXiv preprint arXiv:2503.00370\n.\nCited by:\nÂ§I\n.\n[39]\nN. Ravi, V. Gabeur, Y. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson,\net al.\n(2024)\nSam 2: segment anything in images and videos\n.\narXiv\n.\nCited by:\nÂ§\nIII-A\n2\n,\nÂ§\nIII-D\n1\n.\n[40]\nJ. L. SchÃ¶nberger and J. Frahm\n(2016)\nStructure-from-motion revisited\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nÂ§I\n.\n[41]\nJ. L. SchÃ¶nberger, E. Zheng, M. Pollefeys, and J. Frahm\n(2016)\nPixelwise view selection for unstructured multi-view stereo\n.\nIn\nEuropean Conf. on Computer Vision (ECCV)\n,\nCited by:\nÂ§\nII-A\n.\n[42]\nM. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, and P. Agrawal\n(2024)\nReconciling reality through simulation: a real-to-sim-to-real approach for robust manipulation\n.\narXiv preprint arXiv:2403.03949\n.\nCited by:\nÂ§I\n.\n[43]\nH. Vogel\n(1979)\nA better way to construct the sunflower head\n.\nMathematical biosciences\n44\n(\n3-4\n),\npp.Â 179â€“189\n.\nCited by:\nÂ§\nIII-C\n1\n.\n[44]\nP. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang\n(2021)\nNeuS: learning neural implicit surfaces by volume rendering for multi-view reconstruction\n.\nIn\nAdvances in Neural Information Processing Systems (NIPS)\n,\npp.Â 27171â€“27183\n.\nCited by:\nÂ§\nII-A\n.\n[45]\nZ. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli\n(2004)\nImage quality assessment: from error visibility to structural similarity\n.\nIEEE Trans. Image Process.\n13\n(\n4\n),\npp.Â 600â€“612\n.\nCited by:\nÂ§\nIII-A\n2\n.\n[46]\nS. Wu, Z. Lv, Y. Zhu, D. Frost, Z. Li, L. Yan, C. Ren, R. Newcombe, and Z. Dong\n(2025)\nMonocular online reconstruction with enhanced detail preservation\n.\nIn\nSIGGRAPH\n,\npp.Â 1â€“11\n.\nCited by:\nÂ§\nIII-B\n2\n,\nÂ§\nIII-B\n2\n.\n[47]\nY. Xie, Y. Cai, Y. Zhang, L. Yang, and J. Pan\n(2025)\nGauSS-mi: gaussian splatting shannon mutual information for active 3d reconstruction\n.\narXiv preprint arXiv:2504.21067\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIV-A\n1\n,\nÂ§\nIV-C\n1\n,\nTABLE I\n,\nTABLE II\n.\n[48]\nZ. Xu, R. Jin, K. Wu, Y. Zhao, Z. Zhang, J. Zhao, F. Gao, Z. Gan, and W. Ding\n(2025)\nHgs-planner: hierarchical planning framework for active scene reconstruction using 3d gaussian splatting\n.\nIn\nIEEE Intl. Conf. on Robotics and Automation (ICRA)\n,\npp.Â 14161â€“14167\n.\nCited by:\nÂ§\nII-B\n.\n[49]\nS. Xue, J. Dill, P. Mathur, F. Dellaert, P. Tsiotra, and D. Xu\n(2024)\nNeural visibility field for uncertainty-driven active mapping\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 18122â€“18132\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[50]\nD. Yan, J. Liu, F. Quan, H. Chen, and M. Fu\n(2023)\nActive implicit object reconstruction using uncertainty-guided next-best-view optimization\n.\nIEEE Robotics and Automation Letters\n8\n(\n10\n),\npp.Â 6395â€“6402\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n.\n[51]\nC. Yang, S. Li, J. Fang, R. Liang, L. Xie, X. Zhang, W. Shen, and Q. Tian\n(2024)\nGaussianObject: high-quality 3d object reconstruction from four views with gaussian splatting\n.\nACM Trans. Graphics\n43\n(\n6\n),\npp.Â 1â€“13\n.\nCited by:\nÂ§\nII-A\n.\n[52]\nS. Ye, Y. He, M. Lin, J. Sheng, R. Fan, Y. Han, Y. Hu, R. Yi, Y. Wen, Y. Liu,\net al.\n(2024)\nPVP-recon: progressive view planning via warping consistency for sparse-view surface reconstruction\n.\nACM Trans. Graphics\n43\n(\n6\n),\npp.Â 1â€“13\n.\nCited by:\nÂ§I\n,\nÂ§\nII-B\n,\nÂ§\nII-B\n,\nÂ§\nIV-B\n1\n,\nÂ§\nIV-B\n.\n[53]\nJ. Y. Yen\n(1971)\nFinding the k shortest loopless paths in a network\n.\nmanagement Science\n17\n(\n11\n),\npp.Â 712â€“716\n.\nCited by:\nÂ§\nIII-C\n2\n.\n[54]\nH. Yu, B. Jia, Y. Chen, Y. Yang, P. Li, R. Su, J. Li, Q. Li, W. Liang, S. Zhu,\net al.\n(2025)\nMETASCENES: towards automated replica creation for real-world 3d scans\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 1667â€“1679\n.\nCited by:\nÂ§I\n.\n[55]\nJ. Yu, L. Fu, H. Huang, K. El-Refai, R. A. Ambrus, R. Cheng, M. Z. Irshad, and K. Goldberg\n(2025)\nReal2render2real: scaling robot data without dynamics simulation or robot hardware\n.\narXiv preprint arXiv:2505.09601\n.\nCited by:\nÂ§I\n.\n[56]\nJ. Zhang, Z. Wan, and J. Liao\n(2022)\nAdaptive joint optimization for 3d reconstruction with differentiable rendering\n.\nIEEE Trans. Visualization and Comp. Graphics\n29\n(\n6\n),\npp.Â 3039â€“3051\n.\nCited by:\nÂ§\nII-A\n.\n[57]\nW. Zhang, Q. Cheng, D. Skuddis, N. Zeller, D. Cremers, and N. Haala\n(2025)\nHi-slam2: geometry-aware gaussian slam for fast monocular scene reconstruction\n.\nIEEE Trans. Robotics\n41\n,\npp.Â 6478â€“6493\n.\nCited by:\nÂ§\nIII-D\n2\n.\n[58]\nW. Zhang, E. Y. Jia, J. Zhou, B. Ma, K. Shi, Y. Liu, and Z. Han\n(2025)\nNeRFPrior: learning neural radiance field as a prior for indoor scene reconstruction\n.\nIn\nIEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 11317â€“11327\n.\nCited by:\nÂ§\nII-A\n.\n[59]\nZ. Zhang, B. Huang, H. Jiang, L. Zhou, X. Xiang, and S. Shen\n(2025)\nQuadratic gaussian splatting: high quality surface reconstruction with second-order geometric primitives\n.\nIn\nIntl. Conf. on Computer Vision (ICCV)\n,\npp.Â 28260â€“28270\n.\nCited by:\nÂ§\nII-A\n.",
    "preview_text": "Autonomous high-fidelity object reconstruction is fundamental for creating digital assets and bridging the simulation-to-reality gap in robotics. We present ObjSplat, an active reconstruction framework that leverages Gaussian surfels as a unified representation to progressively reconstruct unknown objects with both photorealistic appearance and accurate geometry. Addressing the limitations of conventional opacity or depth-based cues, we introduce a geometry-aware viewpoint evaluation pipeline that explicitly models back-face visibility and occlusion-aware multi-view covisibility, reliably identifying under-reconstructed regions even on geometrically complex objects. Furthermore, to overcome the limitations of greedy planning strategies, ObjSplat employs a next-best-path (NBP) planner that performs multi-step lookahead on a dynamically constructed spatial graph. By jointly optimizing information gain and movement cost, this planner generates globally efficient trajectories. Extensive experiments in simulation and on real-world cultural artifacts demonstrate that ObjSplat produces physically consistent models within minutes, achieving superior reconstruction fidelity and surface completeness while significantly reducing scan time and path length compared to state-of-the-art approaches. Project page: https://li-yuetao.github.io/ObjSplat-page/ .\n\nObjSplat: Geometry-Aware Gaussian Surfels for\nActive Object Reconstruction\nYuetao Li, Zhizhou Jia, Yu Zhang, Qun Hao, Shaohui Zhang\nThe corresponding author:\nzhangshaohui@bit.edu.cn,\nSchool of Optics and Photonics, Beijing Institute of Technology, China\nAbstract\nAutonomous high-fidelity object reconstruction is fundamental for creating digital assets and bridging the simulation-to-reality gap in robotics. We present ObjSplat, an active reconstruction framework that leverages Gaussian surfels as a unified representation to progressively reconstruct unknown objects with both photorealistic appearance and accurate geometry. Addres",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "Gaussian surfels",
        "active reconstruction",
        "viewpoint evaluation",
        "next-best-path planner",
        "object reconstruction"
    ],
    "one_line_summary": "ObjSplatæ˜¯ä¸€ä¸ªä¸»åŠ¨å¯¹è±¡é‡å»ºæ¡†æ¶ï¼Œä½¿ç”¨é«˜æ–¯é¢å…ƒç»Ÿä¸€è¡¨ç¤ºï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥çš„è§†ç‚¹è¯„ä¼°å’Œå¤šæ­¥å‰ç»è§„åˆ’ï¼Œé«˜æ•ˆé‡å»ºæœªçŸ¥ç‰©ä½“çš„å¤–è§‚å’Œå‡ ä½•ç»“æ„ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-11T17:14:33Z",
    "created_at": "2026-01-21T12:09:06.460555",
    "updated_at": "2026-01-21T12:09:06.460563",
    "recommend": 0
}