{
  "id": "2601.08953v1",
  "title": "Fairness risk and its privacy-enabled solution in AI-driven robotic applications",
  "authors": [
    "Le Liu",
    "Bangguo Yu",
    "Nynke Vellinga",
    "Ming Cao"
  ],
  "abstract": "Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.",
  "url": "https://arxiv.org/abs/2601.08953v1",
  "html_url": "https://arxiv.org/html/2601.08953v1",
  "html_content": "\\equalcont\nThese authors contributed equally to this work.\n\\equalcont\nThese authors contributed equally to this work.\n[1,3]\n\\fnm\nMing\n\\sur\nCao\n1]\n\\orgdiv\nFaculty of Science and Engineering,\n\\orgname\nUniversity of Groningen,\n\\orgaddress\n\\city\nGroningen,\n\\postcode\n9747 AG,\n\\country\nThe Netherlands\n2]\n\\orgdiv\nFaculty of Law,\n\\orgname\nUniversity of Groningen,\n\\orgaddress\n\\city\nGroningen,\n\\postcode\n9712 GH,\n\\country\nThe Netherlands\n3]\n\\orgdiv\nELSA(ethical, legal, societal aspects of AI) Lab for Technical Industry,\n\\orgname\nDutch Research Council,\n\\orgaddress\n\\city\nGroningen,\n\\postcode\n9747 AG,\n\\country\nThe Netherlands\nFairness risk and its privacy-enabled solution in AI-driven robotic applications\n\\fnm\nLe\n\\sur\nLiu\nle.liu@rug.nl\n\\fnm\nBangguo\n\\sur\nYu\nb.yu@rug.nl\n\\fnm\nNynke\n\\sur\nVellinga\nn.e.vellinga@rug.nl\nm.cao@rug.nl\n[\n[\n[\nAbstract\nComplex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.\nkeywords:\nRobotic Decision-making, Large Language Model, Fairness, Privacy\n1\nIntroduction\nFairness is a longstanding human concern: laws strive for equal treatment and justice\n[\nfranck1998fairness\n]\n, as reflected in foundational legal instruments such as the Universal Declaration of Human Rights\n[\nudhr1948\n, Arts.¬†1 and¬†10]\nand the Charter of Fundamental Rights of the European Union\n[\neu_charter\n, Art.¬†35]\n. Although fairness has long been examined in human decision-making, it remains insufficiently addressed in AI systems\n[\nteo2023measuring\n]\n. As AI increasingly powers robotic applications‚Äîfrom sensing and navigation to autonomous decision-making‚Äîit now shapes a growing range of consequential outcomes in modern robotics. The rapid adoption of such systems has outpaced understanding of their societal impacts, and mounting evidence of group-level disparities highlights persistent fairness concerns\n[\nweidinger2021ethical\n]\n. This makes it essential that principles of fairness extend to the design and deployment of AI-driven robotic systems.\nIn robotics, fairness concerns whether behaviours‚Äîsuch as classification, planning, control decisions, and human‚Äìrobot interactions‚Äîsystematically disadvantage individuals or groups defined by protected or socially salient attributes\n[\nhardt2016equality\n]\n(e.g., gender, race, age, disability, and religion), which may fall under high-risk categories in the EU Artificial Intelligence Act\n[\nEUAIAct2024\n]\n. Sources of unfairness span the entire robotics pipeline, from biased training data and modeling choices to deployment contexts that exacerbate disparities. These effects can accumulate, rendering a system unfair even if each stage introduces only minor bias\n[\nbender2021parrots\n]\n. To quantify such disparities, researchers have proposed complementary notions of group fairness\n[\nzemel2013learning\n,\nzhao2017men\n]\n, individual fairness\n[\ndwork2012fairness\n]\n, and contextual fairness\n[\nhuo2025large\n]\n.\nPrivacy is another ethical concern for AI-driven robotic systems. Legal frameworks such as the Universal Declaration of Human Rights\n[\nudhr1948\n, Art.¬†12]\nand the General Data Protection Regulation (GDPR)\n[\ngdpr\n, Art.¬†5(1)(a)]\nunderscore the fundamental importance of privacy. In practice, robotic systems face risks including unintended retention of sensitive sensor data and inference attacks on learned models. Noise-based mechanisms such as differential privacy\n[\ndwork2006calibrating\n]\nare commonly adopted safeguards and may also influence system behavior, creating opportunities to improve fairness while protecting sensitive information.\nLarge language models (LLMs), with parameter counts in the billions, have reshaped the paradigm of natural language processing. In contrast to earlier models such as GPT-2\n[\nradford2019language\n]\n, contemporary LLMs demonstrate substantially stronger abilities in natural language understanding and generation, owing to their scale, larger training corpora, and more sophisticated architectures\n[\nNEURIPS2022_b1efde53\n,\nsurvey\n]\n. Recent advances in large vision and language models (VLMs) have markedly improved the joint modeling of visual and textual information\n[\nopenai2024gpt4o\n,\nNEURIPS2023_6dcf277e\n]\n. These models achieve strong performance on robotic tasks that require generating or verifying factual statements, supported by robust scene understanding\n[\nGu2023\n,\nKirillov_2023_ICCV\n]\nand reasoning capabilities\n[\nChen_2024_CVPR\n,\nopenai2024o1\n]\n. Among available systems, OpenAI‚Äôs GPT-family models\n[\nopenai2024o1\n,\nopenai2024gpt4o\n,\nopenai2024gpt4omini\n]\nhave been widely adopted in academic research due to strong empirical performance. Accordingly, we use an OpenAI GPT multimodal model as the vision-language engine in our study. We examine fairness in LLM-driven robotics applications, where robots‚Äô actions can lead to systematic group disparities. Although fairness concerns are well documented in AIs\n[\nteo2023measuring\n,\nlovato2024foregrounding\n,\nchoudhry2024bias\n]\n, their consequences for robotic systems have remained underexplored. We show that robotic navigation for task allocation exhibits significant and persistent disparities across groups. To quantify these effects, we introduce utility-aware fairness metrics that capture group-specific outcomes through both individual- and group-level formulations. We then establish that fairness can be enforced through differential-privacy parameters, providing a mathematically interpretable guarantee. Building on this insight, we propose a privacy-based remedy, demonstrating that enforcing privacy measurably improves fairness.\nIn a vision‚Äìlanguage navigation case study, we show that privacy can serve as an effective mechanism for promoting fairness. The robot receives human-resource (HR) information together with map data, navigates to HR personnel and then allocates tasks according to its learned policy. In the baseline, the system inadvertently exploits sensitive attributes embedded in the HR records, leading to systematically more onerous assignments for a particular group. When privacy is enforced in the decision pipeline‚Äîby randomizing the sensitive attributes‚Äîthe resulting allocations become markedly fairer. This demonstrates that, in reasonable settings, fairness can be achieved through privacy safeguards alone.\nWhereas most strategies for fair decision-making treat fairness as an explicit constraint, our approach enforces fairness through privacy alone. We formalize a utility-aware notion of fairness that captures the real-world utilities of robotic decisions for different individuals and groups, grounding the analysis in the interplay between fairness and privacy. More broadly, the procedure advances ethically responsible robotics and strengthens public trust by addressing privacy and fairness simultaneously.\nResults\nRobotic Navigation using LLM is inherently biased\nRobotic systems that rely on generative AI for decision-making can inherit the foundation models‚Äô bias, raising significant ethical concerns for the deployment of LLM-driven robotics. We first present the fairness concerns observed in this VLM-driven robotic navigation. Another example on package delivery is presented in Supporting Information Section¬†D.\nPath planning\nIn the robot navigation task derived from an environment map, one identifies the most suitable path using a query and a set of candidate paths. The path identifier\nU\n‚àà\n{\n0\n,\n1\n,\n‚Ä¶\n,\nN\n}\nU\\in\\{0,1,\\dots,N\\}\nis selected using a VLM by providing the descriptive query\nq\nq\n, the set of candidates, and a natural language prompt indicating that a matching answer is desired. In other words, the VLM models a conditional distribution\n‚Ñô\n‚Äã\n(\nU\n‚à£\nq\n)\n{\\mathbb{P}}(U\\mid q)\n. The query\nq\nq\ncontains two types of information:\nA\nA\nis sensitive, while\nX\nX\nnot. Upon receiving the query\nq\nq\ncontaining\nX\nX\nand\nA\nA\n, the VLM can unfairly plan a path. In the S3DIS dataset\n[\nArmeni2016a\n]\n, we selected\nArea_5a\nroom. A pair of start and destination positions from the annotated rooms is selected to evaluate the fairness in this task. The related navigational instruction is:\nDeliver a confidential document concerning Maternity leave/Holiday leave/Sick leave to one of the two available HR offices\n. Therefore,\nX\nX\nin the query\nq\nq\nrepresents the document type and office location information of the HR staff, with all offices set at the same distance from the starting point. The sensitive group-membership information\nA\nA\nis shown in TABLE\n1\n.\nTable 1:\nThe Sensitive Group-membership Information\nHR\nA\nA\nHR_1\nTom, 25 years old, Asian\nHR_2\nMary, 55 years old, American\nUnfairness arises\nUsing the architecture shown in Fig.\n1\n, we collect the optimal paths generated by GPT-4o\n[\nopenai2024gpt4o\n]\n, GPT-4o-mini\n[\nopenai2024gpt4omini\n]\n, and o1\n[\nopenai2024o1\n]\nover 100 runs.\nFigure 1:\nSystem architecture of the proposed robot navigation framework.\nA point cloud map is used to generate top-view and topological representations. Candidate paths are extracted from the topological map and projected onto the top-view map. A vision‚Äìlanguage model then selects an optimal route from these candidates, which the robot follows to complete the navigation task.\nFigure 2:\nUnfairness results in LLM-based robot navigation.\nUnfairness is observed in LLM-based robot navigation even if the document types are different, as the GPT models consistently select HR_2, leading to an unfair workload.\nAs shown in Fig.\n2\n, the VLM consistently chooses HR_2, which results in an unfair distribution of tasks among the HRs. An explanation is that Mary‚Äôs seemingly richer HR experience, associated with her age, gender and cultural background, makes her appear more capable of handling sensitive leave issues.\nMoreover, even when the underlying large language model switches, the unfairness phenomenon remains evident. This suggests that potential bias and unfair decision-making persist across different large language models.\nThe results are fundamental, as they illustrate the high chance for unfair robotic decision-making when using VLM-based systems. Such biased behaviours could hinder the broader adoption of VLM-driven robots in real-world applications by introducing legal risks\n[\nEUAIAct2024\n]\n.\nNext, we show it is possible to define a fairness metric that effectively captures utility or cost (e.g., task burden in this robot navigation).\nA new qualitative fairness metric\nFairness concerns the quantification of disparities in outcomes across groups. In VLM-driven robotic decision-making, users submit requests to a robotic system that relies on VLMs to generate decisions. During this interaction, both sensitive group-membership attributes\nA\nA\n(e.g., race, nationality, gender) and other personal features\nX\nX\n(e.g., office number, working hours, salary) may be conveyed to the VLM. The VLM then produces a decision\nU\nU\n, which is executed by the robot. To ensure equitable treatment across groups, a fairness metric must provide clear and consistent guidance while accounting for utility‚Äîsuch as differences in workload allocation among HR staff. Accordingly, we introduce a new fairness metric in this section. For more details showing the connection between our new fairness metric and those in the literature, see Supporting Information Section¬†B.\nGeneral Properties\nGenerally, a fairness metric should satisfy some fundamental properties to ensure interpretability, consistency, and practical relevance in decision-making. The following three properties are proposed to capture how the metric responds to disparities, and establish a meaningful baseline:\n‚Ä¢\nMonotonicity\n‚Äî the fairness metric increases with inter-group disparity, ensuring a ‚Äúworse-is-larger‚Äù ordering and avoiding perverse rankings.\n‚Ä¢\nNon-negativity\n‚Äî is always non-negative and equals zero only under perfect parity, providing a clear baseline for optimization.\n‚Ä¢\nUtility awareness\n‚Äî reflects disparities in utility outcomes, aligning the metric with consequential effects rather than surrogate statistics.\nQuantifiable Fairness\nWe fix\nP\n‚Äã\n(\nU\n‚à£\nX\n,\nA\n)\nP(U\\mid X,A)\nto represent the pretrained behaviour of the VLM-driven robotic system in generating the stochastic response\nU\nU\n, where\nX\nX\nand\nA\nA\ndenote the attributes of the individuals with whom the robotic system interacts. To measure worst-case group disparities, we introduce\nlocal\ng\ng\n-fairness\nL\n‚Äã\n(\nP\n,\ng\n)\nL(P,g)\n, which is a function of the joint distribution\nP\nP\nand a utility function\ng\ng\n:\nL\n‚Äã\n(\nP\n,\ng\n)\n:=\nlog\n‚Äã\nsup\nx\n‚àà\nùí≥\n,\na\n,\na\n‚Ä≤\n‚àà\nùíú\nùîº\n‚Äã\n[\ng\n‚Äã\n(\nU\n,\nX\n,\nA\n)\n‚à£\nX\n=\nx\n,\nA\n=\na\n]\nùîº\n‚Äã\n[\ng\n‚Äã\n(\nU\n,\nX\n,\nA\n)\n‚à£\nX\n=\nx\n,\nA\n=\na\n‚Ä≤\n]\n,\n\\displaystyle L(P,g):=\\log\\sup_{x\\in\\mathcal{X},\\,a,a^{\\prime}\\in\\mathcal{A}}\\frac{\\mathbb{E}\\!\\left[g(U,X,A)\\mid X=x,\\,A=a\\right]}{\\mathbb{E}\\!\\left[g(U,X,A)\\mid X=x,\\,A=a^{\\prime}\\right]},\n(1)\nwhere\nùîº\n\\mathbb{E}\ndenotes expectation over random variables.\nL\n‚Äã\n(\nP\n,\ng\n)\nL(P,g)\nserves as a fairness metric:\nL\n‚Äã\n(\nP\n,\ng\n)\n=\n0\nL(P,g)=0\nindicates parity, and larger values correspond to greater disparity. Given\nX\nX\n, it compares expected utilities across groups within the same context.\nWe also consider\nglobal\ng\ng\n-fairness\nL\n¬Ø\n‚Äã\n(\nP\n,\ng\n)\n\\bar{L}(P,g)\n, defined analogously but after averaging over\nX\nX\n. Both metrics satisfy monotonicity, non-negativity, and utility awareness, but local fairness is stricter: equal treatment in every context implies global fairness, whereas global fairness may mask context-specific disparities. The formal mathematical definitions of\nL\n‚Äã\n(\nP\n,\ng\n)\nL(P,g)\nand\nL\n¬Ø\n‚Äã\n(\nP\n,\ng\n)\n\\bar{L}(P,g)\nare given in Supporting Information Section¬†A.\nRemark 1.1\n.\nOur fairness metrics relate to, but remain distinct from, established notions in algorithmic fairness, which predominantly focus on classification. The local\ng\ng\n-fairness aligns with the principle of individual fairness introduced in\n[\ndwork2012fairness\n]\n. In contrast to the conditional-independence criteria of\n[\nkleinberg2016inherent\n]\n, our utility-centric metrics quantify differences in expected utility‚Äîat both the individual and group levels‚Äîthereby capturing the consequences of AI-driven robotic decisions rather than the statistical behavior of prediction rules.\nIn the robotic navigation example discussed earlier, one can measure fairness by defining a cost function\ng\ng\nthat represents the task burden. Using this function, one can quantify the difference between HRs‚Äô workloads through\nL\nL\nor\nL\n¬Ø\n\\bar{L}\n.\nWhile in principle, one may promote fairness during training, via e.g., regularization\n[\nzemel2013learning\n,\nzeng2023deep\n]\n, constrained optimization\n[\nzafar2019fairness\n,\nroh2020fr\n]\n, or reweighting\n[\nkamiran2012data\n]\n, users are typically more interested in post hoc guarantees: how to secure fairness at inference time. We now present our key finding that an appropriate privacy mechanism can induce the desired fairness guarantees.\nFairness can be established through preserving privacy\nIn this section, we demonstrate how differential privacy (DP)\n[\ndwork2006calibrating\n]\nconstraints fundamentally shape our fairness criteria, leading to results that are not only qualitative but also mathematically interpretable and quantifiable. Rather than merely illustrating that privacy can promote fairness, our analysis provides a principled characterization of this relationship. A detailed definition of differential privacy is provided in Supporting Information Section¬†A.\nDifferential Privacy\nIntuitively, the\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\n-differential privacy guarantee ensures that a system‚Äôs output reveals essentially little information about whether the input was\na\na\nor\na\n‚Ä≤\na^{\\prime}\n: any measurable event is almost as likely under either input, up to a multiplicative factor\ne\nŒµ\nA\ne^{\\varepsilon_{A}}\nand an additive slack\nŒ¥\nA\n\\delta_{A}\n. Smaller\nŒµ\nA\n\\varepsilon_{A}\nand\nŒ¥\nA\n\\delta_{A}\nyield stronger protection, and\nŒ¥\nA\n=\n0\n\\delta_{A}=0\nrecovers pure DP. Differential privacy provides a rigorous guarantee and is widely used because of its robustness and simplicity. For privacy protection, we use an\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\n-differentially private release\nA\n~\n\\tilde{A}\nof\nA\nA\nand an\n(\nŒµ\nX\n,\nŒ¥\nX\n)\n(\\varepsilon_{X},\\delta_{X})\n-differentially private release\nX\n~\n\\tilde{X}\nof\nX\nX\nfor decision making. In this paper, we focus on privacy parameters\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\n, as it is closely tied to our fairness metrics; by contrast, the implications of safeguarding the privacy of\nX\nX\nfor fairness are not straightforward. Privacy protections for\nX\nX\ndo not uniformly promote fairness and may, in some settings, worsen fairness concerns. We examine these conditions in detail in the Supporting Information Section C.\nTurning Privacy into Fairness Guarantees\nFigure 3:\nSystem Architecture of privacy filters.\nRaw features\nX\nX\nand the sensitive attribute\nA\nA\nfrom the agent are privatized by separate filters to\nX\n~\n\\tilde{X}\nand\nA\n~\n\\tilde{A}\n, satisfying\n(\nŒµ\nX\n,\nŒ¥\nX\n)\n(\\varepsilon_{X},\\delta_{X})\n-DP and\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\n-DP, respectively. Because the attributes\nX\nX\nand\nA\nA\nare privatized before being transmitted to the robotic system, the VLM-driven robotic system generates its response\nU\nU\naccording to the distribution\nP\n‚Äã\n(\nU\n‚à£\nX\n~\n,\nA\n~\n)\nP(U\\mid\\tilde{X},\\tilde{A})\n.\nThe VLM-driven robot receives noise-injected information and subsequently makes the decision. Concretely, we release\nA\n~\n\\tilde{A}\nand\nX\n~\n\\tilde{X}\nfrom\n(\nA\n,\nX\n)\n(A,X)\nvia randomized mechanisms that satisfy\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\n-differential privacy with respect to\nA\nA\nand\n(\nŒµ\nX\n,\nŒ¥\nX\n)\n(\\varepsilon_{X},\\delta_{X})\n-differential privacy with respect to\nX\nX\n. The robot utilizes\n(\nA\n~\n,\nX\n~\n)\n(\\tilde{A},\\tilde{X})\nto generate its decision. This architecture is illustrated in Fig.\n3\n.\nWe obtained the formal connection between the fairness metrics (local\nL\nL\nand global\nL\n¬Ø\n\\bar{L}\n) and differential privacy. Our main results show that the privacy parameters\nŒµ\nA\n\\varepsilon_{A}\nand\nŒ¥\nA\n\\delta_{A}\nestablish upper bounds for\nL\n‚Äã\n(\nP\n,\ng\n)\nL(P,g)\nand\nL\n¬Ø\n‚Äã\n(\nP\n,\ng\n)\n\\bar{L}(P,g)\n:\nL\n¬Ø\n‚Äã\n(\nP\n,\ng\n)\n‚â§\nL\n‚Äã\n(\nP\n,\ng\n)\n‚â§\nŒµ\nA\n+\nlog\n‚Å°\n(\n1\n+\nL\nA\n‚Äã\ndiam\n‚Å°\n(\nùíú\n)\n+\nŒ¥\nA\n‚Äã\nŒ≥\nœÑ\n)\n,\n\\displaystyle\\bar{L}(P,g)\\leq L(P,g)\\leq\\varepsilon_{A}+\\log\\!\\Big(1+\\dfrac{L_{A}\\,\\operatorname{diam}(\\mathcal{A})+\\delta_{A}\\gamma}{\\tau}\\Big),\n(2)\nwhere\nL\nA\nL_{A}\n,\ndiam\n‚Å°\n(\nùíú\n)\n\\operatorname{diam}(\\mathcal{A})\n, and\nŒ≥\n\\gamma\nare positive constants associated with\nùíú\n\\mathcal{A}\nand\ng\ng\n, as defined in Supporting Information Section¬†C.\nProofs and details are also deferred to the Supporting Information Section¬†C.\nIn essence, smaller values of\nŒµ\nA\n\\varepsilon_{A}\nand\nŒ¥\nA\n\\delta_{A}\nlead to lower fairness metrics, indicating that stronger privacy guarantees yield improved fairness. Thus,\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\n-DP for the sensitive attribute directly controls the fairness metrics. By calibrating the privacy mechanism for\nA\nA\n, one can select noise levels that guarantee target bounds on\nL\nL\nand\nL\n¬Ø\n\\bar{L}\n.\nExperimental Results\nTo evaluate the influence of privacy in a fairness-aware navigation task, we use different privacy parameters\nŒµ\nA\n\\varepsilon_{A}\nfor each HR information about the age (25 and 55 years old), gender (Tom and Mary), and race (Asian and American), and sample 50 times for each privacy parameters\nŒµ\nA\n\\varepsilon_{A}\n. The experiment results are shown in Fig.\n4\n. The experimental details are provided in the Method section. Another example on package delivery is presented in Supporting Information Section¬†D.\nIt should be noted that\ng\n:=\nùüè\n{\nchoose\n‚Äã\na\n}\ng:=\\mathbf{1}_{\\{\\text{choose }a\\}}\nserves as the cost function, indicating that the robot assigns the task to HR\na\na\n. Therefore, a fair decision in this context is straightforward: the robot assigns equal probability to selecting each HR.\nFigure 4:\nFairness-Privacy results in LLM-based robot navigation.\nIn this task,\nL\n‚Äã\n(\nP\n,\ng\n)\n=\n0\nL(P,g)=0\nindicates a fair workload across document types, while\nL\n¬Ø\n‚Äã\n(\nP\n,\ng\n)\n=\n0\n\\bar{L}(P,g)=0\ndenotes fairness in the average workload aggregated over the three document types. In this experiment, we set\nŒ¥\nA\n=\n0\n\\delta_{A}=0\n. As shown, the fairness metric increases with\nŒµ\nA\n\\varepsilon_{A}\n, indicating that the privacy parameter directly influences fairness; stronger privacy guarantees therefore promote fairer outcomes.\nAs shown in Fig.\n4\n, when the privacy parameter\nŒµ\nA\n\\varepsilon_{A}\ndecreases, the robot tends to assign tasks more fairly across different HRs, regardless of the underlying LLM. This demonstrates the effectiveness of our proposed method.\nDiscussions\nOur results show that privacy can serve not only its traditional role but also function as a principled instrument for shaping fairness. By introducing differential privacy at the interface between inputs and decisions, we derive explicit and testable bounds on both local and global utility-aware fairness metrics. In essence, privacy constrains the extent to which a system can differentially respond to group membership, and this induced indistinguishability translates directly into fairness guarantees. In our VLM case study, applying privacy to group-sensitive inputs significantly reduced allocation disparities, showing that privacy can act as a practical fairness tool when retraining is not feasible.\nThis reframing offers two practical advantages. First, it turns the often adversarial trade-off between privacy and fairness‚Äîboth typically treated as constraints in decision-making‚Äîinto reciprocity: a single privacy mechanism can simultaneously provide confidentiality and guide fairness. Second, it enables a quantitative\nfairness certificate\n: given a privacy budget\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\nfor the sensitive attribute, one can guarantee that worst-case disparities remain bounded below a prescribed threshold.\nThe assumptions required for these guarantees are mild. The independence of features and sensitive attributes isolates fairness effects from compositional confounding. If\nX\nX\nis correlated with\nA\nA\n, one can decompose or project\nX\nX\nto enforce independence. Uniform boundedness and a positive lower bound on expected utility can be achieved through standard rescaling or truncation. These conditions avoid strong modeling assumptions and remain compatible with off-the-shelf generative systems and decision modules.\nConceptually, the mechanism-level view is important. Because differential privacy is invariant to post-processing and composes across repeated uses, privacy added at the input stage carries through downstream components without being weakened by system design. This property is particularly valuable for deployed systems with multiple downstream architectures.\nAt the same time, privacy is not a panacea. There is also the inevitable privacy‚Äìaccuracy tension: while our bounds quantify how fairness scales with\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\n, overly small budgets can degrade utility if the task genuinely requires group-specific information for accurate decisions (for example, clinical issue).\nMethodologically, our analysis offers practical guidance. The privacy parameters\n(\nŒµ\nA\n,\nŒ¥\nA\n)\n(\\varepsilon_{A},\\delta_{A})\ncan be calibrated to meet policy-defined fairness goals, using our bounds as conservative design constraints.\nA key limitation is scope: we focus on fairness under\n(\nŒµ\n,\nŒ¥\n)\n(\\varepsilon,\\delta)\n-differential privacy. Exploring alternative privacy frameworks‚Äîsuch as testing-based DP or concentrated/Gaussian DP‚Äîmay yield tighter or more interpretable fairness‚Äìprivacy trade-offs.\nIn summary, the central message is constructive: carefully designed privacy can improve fairness. This perspective reframes privacy not only as a safeguard against information leakage, but also as a policy lever for governing the social behaviour of AI-driven robotics applications. By making the fairness effects of privacy explicit and tunable, our results open the door to integrated deployments in which confidentiality, fairness and task performance are engineered jointly.\nMethods\nDataset used in VLM-driven robotic navigation\nOur experiments are conducted on the S3DIS dataset\n[\nArmeni2016a\n]\n, which provides mutually registered multi-modal 3D data with instance-level semantic and geometric annotations. The dataset spans over 6,000\nm\n2\nm^{2}\nacross six large-scale indoor areas from three different buildings and includes both raw and semantically annotated 3D meshes and point clouds.\nImplementation details of privacy filter\nThe proposed framework is illustrated in Fig.\n5\n. The agent first acquires a scene point cloud and constructs top-view and traversability maps, from which a topological map is derived. Candidate paths from the start to the destination are then generated using the A* algorithm\n[\nHart1968\n]\n. A vision‚Äìlanguage model subsequently uses the privacy-filtered information to select an optimal path among the candidates. The agent then follows the selected path to reach the destination.\nFigure 5:\nSystem architecture of the proposed fairness-aware robot navigation framework with privacy filter.\nA point cloud map is processed to generate top-view and topological representations. Candidate paths are extracted from the topological map and projected onto the top-view map. A privacy filter perturbs human-related attributes, and the filtered information, together with the candidate paths, is provided to a vision‚Äìlanguage model, which selects an optimal route under privacy constraints for robot navigation.\nWe construct the top-view and traversability maps from the scene point cloud. To generate the top-view map\nM\ntop\nM_{\\mathrm{top}}\n, ceiling points are first removed from the raw point cloud, and the remaining environmental points are projected onto a 2D grid according to their heights. For each grid cell, the highest point is retained to preserve vertical structure in the top-view representation. The traversability map\nM\ntra\nM_{\\mathrm{tra}}\nis then constructed by identifying grid cells containing points within a specified height range as traversable, while cells absent from the top-view map are designated as obstacles. Both maps are generated using a shared coordinate system and resolution to ensure spatial alignment and consistent scale. A topological map is further derived from the annotated point cloud, providing room-level location information.\nFor each candidate route, the A* algorithm\n[\nHart1968\n]\nis applied to the traversability map\nM\ntra\nM_{\\mathrm{tra}}\nto compute a minimum-cost grid path from the start to the target destination by combining the accumulated path cost with a heuristic estimate of the remaining distance. The complete set of candidate paths is thus obtained.\nThe textual prompt encodes the task objective, environmental assumptions, and expected output format, clarifying the task structure and facilitating effective candidate selection. An example is shown below.\nYou are an intelligent delivering agent tasked with selecting one HR for delivering a document.\nYou will receive:\n1. The profiles of two candidate HR recipients (HR1 and HR2).\n2. Two top-down floorplan maps as images. Each image shows the route to its corresponding HR office highlighted in red, and the HR index is printed in the upper-left corner of the image: ‚Äú1‚Äù for HR1, ‚Äú2‚Äù for HR2.\nYour current mission is to deliver a document to one of two available HR offices. Based on the HR information, the routes shown on the maps, and the item to be delivered, choose the most suitable HR recipient.\nRespond by selecting either HR1 or HR2, assigning a value of 1.0 to your chosen option, and briefly stating the reason. If you believe both have the same likelihood, randomly select one. Your response should be a JSON object:\nOutput Example:\n{\n‚ÄúHR1‚Äù: ‚Äú0.0‚Äù,\n‚ÄúHR2‚Äù: ‚Äú1.0‚Äù,\n‚Äúreason‚Äù: ‚ÄúExplain why the chosen HR was selected and why the other was not.‚Äù\n}\nTo protect privacy, randomized noise is injected into two candidate HR profiles. Across 50 trials, each attribute (name, age, race) is sampled according to preset probabilities; for example, for HR-1, the name is sampled as Tom with 70% probability and Mary with 30%. For each attribute, a random variable\ns\n‚àº\nUniform\n‚Äã\n(\n0\n,\n1\n)\ns\\sim\\mathrm{Uniform}(0,1)\nis drawn, and Tom is selected if\ns\n<\np\ns<p\n, where\np\np\ndenotes the sampling probability; otherwise, the alternative is chosen. The same procedure is applied to all attributes. The privacy parameter is defined as\nŒµ\nA\n=\np\n/\n(\n1\n‚àí\np\n)\n\\varepsilon_{A}=p/(1-p)\n.\nEach candidate path is then integrated into the top-view map\nM\ntop\nM_{\\mathrm{top}}\nas a visual prompt with an associated path identifier, yielding a set of candidate maps. These candidates, together with textual navigation prompts and external scene knowledge, are provided to the VLM. The VLM infers the optimal path among the candidates and outputs the selected path identifier, taking into account the scene layout, robot state, environmental context, and task instructions. This formulation eliminates the need for few-shot demonstrations while enabling the incorporation of rich scene knowledge for dynamic, interactive navigation. To enhance transparency, the VLM output includes both the selected path and a description of the reasoning process underlying the decision, as illustrated in Fig.\n5\n.\nRelated Work\nInterest in the foundations of fairness in machine learning has grown rapidly in recent years. A central strand of this work concerns algorithmic fairness. For example, ref.\n[\ndwork2012fairness\n]\nintroduced a formulation of individual fairness grounded in classification, while\n[\nkleinberg2016inherent\n]\nestablished key impossibility results among widely used fairness criteria. Fairness in data-driven educational systems has been surveyed in\n[\nkizilcec2022algorithmic\n]\n, and broader perspectives can be found in\n[\nmitchell2021algorithmic\n,\nbarocas2023fairness\n]\n. Despite this progress, the interaction between privacy and fairness remains comparatively understudied, leaving open fundamental questions about how these principles constrain‚Äîand potentially reinforce‚Äîone another.\nSafeguarding data privacy is now foundational: modern analytics draw power from rich, individual-level data, yet the same richness amplifies the risk of disclosure and misuse. Differential privacy offers a practical remedy. By adding carefully calibrated randomness, it ensures that reported statistics or released datasets can be analyzed directly while limiting what can be inferred about any single person‚Äîunlike encryption, which typically requires decryption before analysis. When noise is injected into a release, several metrics can quantify privacy leakage, including mutual information\n[\nwang2016relation\n,\nliao2017hypothesis\n]\n, k-anonymity\n[\nsweeney2002kanon\n]\n, t-closeness\n[\nli2007tcloseness\n]\n, and, most prominently, differential privacy\n[\ndwork2006calibrating\n,\ndwork2014foundations\n]\n. Among these criteria, differential privacy offers a clear and rigorous framework: it limits any adversary‚Äôs ability to perform hypothesis testing about an individual‚Äôs data\n[\nballe2020hypothesis\n,\ndong2022gaussian\n]\n, and it remains robust even in the presence of side information. Consequently, we adopt differential privacy as our formal criterion for privacy.\nSome works have shown that privacy and fairness can be at odds: adding noise to achieve privacy may disproportionately impact minority groups, thereby amplifying disparities in model performance\n[\nbagdasaryan2019differential\n,\ncummings2019compatibility\n,\nmozannar2020fair\n]\n. Conversely, other studies argue that privacy constraints can also promote fairness, as privacy reduces the ability of models to distinguish between sensitive groups\n[\nkhalili2021improving\n]\n. This duality has motivated recent theoretical and empirical investigations into the joint design of privacy-preserving and fairness-aware learning algorithms.\nIn the context of robotics and embodied AI, this relationship remains relatively underexplored. While fairness has been studied in human‚Äìrobot interaction and task allocation\n[\nwang2023fairness\n,\nkim2022equitable\n]\n, the effect of privacy mechanisms on fairness in robotic decision-making‚Äîparticularly when guided by LLMs and LLMs has received limited attention. Our work contributes to this emerging line of research by demonstrating how differential privacy applied to sensitive attributes can serve as a controllable mechanism to enforce fairness in robot task assignment.\nReferences",
  "preview_text": "Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, we show that Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. Here we provide a utility-aware fairness metric for robotic decision making and analyze fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics. This yields a unified framework that formalizes and quantifies fairness and its interplay with privacy, which is tested in a robot navigation task. In view of the fact that under legal requirements, most robotic systems will enforce user privacy, the approach shows surprisingly that such privacy budgets can be jointly used to meet fairness targets. Addressing fairness concerns in the creative combined consideration of privacy is a step towards ethical use of AI and strengthens trust in autonomous robots deployed in everyday environments.\n\n\\equalcont\nThese authors contributed equally to this work.\n\\equalcont\nThese authors contributed equally to this work.\n[1,3]\n\\fnm\nMing\n\\sur\nCao\n1]\n\\orgdiv\nFaculty of Science and Engineering,\n\\orgname\nUniversity of Groningen,\n\\orgaddress\n\\city\nGroningen,\n\\postcode\n9747 AG,\n\\country\nThe Netherlands\n2]\n\\orgdiv\nFaculty of Law,\n\\orgname\nUniversity of Groningen,\n\\orgaddress\n\\city\nGroningen,\n\\postcode\n9712 GH,\n\\country\nThe Netherlands\n3]\n\\orgdiv\nELSA(ethical, legal, societal aspects of AI) Lab for Technical Industry,\n\\orgname\nDutch Research Council,\n\\orgaddress\n\\city\nGroningen,\n\\postcode\n9747 AG,\n\\country\nThe Netherlands\nFairness risk and its privacy-enabled solution in AI-driven robotic applications\n\\fnm\nLe\n\\sur\nLiu\nle.liu@rug.nl\n\\fnm\nBangguo\n\\sur\nYu\nb.yu@rug.nl\n\\fnm\nNynke\n\\sur\nVellinga\nn.",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "fairness",
    "privacy",
    "robotic applications",
    "AI-driven",
    "ethical AI"
  ],
  "one_line_summary": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜAIÈ©±Âä®Êú∫Âô®‰∫∫Â∫îÁî®‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄßÈ£éÈô©ÂèäÂÖ∂ÈÄöËøáÈöêÁßÅÈ¢ÑÁÆóÂÆûÁé∞ÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂ±û‰∫é‰º¶ÁêÜAIÈ¢ÜÂüüÔºå‰∏éÂº∫ÂåñÂ≠¶‰π†„ÄÅVLAÁ≠âÊäÄÊúØÂÖ≥ÈîÆËØçÊó†ÂÖ≥„ÄÇ",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "flag": "",
  "published_date": "2026-01-13T19:43:55Z",
  "created_at": "2026-01-20T17:49:46.832998",
  "updated_at": "2026-01-20T17:49:46.833008"
}