{
  "id": "2512.02011v1",
  "title": "Learning Dexterous Manipulation Skills from Imperfect Simulations",
  "authors": [
    "Elvis Hsieh",
    "Wen-Han Hsieh",
    "Yen-Jen Wang",
    "Toru Lin",
    "Jitendra Malik",
    "Koushil Sreenath",
    "Haozhi Qi"
  ],
  "abstract": "Reinforcement learning and sim-to-real transfer have made significant progress in dexterous manipulation. However, progress remains limited by the difficulty of simulating complex contact dynamics and multisensory signals, especially tactile feedback. In this work, we propose \\ours, a sim-to-real framework that addresses these limitations and demonstrates its effectiveness on nut-bolt fastening and screwdriving with multi-fingered hands. The framework has three stages. First, we train reinforcement learning policies in simulation using simplified object models that lead to the emergence of correct finger gaits. We then use the learned policy as a skill primitive within a teleoperation system to collect real-world demonstrations that contain tactile and proprioceptive information. Finally, we train a behavior cloning policy that incorporates tactile sensing and show that it generalizes to nuts and screwdrivers with diverse geometries. Experiments across both tasks show high task progress ratios compared to direct sim-to-real transfer and robust performance even on unseen object shapes and under external perturbations. Videos and code are available on https://dexscrew.github.io.",
  "url": "https://arxiv.org/abs/2512.02011v1",
  "html_url": "https://arxiv.org/html/2512.02011v1",
  "html_content": "Learning Dexterous Manipulation Skills from Imperfect Simulations\nElvis Hsieh\n‚àó\n, Wen-Han Hsieh\n‚àó\n, Yen-Jen Wang\n‚àó\n, Toru Lin, Jitendra Malik, Koushil Sreenath\n‚Ä†\n, Haozhi Qi\n‚Ä†\nUC Berkeley\n‚àó\nEqual contribution (listed in alphabetical order).\n‚Ä†\nEqual advising.\nAbstract\nReinforcement learning and sim-to-real transfer have made significant progress in dexterous manipulation. However, progress remains limited by the difficulty of simulating complex contact dynamics and multisensory signals, especially tactile feedback. In this work, we propose DexScrew, a sim-to-real framework that addresses these limitations and demonstrates its effectiveness on nut-bolt fastening and screwdriving with multi-fingered hands. The framework has three stages. First, we train reinforcement learning policies in simulation using simplified object models that lead to the emergence of correct finger gaits. We then use the learned policy as a skill primitive within a teleoperation system to collect real-world demonstrations that contain tactile and proprioceptive information. Finally, we train a behavior cloning policy that incorporates tactile sensing and show that it generalizes to nuts and screwdrivers with diverse geometries. Experiments across both tasks show high task progress ratios compared to direct sim-to-real transfer and robust performance even on unseen object shapes and under external perturbations.\nI\nIntroduction\nReinforcement learning (RL) paired with sim-to-real transfer has recently delivered a number of promising results in dexterous manipulation\n[\nopenai2018learning\n,\nopenai2019solving\n,\nhanda2023dextreme\n,\nchen2023visual\n,\nqi2022hand\n]\n. Policies trained in massively parallel simulation\n[\nmakoviychuk2021isaac\n]\nwith domain randomization\n[\npeng2018sim\n]\nhave demonstrated strong robustness and generalization capabilities in the real world.\nHowever, in practice, sim-to-real transfer faces two major limitations. First, due to the complexity of physics simulation, only a limited range of tasks can be accurately modeled. Prior work either relies on specialized techniques for high-fidelity simulation\n[\nnarang2022factory\n,\nheiden2021disect\n]\nor seeks generalization through domain randomization\n[\nlee2020learning\n,\nkumar2021rma\n,\nopenai2018learning\n,\nloquercio2021learning\n]\n. However, as the task becomes more dynamic, the sim-to-real gap grows\n[\nwang2024lessons\n]\n, and simulation alone becomes insufficient. Second, existing sensing modalities have an intrinsic sim-to-real gap. While vision can be partially mitigated through domain randomization\n[\nsingh2024dextrah\n]\n, tactile sensing remains difficult to approximate reliably. Although some work aims to improve tactile simulation\n[\nwang2022tacto\n,\nakinola2025tacsl\n]\nor trains policies using alternative proxy representations\n[\nqi2023general\n,\nyang2024anyrotate\n,\nyin2025learning\n]\n, these approaches cannot leverage the full power of tactile sensing. These limitations remain widely viewed as major constraints on the complexity of tasks that can be achieved.\nOn the other hand, teleoperation and imitation learning\n[\nchi2025diffusion\n,\nzhao2023learning\n]\nremove the need for simulation entirely. In this setting, policies can learn directly from real-world interactions and sensorimotor signals, which avoids the challenges introduced by sim-to-real transfer. However, teleoperating dexterous hands is challenging because of the intrinsic morphology differences between human and robot hands\n[\narunachalam2022dexterous\n,\narunachalam2023holo\n,\nqin2023anyteleop\n]\n. As a result, it is difficult to collect datasets that are large and diverse enough to achieve the desired behavior and generalization.\nMotivated by these observations, we introduce DexScrew, a framework that combines the strengths of both approaches to expand the capability of sim-to-real reinforcement learning under imperfect simulation. The key idea is that\nthe motion primitives underlying contact-rich dexterous manipulation do not need to be learned from a perfect physics model\n. A simplified simulator is sufficient to induce the core rotational behaviors required for these tasks. Once this motion is learned, the resulting policy can be used as a skill primitive to collect real-world demonstrations, from which a new policy can be learned. In this way, sensing modalities and physical interactions that are difficult to simulate can be obtained directly from real-world data, while the fine-grained motions that are hard to teleoperate are provided by the simulation-trained policy. We demonstrate this idea through two tasks, nut-bolt fastening and screwdriving with a multifingered hand. Both tasks are traditionally viewed as requiring complex contact dynamics understanding and tactile sensing. We instead show that effective policies can be learned without relying on high-fidelity simulation.\nMore specifically, our framework consists of three stages. First, we train reinforcement learning (RL) policies in simulation using a simplified physics model. Instead of modeling the thread structure of the nut and screw, we approximate their interaction with a revolute joint that connects two simple geometric shapes, which allows the policy to efficiently learn rotational behavior. Second, we use this learned skill as a primitive within a teleoperation system to collect real-world demonstrations. The operator controls the arm motion and triggers the finger rotation skill rather than issuing low-level joint commands, which enables efficient collection of tactile data during teleoperated execution. Finally, using the resulting multisensory dataset, we train a behavior cloning policy that coordinates arm and finger motions while leveraging tactile feedback.\nWe evaluate our framework on two tasks: nut-bolt fastening and screwdriving. Policies trained with simplified dynamics can generate reasonable rotational behavior but cannot complete the tasks. By learning from real-world multisensory demonstrations, our method overcomes these limitations and achieves stable and reliable performance under challenging contact conditions. These results show that complex contact-rich manipulation skills can be bootstrapped from simplified simulators and that real-world tactile feedback is essential. Our framework provides a scalable path toward dexterous manipulation and supports broader deployment of general-purpose robot hands in unstructured environments.\nII\nRelated Work\nDexterous manipulation has been a long-standing challenge in robotics\n[\nbicchi2002hands\n,\nokamura2000overview\n]\n. Early work focused on classical model-based control and analytic grasp planning\n[\nrus1999hand\n,\nmordatch2012contact\n,\nfearing1986implementing\n,\nmorgan2022complex\n,\npatidar2023hand\n]\n. Recent years have seen rapid progress in learning-based approaches, which can be grouped into two primary directions: sim-to-real learning paired with reinforcement learning\n[\nopenai2018learning\n,\nqi2022hand\n,\nchen2023visual\n]\nand imitation learning from teleoperation\n[\nlin2024learning\n,\narunachalam2023holo\n,\nqin2023anyteleop\n]\nor human data\n[\nwang2024dexcap\n,\nxu2025dexumi\n]\n.\nBoth directions, however, face notable limitations. Sim-to-real methods benefit from large-scale simulated data and can generalize across diverse objects, yet they remain limited by inaccuracies in modeling complex contact dynamics and sensing, a challenge that becomes more significant as task complexity increases\n[\nwang2024lessons\n,\nhuang2023dynamic\n]\n. Imitation learning benefits from multisensory real-world data, yet collecting high-quality dexterous demonstrations is considerably more difficult than collecting data for simpler end-effectors. Our work seeks to combine the strengths of both approaches. We use large-scale simulation to learn motion primitives while leveraging real-world data to close the dynamics and sensing gaps. Moreover, our skill-based framework enables efficient collection of dexterous real-world data by using the simulation-trained policy itself as a reusable skill primitive.\nIn the context of nut fastening and screwdriving, there has been recent work combining sim-to-real transfer with teleoperation. For example, Liu et al.\n[\nliu2025dexndm\n]\nbuild a residual model from real-world interactions to compensate for the sim-to-real gap and achieve robust in-hand manipulation. Yin et al.\n[\nyin2025dexteritygen\n]\nuse simulation-trained policies as stability controllers to enable complex manipulation skills. Both approaches can be integrated with teleoperated arm control to complete these tasks. However, they do not produce autonomous policies that incorporate tactile sensing. Another line of work is Kumar et al.\n[\nkumar2025diffusion\n]\n, who demonstrate screwdriver turning by combining learning with trajectory optimization. Noseworthy et al.\n[\nnoseworthy2025forge\n]\npresent an autonomous sim-to-real policy but only show results with a parallel-jaw gripper and do not demonstrate regrasping.\nAnother way to address the sim-to-real gap is to refine the policies in the real world. For example, Transic\n[\njiang2024transic\n]\nshows that sim-to-real policies can adapt to complex real-world dynamics with only a few human interventions as demonstrations, although the demonstrations are primarily performed with simple end-effectors. In contrast, we apply this idea to dexterous hands. Maddukuri et al.\n[\nmaddukuri2025sim\n]\nshow that co-training with both simulation and real-world data can reduce the gap and improve manipulation performance.\nFigure 1:\nAn overview of our approach.\nWe first train a reinforcement learning policy in simulation using a simplified object model, which serves as a motion prior for nut-bolt fastening and screwdriving. We then collect real-world trajectories by using the learned policy as a skill primitive during teleoperation. Finally, we train a behavior cloning policy on the collected data to obtain coordinated behavior between the arm and the fingers.\nIII\nDexterity from Imperfect Simulation\nAn overview of our method is shown in Figure\n1\n. It consists of three stages. First, we train a reinforcement learning (RL) policy in simulation using a simplified object model (Section\nIII-A\n). The resulting policy learns the desired finger motions but does not experience real-world dynamics and lacks tactile feedback. To address this, we collect real-world trajectories using the learned policy as a skill primitive for teleoperation (Section\nIII-B\n). Finally, using this dataset, we train a new multisensory policy using behavior cloning (Section\nIII-C\n).\nIII-A\nTraining a Reinforcement Learning Policy in Simulation\nSimplified Object Modeling.\nOur goal is to design a simulation environment that enables fast training and encourages the emergence of desired finger gaits for rotation. To achieve this, we construct a simplified simulated object (Figure\n2\n) that captures the essence of rotational motion. The object consists of a fixed cylindrical base with a nut or handle attached via a revolute joint. This setup allows the policy to learn rotational motion efficiently without relying on expensive contact-rich simulations. A similar idea was explored in\n[\nlin2024twisting\n]\nto model bottle caps using a heuristic friction design. In contrast, we further simplify the model, since we can leverage real-world data to compensate for the resulting dynamics mismatch.\nSpecifically, for the nut-bolt task, we use a thick triangular shape as the training object (Figure\n2\nA). The extra thickness is used to prevent the policy from learning suboptimal strategies that apply a large force from the bottom. The learned policy also discovers a high-clearance gait that transfers well to diverse real-world nuts such as hexagonal and cube-shaped nuts. For the screwdriver task, where the primary difficulty arises from slippage around the handle, we use spherical primitives to keep the learned behavior conservative (Figure\n2\nB). This observation, that different training shapes lead to different rotational gaits, is also consistent with the findings in\n[\nqi2022hand\n]\n. Note that these objects do not need to be visually aligned with real world objects, as they are only used to learn the coarse motions used for real world data collection, as we discussed in Section\nIII-B\n.\nFigure 2:\nSimplified Object Models.\nEach nut or handle is modeled as a rigid body attached to a fixed base through a revolute joint. This abstraction ignores thread-level mechanics while retaining the essential rotational dynamics needed for learning.\nTraining Pipeline.\nFollowing\n[\nqi2023general\n,\nwang2024lessons\n]\n, we first train an oracle policy and then distill it into a sensorimotor policy. The oracle policy\nùíá\n\\bm{f}\nis trained with access to an embedding of privileged information\n[\nchen2020learning\n]\nùíõ\nt\n\\bm{z}_{t}\n. The sensorimotor policy operates without privileged sensing and instead conditions on a predicted embedding\nùíõ\n^\nt\n=\nœï\n‚Äã\n(\nùíâ\nt\n)\n\\hat{\\bm{z}}_{t}=\\bm{\\phi}(\\bm{h}_{t})\ninferred from proprioceptive history\nùíâ\nt\n\\bm{h}_{t}\nby a prediction module\nœï\n\\bm{\\phi}\n.\nPrivileged Information.\nThe oracle policy has access to ground-truth environment and object properties, including object attributes (e.g., position, scale, mass, center of mass, friction coefficients), hand pose and finger configurations, and low-level controller parameters. The full set of privileged inputs is documented in the appendix.\nActions.\nAt each step, the policy outputs a relative target position. The position command is computed as\nùíÇ\nt\nHand\n=\nŒ∑\n‚Äã\nùíá\n‚Äã\n(\nùíê\nRL\n)\n+\nùíÇ\nt\n‚àí\n1\nHand\n\\bm{a}_{t}^{\\text{Hand}}=\\eta\\bm{f}(\\bm{o}^{\\text{RL}})+\\bm{a}_{t-1}^{\\text{Hand}}\n, where\nŒ∑\n\\eta\nis the action scale. This command is sent to the robot and converted into torque via a low-level PD controller. Here,\nùíê\nRL\n\\bm{o}^{\\text{RL}}\ncontains the robot‚Äôs proprioceptive state, including joint positions and previous target positions from a sliding window of recent 3 timesteps.\nReward.\nThe goal of the policy in simulation is to rotate the simplified object around the revolute joint. The reward consists of a task reward, energy penalties, and stability penalties (time index\nt\nt\nomitted for simplicity). Each component includes several terms defined in the appendix:\nr\nt\n=\nŒª\ntask\n‚Äã\nr\ntask\n+\nŒª\nenergy\n‚Äã\nr\nt\nenergy\n+\nŒª\nstability\n‚Äã\nr\nt\nstability\n.\nr_{t}=\\lambda_{\\text{task}}r^{\\text{task}}+\\lambda_{\\text{energy}}r^{\\text{energy}}_{t}+\\lambda_{\\text{stability}}r^{\\text{stability}}_{t}.\nOracle Policy Training.\nWe train the oracle policy using proximal policy optimization (PPO)\n[\nschulman2017proximal\n]\nwith the reward described above. The robot state and privileged information are each encoded with separate MLPs. These embeddings are concatenated and passed through an MLP to produce the final action and value predictions. We train the policy for 1.5\n√ó\n\\times\n10\n9\n{}^{\\text{9}}\nenvironment steps.\nSensorimotor Policy Training.\nThe sensorimotor policy receives proprioceptive states and a latent code\nùíõ\n^\nt\n=\nœï\n‚Äã\n(\nùíâ\nt\n)\n\\hat{\\bm{z}}_{t}=\\bm{\\phi}(\\bm{h}_{t})\ninferred from a 30-timestep history. We train the policy using DAgger\n[\nross2011reduction\n]\n: at each step, the sensorimotor policy acts in the environment, while the oracle policy provides target actions and ground-truth privileged embeddings. The training objective is\n‚Ñí\n=\n‚Äñ\na\nt\nHand\n‚àí\na\n^\nt\nHand\n‚Äñ\n2\n2\n+\n‚Äñ\nz\nt\n‚àí\nz\n^\nt\n‚Äñ\n2\n2\n,\n\\mathcal{L}=\\|a^{\\text{Hand}}_{t}-\\hat{a}^{\\text{Hand}}_{t}\\|_{2}^{2}+\\|z_{t}-\\hat{z}_{t}\\|_{2}^{2},\nwhere\na\n^\nt\nHand\n\\hat{a}^{\\text{Hand}}_{t}\ndenotes the actions produced by the sensorimotor policy. The embedding predictor\nœï\n\\phi\nis optimized using Adam\n[\nkingma2014adam\n]\nuntil convergence.\nRandomization.\nWe apply domain randomization during training to improve the robustness of the RL policy\n[\npeng2018sim\n]\n. Following\n[\nwang2024lessons\n]\n, we randomize the nut/handle mass, center of mass, friction coefficient, size, and PD gains, and we also add observation noise. Detailed parameters are provided in the appendix.\nTermination Conditions.\nTo prevent the policy from getting stuck in unrecoverable states, we terminate an episode when any of the following conditions are met: (1) the distance between the thumb or index finger and the nut/handle exceeds a reset threshold (\n7\ncm\n7\\text{\\,}\\mathrm{cm}\nfor nuts,\n10\ncm\n10\\text{\\,}\\mathrm{cm}\nfor handles); (2) the nut or handle remains stagnant over a sliding time window (\n3.5\ns\n3.5\\text{\\,}\\mathrm{s}\nfor nuts,\n3\ns\n3\\text{\\,}\\mathrm{s}\nfor handles); or (3) near-zero contact forces are detected for the same duration. These conditions accelerate training by penalizing failure modes such as drifting away from the object or failing to maintain contact.\nFigure 3:\nTeleoperation Interface.\nThe human operator controls the wrist position using the VR controller buttons and adjusts yaw and pitch through the joystick. This setup allows the operator to guide the arm motion while relying on the learned finger-rotation skill during data collection.\nIII-B\nReal World Data Collection with Learned Policy\nThe policy trained in simulation with the simplified object model learns the desired rotational behavior; however, it inevitably misses key physical dynamics such as thread interactions. These effects are difficult to model but are crucial for reliable real-world fastening. It also lacks tactile information, which is hard to simulate but crucial for fine-grained wrist adjustments.\nTo bridge this gap, we introduce a skill-based\nassisted teleoperation\nfor real-world data collection. The core idea is to reuse the simulation-trained policy as a skill primitive for finger motion control. Instead of commanding every joint individually, the human operator controls only the wrist movement and decides when to activate the skill primitive (Figure\n3\n). Wrist position is specified using the Quest VR\n1\n1\n1\nhttps://www.meta.com/quest/products/quest-2\ncontroller‚Äôs joystick. This approach is inspired by\n[\nlin2024learning\n]\n, but we use a much finer-grained skill for data collection.\nThis framework offers several advantages. First, it delegates complex finger motions to a robust simulation-trained policy that generalizes across different objects, eliminating the need for humans to learn finger coordination under morphological differences. Second, using a joystick for arm control enables precise and intuitive wrist positioning. Finally, collecting data in the real world provides the multisensory observations necessary for these tasks.\nConcretely, at each timestep we record two actions: (1) the action generated by the RL policy\nœÄ\nRL\n\\pi_{\\rm RL}\n, which defaults to the current hand joint positions when the policy is not activated, and (2) the arm action generated by human teleoperation. Formally, we define\nùíÇ\nt\n=\n[\nùíÇ\nt\nHand\n,\nùíÇ\nt\nArm\n]\n\\bm{a}_{t}=[\\bm{a}_{t}^{\\text{Hand}},\\bm{a}_{t}^{\\text{Arm}}]\n, where\nùíÇ\nt\nHand\n‚àà\n‚Ñù\n12\n\\bm{a}_{t}^{\\text{Hand}}\\in\\mathbb{R}^{12}\ndenotes the hand target joint positions, and\nùíÇ\nt\nArm\n‚àà\n‚Ñù\n6\n\\bm{a}_{t}^{\\text{Arm}}\\in\\mathbb{R}^{6}\ndenotes the arm joint positions. We also record the multisensory observation\n(\nùíí\nt\n,\nùíÑ\nt\n)\n\\big(\\bm{q}_{t},\\bm{c}_{t}\\big)\n, where\nùíí\nt\n=\n[\nùíí\nt\nHand\n,\nùíí\nt\nArm\n]\n\\bm{q}_{t}=[\\bm{q}_{t}^{\\text{Hand}},\\bm{q}_{t}^{\\text{Arm}}]\ncontains all joint positions, and\nùíÑ\nt\n\\bm{c}_{t}\nrepresents the raw tactile signals from all five fingers.\nTactile Signal.\nIn this work, we use the XHand‚Äôs built-in tactile sensors to capture contact information. Each fingertip is equipped with a pressure-based tactile array comprising 120 sensing elements, each measuring three-axis forces with a minimum detectable force of\n0.05\nN\n0.05\\text{\\,}\\mathrm{N}\n. At each timestep, we record the tactile signal as\nùêú\nt\n‚àà\n‚Ñù\n5\n√ó\n120\n√ó\n3\n\\mathbf{c}_{t}\\in\\mathbb{R}^{5\\times 120\\times 3}\n, which includes the signals from all five fingers across three axes.\nIII-C\nBehavior Cloning with Multisensory Data\nWith the dataset\nùíü\nReal\n\\mathcal{D}_{\\text{Real}}\ncollected using the skill-based\nassisted teleoperation\n, we can train a behavior cloning (BC) policy\nœÄ\nBC\n\\pi_{\\text{BC}}\nusing the paired multisensory observations and expert actions. Vision is not used in our work.\nNeural Network Architecture.\nWe use a feedforward network as the policy. The past\nK\nK\ntimesteps of observations\n(\nùíí\nt\n‚àí\nK\n+\n1\n:\nt\n,\nùíÑ\nt\n‚àí\nK\n+\n1\n:\nt\n)\n(\\bm{q}_{t-K+1:t},\\bm{c}_{t-K+1:t})\nare concatenated into a single feature vector. Tactile signals are first flattened and passed through an MLP. The fused feature vector is then processed by an hourglass encoder\n[\nnewell2016stacked\n]\n, which outputs the action predictions.\nWe also apply an action chunking strategy\n[\nzhao2023learning\n,\nchi2025diffusion\n]\n, where the policy predicts a sequence of future actions\nùíÇ\n^\nt\n:\nt\n+\nH\n\\hat{\\bm{a}}_{t:t+H}\nrather than a single-step action. We use\nK\n=\n5\nK=5\nand\nH\n=\n16\nH=16\nunless otherwise noted.\nTABLE I:\nReal-world fastening performance on square, triangular, hexagonal, and cross-shaped nuts. We report progress ratio and rotation time (mean ¬± standard deviation over 10 trials) for different observation modalities. Tactile sensing and temporal history both improve performance, and their combination yields the highest accuracy and fastest execution. * indicates that only one completely successful trial was recorded, so no standard deviation is reported.\nSquare Nuts\nTriangular Nuts\nHexagonal Nuts\nCross-Shaped Nuts\nTactile\nHist.\nProg. Ratio (%)\n‚Üë\n\\uparrow\nTime (s)\n‚Üì\n\\downarrow\nProg. Ratio (%)\n‚Üë\n\\uparrow\nTime (s)\n‚Üì\n\\downarrow\nProg. Ratio (%)\n‚Üë\n\\uparrow\nTime (s)\n‚Üì\n\\downarrow\nProg. Ratio (%)\n‚Üë\n\\uparrow\nTime (s)\n‚Üì\n\\downarrow\n63.75\n¬±\n\\pm\n33.05\n148.76\n¬±\n\\pm\n30.57\n30.00\n¬±\n\\pm\n39.62\n229.39*\n75.00\n¬±\n\\pm\n29.46\n102.36\n¬±\n\\pm\n25.00\n63.75\n¬±\n\\pm\n33.05\n101.07\n¬±\n\\pm\n0.51\n‚úì\n62.50\n¬±\n\\pm\n33.85\n202.21\n¬±\n\\pm\n59.69\n66.25\n¬±\n\\pm\n27.67\n134.19\n¬±\n\\pm\n46.72\n75.00\n¬±\n\\pm\n16.67\n205.02\n¬±\n\\pm\n82.44\n82.50\n¬±\n\\pm\n10.54\n127.17\n¬±\n\\pm\n47.98\n‚úì\n87.50\n¬±\n\\pm\n20.41\n129.88\n¬±\n\\pm\n48.06\n80.00\n¬±\n\\pm\n32.91\n111.47\n¬±\n\\pm\n55.10\n85.00\n¬±\n\\pm\n32.17\n68.97\n¬±\n\\pm\n13.99\n100.00\n¬±\n\\pm\n00.00\n91.49\n¬±\n\\pm\n44.89\n‚úì\n‚úì\n97.50\n¬±\n\\pm\n7.91\n124.20\n¬±\n\\pm\n33.22\n96.25\n¬±\n\\pm\n8.44\n117.79\n¬±\n\\pm\n52.13\n95.00\n¬±\n\\pm\n10.54\n75.07\n¬±\n\\pm\n17.41\n98.75\n¬±\n\\pm\n3.95\n84.21\n¬±\n\\pm\n57.19\nTraining.\nThe policy is trained with supervised learning using the loss\n‚Ñí\nBC\n=\n‚àë\nt\n=\n1\nT\n‚àë\nh\n=\n0\nH\n‚Äñ\nùíÇ\n^\nt\n+\nh\n‚àí\nùíÇ\nt\n+\nh\n‚Äñ\n2\n2\n,\n\\mathcal{L}_{\\text{BC}}=\\sum_{t=1}^{T}\\sum_{h=0}^{H}\\left\\|\\,\\hat{\\bm{a}}_{t+h}-\\bm{a}_{t+h}\\,\\right\\|_{2}^{2},\nwhere\nùíÇ\n^\nt\n:\nt\n+\nH\n\\hat{\\bm{a}}_{t:t+H}\nis the action chunk predicted by\nœÄ\nBC\n\\pi_{\\text{BC}}\n, and\nùíÇ\nt\n:\nt\n+\nH\n\\bm{a}_{t:t+H}\nis the corresponding expert sequence. This objective encourages consistent predictions over the full horizon.\nWe train the policy using Adam\n[\nkingma2014adam\n]\nfor 200 epochs and normalize observations following\n[\nbarreiros2025careful\n]\n.\nIV\nExperiments\nIn this section, we first introduce the experiment setup (Section\nIV-A\n). We then evaluate the performance of our policies on two challenging tasks, nut-bolt fastening (Section\nIV-B\n) and screwdriving (Section\nIV-C\n). We conclude with qualitative experiments that provide additional analysis and design ablations in simulation.\nIV-A\nExperiment Setup\nHardware Setup.\nOur system consists of a UR5e robot arm (6 DoF) and a 12-DoF XHand. The XHand has five fingers: the thumb and index finger each have 3 DoF, and the remaining fingers have 2 DoF. Only the thumb and index provide abduction/adduction.\nSimulation.\nWe train our policies in IsaacGym\n[\nmakoviychuk2021isaac\n]\nusing 8,192 parallel environments. Each environment contains a simulated XHand and the simplified object model described in Section\nIII-A\n. The simulation runs at\n200\nHz\n200\\text{\\,}\\mathrm{Hz}\n, with control applied at\n20\nHz\n20\\text{\\,}\\mathrm{Hz}\n. Each episode lasts up to 800 simulation steps (\n40\ns\n40\\text{\\,}\\mathrm{s}\n).\nObject Set.\nFor the nut-bolt task, we train on triangular nuts. For the screwdriver task, we approximate handles by octagon- and dodecagon-shaped nuts. This multi-geometry training in simulation helps the policy generalize to diverse real-world shapes.\nMetrics.\nIn simulation, we report the episode reward and episode length during training. In real-world evaluation, we measure the\nprogress ratio\n, defined as the number of successful rotations divided by the total number of rotations required for full fastening. We also report the\ntime\nfor trials that fully complete the fastening process (progress ratio = 100), defined as the time needed to fully fasten the nut or fully tighten the screw. Note that some baseline methods do not achieve a single successful fastening or screwing attempt across ten trials. In such cases, we cannot report the standard deviation or the completion-time metric.\nIV-B\nNut-Bolt Experiments\nWe first evaluate the system on the nut-bolt task. This task requires the fingers to establish correct contact patterns, sense progress through tactile feedback, and adjust the arm position accordingly. We choose this task because nut-bolt interactions are difficult to simulate efficiently, and completing the task relies heavily on tactile sensing, making it a strong testbed for our method.\nWe note that direct sim-to-real transfer can rotate the nut, but it cannot drive the nut downward because the arm does not move. Since thread interactions are not simulated, the nut remains at the same height even after completing full revolutions in simulation.\nSetting.\nIn simulation, we use the thick triangular nut shown in Figure\n2\n(left). Training with this shape produces high-clearance gaits that transfer well and can rotate both square and triangular nuts in the real world.\nDuring skill-based assisted teleoperation, we collect 50 trajectories each for the square and triangular nuts. Each trajectory lasts about 80 seconds. We then train a behavior cloning policy using the combined dataset. We evaluate performance on four types of nuts, which include square and triangular nuts as well as two unseen shapes, namely hexagonal nuts and cross-shaped nuts. Our main results are shown in Table\nI\n.\nObservation History.\nWe first study the effect of providing a short temporal history in the observation. Adding history significantly improves progress ratio and reduces execution time across all modalities and nut geometries. Temporal cues help the policy track fine-grained rotational progress and distinguish local geometric features. The benefit is especially clear for non-tactile policies, where history stabilizes performance and narrows much of the gap to tactile-based policies on easier geometries. When combined with tactile sensing, history yields the strongest overall performance, achieving the highest accuracy and fastest completion times across all nut types, including unseen shapes.\nTactile Information.\nAcross all nut geometries, adding tactile input generally improves progress ratio. The effect is most pronounced on challenging shapes such as triangular and cross-shaped nuts, where progress ratios rise from roughly 30 to 65 percent for triangular nuts and from about 60 to 80 percent for cross-shaped nuts. This underscores the importance of tactile feedback for maintaining stable contact and detecting effective rotation. We also observe that certain non-tactile settings achieve high performance, even on unseen shapes, particularly on well-constrained geometries such as hexagonal and cross-shaped nuts. In these cases, the geometry provides strong passive guidance, and the finger gait learned in simulation is often sufficient to maintain contact without relying on tactile cues.\nFailure Modes.\nWe observe two main failure modes. First, the policy without observation history struggles to infer object shape from single-step proprioception. As a result, it fails to generalize across nut geometries, since different nuts require different rotational gaits. The policy cannot adjust its gait because the limited sensing information prevents it from identifying the correct motion pattern. Second, non-tactile policies frequently drift into unstable contact states and lose alignment with the bolt. Once misalignment occurs, the nut cannot sustain continuous rotation. In contrast, tactile policies can recover by adjusting wrist orientation or applying corrective downward force to re-establish contact.\nIV-C\nScrewdriving Experiments\nTABLE II:\nReal-world screwdriving performance\n. We report progress ratio and rotation time (mean ¬± standard deviation over 10 trials) for direct sim-to-real, expert replay, and our behavior cloning (BC) ablations. Tactile sensing and temporal history each improve performance, and their combination achieves the highest progress ratio and fastest execution. Here * indicates that the policy never fully completed the task, so no rotation time is reported.\nScrewdriver Task\nMethod\nTactile\nHist.\nProg. Ratio (%)\n‚Üë\n\\uparrow\nTime (s)\n‚Üì\n\\downarrow\nDirect Sim2Real\n41.60\n¬±\n\\pm\n26.21\nN.A.*\nExpert Replay\n50.80\n¬±\n\\pm\n19.27\nN.A.*\nOurs\n69.20\n¬±\n\\pm\n35.25\n266.33\n¬±\n\\pm\n91.60\n‚úì\n67.63\n¬±\n\\pm\n35.65\n264.06\n¬±\n\\pm\n76.57\n‚úì\n87.50\n¬±\n\\pm\n18.61\n195.15\n¬±\n\\pm\n44.83\n‚úì\n‚úì\n95.00\n¬±\n\\pm\n13.24\n187.87\n¬±\n\\pm\n24.87\nWe also demonstrate that our method extends to the more challenging screwdriving task. Compared to nut-bolt fastening, screwdriving is inherently less stable: the shaft is not kinematically constrained along the screw axis, and even small tilts or misalignments can lead to slipping or loss of contact. As a result, the task requires more fine-grained control to maintain continuous rotation. The complex interaction between the screwdriver and the screw is also difficult to simulate accurately, which is why we include this task in our study.\nSetting.\nIn simulation, we use a mix of octagonal and dodecagonal handles as the training objects, as shown in Figure\n2\n(right). This curated set encourages the learned rotational gaits to remain conservative in clearance and maintain stability. In the real world, we collect 72 trajectories, each lasting between 120 and 180 seconds.\nSim-to-Real Policy.\nUnlike the nut-bolt experiments, the screwdriving task can still make progress even when the wrist does not move, since downward motion is not required. We first evaluate a direct sim-to-real policy that uses only proprioception. The results are shown in Table\nII\n. The direct sim-to-real policy achieves a 41.60% progress ratio, indicating that it can produce meaningful behavior, which is a prerequisite for collecting expert trajectories. However, because it inevitably makes mistakes, it never completes the task even once, and therefore we cannot report statistics for completion time. In contrast, during data collection, the human operator can adjust the wrist position to recover from such mistakes.\nAs a result, when we replay the expert data from the data we collected, it can achieve higher success rate to be 50.80%. However, because it cannot adapt changes during deployment. It also fails at completely finishing the task. Next, we show the results of our policies.\nFigure 4:\nTop\n: The policy with tactile information maintains a consistent alternating pattern of thumb and index finger contact, which supports stable engagement as the nut is rotated downward.\nBottom\n: The policy without tactile information does not maintain a clear contact pattern. This leads to unsuccessful engagement and prevents proper downward wrist motion. The resulting pattern reflects the index finger pressing against the bolt after losing stable contact.\nMain Results.\nOur behavioral cloning policies show clear improvements over the direct sim-to-real and expert-replay baselines. Adding tactile sensing or temporal history individually improves progress ratio. Combining both modalities gives the strongest performance.\nThe baseline behavior cloning model already achieves a 69.20% progress ratio, which substantially outperforms expert replay. This phenomenon, where a behavior cloning policy surpasses the policy that generated the data, is consistent with filtered behavior cloning\n[\nkumar2022should\n]\n. In this approach, only successful trials are used for training. A similar effect has also been reported in\n[\nwang2024lessons\n]\n.\nUsing history alone produces a comparable improvement of 67.63%. This indicates that temporal information helps the policy track rotational progress and recover from partial failures. When history is not used, tactile information provides only limited benefit. However, once history is included, the two modalities become complementary. The progress ratio increases to 95.00%, and the average rotation time decreases substantially. These results show that tactile feedback and temporal history work together to produce stable, consistent, and efficient screwdriving behaviors.\nFailure Modes.\nWe observe that open-loop baselines frequently fail due to gradual handle slipping and accumulated orientation drift. Without feedback, small misalignments grow over time. Among behavioral cloning methods, the policy trained without observation history struggles to stabilize the screwdriver since it lacks the temporal information needed to infer handle pose and orientation. Non-tactile policies fail to detect subtle torque imbalances and often lose stable contact under slight perturbations. With both tactile feedback and temporal history, BC with tactile and history compensates for these effects by adjusting wrist orientation and applying appropriate forces.\nIV-D\nQualitative Experiments\nFigure 5:\nTop row:\nThe policy recovers back to the nut-bolt fastening motion when the fingers are dragged by an external force.\nBottom row:\nThe policy recovers back to the screwdriving motion when the screwdriver is rotated counterclockwise during the clockwise rotation by the policy.\nOut-of-distribution Robustness.\nWe study the robustness of the learned policy under external perturbations that are not encountered during training. These disturbances include (1) dragging the fingers away from the object or rotating the screwdriver in the opposite direction of the intended direction. In Figure\n5\n, we show that despite these perturbations, the policy consistently recovers to stable fastening behavior. Specifically, it recovers from reverse rotation of the fastening object by re-establishing contact and restoring the correct rotational direction. When finger contact is disrupted or temporarily blocked, the policy repositions the fingers and wrist to regain stable engagement.\nTactile Visualization.\nTactile signals provide structured spatiotemporal patterns that the policy uses to infer contact phases. As shown in Fig.\n4\n, stable activation signatures emerge when the nut or screwdriver handle is correctly engaged. The policy learns to preserve these patterns by adjusting wrist orientation and contact force, effectively using tactile feedback as a local reference for alignment. When these patterns deviate, corrective actions such as re-engagement or downward pressing are triggered.\nIV-E\nSimulation Ablations\nIn simulation, we verify the design choices used to train the screwdriving policies. We evaluate our two-stage training procedure against two alternatives: training without any privileged information, and using an asymmetric actor-critic\n[\npinto2017asymmetric\n]\narchitecture where the critic has access to full tactile information while the actor does not. The actor in this setting is directly deployable in the real world. The results are shown in Figure\n6\n.\nWe compare episode reward and episode length across these training strategies. When both the actor and critic have access to privileged information, our method (Ours, Oracle) achieves the highest performance. Removing privileged information from the actor, as in the asymmetric actor-critic variant, leads to a noticeable drop in performance. Removing privileged information entirely results in a further decline. These results show that privileged information plays an important role during policy learning. We also observe that a sensorimotor policy can approach similar performance when proprioceptive history is provided as input. We also experimented with training the asymmetric and non-privileged models for longer horizons but have not observed further improvement. This suggests that the performance gap is not due to insufficient training but instead reflects the importance of privileged information during learning.\nFigure 6:\nSimulation ablations of screwdriving policy training. We compare our privileged-information oracle policy, its sensorimotor policy, an asymmetric actor‚Äìcritic variant, and a policy trained without privileged information. Providing privileged information during training leads to significantly higher reward and more stable episode lengths. Each curve shows the mean and standard deviation over 5 seeds.\nV\nConclusion and Future Work\nWe present a framework for learning dexterous manipulation skills for contact-rich tasks using imperfect simulation. The approach first learns transferable rotational skills through reinforcement learning with simplified object modeling. It then uses these skills for skill-based teleoperation to collect real-world trajectories, and finally incorporates tactile feedback and learns a sensorimotor policy through behavior cloning. Experiments on nut-bolt fastening and screwdriver usage show that simulation alone cannot capture the complex dynamics required for reliable task execution. However, when behavior cloning is combined with tactile sensing and temporal history, the resulting policies become robust and reliable across diverse and unseen object geometries. This staged pipeline provides a practical and scalable solution for contact-rich manipulation and highlights the value of tactile sensing and skill-based teleoperation as effective bridges between simulation and real-world deployment.\nLimitations.\nAlthough our skill-based teleoperation reduces the burden on the human operator, it remains a constraint for scalable data acquisition. Fully autonomous data collection or learning skill-level guidance from human videos would further improve efficiency. In our current tasks, the nut is already installed and the screwdriver is already inserted. Extending the approach to fully long-horizon assembly will require vision sensing and possibly high-accuracy force-torque sensing. A broader evaluation across more contact-rich manipulation tasks is also needed to assess generality.\nAcknowledgment\nThis work is supported in part by the program ‚ÄúDesign of Robustly Implementable Autonomous and Intelligent Machines (TIAMAT)‚Äù, Defense Advanced Research Projects Agency award number HR00112490425. We thank Mengda Xu for his valuable feedback.\nAppendix\nV-A\nPrivileged Information for Oracle Policy\nNut-Bolt Task.\nThe oracle policy receives privileged information about object properties, fingertip states, nut-specific dynamics, hand states, and PD controller gains. The full list of privileged inputs is provided in Table\nIII\n.\nScrewdriver Task.\nThe screwdriver task uses a subset of the privileged information from the nut-bolt task. Specifically, it excludes hand base position, hand orientation, hand joint positions, and PD controller gains. All other privileged inputs remain the same.\nV-B\nReward\nOur reward function is a weighted combination of task rewards, energy penalties, and stability penalties:\n‚Ä¢\nTask Rewards\nencourage successful rotation:\n‚àò\n\\circ\nRotation Rewards:\nr\nt\nrot\n=\nclip\n‚Äã\n(\nœâ\nt\n,\nœâ\nmin\n,\nœâ\nmax\n)\nr^{\\text{rot}}_{t}=\\text{clip}(\\omega_{t},\\omega_{\\min},\\omega_{\\max})\n. It encourages positive angular velocity\nœâ\n\\omega\nalong the fastening axis, clipped to\n[\n‚àí\n4.0\n,\n4.0\n]\n[-4.0,4.0]\nrad/s.\n‚àò\n\\circ\nProximity Rewards:\nr\nt\nprox\n=\nmax\n‚Å°\n(\n0\n,\n1\n‚àí\nd\nt\n/\nd\nthresh\n)\nr^{\\text{prox}}_{t}=\\max(0,\\,1-d_{t}/d_{\\text{thresh}})\n. It encourages fingers to remain close to the object, where\nd\nt\nd_{t}\nis the mean distance from thumb and index finger to the nut/handle.\n‚Ä¢\nEnergy Penalties\ndiscourage inefficient motions:\n‚àò\n\\circ\nTorque Penalty:\nr\nt\ntorque\n=\n‚àí\n‚Äñ\nœÑ\nt\n‚Äñ\n2\nr^{\\text{torque}}_{t}=-\\|\\tau_{t}\\|^{2}\npenalizes large joint torques.\n‚àò\n\\circ\nWork Penalty:\nr\nt\nwork\n=\n‚àí\n(\n|\nœÑ\nt\n|\n‚ä§\n‚Äã\n|\nq\nÀô\nt\n|\n)\n2\nr^{\\text{work}}_{t}=-(|\\tau_{t}|^{\\top}|\\dot{q}_{t}|)^{2}\npenalizes excessive joint power.\n‚Ä¢\nStability penalties\nmaintain stable behavior:\n‚àò\n\\circ\nPose Difference Penalty:\nr\nt\npose\n=\n‚àí\n‚Äñ\nq\nt\n‚àí\nq\n0\n‚Äñ\n2\nr^{\\text{pose}}_{t}=-\\|q_{t}-q^{0}\\|^{2}\npenalizes deviations from the initial finger configuration (thumb joints masked).\n‚àò\n\\circ\nLarge Rotation Penalty:\nr\nt\nrp\n=\n‚àí\nmax\n‚Å°\n(\n0\n,\nœâ\nt\n‚àí\nœâ\nthresh\n)\nr^{\\text{rp}}_{t}=-\\max(0,\\omega_{t}-\\omega_{\\text{thresh}})\npenalizes excessive angular velocity above threshold\nœâ\nthresh\n\\omega_{\\text{thresh}}\n(10.0 rad/s for nut-bolt, curriculum from 7.5 to 15.0 rad/s for screwdriver).\nWe sum the above rewards with weights listed in Table\nIV\n.\nTABLE III:\nPrivileged information for nut-bolt task. The screwdriver task uses a subset of these features (excludes hand pose and PD gains).\nPrivileged Information\nDimension\nObject position\n3\nObject scale\n1\nObject mass\n1\nObject friction coefficient\n1\nObject center of mass\n3\nObject orientation (quaternion)\n4\nObject linear velocity\n3\nObject angular velocity\n3\nObject restitution\n1\nFingertip positions (2 fingers)\n6\nFingertip orientations (2 fingers)\n8\nFingertip linear velocities (2 fingers)\n6\nFingertip angular velocities (2 fingers)\n6\nNut contact indicator\n1\nNut position\n3\nNut joint velocity\n1\nNut joint position\n1\nScrew joint friction\n1\nHand scale\n1\nHand position\n3\nHand orientation (quaternion)\n4\nHand joint positions\n12\nPD controller gains (\nk\np\nk_{p}\n)\n12\nPD controller gains (\nk\nd\nk_{d}\n)\n12\nTotal\n97\nTABLE IV:\nReward function hyper-parameters for nut-bolt and screwdriver tasks.\nReward Component\nnut-bolt\nScrewdriver\nTask Rewards\nŒª\nrotate\n\\lambda_{\\text{rotate}}\n6.0\n2.5\nŒª\nproximity\n\\lambda_{\\text{proximity}}\n2.0\n2.0\nEnergy Penalties\nŒª\ntorque\n\\lambda_{\\text{torque}}\n-0.1\n-3.0\nŒª\nwork\n\\lambda_{\\text{work}}\n-0.01\n-0.01\nStability Penalties\nŒª\npose\n\\lambda_{\\text{pose}}\n-0.5\n-0.1\nŒª\nrotate-penalty\n\\lambda_{\\text{rotate-penalty}}\n-0.3\n-0.3\nŒª\npc-z\n\\lambda_{\\text{pc-z}}\n-1.0\n-1.0\nTABLE V:\nDomain Randomization Parameters.\nObject scale is discretely sampled from the specified set and multiplied by the base scale. Mass, center of mass, friction, restitution, and PD controller gains are uniformly sampled at environment initialization. Observation and action noise are sampled i.i.d. from Gaussian distributions at each timestep. Following\n[\nopenai2018learning\n]\n, we apply a random disturbance force with magnitude\n2.0\n‚Äã\nm\n2.0m\n(where\nm\nm\nis object mass) with probability 0.25 at each timestep.\nParameter\nRange\nObject Scale (nut-bolt)\n√ó\n\\times\n[0.95, 1.05]\nObject Scale (Screwdriver)\n√ó\n\\times\n[0.85, 1.25]\nMass\n[0.04, 0.06]\nkg\n\\text{\\,}\\mathrm{kg}\nCenter of Mass\n[-0.001, 0.001]\nm\n\\text{\\,}\\mathrm{m}\nCoefficient of Friction\n[0.5, 8.0]\nObject Restitution\n[0.0, 1.0]\nPD Controller Stiffness (\nk\np\nk_{p}\n)\n[2.7, 3.3]\nPD Controller Damping (\nk\nd\nk_{d}\n)\n[0.009, 0.011]\nObservation Noise (rotation)\nùí©\n‚Äã\n(\n0\n,\n0.01\n)\n\\mathcal{N}(0,0.01)\n(rad)\nObservation Noise (translation)\nùí©\n‚Äã\n(\n0\n,\n0.005\n)\n\\mathcal{N}(0,0.005)\n(m)\nAction Noise (rotation)\nùí©\n‚Äã\n(\n0\n,\n0.01\n)\n\\mathcal{N}(0,0.01)\n(rad)\nAction Noise (translation)\nùí©\n‚Äã\n(\n0\n,\n0.005\n)\n\\mathcal{N}(0,0.005)\n(m)\nExternal Force Scale\n2.0\n‚Äã\nm\n2.0m\nExternal Force Probability\n0.25 per timestep\nV-C\nTraining Hyperparameters\nThe inputs to our oracle policy contains the proprioceptive observations consist of\nq\nt\nq_{t}\n(joint positions over the last 3 timesteps) and\na\nt\n‚àí\n1\na_{t-1}\n(previous joint targets over the last 3 timesteps). The privileged information includes\np\nt\np_{t}\n(5 fingertip positions),\nw\nt\nw_{t}\n(object state with 3D position, quaternion orientation, and angular velocity), and additional features detailed in Table\nIII\n. We follow the domain randomization parameters in Table\nV\n.\nWe train our oracle policy with PPO, and the training hyperparameters are shown in Table\nVI\n. Specifically, we train with 8,192 parallel environments. Each environment gathers 12 steps of data to train in each epoch of PPO. The data is split into minibatches of size 16,384 and optimized with PPO loss.\nŒ≥\n\\gamma\nand\nŒª\n\\lambda\nare used for computing generalized advantage estimate (GAE) returns. We use the Adam optimizer to train PPO and adopt gradient clipping to stabilize training. We train 1.5 billion environment steps in total, which takes less than one day on a single GPU. We train our sensorimotor policy with on-policy behavioral cloning, and the training hyperparameters are shown in Table\nVII\n.\nTABLE VI:\nHyperparameters for training the oracle policy.\nHyperparameter\nValue\n# environments\n8192\n# steps\n12\n# minibatch size\n16384\n# Environment steps\n3\n√ó\n\\times\n10\n9\ndiscount factor (\nŒ≥\n\\gamma\n)\n0.99\nGAE (\nŒª\n\\lambda\n)\n0.95\nlearning rate\n5e-3\nclip range\n0.2\nentropy coefficient\n0.0\nkl threshold\n0.02\nmax gradient norm\n1.0\nTABLE VII:\nHyperparameters for training the sensorimotor policy in simulation.\nHyperparameter\nValue\n# environments\n48\n# steps\n512\n# minibatches\n4096\nlearning rate\n1e-3",
  "preview_text": "Reinforcement learning and sim-to-real transfer have made significant progress in dexterous manipulation. However, progress remains limited by the difficulty of simulating complex contact dynamics and multisensory signals, especially tactile feedback. In this work, we propose \\ours, a sim-to-real framework that addresses these limitations and demonstrates its effectiveness on nut-bolt fastening and screwdriving with multi-fingered hands. The framework has three stages. First, we train reinforcement learning policies in simulation using simplified object models that lead to the emergence of correct finger gaits. We then use the learned policy as a skill primitive within a teleoperation system to collect real-world demonstrations that contain tactile and proprioceptive information. Finally, we train a behavior cloning policy that incorporates tactile sensing and show that it generalizes to nuts and screwdrivers with diverse geometries. Experiments across both tasks show high task progress ratios compared to direct sim-to-real transfer and robust performance even on unseen object shapes and under external perturbations. Videos and code are available on https://dexscrew.github.io.\n\nLearning Dexterous Manipulation Skills from Imperfect Simulations\nElvis Hsieh\n‚àó\n, Wen-Han Hsieh\n‚àó\n, Yen-Jen Wang\n‚àó\n, Toru Lin, Jitendra Malik, Koushil Sreenath\n‚Ä†\n, Haozhi Qi\n‚Ä†\nUC Berkeley\n‚àó\nEqual contribution (listed in alphabetical order).\n‚Ä†\nEqual advising.\nAbstract\nReinforcement learning and sim-to-real transfer have made significant progress in dexterous manipulation. However, progress remains limited by the difficulty of simulating complex contact dynamics and multisensory signals, especially tactile feedback. In this work, we propose DexScrew, a sim-to-real framework that addresses these limitations and demonstrates its effectiveness on nut-bolt fastening and screwdriving with multi-fingered hands. The framework has three stages. First, we train reinforcement learning policies in simulat",
  "is_relevant": false,
  "relevance_score": 1.0,
  "extracted_keywords": [
    "reinforcement learning",
    "sim-to-real transfer",
    "tactile sensing",
    "behavior cloning",
    "dexterous manipulation"
  ],
  "one_line_summary": "Êú¨ÊñáÊèêÂá∫‰∏ÄÁßç‰ªé‰∏çÂÆåÁæé‰ªøÁúü‰∏≠Â≠¶‰π†ÁÅµÂ∑ßÊìç‰ΩúÊäÄËÉΩÁöÑsim-to-realÊ°ÜÊû∂ÔºåÂ∫îÁî®‰∫éÂ§öÊåáÊâãÁöÑËû∫ÊØçÊãßÁ¥ßÂíåËû∫‰∏ùÊóãÂÖ•‰ªªÂä°„ÄÇ",
  "detailed_summary": "",
  "qa_pairs": [],
  "is_hidden": false,
  "is_starred": false,
  "published_date": "2025-12-01T18:59:45Z",
  "created_at": "2026-01-09T09:59:25.872598",
  "updated_at": "2026-01-09T09:59:25.872609"
}