{
    "id": "2601.23266v1",
    "title": "IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models",
    "authors": [
        "Seyed Ahmad Hosseini Miangoleh",
        "Amin Jalal Aghdasian",
        "Farzaneh Abdollahi"
    ],
    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£è‡ªé€‚åº”å‰ç»è§„åˆ’å™¨ï¼ˆIRL-DALï¼‰çš„æ–°å‹é€†å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹å§‹äºå¯¹ä¸“å®¶æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰æ§åˆ¶å™¨çš„æ¨¡ä»¿å­¦ä¹ ï¼Œä»¥æä¾›ç¨³å®šçš„åˆå§‹åŒ–ã€‚ç¯å¢ƒåé¦ˆé¡¹ä¸é€†å¼ºåŒ–å­¦ä¹ åˆ¤åˆ«å™¨ä¿¡å·ç›¸ç»“åˆï¼Œç¡®ä¿ä¸ä¸“å®¶ç›®æ ‡ä¿æŒä¸€è‡´ã€‚éšåï¼Œé€šè¿‡ç»“åˆæ‰©æ•£ç¯å¢ƒåé¦ˆä¸å®šå‘é€†å¼ºåŒ–å­¦ä¹ å¥–åŠ±çš„æ··åˆå¥–åŠ±å‡½æ•°è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚ä½œä¸ºå®‰å…¨ç›‘ç£å™¨çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹è´Ÿè´£è§„åˆ’å®‰å…¨è·¯å¾„ï¼Œç¡®ä¿è½¦è¾†ä¿æŒåœ¨è½¦é“å†…ã€è§„é¿éšœç¢ç‰©å¹¶å®ç°å¹³ç¨³è¡Œé©¶ã€‚æ­¤å¤–ï¼Œå¯å­¦ä¹ çš„è‡ªé€‚åº”æ©ç ï¼ˆLAMï¼‰é€šè¿‡æ ¹æ®è½¦é€ŸåŠå‘¨è¾¹é£é™©åŠ¨æ€è°ƒæ•´è§†è§‰æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆæå‡äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨åŸºäºFSMçš„æ¨¡ä»¿å­¦ä¹ åï¼Œé‡‡ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å¯¹ç­–ç•¥è¿›è¡Œå¾®è°ƒã€‚è®­ç»ƒåœ¨Webotsä»¿çœŸå™¨ä¸­é‡‡ç”¨ä¸¤é˜¶æ®µè¯¾ç¨‹å­¦ä¹ è¿›è¡Œï¼Œæœ€ç»ˆå®ç°äº†96%çš„æˆåŠŸç‡ï¼Œå¹¶å°†æ¯åƒæ­¥ç¢°æ’æ¬¡æ•°é™è‡³0.05ï¼Œä¸ºå®‰å…¨å¯¼èˆªè®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚åº”ç”¨è¯¥æ–¹æ³•åï¼Œæ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿå®ç°è½¦é“å†…è¡Œé©¶ï¼Œè¿˜èƒ½ä»¥ä¸“å®¶æ°´å¹³åº”å¯¹ä¸å®‰å…¨è·¯å†µï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚æœ¬ç ”ç©¶ä»£ç å·²å…¬å¼€ã€‚",
    "url": "https://arxiv.org/abs/2601.23266v1",
    "html_url": "https://arxiv.org/html/2601.23266v1",
    "html_content": "IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models\nSeyed Ahmad Hosseini Miangoleh\n1\n,\nAmin Jalal Aghdasian\n1\n,\nFarzaneh Abdollahi\n1\n1\nDepartment of Electrical Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran\nAbstract\nThis paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available\n1\n1\n1\nPaper Website\nI\nIntroduction\nThe main challenge in autonomous vehicles is building systems that can operate at human levels of safety and reliability in highly dynamic environments\n[\ncrosato2024social\n,\nkiran2021deep\n]\n. This challenge mainly comes from the need to avoid obstacles in a strong and reliable way. Obstacle avoidance is the key safety task that shows whether a system can work in the real world. Even small mistakes in rare situations can cause very serious failures. This means the agent must remain safe even when encountering situations it was not trained on\n[\nross2011reduction\n]\n. To address these challenges, recent research has investigated a range of advanced methods. These approaches aim to enhance the safety, reliability, and adaptability of autonomous systems operating in dynamic environments.\nI-A\nRelated Work\nThis work falls within the overlap of four main areas: hybrid learning, reward inference, generative planning, and adaptive perception. The following section reviews some recent studies in each of these areas.\nI-A\n1\nHybrid Imitation and Reinforcement Learning\nImitation Learning (IL), especially Behavioral Cloning (BC), is widely used in autonomous driving\n[\ncui2023safe\n]\n. It provides a data-efficient way to learn a mapping from expert demonstrations to control actions using supervised learning\n[\nli2022driver\n,\nwang2024implicit\n]\n. The primary advantage of BC is its computational efficiency and its requirement for no explicit knowledge of the underlying environment dynamics. Within BC, methods such as Conditional Imitation Learning improve performance by conditioning the policy on high-level commands. This helps address the challenge of having many possible correct actions for a given situation\n[\neraqi2022dynamic\n]\n. Even though it is efficient, IL has a problem called covariate shift. In this case, small mistakes grow over time when the agent reaches states that were not in the training data. Unlike traditional methods, RL can develop strong and resilient behaviors by learning from trial-and-error experience. However, it usually needs a lot of data and depends on hand-designed reward signals that can be unreliable\n[\nyuan2025model\n]\n.\nThe hybrid method combines IL and RL. This lets the self-driving car first learn basic behavior from examples and then improve it through trial, feedback, and optimization\n[\nradwan2021obstacles\n]\n. In\n[\nmahmoudi2023reinforcement\n]\n, a hybrid method is used that combines BC with the PPO algorithm in the Unity Agents environment. This setup trains racecar agents to drive and avoid obstacles. These combinations capitalize on expert supervision for fast convergence while allowing the agent to explore recovery and adaptation strategies\n[\npinto2021curriculum\n]\n. In\n[\nlu2023imitation\n]\n, the BC-SAC model fuses BC and Soft Actor-Critic to enhance policy robustness and safety in autonomous driving. By employing supervised imitation and reinforcement optimization, one achieves higher generalization and a 38% reduction in failure rate across complex real-world scenarios.\nI-A\n2\nInverse Reinforcement Learning for Reward Shaping\nThe problem of designing a good reward for complex tasks can be handled with Inverse Reinforcement Learning (IRL). IRL tries to find the hidden reward rules directly from expert demonstrations\n[\nzhao2024survey\n,\nlanzaro2025evaluating\n]\n. The Conditional Predictive Behavior Planning model combines Conditional Motion Prediction and Maximum Entropy IRL to make driving more like a human. It predicts how nearby cars will react to each possible move and scores these moves using expert driving data\n[\nhuang2023conditional\n]\n. Adversarial inverse reinforcement learning (AIRL) uses a GAN to learn the reward and the driving policy at the same time, which helps the agent adapt to new environments. Safety-aware AIRL then adds safety rules to block risky actions and lower the chance of crashes\n[\nwang2021decision\n]\n.\nI-A\n3\nAdaptive Perception and Attention in Driving\nWith the growing use of attention mechanisms in deep learning\n[\nlu2024epitester\n,\nxi2023ema\n]\n, these methods have been added to end-to-end driving models. They help the system focus more on key visual elements such as vehicles, pedestrians, and road signs\n[\nwang2024pedestrian\n]\n. Effective driving requires perception systems that can adapt attention dynamically to changing contexts. In\n[\nschwonberg2023survey\n]\n, it uses spatiotemporal, uncertainty-aware attention over multimodal sensors with crossmodal alignment and multiscale fusion to prioritize important actors and regions for downstream planning. In\n[\nchi2024dynamic\n]\n, it adds a temporal residual block, multiscale feature fusion, and global plus double attention to use time and image cues better. In\n[\nma2024adaptive\n]\n, it combines adaptive channel attention and grouped spatial attention with channel shuffle to highlight important features. It plugs into standard CNNs, can replace a 3Ã—3 convolution layer, and improves accuracy with little extra compute.\nI-A\n4\nDiffusion Models for Safe Motion Planning\nDiffusion models\n[\npeng2025diffusion\n]\nare now leading tools for generating data. They are also being used more and more for planning tasks. When generating trajectories, they can produce diverse future motions that still obey real-world physics\n[\nwang2024trajectory\n]\n. Their key advantage is their flexibility. By guiding the backward diffusion steps with extra rules or helper models, they can add safety rules such as avoiding crashes and keeping motions that the vehicle can really follow\n[\npearce2023plannable_journal\n,\nchi2023diffusion\n]\n. Current methods use diffusion models as direct policy models, mainly in offline reinforcement learning settings\n[\nwang2023diffusion\n]\n. Moreover, some methods use stand-alone planners that first produce open-loop trajectories. A separate controller then follows these trajectories\n[\njanner2022planning\n]\n.\nI-B\nResearch Gap\nDespite progress in each area, a key gap appears at their intersection. Current stacks rarely combine generative planning, reward inference, and online policy learning into one system. This causes three main limits:\n1.\nLack of an end-to-end unified loop: Most diffusion-based planners generate trajectories open loop and are executed separately from the RL policy, creating a distribution mismatch between planned motions and closed-loop control. This separation weakens robustness under disturbances and hinders consistent transfer from trajectory proposals to joint steeringâ€“speed commands.\n2.\nNon-adaptive safety trade-offs: Fixed cost weights for lane keeping, collision avoidance, and stability cannot re-balance as scene risk and sensor uncertainty change. Without real-time, perception-conditioned guidance, systems oscillate between over-conservative and overly aggressive behavior and fail to maintain safety while preserving efficiency.\n3.\nInefficient learning signals: Reliance on sparse, hand-crafted rewards leads to sample-hungry training, unstable convergence, and sub-expert driving. The absence of dense rewards inferred from demonstrations and a staged curriculum limits generalization to out-of-distribution states and degrades control smoothness.\nI-C\nMotivation\nReliable autonomous driving must jointly address three coupled challenges: learning efficiency, decision safety, and perceptual adaptability.\nLearning\nPure RL needs a lot of data and becomes unstable when reward signals are rare. IL uses data well but still drifts under covariate shift, and detailed rewards made by people often fail to match expert goals in complex traffic.\nSafety and Planning\nReactive policies are fast to respond, but cannot plan very far ahead. Generative planners can guess future events, but they often cannot run in real time or give strong safety guarantees, so there is still a gap between long-term planning and moment-to-moment control.\nPerception\nStandard vision encoders treat almost all regions of an image uniformly and struggle to leverage contextual information effectively. In driving, however, attention mechanisms should be dynamic: prioritizing near-field lane coherence to ensure lateral stability at high speeds, while intensifying focus on immediate proximity when potential hazards are detected.\nThese limitations highlight the need for a unified framework that connects stable imitation with exploratory reinforcement learning.\nI-D\nContributions\nWe introduce a IRLâ€“DAL, a cohesive framework that confronts the above challenges through three complementary components:\n(1) Hybrid ILâ€“IRLâ€“RL Training\nA structured pipeline that integrates BC for initialization with PPO fineâ€‘tuning under a hybrid reward combining sparse environment feedback and dense GAILâ€‘based intent rewards. This ensures stability, sample efficiency, and policy alignment with expert intent.\n(2) Diffusion Planner for Onâ€‘Demand Safety\nA conditional diffusion model serves as a shortâ€‘horizon, riskâ€‘aware planner, activated only in uncertain or highâ€‘risk states. It generates candidate trajectories optimized by an energyâ€‘based objective penalizing collisions and abrupt control variations, allowing the main policy to internalize safer behaviors via planner feedback.\n(3) Learnable Adaptive Mask (LAM)\nA lightweight perception module that dynamically modulates spatial attention based on vehicle kinematics and LiDAR proximity. The mask directs the visual encoder toward context-critical regionsâ€”amplifying lower-field road features at high speed for precise lane keeping and highlighting proximate surroundings near hazardsâ€”achieving interpretable and efficient attention allocation without heavy selfâ€‘attention overhead.\nFigure 1:\nOverview of the IRL-DAL architecture. The training process unfolds in two phases: the policy is first initialized by Behavioral Cloning (BC) and then fine-tuned with Proximal Policy Optimization (PPO) using a hybrid reward\nr\ntotal\nr_{\\text{total}}\n. A LAM enhances state-aware perception. During RL rollouts, the Diffusion-based Adaptive Lookahead (DAL) planner serves as a safety supervisor, correcting unsafe PPO actions through energy-guided sampling so that only safe experiences are stored in the replay buffer\nğ’Ÿ\nPPO\n\\mathcal{D}_{\\text{PPO}}\n.\nII\nProblem formulation\nThe paper first formalizes the autonomous driving task as a partially observable Markov decision process POMDP defined by the tuple:\n(\nğ’®\n,\nğ’œ\n,\nğ’¯\n,\nâ„›\n,\nğ’ª\n,\nğ’µ\n,\nÎ³\n)\n(\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathcal{R},\\mathcal{O},\\mathcal{Z},\\gamma)\nwhich captures the agent partial observability and reliance on noisy sensor data.\nState Space (\nğ’®\n\\mathcal{S}\n)\nThe latent true state includes the ego car position, orientation, and speed. It also includes nearby moving objects, the road layout. The policy cannot see this state directly.\nAction Space (\nğ’œ\n\\mathcal{A}\n)\nEach action is a continuous two-dimensional vector:\na\nt\n=\n[\nsteering\n,\nspeed\n]\na_{t}=[\\,\\text{steering},\\,\\text{speed}\\,]\nwhich is later mapped by a low-level controller into executable control commands. This design provides smooth, normalized continuous control.\nTransition Function (\nğ’¯\n\\mathcal{T}\n)\nThe stochastic dynamics:\nğ’¯\nâ€‹\n(\ns\nt\n+\n1\nâˆ£\ns\nt\n,\na\nt\n)\n\\mathcal{T}(s_{t+1}\\mid s_{t},a_{t})\ndefines how the true state evolves given the action.\nObservation Space (\nğ’ª\n\\mathcal{O}\n)\nThe multimodal observation at each time step is given by:\no\nt\n=\n{\nI\nt\n,\nL\nt\n,\nK\nt\n}\no_{t}=\\{I_{t},L_{t},K_{t}\\}\nWhere:\nâ€¢\nCamera Image\n(\nI\nt\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nI_{t}\\in\\mathbb{R}^{H\\times W\\times 3}\n): front-facing RGB frame.\nâ€¢\nLiDAR Scan\n(\nL\nt\nâˆˆ\nâ„\n180\nL_{t}\\in\\mathbb{R}^{180}\n): range readings from a forward LiDAR.\nâ€¢\nVehicle Kinematics\n(\nK\nt\nK_{t}\n): normalized ego-vehicle speed.\nThe LAM is generated internally based on\nK\nt\nK_{t}\nand\nL\nt\nL_{t}\n. Thus, they belong to the perception module rather than the raw observation space.\nObservation Function (\nğ’µ\n\\mathcal{Z}\n)\nThe conditional distribution:\nğ’µ\nâ€‹\n(\no\nt\nâˆ£\ns\nt\n+\n1\n,\na\nt\n)\n\\mathcal{Z}(o_{t}\\mid s_{t+1},a_{t})\nModels the sensing process that generates observations.\nReward Function (\nâ„›\n\\mathcal{R}\n)\nSince the agent perceives the environment only through observations, the reward depends on both\no\nt\no_{t}\nand\na\nt\na_{t}\n. IRL-DAL uses a hybrid reward. It combines an environment-defined term with an intrinsic term learned by the IRL discriminator.\nr\nt\nâ€‹\n(\no\nt\n,\na\nt\n)\n=\n(\n1\nâˆ’\nw\nIRL\n)\nâ€‹\nr\nenv\nâ€‹\n(\no\nt\n,\na\nt\n)\n+\nw\nIRL\nâ€‹\nr\nIRL\nâ€‹\n(\no\nt\n,\na\nt\n)\nr_{t}(o_{t},a_{t})=(1-w_{\\text{IRL}})\\,r_{\\text{env}}(o_{t},a_{t})+w_{\\text{IRL}}\\,r_{\\text{IRL}}(o_{t},a_{t})\nwhere\nw\nIRL\nw_{\\text{IRL}}\nis a phase-dependent weight used during the mixed training phase, the influence of the learned reward throughout training. Both terms depend solely on observable quantities, ensuring consistency under partial observability.\nDiscount Factor (\nÎ³\n\\gamma\n)\nA scalar\nÎ³\n\\gamma\nthat balances short-term rewards with long-term performance.\nObjective\nThe agent aims to learn a stochastic policy\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\no\nt\n)\n\\pi_{\\theta}(a_{t}\\mid o_{t})\n, parameterized by\nÎ¸\n\\theta\n, that maximizes the expected discounted return:\nğ”¼\nÏ€\nÎ¸\n,\nğ’¯\n,\nğ’µ\nâ€‹\n[\nâˆ‘\nt\n=\n0\nâˆ\nÎ³\nt\nâ€‹\nr\nt\n]\n\\mathbb{E}_{\\pi_{\\theta},\\,\\mathcal{T},\\,\\mathcal{Z}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\right]\nIII\nMethodology\nFigure\n1\nprovides an overview of the entire architecture. Our proposed framework integrates safety, stability, and expert-like decision-making within a single autonomous driving system. As illustrated in Figure\n1\n, the architecture combines four interacting components that collectively enable robust and adaptive behavior: (1) context-aware perception via the LAM, (2) FSM-aware structured replay buffers, (3) PPO fine-tuning with a hybrid reward, and (4) diffusion-based safety supervision with experience correction.\nFigure 2:\nArchitecture of the LAM and its integration with the PPO policy. Normalized speed\nv\nt\nnorm\nv_{t}^{\\text{norm}}\nand hazard level\nh\nt\nh_{t}\nmodulate a vertical gradient mask via learnable parameters\nÎ±\nspeed\n\\alpha_{\\text{speed}}\nand\nÎ±\nlidar\n\\alpha_{\\text{lidar}}\n. The resulting mask is concatenated with the RGB image to form a 4-channel input, enabling context-aware visual processing. LAM is trained end-to-end via BC gradients, allowing the agent to discover adaptive attention patterns that prioritize safety-critical regions.\nIII-A\nPerception Module\nThe perception module uses a learnable attention mechanism that dynamically focuses on the driving situation. This adaptive mask helps the agent build a compact but informative state representation\ns\nt\ns_{t}\nfrom high-dimensional, multi-sensor inputs. The quality of the driving policy strongly depends on how well it sees and encodes the environment. Standard end-to-end methods often treat all image pixels equally, wasting model capacity. To fix this, we introduce the LAM, a small differentiable module that adds safety-related prior knowledge to the visual input using top-down attention.\nFigure\n2\nshows the LAM architecture and how it connects to the PPO policy. The module takes two inputs: the current vehicle speed\nv\nt\nv_{t}\nand the minimum LiDAR distance\nd\nmin\n,\nt\n=\nmin\nâ¡\n(\nmin\nâ¡\n(\nl\nt\n)\n,\nd\nmax\n)\nd_{\\min,t}=\\min(\\min(l_{t}),d_{\\max})\n, where\nl\nt\nâˆˆ\nâ„\n180\nl_{t}\\in\\mathbb{R}^{180}\nis the raw LiDAR range vector, clipped at the maximum range\nd\nmax\nd_{\\max}\n. These signals are then scaled to the interval\n[\n0\n,\n1\n]\n[0,1]\n.\nv\nt\nnorm\n\\displaystyle v_{t}^{\\text{norm}}\n=\nclamp\nâ¡\n(\nv\nt\nv\nmax\n,\n0\n,\n1\n)\n\\displaystyle=\\operatorname{clamp}\\left(\\frac{v_{t}}{v_{\\max}},0,1\\right)\n(1)\nh\nt\n\\displaystyle h_{t}\n=\nclamp\nâ¡\n(\nd\nsafe\nâˆ’\nd\nmin\n,\nt\nd\nsafe\n,\n0\n,\n1\n)\n\\displaystyle=\\operatorname{clamp}\\left(\\frac{d_{\\text{safe}}-d_{\\min,t}}{d_{\\text{safe}}},0,1\\right)\n(2)\nwhere\nv\nmax\nv_{\\max}\nand\nd\nsafe\nd_{\\text{safe}}\nare predefined maximum speed and safety distance thresholds, respectively. LAM computes context-dependent modulation factors using two learnable scalar parameters\nÎ±\nspeed\n,\nÎ±\nlidar\nâˆˆ\nâ„\n\\alpha_{\\text{speed}},\\alpha_{\\text{lidar}}\\in\\mathbb{R}\n. They are initialized to\n0.5\n0.5\n:\nf\nspeed\n=\n1\n+\nÎ±\nspeed\nâ‹…\nv\nt\nnorm\n,\nf\nhazard\n=\n1\n+\nÎ±\nlidar\nâ‹…\nh\nt\nf_{\\text{speed}}=1+\\alpha_{\\text{speed}}\\cdot v_{t}^{\\text{norm}},\\quad f_{\\text{hazard}}=1+\\alpha_{\\text{lidar}}\\cdot h_{t}\n(3)\nThese factors scale a base lower-bound intensity weight\nw\nbase, lower\n=\n1.0\nw_{\\text{base, lower}}=1.0\n, while the upper-bound weight is fixed at\nw\nbase, upper\n=\n0.0\nw_{\\text{base, upper}}=0.0\n. The resulting dynamic lower intensity is:\nw\nlower\n=\nw\nbase, lower\nâ‹…\nf\nspeed\nâ‹…\nf\nhazard\nw_{\\text{lower}}=w_{\\text{base, lower}}\\cdot f_{\\text{speed}}\\cdot f_{\\text{hazard}}\n(4)\nA smooth vertical gradient mask is then constructed for each row\ny\nâˆˆ\n[\n0\n,\nH\nâˆ’\n1\n]\ny\\in[0,H-1]\nof the input image:\nM\nt\nâ€‹\n(\ny\n)\n=\nw\nbase, upper\n+\n(\nw\nlower\nâˆ’\nw\nbase, upper\n)\nâ‹…\ny\nH\nâˆ’\n1\nM_{t}(y)=w_{\\text{base, upper}}+(w_{\\text{lower}}-w_{\\text{base, upper}})\\cdot\\frac{y}{H-1}\n(5)\nTo ensure numerical stability and bounded output, the mask is normalized:\nM\n^\nt\n=\nM\nt\nmax\nâ¡\n(\nM\nt\n)\n+\nÎµ\n,\nÎµ\n=\n10\nâˆ’\n6\n\\hat{M}_{t}=\\frac{M_{t}}{\\max(M_{t})+\\varepsilon},\\quad\\varepsilon=10^{-6}\n(6)\nyielding\nM\n^\nt\nâˆˆ\nâ„\n1\nÃ—\nH\nÃ—\nW\nÃ—\n1\n\\hat{M}_{t}\\in\\mathbb{R}^{1\\times H\\times W\\times 1}\n.\nThe normalized RGB image\nI\nt\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nI_{t}\\in\\mathbb{R}^{H\\times W\\times 3}\n(scaled to\n[\n0\n,\n1\n]\n[0,1]\n) is concatenated channel-wise with\nM\n^\nt\n\\hat{M}_{t}\nto form a 4-channel input tensor:\nI\nt\nâ€²\n=\nconcat\nâ¡\n(\nI\nt\n/\n255.0\n,\nM\n^\nt\n)\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n4\nI^{\\prime}_{t}=\\operatorname{concat}(I_{t}/255.0,\\hat{M}_{t})\\in\\mathbb{R}^{H\\times W\\times 4}\n(7)\nThis augmented input is processed by a shared convolutional backbone, which also fuses embedded LiDAR features before feeding into the actor and critic heads.\nThe LAM parameters\nÎ¸\nLAM\n=\n{\nÎ±\nspeed\n,\nÎ±\nlidar\n}\n\\theta_{\\text{LAM}}=\\{\\alpha_{\\text{speed}},\\alpha_{\\text{lidar}}\\}\nare optimized end-to-end during BC alongside the policy using the Adam optimizer with learning rate\nÎ·\nBC\n\\eta_{\\text{BC}}\nand L2 regularization applied selectively to policy weights:\nâ„’\nBC\n=\n1\nB\nâ€‹\nâˆ‘\ni\n=\n1\nB\nâ€–\nÏ€\nÎ¸\nâ€‹\n(\ns\nt\n(\ni\n)\n)\nâˆ’\na\nexpert\n(\ni\n)\nâ€–\n2\n+\nÎ»\nL2\nâ€‹\nâˆ‘\np\nâˆˆ\nÎ¸\npolicy\nâ€–\np\nâ€–\n2\n\\mathcal{L}_{\\text{BC}}=\\frac{1}{B}\\sum_{i=1}^{B}\\left\\|\\pi_{\\theta}(s_{t}^{(i)})-a_{\\text{expert}}^{(i)}\\right\\|^{2}+\\lambda_{\\text{L2}}\\sum_{p\\in\\theta_{\\text{policy}}}\\|p\\|^{2}\n(8)\nwhere gradients flow through the 4-channel observation to update\nÎ¸\nLAM\n\\theta_{\\text{LAM}}\n. Training includes gradient clipping (max norm\nG\nmax\nG_{\\max}\n) and learning rate scheduling via plateau detection.\nAs shown in Figure\n2\n, the learned masks adapt dynamically to the driving context. Rather than shifting the geometric center of attention, the mechanism modulates the intensity of the spatial gradient. At high speeds, the mask amplitude significantly increases (via\nf\nspeed\nf_{\\text{speed}}\n), which strengthens the feature extraction across the entire driveable area, effectively expanding the usable visual range while maintaining a strong prior on the immediate lane path. Similarly, when proximity to obstacles is detected (\nd\nmin\n,\nt\nd_{\\min,t}\n), the hazard factor (\nf\nhazard\nf_{\\text{hazard}}\n) further amplifies the mask intensity. This ensures that the network receives a sharper, high-contrast signal of the immediate surroundings for precise collision avoidance, boosting safety without relying on hand-crafted heuristic rules.\nIII-B\nMulti-Phase Learning Curriculum\nPolicy learning happens in two main phases. First, BC gives the agent a stable and safe starting point by training the policy network on expert demonstrations. This warm start reduces risky, high-variance exploration and ensures the agent begins with reasonable behavior. Then, the policy is improved with PPO\n[\nschulman2017proximal\n]\n, which allows controlled on-policy exploration and lets the agent go beyond simple imitation.\nTo keep PPO updates aligned with expert behavior, an adversarial IRL module based on GAIL\n[\nho2016generative\n]\nis added. This module provides dense, behavior-focused reward signals. The hybrid reward is defined as:\nr\nt\n=\nw\nenv\nâ€‹\nr\nenv\nâ€‹\n(\ns\nt\n,\na\nt\n)\n+\nw\nirl\nâ€‹\nr\nirl\nâ€‹\n(\ns\nt\n,\na\nt\n)\nr_{t}=w_{\\text{env}}r_{\\text{env}}(s_{t},a_{t})+w_{\\text{irl}}r_{\\text{irl}}(s_{t},a_{t})\n(9)\nwhere\nr\nenv\nâ€‹\n(\ns\nt\n,\na\nt\n)\nr_{\\text{env}}(s_{t},a_{t})\ndenotes the environment reward and\nw\nenv\n+\nw\nirl\n=\n1\nw_{\\text{env}}+w_{\\text{irl}}=1\n. The IRL reward is:\nr\nirl\nâ€‹\n(\ns\nt\n,\na\nt\n)\n=\nâˆ’\nlog\nâ¡\n(\n1\nâˆ’\nD\nÏˆ\nâ€‹\n(\ns\nt\n,\na\nt\n)\n+\nÎµ\n)\nr_{\\text{irl}}(s_{t},a_{t})=-\\log(1-D_{\\psi}(s_{t},a_{t})+\\varepsilon)\n(10)\nit is bounded within a predefined range.\nThe curriculum begins with an FSM-partitioned expert dataset\nD\nexpert\nD_{\\text{expert}}\n, enabling balanced sampling across driving modes. During the imitation phase, both policy and diffusion planner are trained via BC with interval\nT\nBC\ninit\nT_{\\text{BC}}^{\\text{init}}\n. In the mixed phase, PPO updates use hybrid rewards, with discriminator trained every\nT\ndisc\nT_{\\text{disc}}\nsteps and BC continued adaptively every\nT\nBC\nmixed\nT_{\\text{BC}}^{\\text{mixed}}\nsteps. Safety interventions, which are triggered when\nd\nmin\n,\nt\n<\nd\nsafe\nd_{\\min,t}<d_{\\text{safe}}\nor\n|\nd\nlane\n,\nt\n|\n>\ne\nlane\n|d_{\\text{lane},t}|>e_{\\text{lane}}\n, are corrected and stored with a metadata flag indicating diffusion-based intervention. This progressive design, which moves from imitation to hybrid RL and then to safety distillation, leads to robust, safe, and efficient learning.\nIII-C\nExpert Data Generation via FSM-Aware Experience Replay\nA key part of the framework is a reliable and high-quality source of expert demonstrations. Instead of dealing with the noise and inconsistency of real human driving logs, an expert policy is built from scratch using a deterministic FSM policy\nÏ€\nâˆ—\n\\pi^{*}\n. As shown in Figure\n3\n, this FSM controller moves smoothly between four behavior modes: Lane Following, Obstacle Avoidance, Driving Straight, and Returning, with transitions defined by simple sensor-based rules that describe the current driving situation. What sets this approach apart is the FSM-aware experience replay strategy. Each collected transition\n(\no\nt\n,\na\nt\nâˆ—\n,\ns\nFSM\n,\nt\n)\n(o_{t},a_{t}^{*},s_{\\text{FSM},t})\nis stored in a separate buffer\nD\ns\nD_{s}\nthat corresponds to its active FSM state\ns\ns\n. The full expert dataset is then formed as follows:\nD\nexpert\n=\nâ‹ƒ\ns\nD\ns\nD_{\\text{expert}}=\\bigcup_{s}D_{s}\nThis clear, state-based structure helps solve a common problem in autonomous driving datasets, where rare but important events are often underrepresented, such as passing through a narrow gap or recovering from a strong lane drift. By sampling mini-batches evenly across FSM states, the training process sees a balanced mix of normal cruising and challenging edge cases, instead of being dominated by simple highway driving.\nFigure 3:\nThe FSM expert policy in action. It switches smoothly between the modes (Lane Following, Obstacle Avoidance, Driving Straight, Returning) using sensor-based transition rules. The FSM-aware experience replay stores each sample in its matching state buffer, which gives balanced exposure to both normal and risky driving situations.\nIII-C\n1\nPhase 1: Foundational Pre-training\nThe training process starts with a supervised warm up over\nN\nimitation\nN_{\\text{imitation}}\ntimesteps, using only\nD\nexpert\nD_{\\text{expert}}\nto build a safe and reliable behavioral base. This phase protects the agent from unstable and unsafe exploration that often appears at the start of RL. Under BC, the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nis trained on compact embeddings\ns\nt\n=\nÏ•\nâ€‹\n(\no\nt\n)\ns_{t}=\\phi(o_{t})\nfrom the perception module. It learns to match the expert actions using a mean-squared error loss. Balanced sampling from the FSM-partitioned buffers ensures that all driving modes are well represented during training:\nâ„’\nBC\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\n(\no\nt\n,\na\nt\nâˆ—\n)\nâˆ¼\nD\nexpert\nbalanced\nâ€‹\n[\nâ€–\nÏ€\nÎ¸\nâ€‹\n(\ns\nt\n)\nâˆ’\na\nt\nâˆ—\nâ€–\n2\n]\n+\nÎ»\nL2\nâ€‹\nâˆ‘\np\nâˆˆ\nÎ¸\npolicy\nâ€–\np\nâ€–\n2\n\\mathcal{L}_{\\text{BC}}(\\theta)=\\mathbb{E}_{(o_{t},a_{t}^{*})\\sim D_{\\text{expert}}^{\\text{balanced}}}\\left[\\|\\pi_{\\theta}(s_{t})-a_{t}^{*}\\|^{2}\\right]+\\lambda_{\\text{L2}}\\sum_{p\\in\\theta_{\\text{policy}}}\\|p\\|^{2}\n(11)\nThis loss keeps the policy close to expert behavior long before any reward signal is used.\nDiffusion planner training runs concurrently with BC. A conditional 1D U-Net diffusion planner is trained to generate smooth and feasible motion sequences. It is trained on consecutive expert action chunks\n{\na\nt\nâˆ—\n,\nâ€¦\n,\na\nt\n+\nH\nâˆ’\n1\nâˆ—\n}\n\\{a_{t}^{*},\\dots,a_{t+H-1}^{*}\\}\ntaken from\nD\nexpert\nD_{\\text{expert}}\nand optimized with the standard DDPM denoising objective\n[\nho2020denoising\n]\n. In this way, the planner learns the smooth and physically consistent control patterns of the FSM expert and is prepared to act later as a reliable safety net.\nIII-C\n2\nPhase 2: Online Fine-tuning with Adversarial Reinforcement Learning\nAfter a strong base is built, the system runs for\nN\nmixed\nN_{\\text{mixed}}\ntimesteps of online refinement with PPO. During this phase, the policy is improved through interaction with the environment, using a hybrid reward that combines clear task feedback with imitation-based signals. Under GAIL, a discriminator\nD\nÏˆ\nD_{\\psi}\nis trained to go beyond simple action matching. It learns to tell the difference between policy rollouts\n(\ns\nt\n,\na\nt\n)\n(s_{t},a_{t})\nand true expert pairs\n(\ns\nt\nâˆ—\n,\na\nt\nâˆ—\n)\n(s_{t}^{*},a_{t}^{*})\nusing a binary cross-entropy loss:\nâ„’\nDisc\nâ€‹\n(\nÏˆ\n)\n=\n\\displaystyle\\mathcal{L}_{\\text{Disc}}(\\psi)=\nâˆ’\nğ”¼\n(\ns\nt\n,\na\nt\n)\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n[\nlog\nâ¡\n(\n1\nâˆ’\nD\nÏˆ\nâ€‹\n(\ns\nt\n,\na\nt\n)\n)\n]\n\\displaystyle-\\mathbb{E}_{(s_{t},a_{t})\\sim\\pi_{\\theta}}[\\log(1-D_{\\psi}(s_{t},a_{t}))]\n(12)\nâˆ’\nğ”¼\n(\ns\nt\nâˆ—\n,\na\nt\nâˆ—\n)\nâˆ¼\nD\nexpert\nâ€‹\n[\nlog\nâ¡\n(\nD\nÏˆ\nâ€‹\n(\ns\nt\nâˆ—\n,\na\nt\nâˆ—\n)\n)\n]\n\\displaystyle-\\mathbb{E}_{(s_{t}^{*},a_{t}^{*})\\sim D_{\\text{expert}}}[\\log(D_{\\psi}(s_{t}^{*},a_{t}^{*}))]\nOnce training is complete, the discriminator provides a dense and continuous imitation reward:\nr\nirl\nâ€‹\n(\ns\nt\n,\na\nt\n)\n=\nâˆ’\nlog\nâ¡\n(\n1\nâˆ’\nD\nÏˆ\nâ€‹\n(\ns\nt\n,\na\nt\n)\n+\nÎµ\n)\nr_{\\text{irl}}(s_{t},a_{t})=-\\log(1-D_{\\psi}(s_{t},a_{t})+\\varepsilon)\n(13)\nBalanced sampling across FSM states ensures that the discriminator sees the full range of expert behaviors.\nIn the hybrid reward formulation, the final PPO reward is designed to balance task completion with expert-level behavior.\nr\nt\nâ€‹\n(\ns\nt\n,\na\nt\n)\n=\nw\nenv\nâ€‹\nr\nenv\nâ€‹\n(\ns\nt\n,\na\nt\n)\n+\nw\nirl\nâ€‹\nr\nirl\nâ€‹\n(\ns\nt\n,\na\nt\n)\nr_{t}(s_{t},a_{t})=w_{\\text{env}}r_{\\text{env}}(s_{t},a_{t})+w_{\\text{irl}}r_{\\text{irl}}(s_{t},a_{t})\n(14)\nwhere\nw\nenv\n+\nw\nirl\n=\n1\nw_{\\text{env}}+w_{\\text{irl}}=1\n. This combination helps the agent achieve high-level goals, such as steady progress and zero collisions, while also matching the smooth, anticipatory style of the FSM expert.\nIII-D\nSafety Assurance via Guided Diffusion and Experience Correction\nFinally, a diffusion-based planner is added as a safety-focused backup system. The DAL module is activated only in high-risk situations and quickly generates short, feasible paths that keep the vehicle safe. At the same time, it improves learning by storing safe, corrected experiences in the policy memory, so the agent becomes better over time without reinforcing unsafe behaviors. Together, these parts (perception, curriculum, expert replay, and safety) form an integrated pipeline that combines fast reactions with careful planning and leads to behavior that is effective, safe, and close to human driving.\nEven when the PPO policy learns complex driving patterns, it is still hard to stay safe in rare or unexpected situations. To reduce these risks, the framework uses two safety components:\n1.\nAn on-demand, energy-guided diffusion planner that is activated to generate safe actions when the situation becomes dangerous.\n2.\nAn experience correction mechanism that sends these safe corrections back to the main policy so that temporary fixes become lasting improvements.\nIII-D\n1\nOn-Demand, Energy-Guided Trajectory Generation\nThe diffusion planner trained in the first phase now acts as a safety-critical motion generator. It stays inactive most of the time to save computation and is only turned on in high-risk states. A state is treated as high-risk when the closest LiDAR reading becomes too small or the vehicle moves far away from the lane center:\nd\nmin\n,\nt\n<\nd\ntrigger\nâˆ¨\n|\nd\nlane\n,\nt\n|\n>\ne\nlane\nd_{\\min,t}<d_{\\text{trigger}}\\quad\\vee\\quad|d_{\\text{lane},t}|>e_{\\text{lane}}\n(15)\nletting the PPO policy drive freely under normal conditions.\nWhen DAL is called, it generates a short safe trajectory\nğ’œ\n=\na\nt\n,\nâ€¦\n,\na\nt\n+\nH\nâˆ’\n1\n\\mathcal{A}={a_{t},\\dots,a_{t+H-1}}\nbased on the current context embedding. This is done with a guided reverse diffusion process. At each denoising step\nk\nk\n, the U-Net predicts a cleaner trajectory\nğ’œ\n^\nk\n0\n\\hat{\\mathcal{A}}^{0}_{k}\n, which is then moved toward safer behavior by following the gradient of a composite energy function\nE\nâ€‹\n(\nğ’œ\n,\no\nt\n)\nE(\\mathcal{A},o_{t})\n:\nğ’œ\n~\nk\n0\n=\nğ’œ\n^\nk\n0\nâˆ’\nw\ng\nâ‹…\nâˆ‡\nE\nâ€‹\n(\nğ’œ\n^\nk\n0\n,\no\nt\n)\nâ€–\nâˆ‡\nE\nâ€–\n+\nÎµ\n\\tilde{\\mathcal{A}}^{0}_{k}=\\hat{\\mathcal{A}}^{0}_{k}-w_{g}\\cdot\\frac{\\nabla E(\\hat{\\mathcal{A}}^{0}_{k},o_{t})}{\\|\\nabla E\\|+\\varepsilon}\n(16)\nwhere\nw\ng\nw_{g}\ndetermines how strongly the planner pushes trajectories toward safe regions during sampling, and\nğ’œ\n~\nk\n0\n\\tilde{\\mathcal{A}}^{0}_{k}\ndenotes the resulting safety-guided trajectory obtained after applying the energy gradient.\nThe energy\nE\nE\nis a weighted sum of five simple terms that together enforce lane keeping, obstacle clearance, smooth control, stability, and an optional expert-alignment term.\nE\n=\nE\nlane\n+\nE\nlidar\n+\nE\njerk\n+\nE\nstability\n+\nE\nexpert\nE=E_{\\text{lane}}+E_{\\text{lidar}}+E_{\\text{jerk}}+E_{\\text{stability}}+E_{\\text{expert}}\n(17)\nLane Adherence â€”\nE\nlane\nE_{\\text{lane}}\n:\nThis term penalizes deviation from the lane center. Instead of computationally expensive forward modeling, the energy term utilizes the current lateral error state distance (\nd\nlat\nd_{\\text{lat}}\n) to strictly guide the diffusion sampling process toward immediate correction.\nE\nlane\n=\nw\nlane\nâ€‹\n(\nd\nlane\n,\nt\ns\nlane\n)\n2\nE_{\\text{lane}}=w_{\\text{lane}}\\left(\\frac{d_{\\text{lane},t}}{s_{\\text{lane}}}\\right)^{2}\n(18)\nwith a risk-adaptive weight\nw\nlane\nw_{\\text{lane}}\nthat increases under higher hazard levels:\nw\nlane\n=\nw\nbase\nlane\nâ€‹\n(\n1\n+\nÎ±\nhazard\nâ€‹\nh\nt\n)\nw_{\\text{lane}}=w_{\\text{base}}^{\\text{lane}}(1+\\alpha_{\\text{hazard}}h_{t})\nwhere\nw\nbase\nlane\nw_{\\text{base}}^{\\text{lane}}\nis the nominal weighting coefficient for lane keeping,\nÎ±\nhazard\n\\alpha_{\\text{hazard}}\ncontrols how aggressively the weight increases under hazardous conditions,\nand\nh\nt\nh_{t}\ndenotes the instantaneous hazard indicator.\nObstacle Avoidance â€”\nE\nlidar\nE_{\\text{lidar}}\n:\nTo keep a safe distance from obstacles, trajectories generated while the vehicle is in close proximity to obstacles are penalized based on the current sensor state (\nd\nmin\n,\nt\nd_{\\min,t}\n).\nE\nlidar\n=\nw\nlidar\nâ‹…\nmax\n(\n0\n,\nd\nsafe\nplan\nâˆ’\nd\nmin\n,\nt\ns\nlidar\n)\n2\nE_{\\text{lidar}}=w_{\\text{lidar}}\\cdot\\max\\left(0,\\frac{d_{\\text{safe}}^{\\text{plan}}-d_{\\min,t}}{s_{\\text{lidar}}}\\right)^{2}\n(19)\nwhere the weight\nw\nlidar\nw_{\\text{lidar}}\nincreases with the hazard level, according to\nw\nlidar\n=\nw\nbase\nlidar\nâ€‹\n(\n1\n+\nh\nt\n)\nw_{\\text{lidar}}=w_{\\text{base}}^{\\text{lidar}}(1+h_{t})\n.\nControl Smoothness â€”\nE\njerk\nE_{\\text{jerk}}\n:\nTo avoid jerky or hard-to-drive behavior, large changes between consecutive actions are penalized:\nE\njerk\n=\nw\njerk\nâ‹…\n1\nH\nâˆ’\n1\nâ€‹\nâˆ‘\ni\n=\n1\nH\nâˆ’\n1\nâ€–\na\ni\nâˆ’\na\ni\nâˆ’\n1\nâ€–\n2\nE_{\\text{jerk}}=w_{\\text{jerk}}\\cdot\\frac{1}{H-1}\\sum_{i=1}^{H-1}\\|a_{i}-a_{i-1}\\|^{2}\n(20)\nwhere\nw\njerk\nw_{\\text{jerk}}\nis the weighting coefficient for smoothness and\nH\nH\nis the trajectory horizon.\nStability â€”\nE\nstability\nE_{\\text{stability}}\n:\nSteady, centered control is encouraged by penalizing large steering or speed deviations from a neutral reference:\nE\nstability\n=\nw\nstab\nâ‹…\n1\nH\nâ€‹\nâˆ‘\ni\n=\n0\nH\nâˆ’\n1\n(\nsteer\ni\n2\n+\n(\nspeed\ni\nâˆ’\nv\nref\n)\n2\n)\nE_{\\text{stability}}=w_{\\text{stab}}\\cdot\\frac{1}{H}\\sum_{i=0}^{H-1}\\left(\\text{steer}_{i}^{2}+(\\text{speed}_{i}-v_{\\text{ref}})^{2}\\right)\n(21)\nwhere\nw\nstab\nw_{\\text{stab}}\nis the stability weight and\nv\nref\nv_{\\text{ref}}\ndenotes the target reference speed.\nExpert Alignment â€”\nE\nexpert\nE_{\\text{expert}}\n(optional):\nWhen an expert reference is available, an additional term keeps the trajectory close to the expert demonstration:\nE\nexpert\n=\nw\nexp\nâ‹…\n1\nH\nâ€‹\nâˆ‘\ni\n=\n0\nH\nâˆ’\n1\nâ€–\na\ni\nâˆ’\na\ni\nexpert\nâ€–\n2\nE_{\\text{expert}}=w_{\\text{exp}}\\cdot\\frac{1}{H}\\sum_{i=0}^{H-1}\\|a_{i}-a_{i}^{\\text{expert}}\\|^{2}\n(22)\nwhere\nw\nexp\nw_{\\text{exp}}\ncontrols the strength of expert-matching and\na\ni\nexpert\na_{i}^{\\text{expert}}\nis the corresponding expert action.\nThe hazard level\nh\nt\nh_{t}\nadapts all risk-sensitive weights:\nh\nt\n=\nclamp\nâ¡\n(\n1\nâˆ’\ntanh\nâ¡\n(\nd\nmin\n,\nt\ns\nh\n)\n,\n0\n,\n1\n)\nh_{t}=\\operatorname{clamp}\\!\\left(1-\\tanh\\!\\left(\\frac{d_{\\min,t}}{s_{h}}\\right),0,1\\right)\n(23)\nwhere\nd\nmin\n,\nt\nd_{\\min,t}\nis the minimum LiDAR distance at time\nt\nt\n,\ns\nh\ns_{h}\nis a scaling factor determining sensitivity to obstacles, and the clamp limits the value to the range\n[\n0\n,\n1\n]\n[0,1]\n.\nConditioning is performed using a compact context vector\nc\nt\nâˆˆ\nâ„\n64\nc_{t}\\in\\mathbb{R}^{64}\n, which encodes current hazard level, lateral deviation, speed, steering, and LiDAR statistics. Each element is normalized, and unused entries are padded for dimensional consistency.\nIII-D\n2\nAction Blending and Execution\nOnly the first action\na\nt\nDAL\na_{t}^{\\text{DAL}}\nfrom the refined trajectory is used. During the mixed phase, this action is smoothly blended with the PPO action using a dynamic blend weight:\nw\nb\n=\n{\n1.0\n,\nd\nmin\n,\nt\n<\nd\ncritical\nw\nbase\nblend\n+\nw\nscale\nblend\nâ€‹\nh\nblend\notherwise\nw_{b}=\\begin{cases}1.0,&d_{\\min,t}<d_{\\text{critical}}\\\\\nw_{\\text{base}}^{\\text{blend}}+w_{\\text{scale}}^{\\text{blend}}h_{\\text{blend}}&\\text{otherwise}\\end{cases}\n(24)\nwhere:\nh\nblend\n=\ne\nâˆ’\nd\nmin\n,\nt\n/\ns\n1\n+\nk\ntanh\nâ€‹\ntanh\nâ¡\n(\n|\nd\nlane\n,\nt\n|\n/\ns\n2\n)\nh_{\\text{blend}}=e^{-d_{\\min,t}/s_{1}}+k_{\\text{tanh}}\\tanh(|d_{\\text{lane},t}|/s_{2})\n(25)\nand\nw\nb\nw_{b}\nis temporally smoothed via EMA. The final executed action is:\na\nt\nfinal\n=\nw\nb\nâ€‹\na\nt\nDAL\n+\n(\n1\nâˆ’\nw\nb\n)\nâ€‹\na\nt\nPPO\na_{t}^{\\text{final}}=w_{b}a_{t}^{\\text{DAL}}+(1-w_{b})a_{t}^{\\text{PPO}}\n(26)\nIII-D\n3\nExperience Correction\nThe executed transition is stored in the PPO replay buffer with a correction marker, and any unsafe PPO action is replaced by the safe blended output to prevent accidents (Runtime Shielding). However, the experience stored in the replay buffer remains anchored to the FSM expert action. This ensures the policy learns stable rule-based behaviors while the diffusion layer prevents premature episode termination, as illustrated in Fig.\n4\n.\nFigure 4:\nThe safety pipeline in action. DAL is activated by high-risk signals, generates energy-guided safe trajectories, and blends the first action with the PPO output. The corrected experiences are stored in the replay buffer with markers, The diffusion planner acts as an active shield, allowing the agent to safely collect more high-quality expert data even in challenging scenarios.\nIII-E\nSafety-Aware Experience Correction (SAEC)\nTo ensure robust training without premature episode termination, the SAEC mechanism employs the diffusion planner as a runtime safety shield. As described in Algorithm 1, the planner is activated when the agent enters a high-risk state, defined by:\nd\nmin\n,\nt\n<\nd\ntrigger\nâˆ¨\n|\nd\nlane\n,\nt\n|\n>\ne\nlane\nd_{\\min,t}<d_{\\text{trigger}}\\vee|d_{\\text{lane},t}|>e_{\\text{lane}}\nwhere\nd\nmin\n,\nt\nd_{\\min,t}\nis the closest LiDAR reading and\nd\nlane\n,\nt\nd_{\\text{lane},t}\nmeasures the lateral deviation. Once triggered, DAL generates a safe action to prevent immediate collision. However, to ensure the policy converges to a stable kinematic behavior, we do not train on the corrective diffusion action itself. Instead, we utilize the concurrent Rule-Based Expert (FSM) to calculate the ideal ground-truth action (\na\nExpert\na_{\\text{Expert}}\n) for that specific state.\nThe transition stored in the PPO replay buffer\nD\nPPO\nD_{\\text{PPO}}\nis therefore defined as:\nD\nPPO\nâ†\nD\nPPO\nâˆª\n(\no\nt\n,\na\nExpert\n,\nr\nt\n,\no\nt\n+\n1\n)\nD_{\\text{PPO}}\\leftarrow D_{\\text{PPO}}\\cup(o_{t},a_{\\text{Expert}},r_{t},o_{t+1})\nThis mechanism provides two critical benefits:\nâ€¢\nRuntime Survival:\nThe diffusion action is executed to physically prevent collisions, allowing the episode to continue beyond states that would normally cause termination.\nâ€¢\nSafe Expert Labeling:\nBy keeping the agent active in near-accident scenarios, the system allows the FSM expert to label these â€edge casesâ€ with correct recovery actions. The policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nthus learns obstacle avoidance based on the FSMâ€™s consistent logic, while the diffusion layer acts solely as a guardrail during this learning process.\nIII-F\nPolicy Optimization with Safety-Aware Curriculum\nThe core of the system, the driving policy, is trained with a safety-first curriculum that combines the stable reliability of IL with the flexibility of RL. This combination leads to behavior that is not only skilled but also naturally cautious and robust. The full training procedure is summarized in Algorithm\n1\n.\nIII-F\n1\nRL Backbone: PPO\nTo improve the policy through interaction with the environment, PPO is used, an on-policy actor-critic method well suited to smooth, high-dimensional control. Its clipped objective keeps updates stable and prevents large policy changes that could disrupt training.\nThe full optimization objective is:\nL\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\n^\nt\nâ€‹\n[\nL\nt\nCLIP\nâ€‹\n(\nÎ¸\n)\n+\nc\n1\nâ€‹\nL\nt\nVF\nâ€‹\n(\nÏ•\n)\nâˆ’\nc\n2\nâ€‹\nS\nâ€‹\n[\nÏ€\nÎ¸\n]\nâ€‹\n(\ns\nt\n)\n]\nL(\\theta)=\\hat{\\mathbb{E}}_{t}\\left[L_{t}^{\\text{CLIP}}(\\theta)+c_{1}L_{t}^{\\text{VF}}(\\phi)-c_{2}S[\\pi_{\\theta}](s_{t})\\right]\n(26)\nwhere\nL\nt\nCLIP\nL_{t}^{\\text{CLIP}}\nis the clipped policy loss,\nL\nt\nVF\nL_{t}^{\\text{VF}}\nis the value function error, and\nS\nâ€‹\n[\nÏ€\nÎ¸\n]\nS[\\pi_{\\theta}]\nis an entropy term that encourages the policy to keep exploring. Raw observations are first converted into compact embeddings\ns\nt\n=\nÏ•\nâ€‹\n(\no\nt\n)\ns_{t}=\\phi(o_{t})\nby a shared perception backbone (Sec.\nIII-A\n). After this step, the actor and critic each use their own lightweight MLP head.\nIII-F\n2\nMulti-Phase Training Curriculum\nTraining is carried out in two phases. It starts with an imitation warm up and then moves to a reward driven refinement phase.\nPhase 1 â€” Imitation Pre-training:\nOver the first\nN\nimitation\nN_{\\text{imitation}}\ntimesteps, both the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nand diffusion planner are bootstrapped via BC on the FSM-aware expert dataset\nD\nexpert\nD_{\\text{expert}}\n(Sec.\nIII-C\n). Balanced sampling across driving modes guarantees exposure to rare but critical edge cases:\nâ„’\nBC\n=\nğ”¼\n(\no\nt\n,\na\nt\nâˆ—\n)\nâˆ¼\nD\nexpert\nbalanced\nâ€‹\n[\nâ€–\nÏ€\nÎ¸\nâ€‹\n(\ns\nt\n)\nâˆ’\na\nt\nâˆ—\nâ€–\n2\n]\n+\nÎ»\nL2\nâ€‹\nâ€–\nÎ¸\npolicy\nâ€–\n2\n.\n\\mathcal{L}_{\\text{BC}}=\\mathbb{E}_{(o_{t},a_{t}^{*})\\sim D_{\\text{expert}}^{\\text{balanced}}}\\left[\\|\\pi_{\\theta}(s_{t})-a_{t}^{*}\\|^{2}\\right]+\\lambda_{\\text{L2}}\\|\\theta_{\\text{policy}}\\|^{2}.\n(27)\nThis phase plants a\nsafe, expert-grade behavioral prior\n, slashing unsafe exploration right from the start.\nPhase 2 â€” IRL-PPO Fine-tuning with Hybrid Reward:\nOnce a solid foundation is set, the agent goes for\nN\nmixed\nN_{\\text{mixed}}\ntimesteps, refining itself with PPO under a\nhybrid reward\nthat fuses crisp environmental feedback with rich, learned imitation cues:\nr\nt\n=\nw\nenv\nâ€‹\nr\nenv\nâ€‹\n(\ns\nt\n,\na\nt\n)\n+\nw\nirl\nâ€‹\nr\nirl\nâ€‹\n(\ns\nt\n,\na\nt\n)\nr_{t}=w_{\\text{env}}r_{\\text{env}}(s_{t},a_{t})+w_{\\text{irl}}r_{\\text{irl}}(s_{t},a_{t})\n(28)\nwhere\nw\nenv\n+\nw\nirl\n=\n1\nw_{\\text{env}}+w_{\\text{irl}}=1\n.\nIRL Reward from GAIL discriminator:\nr\nirl\nâ€‹\n(\ns\nt\n,\na\nt\n)\n=\nâˆ’\nlog\nâ¡\n(\n1\nâˆ’\nD\nÏˆ\nâ€‹\n(\ns\nt\n,\na\nt\n)\n+\nÎµ\n)\n,\nr\nirl\nâˆˆ\n[\nr\nmin\n,\nr\nmax\n]\nr_{\\text{irl}}(s_{t},a_{t})=-\\log(1-D_{\\psi}(s_{t},a_{t})+\\varepsilon),\\quad r_{\\text{irl}}\\in[r_{\\min},r_{\\max}]\nThe environment reward\nr\nenv\nr_{\\text{env}}\nis dense and FSM aware. It is adapted to the current driving mode and combines terms for precise lane centring, obstacle clearance, and sparse goal completion. A collision gives a large negative reward, a near miss gives a smaller penalty, and a minimum baseline keeps the reward positive in safe states, as illustrated in Fig.\n5\n.\nFigure 5:\nImpact of Hybrid Reward Shaping on Trajectory Smoothness. The blue trajectory illustrates the policy trained solely on the rule-based environment reward (\nr\nenv\nr_{\\text{env}}\n), resulting in oscillatory behavior and excessive lateral deviation. In contrast, the green trajectory incorporates the dense IRL signal (\nr\nIRL\nr_{\\text{IRL}}\n), effectively regularizing the policy towards the expertâ€™s kinematic profile. This results in smoother lane-change maneuvers (left) and tighter collision avoidance (right), correcting the overshooting tendencies observed in the baseline.\nIII-G\nIntegrated Training Pipeline\nAlgorithm\n1\nties everything together: FSM-aware replay, BC, GAIL, PPO, DAL, and SAEC.\n1\n2\nInitialize:\nÏ€\nÎ¸\n\\pi_{\\theta}\n,\nV\nÏ•\nV_{\\phi}\n,\nP\nDAL\nP_{\\text{DAL}}\n,\nD\nÏˆ\nD_{\\psi}\n,\nğ’Ÿ\nbuffer\nâ†\nâˆ…\n\\mathcal{D}_{\\text{buffer}}\\leftarrow\\emptyset\n3\n4\nPhase 1: Imitation (Warm-up & Data Collection)\n5\nfor\nt\nâ†\n0\nt\\leftarrow 0\nto\nN\nimitation\nâˆ’\n1\nN_{\\text{imitation}}-1\ndo\n6\nObserve\no\nt\no_{t}\n, compute expert action\na\nt\nâˆ—\nâ†\nFSM\nâ€‹\n(\no\nt\n)\na_{t}^{*}\\leftarrow\\text{FSM}(o_{t})\n7\nExecute\na\nt\nâˆ—\na_{t}^{*}\n, observe\no\nt\n+\n1\no_{t+1}\n8\nStore\n(\no\nt\n,\na\nt\nâˆ—\n)\n(o_{t},a_{t}^{*})\nin\nğ’Ÿ\nbuffer\n\\mathcal{D}_{\\text{buffer}}\n9\nif\nt\nmod\nT\nBC\n=\n0\nt\\bmod T_{\\text{BC}}=0\nthen\n10\nUpdate\nÏ€\nÎ¸\n\\pi_{\\theta}\nvia BC on\nğ’Ÿ\nbuffer\n\\mathcal{D}_{\\text{buffer}}\n(Eq.\n27\n)\n11\n12\nend if\n13\nif\nt\nmod\nT\ndiffusion\n=\n0\nt\\bmod T_{\\text{diffusion}}=0\nthen\n14\nTrainDiffusion\n(\nğ’Ÿ\nbuffer\n\\mathcal{D}_{\\text{buffer}}\n)\n15\n16\nend if\n17\n18\nend for\n19\n20\nPhase 2: Mixed (RL + DAL Safety + Regularization)\n21\nfor\nt\nâ†\nN\nimitation\nt\\leftarrow N_{\\text{imitation}}\nto\nN\ntotal\nâˆ’\n1\nN_{\\text{total}}-1\ndo\n22\nSample agent action\na\nt\nPPO\nâˆ¼\nÏ€\nÎ¸\nâ€‹\n(\no\nt\n)\na_{t}^{\\text{PPO}}\\sim\\pi_{\\theta}(o_{t})\n23\nCompute expert action\na\nt\nâˆ—\nâ†\nFSM\nâ€‹\n(\no\nt\n)\na_{t}^{*}\\leftarrow\\text{FSM}(o_{t})\n24\n25\nif\nd\nmin\n,\nt\n<\nd\ntrigger\nd_{\\min,t}<d_{\\text{trigger}}\nor\n|\nd\nlane\n,\nt\n|\n>\ne\nlane\n|d_{\\text{lane},t}|>e_{\\text{lane}}\nthen\n26\nSample\nğ’œ\nâˆ¼\nP\nDAL\nâ€‹\n(\no\nt\n)\n\\mathcal{A}\\sim P_{\\text{DAL}}(o_{t})\nwith energy guidance\n27\na\nt\nfinal\nâ†\nblend\nâ€‹\n(\na\nt\nDAL\n,\na\nt\nPPO\n)\na_{t}^{\\text{final}}\\leftarrow\\text{blend}(a_{t}^{\\text{DAL}},a_{t}^{\\text{PPO}})\n28\n29\nelse\n30\na\nt\nfinal\nâ†\na\nt\nPPO\na_{t}^{\\text{final}}\\leftarrow a_{t}^{\\text{PPO}}\n31\n32\nend if\n33\n34\nExecute\na\nt\nfinal\na_{t}^{\\text{final}}\n, observe\nr\nenv\n,\no\nt\n+\n1\nr_{\\text{env}},o_{t+1}\n35\nr\nirl\nâ†\nâˆ’\nlog\nâ¡\n(\n1\nâˆ’\nD\nÏˆ\nâ€‹\n(\no\nt\n,\na\nt\nfinal\n)\n+\nÎµ\n)\nr_{\\text{irl}}\\leftarrow-\\log(1-D_{\\psi}(o_{t},a_{t}^{\\text{final}})+\\varepsilon)\n36\nr\nt\nâ†\nw\nenv\nâ€‹\nr\nenv\n+\nw\nirl\nâ€‹\nr\nirl\nr_{t}\\leftarrow w_{\\text{env}}r_{\\text{env}}+w_{\\text{irl}}r_{\\text{irl}}\n37\n38\nStore\n(\no\nt\n,\na\nt\nfinal\n,\na\nt\nâˆ—\n,\nr\nt\n)\n(o_{t},a_{t}^{\\text{final}},a_{t}^{*},r_{t})\nin\nğ’Ÿ\nbuffer\n\\mathcal{D}_{\\text{buffer}}\n39\n40\nif\nt\nmod\nT\ndisc\n=\n0\nt\\bmod T_{\\text{disc}}=0\nthen\n41\nTrainDiscriminator\n(\nğ’Ÿ\nbuffer\n\\mathcal{D}_{\\text{buffer}}\n)\n42\n43\nend if\n44\nif\nt\nmod\nT\nsync\n=\n0\nt\\bmod T_{\\text{sync}}=0\nthen\n45\nSyncFeatures\n()\n46\n47\nend if\n48\nif\nshould_run_bc_training\n()\nthen\n49\nTrainBC\n(\nğ’Ÿ\nbuffer\n\\mathcal{D}_{\\text{buffer}}\n)\n50\n51\nend if\n52\nUpdate\nÏ€\nÎ¸\n,\nV\nÏ•\n\\pi_{\\theta},V_{\\phi}\nvia PPO using collected rollouts\n53\n54\nend for\nAlgorithmÂ 1\nHybrid IRLâ€“DAL Training Framework\nTABLE I:\nQuantitative performance across architectural variants (10 seeds, mean\nÂ±\n\\pm\nstd). Mean reward normalized to [0, 200]. Trajectory prediction metrics (ADE/FDE) from rollout evaluation. Arrows indicate improvement direction;\nbold\ndenotes best.\nModel\nMean Reward\nâ†‘\n\\uparrow\nColl./1k Steps\nâ†“\n\\downarrow\nSuccess (%)\nâ†‘\n\\uparrow\nBC Loss (\nÃ—\n10\nâˆ’\n2\n\\times 10^{-2}\n)\nâ†“\n\\downarrow\nAction Sim. (%)\nâ†‘\n\\uparrow\nADE (m)\nâ†“\n\\downarrow\nFDE (m)\nâ†“\n\\downarrow\nPPO + Uniform Sampling\n85.2\nÂ±\n\\pm\n4.1\n0.63\nÂ±\n\\pm\n0.12\n78.1\nÂ±\n\\pm\n3.2\n17.1\nÂ±\n\\pm\n1.4\n65.3\nÂ±\n\\pm\n4.1\n5.25\nÂ±\n\\pm\n0.31\n11.8\nÂ±\n\\pm\n0.65\n+ FSM Replay\n120.4\nÂ±\n\\pm\n3.8\n(+41%)\n0.30\nÂ±\n\\pm\n0.08\n88.4\nÂ±\n\\pm\n2.1\n12.3\nÂ±\n\\pm\n1.1\n75.1\nÂ±\n\\pm\n3.5\n4.10\nÂ±\n\\pm\n0.27\n9.5\nÂ±\n\\pm\n0.58\n+ Diffusion Planner\n155.1\nÂ±\n\\pm\n3.2\n(+29%)\n0.15\nÂ±\n\\pm\n0.05\n92.0\nÂ±\n\\pm\n1.8\n13.0\nÂ±\n\\pm\n1.0\n80.2\nÂ±\n\\pm\n3.0\n3.15\nÂ±\n\\pm\n0.22\n7.2\nÂ±\n\\pm\n0.49\n+ LAM + SAEC (Ours)\n180.7\nÂ±\n\\pm\n2.9\n(+16%)\n0.05\nÂ±\n\\pm\n0.03\n96.3\nÂ±\n\\pm\n1.2\n7.4\nÂ±\n\\pm\n0.8\n85.7\nÂ±\n\\pm\n2.4\n2.45\nÂ±\n\\pm\n0.18\n5.1\nÂ±\n\\pm\n0.41\nIV\nSimulations and Results\nIV-A\nExperimental Setup\nAll experiments are done in the Webots simulator. Webots provides realistic vehicle motion and flexible scene settings. The virtual city has multi-lane curved roads, moving obstacles, and different lighting conditions from bright day to dark evening. This setup makes the tests close to real urban driving. For reproducibility, all details of the perception backbone are given. It takes a dictionary-style observation that includes camera images and LiDAR scans.\n1.\nInput Data:\nThe model takes two input streams. The visual stream is a 4-channel tensor\nI\nt\nâ€²\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n4\nI^{\\prime}_{t}\\in\\mathbb{R}^{H\\times W\\times 4}\n, formed from a downscaled RGB image and the LAM output. The auxiliary stream is a 1D LiDAR scan\nL\nt\nâˆˆ\nâ„\nN\nbeams\nL_{t}\\in\\mathbb{R}^{N_{\\text{beams}}}\nthat provides distance measurements in all directions.\n2.\nVisual Encoder:\nA three-layer convolutional network with kernel sizes 5, 3, and 3 processes the visual input. It uses ReLU activations and stride 2 for downsampling, and a final flatten layer produces a compact visual feature vector\nz\nvision\nz_{\\text{vision}}\n.\n3.\nLiDAR Encoder:\nThe LiDAR vector is first normalized and then passed through a three-layer MLP with ReLU activations. This network produces a 32-dimensional embedding\nz\nlidar\nz_{\\text{lidar}}\nthat captures the scene geometry.\n4.\nFeature Fusion:\nThe two latents are concatenated and polished by a small fusion MLP to produce the final state\ns\nt\nâˆˆ\nâ„\n512\ns_{t}\\in\\mathbb{R}^{512}\n:\nğ’”\nt\n=\nMLP\nfusion\nâ€‹\n(\nconcat\nâ€‹\n[\nğ’›\nvision\n,\nğ’›\nlidar\n]\n)\n.\n\\bm{s}_{t}=\\text{MLP}_{\\text{fusion}}(\\text{concat}[\\bm{z}_{\\text{vision}},\\bm{z}_{\\text{lidar}}]).\n(29)\nThis fused vector gives the policy a complete view of the scene, with fine local structure from LiDAR and global context from vision, and it is used in both the imitation warm-up and the later online RL training.\n5.\nEnvironment and Episode Termination:\nA custom Gym wrapper updates observations at 10 Hz. At each step, it provides an\nH\nÃ—\nW\nÃ—\n4\nH\\times W\\times 4\nvisual tensor (RGB plus mask) and an\nN\nbeams\nN_{\\text{beams}}\n-beam LiDAR scan. An episode ends either when a collision occurs (\nd\nmin\n<\nd\ncollision\nd_{\\min}<d_{\\text{collision}}\n) or when the goal is reached. A successful episode is one that reaches the goal without any violations.\n6.\nTraining Procedure:\nTraining followed the two-phase safety-aware curriculum shown in Algorithm\n1\n, for a total of\nN\ntotal\nN_{\\text{total}}\ntimesteps. All transitions were stored in FSM-partitioned replay buffer, so that rare safety-related events are well represented. The main hyperparameters are given in Table\nII\n.\nTABLE II:\nTraining hyperparameters\nParameter\nValue (from code)\nPPO Steps / Batch / Epochs\n2048 / 64 / 10\nDiscount Factor (\nÎ³\n\\gamma\n) / GAE-\nÎ»\n\\lambda\n0.99 / 0.95\nBC / PPO Learning Rate\n3\nÃ—\n10\nâˆ’\n4\n3\\times 10^{-4}\nDiffusion Horizon (\nH\nH\n) / Diffusion Steps (\nT\nT\n)\n8 / 100\nTotal Buffer Capacity\n50,000 transitions\nIRL Reward Weight (\nw\nirl\nw_{\\text{irl}}\n)\n0.3\nEnvironment Reward Weight (\nw\nenv\nw_{\\text{env}}\n)\n0.7\nImitation Phase Duration (\nN\nimitation\nN_{\\text{imitation}}\n)\n20,000 steps\nMixed Phase Duration (\nN\nmixed\nN_{\\text{mixed}}\n)\n30,000 steps\nTotal Training Steps (\nN\ntotal\nN_{\\text{total}}\n)\n50,000 steps\nBC Update Interval (Imitation)\nevery 1,000 steps\nBC Update Interval (Mixed)\nevery 500 steps\nDiffusion Training Interval\nevery 500 steps\nDiscriminator Update Interval (\nT\ndisc\nT_{\\text{disc}}\n)\nevery 2,000 steps\nFeature Sync Interval (\nT\nsync\nT_{\\text{sync}}\n)\nevery 1,000 steps\nL2 Regularization (\nÎ»\nL2\n\\lambda_{\\text{L2}}\n)\n10\nâˆ’\n6\n10^{-6}\nPPO Clip Range\n0.2\nEntropy Coefficient (\nc\n2\nc_{2}\n)\n0.01\nValue Loss Coefficient (\nc\n1\nc_{1}\n)\n0.5\nGradient Clip Norm (\nG\nmax\nG_{\\max}\n)\n0.5\nLearning Rate Scheduler\nReduce on Plateau (patience=5)\nLiDAR Beams (\nN\nbeams\nN_{\\text{beams}}\n)\n180\nImage Size (\nH\nÃ—\nW\nH\\times W\n)\n64\nÃ—\n64\n64\\times 64\nCollision Threshold (\nd\ncollision\nd_{\\text{collision}}\n)\n1.0 m\nDAL Trigger:\nd\ntrigger\nd_{\\text{trigger}}\n/\ne\nlane\ne_{\\text{lane}}\n3.0 m / 120 px\nCritical Blend Threshold (\nd\ncritical\nd_{\\text{critical}}\n)\n1.5 m\nHazard Scale (\ns\nh\ns_{h}\n)\n3.0\nBlend Hazard Scales (\ns\n1\ns_{1}\n,\ns\n2\ns_{2}\n,\nk\ntanh\nk_{\\text{tanh}}\n)\n2.0 / 20 / 0.3\nDAL Guidance Weight (\nw\ng\nw_{g}\n)\n0.1\nEnergy Weights (\nw\njerk\nw_{\\text{jerk}}\n,\nw\nstab\nw_{\\text{stab}}\n,\nw\nexp\nw_{\\text{exp}}\n)\n0.1 / 0.5 / 2.0\nLane Scale (\ns\nlane\ns_{\\text{lane}}\n) / LiDAR Scale (\ns\nlidar\ns_{\\text{lidar}}\n)\n0.5 / 2.0\nLAM Alphas Initial Value\n0.5\nOptimizer\nAdam (\nÎ²\n1\n=\n0.9\n\\beta_{1}=0.9\n,\nÎ²\n2\n=\n0.999\n\\beta_{2}=0.999\n)\nIV-B\nComponent-wise Evaluation of the IRLâ€“DAL Framework\nTo measure the effect of each component, a clear ablation study was carried out. Four model variants were trained under the same conditions, adding components one by one to separate their individual contributions. The evaluated variants are as follows: The Baseline (PPO + Uniform Sampling) is the standard PPO algorithm with a uniform replay buffer. The second variant, + Structured Replay, builds upon the baseline model by incorporating FSM-aware replay, which aims to provide more coverage to safety-critical states. The third variant, + Generative Planner, includes the structured replay along with the diffusion-based safety planner, but without adaptive perception. Finally, the + LAM + SAEC (Full IRL-DAL) represents the full model, incorporating the adaptive mask, safety-aware experience correction, and all components enabled. These results are summarized in Table\nI\nand visualized in Fig.\n6\n, which display reward curves, BC alignment, and intervention decay.\nFigure 6:\nTraining dynamics over 50k steps. After switching to mixed-mode at 20k steps, the full IRLâ€“DAL model (\nDiff+Adapt+SAEC\n) rockets ahead in reward and stability. BC loss stays pinned low, and DAL interventions fade to near-zeroâ€”proof of\nadaptive distillation\n. See Table\nI\nfor final numbers.\nFigure 7:\nQualitative ablation study on a high-curvature turn scenario in Webots. The\nBaseline PPO (Red)\nfails to negotiate the curve, resulting in a collision. Adding\nFSM Replay (Orange)\nimproves lane adherence but exhibits oscillatory behavior. The\nDiffusion Planner (Yellow)\nsuccessfully ensures safety but deviates from the center. The full\nIRLâ€“DAL framework (Green)\ndemonstrates superior trajectory smoothness and precise lane centering, attributing to the LAMâ€™s focused attention on road boundaries.\nStructured replay and safety awareness have a key role in the results. Using only the FSM buffer increases the reward by 41% and reduces collisions by 52% (from 0.63 to 0.30 per 1k steps), which shows that rare events need special attention. Adding DAL increases the reward by another 29% and again cuts collisions by half (from 0.30 to 0.15), so it not only reacts to danger but also helps prevent it (Fig.\n7\n). With the full model, including adaptive perception (LAM) and SAEC, collisions drop to 0.05, which is a 67% reduction compared to the previous step and 12.6 times better than the baseline. At the same time, BC loss stays low and action similarity to the expert increases, so expert-like behavior is reached without sacrificing stability.\nV\nConclusion\nThis work introduced IRL-DAL, a unified framework for safe and adaptive autonomous driving that combines inverse reinforcement learning, diffusion planning, and on-policy control. The method links three key ideas in a single loop: hybrid IL-IRL-RL training, a diffusion-based safety planner, and a LAM for perception. Together, these parts let the agent learn an expert-like driving policy that remains stable under online interaction and reacts safely in high-risk situations.\nThe framework was evaluated in the Webots simulator using a two-phase curriculum. In the first phase, an FSM expert and behavioral cloning provided a safe and reliable starting point for both the policy and the diffusion planner. In the second phase, PPO with a hybrid reward and GAIL-based IRL signals refined the behavior while DAL and SAEC enforced safety and turned interventions into useful training data. The final agent reached a 96% success rate and reduced collisions to 0.05 per 1k steps. Ablation studies showed clear gains from each component: FSM-aware replay improved coverage of rare events, the diffusion planner reduced further failures, and LAM plus SAEC delivered the larger safety and performance gains.\nReferences\nSeyed Ahmad Hosseini Miangoleh\nis currently pursuing the B.S. degree in electrical engineering (control) at Amirkabir University of Technology, Tehran, Iran. His research interests include control systems, robotics, machine learning, deep learning, reinforcement learning, and computer vision.\nAmin Jalal Aghdasian\nreceived the B.S. degree in electrical engineering from Tabriz University, Tabriz, Iran, in 2018, and the M.S. degree in mechatronics engineering from Amirkabir University of Technology, Tehran, Iran, in 2023. His research interests include intelligent automotive systems, robust control, machine learning, neural networks, robotics, and reinforcement learning.\nFarzaneh Abdollahi\n(Senior Member, IEEE) received the B.Sc. degree in electrical engineering from the Isfahan University of Technology, Isfahan, Iran, in 1999, the M.Sc. degree in electrical engineering from the Amirkabir University of Technology, Tehran, Iran, in 2003, and the Ph.D. degree in electrical engineering from Concordia University, Montreal, QC, Canada, in 2008. She is currently an Associate Professor with the Amirkabir University of Technology and an adjunct Professor with Carleton University. Her research interests include intelligent control, robotics, control of nonlinear systems, control of multiagent networks, and robust and switching control.",
    "preview_text": "This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.\n\nIRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models\nSeyed Ahmad Hosseini Miangoleh\n1\n,\nAmin Jalal Aghdasian\n1\n,\nFarzaneh Abdollahi\n1\n1\nDepartment of Electrical Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran\nAbstract\nThis paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combin",
    "is_relevant": true,
    "relevance_score": 7.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "Diffusion",
        "fine tune",
        "offline Reinforcement Learning",
        "goal conditioned reinforcement learning"
    ],
    "one_line_summary": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’Œé€†å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å®ç°å®‰å…¨å¯¼èˆªã€‚",
    "detailed_summary": "## è®ºæ–‡æ‘˜è¦ï¼šIRL-DAL: åŸºäºèƒ½é‡å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„å®‰å…¨è‡ªé€‚åº”è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’\n\n### 1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº\nè‡ªåŠ¨é©¾é©¶çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ„å»ºèƒ½åœ¨é«˜åº¦åŠ¨æ€ç¯å¢ƒä¸­å®ç°äººç±»æ°´å¹³å®‰å…¨æ€§ä¸å¯é æ€§çš„ç³»ç»Ÿã€‚ç°æœ‰æ–¹æ³•å¸¸å°†ç”Ÿæˆå¼è§„åˆ’ã€å¥–åŠ±æ¨æ–­ä¸åœ¨çº¿ç­–ç•¥å­¦ä¹ åˆ†ç¦»ï¼Œå¯¼è‡´**è§„åˆ’ä¸æ§åˆ¶è„±èŠ‚ã€å®‰å…¨æƒè¡¡å›ºå®šã€å­¦ä¹ ä¿¡å·ä½æ•ˆ**ç­‰é—®é¢˜ã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ä¸ªèƒ½ç»Ÿä¸€**å­¦ä¹ æ•ˆç‡ã€å†³ç­–å®‰å…¨ä¸æ„ŸçŸ¥è‡ªé€‚åº”**çš„æ¡†æ¶ã€‚\n\n### 2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°\næœ¬æ–‡æå‡º **IRL-DAL** æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸‰ä¸ªç´§å¯†è€¦åˆçš„ç»„ä»¶ï¼š\n- **æ··åˆIL-IRL-RLè®­ç»ƒæµç¨‹**ï¼šç»“åˆ**è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰** è¿›è¡Œç¨³å®šåˆå§‹åŒ–ï¼Œå¹¶åˆ©ç”¨**ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼ˆGAILï¼‰** æ¨æ–­ä¸“å®¶å¥–åŠ±ï¼Œæœ€åé€šè¿‡**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰** ä¸æ··åˆå¥–åŠ±è¿›è¡Œå¾®è°ƒï¼Œå®ç°æ ·æœ¬é«˜æ•ˆä¸”å¯¹é½ä¸“å®¶æ„å›¾çš„ç­–ç•¥å­¦ä¹ ã€‚\n- **æŒ‰éœ€è§¦å‘çš„èƒ½é‡å¼•å¯¼æ‰©æ•£è§„åˆ’å™¨ï¼ˆDALï¼‰**ï¼šä½œä¸ºä¸€ä¸ª**å®‰å…¨ç›‘ç£å™¨**ï¼Œä»…åœ¨é«˜é£é™©çŠ¶æ€è¢«æ¿€æ´»ã€‚å®ƒé€šè¿‡ä¸€ä¸ªå¤åˆèƒ½é‡å‡½æ•°ï¼ˆæƒ©ç½šç¢°æ’ã€åç¦»è½¦é“ã€æ§åˆ¶æŠ–åŠ¨ç­‰ï¼‰å¼•å¯¼åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œç”ŸæˆçŸ­æœŸå®‰å…¨è½¨è¿¹ï¼Œå¹¶ä¸ä¸»ç­–ç•¥åŠ¨ä½œå¹³æ»‘èåˆã€‚\n- **å¯å­¦ä¹ è‡ªé€‚åº”æ©ç ï¼ˆLAMï¼‰**ï¼šä¸€ä¸ªè½»é‡çº§æ„ŸçŸ¥æ¨¡å—ï¼Œèƒ½æ ¹æ®**è½¦è¾†é€Ÿåº¦**å’Œ**æ¿€å…‰é›·è¾¾æµ‹è·**åŠ¨æ€è°ƒæ•´ç©ºé—´æ³¨æ„åŠ›ã€‚é«˜é€Ÿæ—¶å¢å¼ºå¯¹è½¦é“ä¸‹éƒ¨çš„å…³æ³¨ä»¥ä¿æŒç¨³å®šï¼Œæ¥è¿‘éšœç¢ç‰©æ—¶åˆ™èšç„¦è¿‘åœºåŒºåŸŸä»¥æå‡å®‰å…¨æ€§ã€‚\n\n### 3. ä¸»è¦å®éªŒç»“æœ\nåœ¨Webotsä»¿çœŸå™¨ä¸­é‡‡ç”¨ä¸¤é˜¶æ®µè¯¾ç¨‹è¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ï¼š\n- **æ€§èƒ½æŒ‡æ ‡**ï¼šæœ€ç»ˆæ™ºèƒ½ä½“å®ç°äº†**96%çš„æˆåŠŸç‡**ï¼Œå¹¶å°†ç¢°æ’ç‡é™è‡³**æ¯åƒæ­¥0.05æ¬¡**ï¼Œè®¾ç«‹äº†å®‰å…¨å¯¼èˆªçš„æ–°åŸºå‡†ã€‚\n- **æ¶ˆèå®éªŒ**ï¼šé€é¡¹æ·»åŠ ç»„ä»¶ï¼ˆFSMç»éªŒå›æ”¾ã€æ‰©æ•£è§„åˆ’å™¨ã€LAM+SAECï¼‰å‡å¸¦æ¥æ˜¾è‘—æå‡ã€‚å®Œæ•´æ¡†æ¶ç›¸æ¯”åŸºçº¿PPOï¼Œå¹³å‡å¥–åŠ±æå‡112%ï¼Œç¢°æ’ç‡é™ä½92%ï¼Œè¯æ˜äº†å„æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚\n- **å®šæ€§åˆ†æ**ï¼šå®Œæ•´æ¡†æ¶åœ¨å¼¯é“ç­‰å¤æ‚åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¹³æ»‘çš„è½¨è¿¹å’Œç²¾ç¡®çš„è½¦é“å±…ä¸­èƒ½åŠ›ã€‚\n\n### 4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼\næœ¬å·¥ä½œé€šè¿‡å°†**æ¨¡ä»¿å­¦ä¹ ã€é€†å¼ºåŒ–å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€ç”Ÿæˆå¼è§„åˆ’ä¸è‡ªé€‚åº”æ„ŸçŸ¥**é›†æˆåˆ°ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç»Ÿä¸€æ¡†æ¶ä¸­ï¼Œæœ‰æ•ˆè§£å†³äº†å­¦ä¹ ç¨³å®šæ€§ã€é•¿æœŸå®‰å…¨ä¿è¯ä¸ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¹‹é—´çš„è€¦åˆæŒ‘æˆ˜ã€‚å…¶æå‡ºçš„**å®‰å…¨æ„ŸçŸ¥ç»éªŒæ ¡æ­£ï¼ˆSAECï¼‰æœºåˆ¶**èƒ½å°†å®‰å…¨å¹²é¢„è½¬åŒ–ä¸ºé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼Œä¿ƒè¿›äº†å®‰å…¨è¡Œä¸ºçš„æŒç»­å­¦ä¹ ã€‚è¯¥æ¡†æ¶ä¸ºæ„å»º**æ›´é²æ£’ã€æ‹ŸäººåŒ–ä¸”å¯è§£é‡Š**çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æœ‰å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## è®ºæ–‡æ ¸å¿ƒåˆ†æï¼šIRL-DAL\n\n### **ä¸€ã€ è®ºæ–‡æ—¨åœ¨è§£å†³çš„æ ¸å¿ƒé—®é¢˜**\nè®ºæ–‡æ—¨åœ¨è§£å†³**è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’ä¸­å®‰å…¨æ€§ã€å­¦ä¹ æ•ˆç‡ä¸é€‚åº”æ€§ä¹‹é—´çš„æ ¸å¿ƒçŸ›ç›¾**ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒé’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼š\n1.  **ç³»ç»Ÿå‰²è£‚**ï¼šç”Ÿæˆå¼è§„åˆ’å™¨ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰é€šå¸¸ä»¥å¼€ç¯æ–¹å¼ç”Ÿæˆè½¨è¿¹ï¼Œä¸é—­ç¯å¼ºåŒ–å­¦ä¹ ç­–ç•¥åˆ†ç¦»ï¼Œå¯¼è‡´è§„åˆ’ä¸æ§åˆ¶ä¸åŒ¹é…ï¼Œåœ¨æ‰°åŠ¨ä¸‹é²æ£’æ€§å·®ã€‚\n2.  **å®‰å…¨æƒè¡¡åƒµåŒ–**ï¼šä¼ ç»Ÿçš„å®‰å…¨çº¦æŸï¼ˆå¦‚è½¦é“ä¿æŒã€é¿éšœï¼‰æƒé‡å›ºå®šï¼Œæ— æ³•æ ¹æ®åœºæ™¯é£é™©ï¼ˆå¦‚è½¦é€Ÿã€éšœç¢ç‰©è·ç¦»ï¼‰å’Œæ„ŸçŸ¥ä¸ç¡®å®šæ€§è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œå¯¼è‡´è¡Œä¸ºåœ¨â€œè¿‡åº¦ä¿å®ˆâ€å’Œâ€œè¿‡äºæ¿€è¿›â€ä¹‹é—´æŒ¯è¡ã€‚\n3.  **å­¦ä¹ ä¿¡å·ä½æ•ˆ**ï¼šä¾èµ–äººå·¥è®¾è®¡çš„ç¨€ç–å¥–åŠ±ä¿¡å·ï¼Œå¯¼è‡´è®­ç»ƒæ ·æœ¬éœ€æ±‚å¤§ã€æ”¶æ•›ä¸ç¨³å®šï¼Œä¸”éš¾ä»¥å­¦åˆ°ä¸ä¸“å®¶è¡Œä¸ºå¯¹é½çš„å¹³æ»‘ã€æ‹ŸäººåŒ–é©¾é©¶é£æ ¼ã€‚\n\n### **äºŒã€ æ ¸å¿ƒåˆ›æ–°ç‚¹**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸º **IRL-DAL** çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ä¸‰å¤§äº’è¡¥çš„æŠ€æœ¯ç»„ä»¶ååŒè§£å†³ä¸Šè¿°é—®é¢˜ï¼š\n\n#### **1. æ··åˆIL-IRL-RLè®­ç»ƒæµç¨‹**\n*   **åˆ›æ–°å†…å®¹**ï¼šè®¾è®¡äº†ä¸€ä¸ªç»“æ„åŒ–çš„å¤šé˜¶æ®µå­¦ä¹ è¯¾ç¨‹ã€‚\n    *   **é˜¶æ®µä¸€ï¼ˆæ¨¡ä»¿é¢„çƒ­ï¼‰**ï¼šä½¿ç”¨**è¡Œä¸ºå…‹éš†**åœ¨**æœ‰é™çŠ¶æ€æœºä¸“å®¶æ§åˆ¶å™¨**ç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œåˆå§‹åŒ–ï¼Œä¸ºç­–ç•¥å’Œæ‰©æ•£è§„åˆ’å™¨æä¾›ä¸€ä¸ªå®‰å…¨ã€ç¨³å®šçš„èµ·ç‚¹ã€‚\n    *   **é˜¶æ®µäºŒï¼ˆåœ¨çº¿å¾®è°ƒï¼‰**ï¼šä½¿ç”¨**è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–**è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä½†å…¶å¥–åŠ±å‡½æ•°æ˜¯**æ··åˆå¥–åŠ±**ã€‚\n*   **æŠ€æœ¯ç»†èŠ‚**ï¼š\n    *   **æ··åˆå¥–åŠ±**ï¼š`r_total = w_env * r_env + w_irl * r_irl`\n        *   `r_env`ï¼šç¯å¢ƒå®šä¹‰çš„ç¨€ç–å¥–åŠ±ï¼ˆå¦‚è½¦é“å±…ä¸­ã€é¿éšœï¼‰ã€‚\n        *   `r_irl`ï¼šé€šè¿‡**ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ **ä»ä¸“å®¶æ•°æ®ä¸­æ¨æ–­å‡ºçš„**å¯†é›†æ„å›¾å¥–åŠ±**ï¼Œé©±ä½¿ç­–ç•¥æ¨¡ä»¿ä¸“å®¶çš„å†³ç­–é£æ ¼ã€‚\n    *   **FSMæ„ŸçŸ¥ç»éªŒå›æ”¾**ï¼šä¸“å®¶æ•°æ®æŒ‰FSMçŠ¶æ€ï¼ˆè½¦é“è·Ÿéšã€é¿éšœç­‰ï¼‰åˆ†åŒºå­˜å‚¨ï¼Œè®­ç»ƒæ—¶å‡è¡¡é‡‡æ ·ï¼Œç¡®ä¿æ¨¡å‹èƒ½å……åˆ†å­¦ä¹ åˆ°ç½•è§ä½†å…³é”®çš„å±é™©è¾¹è§’æ¡ˆä¾‹ã€‚\n*   **è§£å†³ä»€ä¹ˆé—®é¢˜**ï¼šè§£å†³äº†**çº¯RLæ ·æœ¬æ•ˆç‡ä½ã€ä¸ç¨³å®š**ä»¥åŠ**çº¯ILå­˜åœ¨åå˜é‡åç§»**çš„é—®é¢˜ã€‚é€šè¿‡BCå¿«é€Ÿè·å¾—å®‰å…¨åŸºçº¿ï¼Œé€šè¿‡IRLæä¾›å¯†é›†ã€ä¸ä¸“å®¶ç›®æ ‡å¯¹é½çš„å­¦ä¹ ä¿¡å·ï¼Œé€šè¿‡RLè¿›è¡Œæ¢ç´¢å’Œä¼˜åŒ–ï¼Œå®ç°äº†**é«˜æ•ˆã€ç¨³å®šä¸”ä¸ä¸“å®¶æ„å›¾å¯¹é½çš„ç­–ç•¥å­¦ä¹ **ã€‚\n\n#### **2. åŸºäºèƒ½é‡å¼•å¯¼çš„æŒ‰éœ€æ‰©æ•£å®‰å…¨è§„åˆ’å™¨**\n*   **åˆ›æ–°å†…å®¹**ï¼šæå‡ºä¸€ä¸ª**æ¡ä»¶æ‰©æ•£æ¨¡å‹**ä½œä¸ºâ€œå®‰å…¨ç›‘ç£å‘˜â€ï¼Œä½†**å¹¶éæŒç»­è¿è¡Œ**ï¼Œè€Œæ˜¯ä»…åœ¨æ£€æµ‹åˆ°é«˜é£é™©çŠ¶æ€æ—¶è¢«è§¦å‘ã€‚\n*   **æŠ€æœ¯ç»†èŠ‚**ï¼š\n    *   **è§¦å‘æœºåˆ¶**ï¼šå½“æœ€è¿‘LiDARè·ç¦» `(d_min < d_trigger)` æˆ–è½¦é“æ¨ªå‘åå·® `(|d_lane| > e_lane)` æ—¶æ¿€æ´»ã€‚\n    *   **èƒ½é‡å¼•å¯¼é‡‡æ ·**ï¼šåœ¨æ‰©æ•£æ¨¡å‹çš„åå‘å»å™ªè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ä¸€ä¸ª**å¤åˆèƒ½é‡å‡½æ•°E**çš„æ¢¯åº¦æ¥å¼•å¯¼è½¨è¿¹ç”Ÿæˆï¼Œä½¿å…¶è¶‹å‘å®‰å…¨åŒºåŸŸã€‚\n        ```python\n        # èƒ½é‡å‡½æ•°ç¤ºä¾‹ï¼ˆç®€åŒ–ï¼‰\n        E = w_lane * E_lane(åç¦»è½¦é“ä¸­å¿ƒ) + \n            w_lidar * E_lidar(æ¥è¿‘éšœç¢ç‰©) + \n            w_jerk * E_jerk(æ§åˆ¶æŠ–åŠ¨) + \n            w_stab * E_stability(ç¨³å®šæ€§)\n        # å¼•å¯¼å…¬å¼\n        A_guided = A_denoised - w_g * (âˆ‡E / (||âˆ‡E|| + Îµ))\n        ```\n    *   **åŠ¨ä½œæ··åˆä¸æ‰§è¡Œ**ï¼šå°†æ‰©æ•£è§„åˆ’å™¨ç”Ÿæˆçš„ç¬¬ä¸€ä¸ªå®‰å…¨åŠ¨ä½œ `(a_t^DAL)` ä¸PPOç­–ç•¥åŠ¨ä½œ `(a_t^PPO)` è¿›è¡ŒåŠ¨æ€åŠ æƒæ··åˆåæ‰§è¡Œã€‚\n*   **è§£å†³ä»€ä¹ˆé—®é¢˜**ï¼šè§£å†³äº†**å¼€ç¯è§„åˆ’ä¸é—­ç¯æ§åˆ¶è„±èŠ‚**ä»¥åŠ**å›ºå®šå®‰å…¨çº¦æŸä¸çµæ´»**çš„é—®é¢˜ã€‚å®ç°äº† **â€œæŒ‰éœ€â€çš„å®æ—¶å®‰å…¨å¹²é¢„**ï¼Œå°†ç”Ÿæˆå¼æ¨¡å‹çš„é•¿æ—¶ç¨‹è§„åˆ’èƒ½åŠ›ä¸RLç­–ç•¥çš„å®æ—¶ååº”èƒ½åŠ›ç›¸ç»“åˆï¼Œå¹¶ä¸”èƒ½é‡æƒé‡å¯æ ¹æ®å®æ—¶é£é™© `(h_t)` åŠ¨æ€è°ƒæ•´ã€‚\n\n#### **3. å¯å­¦ä¹ è‡ªé€‚åº”æ©ç **\n*   **åˆ›æ–°å†…å®¹**ï¼šæå‡ºä¸€ä¸ªè½»é‡çº§çš„**å¯å­¦ä¹ è‡ªé€‚åº”æ©ç **æ¨¡å—ï¼Œç”¨äºå®ç°**ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†è§‰æ³¨æ„åŠ›åŠ¨æ€è°ƒåˆ¶**ã€‚\n*   **æŠ€æœ¯ç»†èŠ‚**ï¼š\n    *   **è¾“å…¥**ï¼šå½’ä¸€åŒ–çš„è½¦è¾†é€Ÿåº¦ `(v_t_norm)` å’ŒåŸºäºLiDARçš„å³æ—¶å±é™©ç¨‹åº¦ `(h_t)`ã€‚\n    *   **æœºåˆ¶**ï¼šé€šè¿‡ä¸¤ä¸ªå¯å­¦ä¹ å‚æ•° `(Î±_speed, Î±_lidar)`ï¼Œç”Ÿæˆä¸€ä¸ª**å‚ç›´æ¢¯åº¦æ©ç **ï¼Œä¸RGBå›¾åƒé€šé“æ‹¼æ¥åè¾“å…¥CNNã€‚\n        *   **é«˜é€Ÿæ—¶**ï¼š`Î±_speed` å¢å¤§æ©ç å¼ºåº¦ï¼Œå¢å¼ºå¯¹è½¦é“ä¸‹éƒ¨ï¼ˆè¿‘å¤„é“è·¯ï¼‰ç‰¹å¾çš„å…³æ³¨ï¼Œä»¥ä¿æŒæ¨ªå‘ç¨³å®šæ€§ã€‚\n        *   **æ¥è¿‘éšœç¢ç‰©æ—¶**ï¼š`Î±_lidar` å¢å¤§æ©ç å¼ºåº¦ï¼Œèšç„¦è½¦è¾†è¿‘å‘¨åŒºåŸŸï¼Œæå‡é¿éšœç²¾åº¦ã€‚\n    *   **è®­ç»ƒ**ï¼šä¸BCç­–ç•¥ä¸€èµ·è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚\n*   **è§£å†³ä»€ä¹ˆé—®é¢˜**ï¼šè§£å†³äº†**æ ‡å‡†è§†è§‰ç¼–ç å™¨å¯¹å›¾åƒåŒºåŸŸå¹³ç­‰å¤„ç†ã€æ— æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯**çš„é—®é¢˜ã€‚æä¾›äº†ä¸€ç§**å¯è§£é‡Šã€ä½è®¡ç®—å¼€é”€çš„æ³¨æ„åŠ›æœºåˆ¶**ï¼Œä½¿æ„ŸçŸ¥ç³»ç»Ÿèƒ½æ ¹æ®é©¾é©¶æƒ…å¢ƒï¼ˆé€Ÿåº¦ã€å±é™©ï¼‰åŠ¨æ€èšç„¦äºå…³é”®åŒºåŸŸï¼Œæå‡äº†çŠ¶æ€è¡¨å¾çš„è´¨é‡ã€‚\n\n#### **4. å®‰å…¨æ„ŸçŸ¥ç»éªŒæ ¡æ­£**\n*   **åˆ›æ–°å†…å®¹**ï¼šå½“æ‰©æ•£è§„åˆ’å™¨ä»‹å…¥äº§ç”Ÿå®‰å…¨åŠ¨ä½œåï¼Œå­˜å…¥PPOç»éªŒå›æ”¾ç¼“å†²åŒºçš„**ä¸æ˜¯è¯¥å®‰å…¨åŠ¨ä½œ**ï¼Œè€Œæ˜¯**FSMä¸“å®¶é’ˆå¯¹å½“å‰çŠ¶æ€æœ¬åº”æ‰§è¡Œçš„åŠ¨ä½œ**ã€‚\n*   **è§£å†³ä»€ä¹ˆé—®é¢˜**ï¼šç¡®ä¿äº†ç­–ç•¥æœ€ç»ˆå­¦ä¹ çš„æ˜¯**ä¸“å®¶åœ¨å±é™©çŠ¶æ€ä¸‹çš„æ¢å¤é€»è¾‘**ï¼Œè€Œéä¸´æ—¶â€œä¿®è¡¥â€åŠ¨ä½œã€‚åŒæ—¶ï¼Œå®‰å…¨å¹²é¢„é¿å…äº†è®­ç»ƒæ—©æœŸå› ç¢°æ’å¯¼è‡´çš„å›åˆè¿‡æ—©ç»ˆæ­¢ï¼Œè®©æ™ºèƒ½ä½“èƒ½åœ¨å®‰å…¨é˜²æŠ¤ä¸‹æ”¶é›†æ›´å¤šé«˜è´¨é‡çš„ä¸“å®¶æ•°æ®ï¼Œ**å°†å®‰å…¨å¹²é¢„è½¬åŒ–ä¸ºæœ‰æ•ˆçš„å­¦ä¹ ç»éªŒ**ã€‚\n\n### **ä¸‰ã€ è§£å†³æ–¹æ¡ˆæ€»ç»“**\nè®ºæ–‡é€šè¿‡ä¸€ä¸ª**é«˜åº¦é›†æˆçš„æ¡†æ¶**ï¼Œå°†**æ¨¡ä»¿å­¦ä¹ ã€é€†å¼ºåŒ–å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€ç”Ÿæˆå¼è§„åˆ’ã€è‡ªé€‚åº”æ„ŸçŸ¥**æœ‰æœºç»“åˆï¼š\n1.  **ç”¨BC+FSMå›æ”¾æ‰“åŸºç¡€**ï¼Œè·å¾—å®‰å…¨ã€å¤šæ ·çš„ä¸“å®¶è¡Œä¸ºå…ˆéªŒã€‚\n2.  **ç”¨IRLï¼ˆGAILï¼‰æä¾›å¯†é›†ã€æ‹Ÿäººçš„å­¦ä¹ ç›®æ ‡**ï¼Œå¼•å¯¼ç­–ç•¥ä¼˜åŒ–æ–¹å‘ã€‚\n3.  **ç”¨PPOè¿›è¡Œåœ¨çº¿æ¢ç´¢å’Œå¾®è°ƒ**ï¼Œæå‡ç­–ç•¥æ€§èƒ½ã€‚\n4.  **ç”¨LAMå®ç°æƒ…å¢ƒåŒ–æ„ŸçŸ¥**ï¼Œæå‡çŠ¶æ€è¡¨å¾çš„æ•ˆç‡å’Œé’ˆå¯¹æ€§ã€‚\n5.  **ç”¨èƒ½é‡å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºæŒ‰éœ€å®‰å…¨ç½‘**ï¼Œåœ¨å±é™©æ—¶æä¾›ä¿éšœå¹¶åŠ¨æ€è°ƒæ•´å®‰å…¨è¾¹ç•Œã€‚\n6.  **ç”¨SAECæœºåˆ¶å°†å®‰å…¨å¹²é¢„è½¬åŒ–ä¸ºç­–ç•¥å­¦ä¹ **ï¼Œå®ç°å®‰å…¨æ€§çš„æŒç»­å†…åŒ–ã€‚\n\n### **å››ã€ å®é™…ä»·å€¼ä¸æ•ˆæœ**\n*   **æ€§èƒ½æå‡**ï¼šåœ¨Webotsä»¿çœŸä¸­ï¼Œå®Œæ•´æ¨¡å‹å–å¾—äº†**96.3%çš„æˆåŠŸç‡**ï¼Œç¢°æ’ç‡é™è‡³**æ¯åƒæ­¥0.05æ¬¡**ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿åŠå„æ¶ˆèç‰ˆæœ¬ã€‚\n*   **å®‰å…¨æ€§**ï¼šé€šè¿‡åŠ¨æ€å®‰å…¨è§„åˆ’å’Œç»éªŒæ ¡æ­£ï¼Œå®ç°äº†å¯¹åˆ†å¸ƒå¤–çŠ¶æ€å’Œå±é™©è¾¹è§’æ¡ˆä¾‹çš„é²æ£’å¤„ç†ã€‚\n*   **æ‹ŸäººåŒ–ä¸å¹³æ»‘æ€§**ï¼šç»“åˆIRLå¥–åŠ±ï¼Œç­–ç•¥èƒ½äº§ç”Ÿæ›´å¹³æ»‘ã€æ›´æ¥è¿‘äººç±»ä¸“å®¶çš„è½¨è¿¹ã€‚\n*   **æ•ˆç‡ä¸é€šç”¨æ€§**ï¼šæ¡†æ¶æä¾›äº†ä»æ¨¡ä»¿åˆ°å¼ºåŒ–å­¦ä¹ ã€ä»æ„ŸçŸ¥åˆ°è§„åˆ’æ§åˆ¶çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œå…¶æ¨¡å—åŒ–è®¾è®¡å…·æœ‰ä¸€å®šçš„é€šç”¨æ€§ï¼Œä¸ºæ„å»ºå®‰å…¨ã€é«˜æ•ˆã€è‡ªé€‚åº”çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
            "timestamp": "2026-02-03T16:02:34.194993",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’å‘å±•è„‰ç»œæ¢³ç†ï¼šä»æ¨¡ä»¿å­¦ä¹ åˆ°å®‰å…¨æ„ŸçŸ¥ä¸€ä½“åŒ–\n\næœ¬æ–‡æå‡ºçš„ **IRL-DAL** æ¡†æ¶æ˜¯è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’é¢†åŸŸå¤šé¡¹å…ˆè¿›æŠ€æœ¯èåˆçš„é›†å¤§æˆè€…ã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥æ¸…æ™°åœ°æ¢³ç†ä¸ºå‡ ä¸ªå…³é”®æ–¹å‘ï¼Œæ¯ä¸€æ­¥éƒ½åœ¨è§£å†³å‰åºå·¥ä½œçš„æ ¸å¿ƒç“¶é¢ˆã€‚\n\n### 1. å­¦ä¹ èŒƒå¼æ¼”è¿›ï¼šä»å•ä¸€æ¨¡ä»¿åˆ°æ··åˆå¼ºåŒ–å­¦ä¹ \n\n**å‘å±•è·¯å¾„**ï¼š çº¯æ¨¡ä»¿å­¦ä¹  (BC) â†’ çº¯å¼ºåŒ–å­¦ä¹  (RL) â†’ æ··åˆæ¨¡ä»¿ä¸å¼ºåŒ–å­¦ä¹  (IL+RL) â†’ **å¼•å…¥é€†å¼ºåŒ–å­¦ä¹ çš„æ··åˆèŒƒå¼ (IL+IRL+RL)**\n\n*   **æ—©æœŸï¼šè¡Œä¸ºå…‹éš† (BC)**\n    *   **æ ¸å¿ƒ**ï¼šç›´æ¥ä»ä¸“å®¶æ•°æ®ä¸­å­¦ä¹ çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼Œæ•°æ®æ•ˆç‡é«˜ã€‚\n    *   **å±€é™**ï¼š**åå˜é‡æ¼‚ç§»**é—®é¢˜ä¸¥é‡ã€‚åœ¨è®­ç»ƒæœªè§çš„çŠ¶æ€ä¸‹ï¼Œå°è¯¯å·®ä¼šç´¯ç§¯æ”¾å¤§ï¼Œå¯¼è‡´ç­–ç•¥å¤±æ•ˆï¼ˆå¦‚é©¶å‡ºé“è·¯ï¼‰ã€‚\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šConditional Imitation Learning ç­‰ã€‚\n\n*   **æ”¹è¿›1ï¼šå¼•å…¥å¼ºåŒ–å­¦ä¹  (RL)**\n    *   **æ ¸å¿ƒ**ï¼šé€šè¿‡è¯•é”™å’Œç¯å¢ƒåé¦ˆä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å­¦ä¹ åˆ°æ›´é²æ£’ã€é€‚åº”æ€§çš„è¡Œä¸ºã€‚\n    *   **å±€é™**ï¼š**æ ·æœ¬æ•ˆç‡æä½**ï¼Œä¸”ä¸¥é‡ä¾èµ–éš¾ä»¥è®¾è®¡çš„**æ‰‹å·¥å¥–åŠ±å‡½æ•°**ã€‚ä¸åˆç†çš„å¥–åŠ±ä¼šå¯¼è‡´ç­–ç•¥å­¦ä¹ ä¸ç¨³å®šæˆ–è¡Œä¸ºæ€ªå¼‚ã€‚\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šä½¿ç”¨PPOã€SACç­‰ç®—æ³•åœ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸­è®­ç»ƒã€‚\n\n*   **æ”¹è¿›2ï¼šæ··åˆæ¨¡ä»¿ä¸å¼ºåŒ–å­¦ä¹  (IL+RL)**\n    *   **æ ¸å¿ƒ**ï¼šå…ˆç”¨BCå¿«é€Ÿè·å¾—ä¸€ä¸ªå®‰å…¨çš„åˆå§‹ç­–ç•¥ï¼Œå†ç”¨RLè¿›è¡Œå¾®è°ƒå’Œæå‡ã€‚ç»“åˆäº†BCçš„æ•°æ®æ•ˆç‡å’ŒRLçš„æ¢ç´¢ä¼˜åŒ–èƒ½åŠ›ã€‚\n    *   **æ”¹è¿›ç‚¹**ï¼šè§£å†³äº†RLå†·å¯åŠ¨æ—¶çš„å±é™©æ¢ç´¢é—®é¢˜ï¼ŒåŠ é€Ÿäº†è®­ç»ƒæ”¶æ•›ã€‚\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šBC+PPOã€BC-SACç­‰æ¡†æ¶ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­å¤±è´¥ç‡é™ä½çº¦38%ã€‚\n\n*   **æœ¬æ–‡çš„è´¡çŒ® (IRL-DAL)ï¼šå¼•å…¥é€†å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¥–åŠ±å¡‘å½¢**\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼šåœ¨IL+RLåŸºç¡€ä¸Šï¼Œå¼•å…¥**åŸºäºGAILçš„é€†å¼ºåŒ–å­¦ä¹ (IRL)**ã€‚\n    *   **æ”¹è¿›ç‚¹**ï¼š\n        1.  **è§£å†³å¥–åŠ±è®¾è®¡éš¾é¢˜**ï¼šä¸å†ä¾èµ–è„†å¼±çš„æ‰‹å·¥å¥–åŠ±ï¼Œè€Œæ˜¯ä»ä¸“å®¶æ¼”ç¤ºä¸­**æ¨æ–­å‡ºéšå«çš„ã€å¯†é›†çš„å¥–åŠ±ä¿¡å·** (`r_irl`)ã€‚\n        2.  **æå‡ç­–ç•¥è´¨é‡**ï¼šä½¿å­¦ä¹ åˆ°çš„ç­–ç•¥ä¸ä»…åœ¨ç›®æ ‡ä¸Šï¼ˆå¦‚åˆ°è¾¾ç»ˆç‚¹ï¼‰è¡¨ç°å¥½ï¼Œæ›´åœ¨**è¡Œä¸ºé£æ ¼ä¸Šï¼ˆå¦‚å¹³æ»‘æ€§ã€é¢„åˆ¤æ€§ï¼‰ä¸ä¸“å®¶å¯¹é½**ã€‚\n        3.  **å½¢æˆç»“æ„åŒ–è®­ç»ƒè¯¾ç¨‹**ï¼šæ„å»ºäº† **â€œBCåˆå§‹åŒ– â†’ IRLæä¾›å¯†é›†å¥–åŠ± â†’ PPOå¾®è°ƒâ€** çš„ä¸‰é˜¶æ®µè¯¾ç¨‹ï¼Œå®ç°äº†**ç¨³å®šã€é«˜æ•ˆä¸”å¯¹é½ä¸“å®¶æ„å›¾**çš„å­¦ä¹ ã€‚\n    *   **å®é™…ä»·å€¼**ï¼šå¦‚å›¾5æ‰€ç¤ºï¼ŒèåˆIRLå¥–åŠ±çš„ç­–ç•¥äº§ç”Ÿäº†æ›´å¹³æ»‘ã€æ›´æ‹Ÿäººçš„è½¨è¿¹ï¼Œå…‹æœäº†ä»…ä½¿ç”¨ç¯å¢ƒå¥–åŠ±æ—¶çš„æŒ¯è¡å’Œè¶…è°ƒé—®é¢˜ã€‚\n\n### 2. è§„åˆ’ä¸å®‰å…¨æœºåˆ¶æ¼”è¿›ï¼šä»ååº”å¼ç­–ç•¥åˆ°ç”Ÿæˆå¼å®‰å…¨ç›‘ç£\n\n**å‘å±•è·¯å¾„**ï¼š ååº”å¼ç­–ç•¥ â†’ å¼€ç¯è½¨è¿¹ç”Ÿæˆå™¨ â†’ **èƒ½é‡å¼•å¯¼çš„é—­ç¯å®‰å…¨æ‰©æ•£è§„åˆ’å™¨**\n\n*   **æ—©æœŸï¼šååº”å¼ç­–ç•¥ (å¦‚çº¯RL/ILç­–ç•¥)**\n    *   **æ ¸å¿ƒ**ï¼šæ ¹æ®å½“å‰è§‚æµ‹ç›´æ¥è¾“å‡ºåŠ¨ä½œï¼Œå“åº”é€Ÿåº¦å¿«ã€‚\n    *   **å±€é™**ï¼š**ç¼ºä¹å‰ç»æ€§**ï¼Œéš¾ä»¥åº”å¯¹éœ€è¦å¤æ‚è¿‚å›æˆ–æå‰åˆ¶åŠ¨çš„å±é™©åœºæ™¯ï¼Œåœ¨åˆ†å¸ƒå¤–çŠ¶æ€ä¸‹çš„å®‰å…¨æ€§ä¿éšœå¼±ã€‚\n\n*   **æ”¹è¿›1ï¼šç”Ÿæˆå¼è½¨è¿¹è§„åˆ’å™¨ (å¦‚æ‰©æ•£æ¨¡å‹)**\n    *   **æ ¸å¿ƒ**ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç­‰ç”Ÿæˆæ¨¡å‹ï¼Œé¢„æµ‹å¤šæ¡æœªæ¥å¯èƒ½çš„è½¨è¿¹ï¼Œä»ä¸­é€‰æ‹©æœ€ä¼˜ã€‚\n    *   **æ”¹è¿›ç‚¹**ï¼šå…·å¤‡äº†**é•¿æ—¶ç¨‹è§„åˆ’èƒ½åŠ›**ï¼Œèƒ½ç”Ÿæˆå¤šæ ·ä¸”ç‰©ç†å¯è¡Œçš„è½¨è¿¹ã€‚\n    *   **å±€é™**ï¼šé€šå¸¸ä½œä¸º**ç‹¬ç«‹çš„ã€å¼€ç¯çš„**æ¨¡å—ï¼Œä¸ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥åˆ†ç¦»ã€‚è¿™å¯¼è‡´äº†**åˆ†å¸ƒä¸åŒ¹é…**é—®é¢˜ï¼šè§„åˆ’çš„è½¨è¿¹å¯èƒ½è¢«ä½å±‚æ§åˆ¶å™¨æ‰§è¡Œå¾—å¾ˆå·®ï¼Œä¸”åœ¨åŠ¨æ€å¹²æ‰°ä¸‹ä¸é²æ£’ã€‚\n    *   **ä»£è¡¨å·¥ä½œ**ï¼šDiffuser, Decision Diffuser ç­‰å°†æ‰©æ•£æ¨¡å‹ç”¨äºç¦»çº¿RLæˆ–å¼€ç¯è§„åˆ’ã€‚\n\n*   **æœ¬æ–‡çš„è´¡çŒ® (IRL-DAL)ï¼šèƒ½é‡å¼•å¯¼çš„æ‰©æ•£è‡ªé€‚åº”å‰ç»è§„åˆ’å™¨**\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼šæå‡ºäº†ä¸€ä¸ª**æŒ‰éœ€æ¿€æ´»ã€èƒ½é‡å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ (DAL)**ï¼Œä½œä¸º**å®‰å…¨ç›‘ç£å™¨**é›†æˆåœ¨é—­ç¯è®­ç»ƒä¸æ‰§è¡Œä¸­ã€‚\n    *   **æ”¹è¿›ç‚¹**ï¼š\n        1.  **é—­ç¯é›†æˆä¸æŒ‰éœ€æ¿€æ´»**ï¼šDALå¹¶éä¸€ç›´è¿è¡Œï¼Œä»…åœ¨é£é™©çŠ¶æ€ï¼ˆå¦‚è·ç¦»éšœç¢ç‰©å¤ªè¿‘ã€åç¦»è½¦é“ï¼‰æ—¶è¢«è§¦å‘ï¼Œ**è®¡ç®—é«˜æ•ˆ**ã€‚å®ƒç”Ÿæˆçš„è½¨è¿¹ç”¨äºå³æ—¶æ ¡æ­£ä¸å®‰å…¨åŠ¨ä½œï¼Œå¹¶ä¸PPOåŠ¨ä½œæ··åˆæ‰§è¡Œã€‚\n        2.  **èƒ½é‡å¼•å¯¼ä¿è¯å®‰å…¨**ï¼šåœ¨æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥åŸºäº**å¤šç›®æ ‡èƒ½é‡å‡½æ•°**çš„æ¢¯åº¦å¼•å¯¼ï¼Œå°†é‡‡æ ·è½¨è¿¹æ¨å‘å®‰å…¨åŒºåŸŸã€‚èƒ½é‡é¡¹æ˜ç¡®æƒ©ç½šç¢°æ’ã€åç¦»è½¦é“ã€æ€¥åˆ¹æ€¥è½¬ç­‰ã€‚\n        3.  **å®‰å…¨æ„ŸçŸ¥ç»éªŒæ ¡æ­£**ï¼šDALçš„å¹²é¢„åŠ¨ä½œè¢«ç”¨æ¥é˜²æ­¢ç¢°æ’ï¼Œä½†å­˜å…¥ç»éªŒå›æ”¾æ± çš„å´æ˜¯å¯¹åº”çŠ¶æ€ä¸‹**FSMä¸“å®¶çš„ç†æƒ³åŠ¨ä½œ**ã€‚è¿™å®ç°äº† **â€œè¿è¡Œæ—¶é˜²æŠ¤â€** ä¸ **â€œå®‰å…¨æ•°æ®æ ‡æ³¨â€** çš„ç»Ÿä¸€ï¼Œè®©ç­–ç•¥ä»è¾¹ç¼˜æ¡ˆä¾‹çš„ä¸“å®¶æ¢å¤åŠ¨ä½œä¸­å­¦ä¹ ï¼Œè€Œéä»ç´§æ€¥é¿é™©çš„æç«¯åŠ¨ä½œä¸­å­¦ä¹ ã€‚\n    *   **å®é™…ä»·å€¼**ï¼šå°†ç”Ÿæˆå¼è§„åˆ’çš„å‰ç»ä¼˜åŠ¿ä¸é—­ç¯ç­–ç•¥çš„å“åº”æ€§ç»“åˆï¼Œæä¾›äº†å¯è¯æ˜çš„å®‰å…¨æå‡ï¼ˆç¢°æ’ç‡é™è‡³0.05/åƒæ­¥ï¼‰ã€‚å¦‚å›¾7æ‰€ç¤ºï¼ŒDALèƒ½æœ‰æ•ˆé¿å…åŸºçº¿ç­–ç•¥çš„ç¢°æ’ã€‚\n\n### 3. æ„ŸçŸ¥æ¶æ„æ¼”è¿›ï¼šä»é™æ€ç¼–ç åˆ°ä¸Šä¸‹æ–‡è‡ªé€‚åº”æ³¨æ„åŠ›\n\n**å‘å±•è·¯å¾„**ï¼š ç»Ÿä¸€å·ç§¯ç¼–ç  â†’ è‡ªæ³¨æ„åŠ›æœºåˆ¶ â†’ **è½»é‡çº§å¯å­¦ä¹ è‡ªé€‚åº”æ©ç **\n\n*   **æ—©æœŸï¼šç»Ÿä¸€å·ç§¯ç¼–ç å™¨**\n    *   **æ ¸å¿ƒ**ï¼šä½¿ç”¨CNNç­‰ç½‘ç»œå¹³ç­‰åœ°å¤„ç†æ•´ä¸ªå›¾åƒã€‚\n    *   **å±€é™**ï¼š**è®¡ç®—èµ„æºæµªè´¹**ï¼Œæ— æ³•æ ¹æ®é©¾é©¶ä¸Šä¸‹æ–‡ï¼ˆå¦‚è½¦é€Ÿã€å±é™©ç¨‹åº¦ï¼‰åŠ¨æ€èšç„¦äºå…³é”®åŒºåŸŸï¼ˆå¦‚è½¦é“çº¿ã€çªç„¶å‡ºç°çš„è¡Œäººï¼‰ã€‚\n\n*   **æ”¹è¿›1ï¼šå¼•å…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶**\n    *   **æ ¸å¿ƒ**ï¼šé€šè¿‡Transformerç­‰ç»“æ„è®©æ¨¡å‹è‡ªè¡Œå­¦ä¹ å›¾åƒä¸­ä¸åŒåŒºåŸŸçš„é‡è¦æ€§ã€‚\n    *   **æ”¹è¿›ç‚¹**ï¼šå®ç°äº†**æ•°æ®é©±åŠ¨çš„æ³¨æ„åŠ›åˆ†é…**ï¼Œèƒ½æ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å’Œå…³é”®ç‰©ä½“ã€‚\n    *   **å±€é™**ï¼š**è®¡ç®—å¼€é”€å¤§**ï¼Œä¸”æ³¨æ„åŠ›æ¨¡å¼å¯èƒ½ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œåœ¨å®æ—¶ç³»ç»Ÿä¸­éƒ¨ç½²æœ‰æŒ‘æˆ˜ã€‚\n\n*   **æœ¬æ–‡çš„è´¡çŒ® (IRL-DAL)ï¼šå¯å­¦ä¹ è‡ªé€‚åº”æ©ç **\n    *   **æ ¸å¿ƒåˆ›æ–°**ï¼šè®¾è®¡äº†ä¸€ä¸ª**è½»é‡çº§ã€å¯è§£é‡Šçš„LAMæ¨¡å—**ï¼Œå®ƒæ ¹æ®è½¦è¾†è¿åŠ¨å­¦ï¼ˆé€Ÿåº¦ï¼‰å’Œæ„ŸçŸ¥ä¿¡æ¯ï¼ˆæœ€è¿‘LiDARè·ç¦»ï¼‰åŠ¨æ€ç”Ÿæˆä¸€ä¸ªç©ºé—´æ³¨æ„åŠ›æ©ç ã€‚\n    *   **æ”¹è¿›ç‚¹**ï¼š\n        1.  **ä¸Šä¸‹æ–‡è‡ªé€‚åº”**ï¼š\n            *   **é«˜é€Ÿæ—¶**ï¼šå¢å¼ºå›¾åƒä¸‹éƒ¨åˆ†ï¼ˆè½¦é“åŒºåŸŸï¼‰çš„æƒé‡ï¼Œ**ä¼˜å…ˆä¿éšœè½¦é“ä¿æŒçš„ç¨³å®šæ€§**ã€‚\n            *   **è¿‘éšœç¢æ—¶**ï¼šè¿›ä¸€æ­¥æ”¾å¤§æ©ç å¼ºåº¦ï¼Œ**èšç„¦äºè¿‘åœºç¯å¢ƒï¼Œä¼˜å…ˆä¿éšœé¿éšœå®‰å…¨æ€§**ã€‚\n        2.  **é«˜æ•ˆä¸è½»é‡**ï¼šä»…é€šè¿‡ä¸¤ä¸ªå¯å­¦ä¹ å‚æ•° (`Î±_speed`, `Î±_lidar`) è°ƒåˆ¶ä¸€ä¸ªå‚ç›´æ¢¯åº¦æ©ç ï¼Œå‡ ä¹ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…ï¼Œå¯è½»æ¾æ’å…¥ç°æœ‰CNN backboneã€‚\n        3.  **ç«¯åˆ°ç«¯å¯å­¦ä¹ **ï¼šLAMå‚æ•°ä¸ç­–ç•¥ä¸€åŒåœ¨BCé˜¶æ®µé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼Œä½¿æ³¨æ„åŠ›æ¨¡å¼ä¸é©¾é©¶ä»»åŠ¡å…±åŒè¿›åŒ–ã€‚\n    *   **å®é™…ä»·å€¼**ï¼šä¸ºæ„ŸçŸ¥ç³»ç»Ÿæ³¨å…¥äº†**å…ˆéªŒçš„é©¾é©¶å¸¸è¯†**ï¼Œå®ç°äº†è®¡ç®—æ•ˆç‡ä¸æ„ŸçŸ¥æ€§èƒ½çš„å¹³è¡¡ï¼Œä½¿ç­–ç•¥åœ¨å¼¯é“ç­‰å¤æ‚åœºæ™¯ä¸‹è¡¨ç°æ›´ç¨³å®šï¼ˆå›¾7ï¼‰ã€‚\n\n### æ€»ç»“ï¼šå‘å±•æ–¹å‘ä¸æ ¸å¿ƒçªç ´\n\n| å‘å±•æ–¹å‘ | æ ¸å¿ƒæ¼”è¿›æ­¥éª¤ | IRL-DALçš„å…³é”®æ”¹è¿› | è§£å†³çš„æ ¸å¿ƒé—®é¢˜ |\n| :--- | :--- | :--- | :--- |\n| **å­¦ä¹ èŒƒå¼** | BC â†’ RL â†’ IL+RL â†’ **IL+IRL+RL** | å¼•å…¥GAILè¿›è¡Œ**å¥–åŠ±æ¨æ–­**ï¼Œå½¢æˆ**ä¸‰é˜¶æ®µæ··åˆè¯¾ç¨‹** | å¥–åŠ±å‡½æ•°è®¾è®¡éš¾ã€æ ·æœ¬æ•ˆç‡ä½ã€ç­–ç•¥ä¸ä¸“å®¶è¡Œä¸ºä¸å¯¹é½ |\n| **è§„åˆ’ä¸å®‰å…¨** | ååº”å¼ç­–ç•¥ â†’ å¼€ç¯ç”Ÿæˆå™¨ â†’ **é—­ç¯å®‰å…¨ç›‘ç£å™¨** | **æŒ‰éœ€æ¿€æ´»ã€èƒ½é‡å¼•å¯¼çš„æ‰©æ•£è§„åˆ’å™¨(DAL)** + **å®‰å…¨æ„ŸçŸ¥ç»éªŒæ ¡æ­£(SAEC)** | å¼€ç¯è§„åˆ’ä¸é—­ç¯æ§åˆ¶ä¸åŒ¹é…ã€ç¼ºä¹å¯ä¿éšœçš„å®‰å…¨æœºåˆ¶ã€å±é™©ç»éªŒç ´åå­¦ä¹  |\n| **æ„ŸçŸ¥æ¶æ„** | é™æ€ç¼–ç  â†’ è‡ªæ³¨æ„åŠ› â†’ **ä¸Šä¸‹æ–‡è‡ªé€‚åº”æ©ç ** | **è½»é‡çº§å¯å­¦ä¹ è‡ªé€‚åº”æ©ç (LAM)**ï¼Œä¾æ®è½¦é€Ÿä¸å±é™©åŠ¨æ€è°ƒåˆ¶æ³¨æ„åŠ› | æ„ŸçŸ¥èµ„æºåˆ†é…ä¸æ™ºèƒ½ã€å¤æ‚æ³¨æ„åŠ›æ¨¡å‹è®¡ç®—å¼€é”€å¤§ã€ç¼ºä¹å¯è§£é‡Šæ€§ |\n\n**IRL-DALçš„æœ€å¤§ä»·å€¼**åœ¨äºå®ƒ**å¹¶éå•ä¸€æŠ€æœ¯çš„çªç ´ï¼Œè€Œæ˜¯é¦–æ¬¡å°†è¿™ä¸‰ä¸ªå…³é”®å‘å±•æ–¹å‘æœ‰æœºåœ°èåˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„ã€ç«¯åˆ°ç«¯çš„æ¡†æ¶å†…**ã€‚å®ƒé€šè¿‡ï¼š\n1.  **IL+IRL+RLè¯¾ç¨‹**ç¡®ä¿å­¦ä¹ **é«˜æ•ˆä¸”æ‹Ÿäºº**ã€‚\n2.  **DAL+SAEC** ç¡®ä¿æ‰§è¡Œè¿‡ç¨‹ä¸è®­ç»ƒè¿‡ç¨‹**å®‰å…¨**ã€‚\n3.  **LAM** ç¡®ä¿æ„ŸçŸ¥**",
            "timestamp": "2026-02-03T16:03:47.482804",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## IRL-DAL è®ºæ–‡çš„å‰ä½œåˆ†æä¸å¯¹æ¯”\n\næ ¹æ®æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œä½œè€…å›¢é˜Ÿï¼ˆSeyed Ahmad Hosseini Miangoleh, Amin Jalal Aghdasian, Farzaneh Abdollahiï¼‰åœ¨æœ¬æ–‡ä¸­å¹¶æœªæ˜ç¡®åˆ—å‡ºå…¶ç›´æ¥çš„å‰ä½œï¼ˆå³åŒä¸€å›¢é˜Ÿæ­¤å‰å‘è¡¨çš„ã€ä¸æœ¬å·¥ä½œç›´æ¥ç›¸å…³çš„è®ºæ–‡ï¼‰ã€‚è®ºæ–‡çš„â€œç›¸å…³å·¥ä½œâ€éƒ¨åˆ†ç»¼è¿°äº†å››å¤§é¢†åŸŸçš„ç ”ç©¶ï¼Œä½†å¹¶æœªæŒ‡æ˜å…¶ä¸­å“ªäº›æ˜¯ä½œè€…å›¢é˜Ÿè‡ªå·±çš„å…ˆå‰å·¥ä½œã€‚\n\nå› æ­¤ï¼Œ**æ— æ³•ä»ç»™å®šæ–‡æœ¬ä¸­æå–å‡ºä½œè€…å›¢é˜Ÿå…·ä½“çš„â€œå‰ä½œâ€åˆ—è¡¨å¹¶è¿›è¡Œè¯¦ç»†å¯¹æ¯”**ã€‚\n\nç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºè®ºæ–‡æ˜ç¡®æŒ‡å‡ºçš„**ç ”ç©¶ç©ºç™½**å’Œ**æœ¬æ–‡è´¡çŒ®**ï¼Œæ¥æ·±å…¥åˆ†æIRL-DALæ¡†æ¶ç›¸å¯¹äº**ç°æœ‰é€šç”¨æ–¹æ³•**ï¼ˆå¯è§†ä¸ºå¹¿ä¹‰ä¸Šçš„â€œå‰ä½œâ€ï¼‰çš„æ ¸å¿ƒåŒºåˆ«ä¸ä¸»è¦æ”¹å–„ã€‚è¿™äº›åŒºåˆ«æ­£æ˜¯æœ¬æ–‡çš„æŠ€æœ¯åˆ›æ–°æ‰€åœ¨ã€‚\n\n### IRL-DAL ä¸ç°æœ‰æ–¹æ³•çš„æ ¸å¿ƒåŒºåˆ«åŠæ”¹è¿›\n\nä¸‹è¡¨å°†IRL-DALæå‡ºçš„é›†æˆæ¡†æ¶ä¸è®ºæ–‡ä¸­ç»¼è¿°çš„ã€å½“å‰ä¸»æµä½†ç›¸äº’å‰²è£‚çš„æŠ€æœ¯è·¯å¾„è¿›è¡Œå¯¹æ¯”ï¼Œä»¥æ¸…æ™°å±•ç¤ºå…¶åˆ›æ–°æ€§ã€‚\n\n| å¯¹æ¯”ç»´åº¦ | ç°æœ‰ä¸»æµæ–¹æ³•ï¼ˆâ€œå‰ä½œâ€çš„æ™®éå±€é™ï¼‰ | IRL-DAL æ¡†æ¶ï¼ˆæœ¬æ–‡çš„æ”¹è¿›ä¸åˆ›æ–°ï¼‰ | **ä¸»è¦æ”¹å–„ä¸å…·ä½“æ”¹åŠ¨** |\n| :--- | :--- | :--- | :--- |\n| **1. å­¦ä¹ æ¡†æ¶ä¸ä¿¡å·** | **å‰²è£‚æˆ–ä½æ•ˆ**ï¼š<br>- **çº¯ILï¼ˆå¦‚BCï¼‰**ï¼šå­˜åœ¨åå˜é‡åç§»ï¼Œåœ¨æœªè§çŠ¶æ€è¡¨ç°å·®ã€‚<br>- **çº¯RL**ï¼šä¾èµ–äººå·¥è®¾è®¡å¥–åŠ±ï¼Œæ ·æœ¬æ•ˆç‡ä½ï¼Œæ”¶æ•›ä¸ç¨³å®šã€‚<br>- **ç®€å•IL+RLæ··åˆ**ï¼šç¼ºä¹ä»ä¸“å®¶æ•°æ®ä¸­æ¨æ–­å¯†é›†å¥–åŠ±çš„æœºåˆ¶ã€‚ | **ç«¯åˆ°ç«¯æ··åˆIL-IRL-RLç®¡é“**ï¼š<br>1. **FSMä¸“å®¶BCåˆå§‹åŒ–**ï¼šæä¾›å®‰å…¨ã€ç¨³å®šçš„è¡Œä¸ºå…ˆéªŒã€‚<br>2. **GAIL-based IRLå¥–åŠ±**ï¼šä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ å¯†é›†çš„â€œæ„å›¾â€å¥–åŠ±(`r_irl`)ã€‚<br>3. **PPOå¾®è°ƒ**ï¼šåœ¨æ··åˆå¥–åŠ±(`r_total = w_env*r_env + w_irl*r_irl`)ä¸‹è¿›è¡Œåœ¨çº¿ä¼˜åŒ–ã€‚ | **æ”¹å–„**ï¼š**å®ç°äº†æ ·æœ¬é«˜æ•ˆã€ç¨³å®šä¸”ä¸ä¸“å®¶ç›®æ ‡å¯¹é½çš„å­¦ä¹ **ã€‚<br>**å…·ä½“æ”¹åŠ¨**ï¼š<br>- å¼•å…¥äº†**FSM-awareçš„ç»éªŒå›æ”¾ç¼“å†²åŒº**ï¼Œå¹³è¡¡é‡‡æ ·å„ç§é©¾é©¶æ¨¡å¼ï¼ˆå¦‚æ­£å¸¸è·Ÿè½¦ã€ç´§æ€¥é¿éšœï¼‰ï¼Œè§£å†³äº†è¾¹ç¼˜æ¡ˆä¾‹æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚<br>- å°†**GAILåˆ¤åˆ«å™¨**æä¾›çš„å¯†é›†å¥–åŠ±ä¸ç¨€ç–ç¯å¢ƒå¥–åŠ±**åŠ¨æ€ç»“åˆ**ï¼Œä¸ºPPOæä¾›äº†æŒç»­ã€é«˜è´¨é‡çš„ä¼˜åŒ–ä¿¡å·ï¼Œé©±åŠ¨ç­–ç•¥æ—¢å®Œæˆä»»åŠ¡åˆæ¨¡ä»¿ä¸“å®¶é£æ ¼ï¼ˆè§å›¾5ï¼‰ã€‚ |\n| **2. å®‰å…¨è§„åˆ’ä¸æ‰§è¡Œ** | **åˆ†ç¦»æˆ–åƒµåŒ–**ï¼š<br>- **å¼€ç¯æ‰©æ•£è§„åˆ’å™¨**ï¼šç”Ÿæˆè½¨è¿¹åç”±ç‹¬ç«‹æ§åˆ¶å™¨è·Ÿè¸ªï¼Œå­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ï¼ŒæŠ—å¹²æ‰°æ€§å¼±ã€‚<br>- **å›ºå®šæƒé‡æˆæœ¬å‡½æ•°**ï¼šåœ¨è§„åˆ’ä¸­ä½¿ç”¨çš„è½¦é“ä¿æŒã€é¿éšœç­‰æˆæœ¬æƒé‡æ˜¯å›ºå®šçš„ï¼Œæ— æ³•æ ¹æ®å®æ—¶é£é™©è°ƒæ•´ã€‚ | **æŒ‰éœ€æ¿€æ´»ã€èƒ½é‡å¼•å¯¼çš„æ‰©æ•£å®‰å…¨ç›‘ç£å™¨(DAL)**ï¼š<br>1. **æ¡ä»¶è§¦å‘**ï¼šä»…åœ¨é«˜é£é™©çŠ¶æ€ï¼ˆå¦‚éšœç¢ç‰©è¿‡è¿‘ã€è½¦é“åç¦»è¿‡å¤§ï¼‰æ¿€æ´»ï¼ŒèŠ‚çœç®—åŠ›ã€‚<br>2. **èƒ½é‡å¼•å¯¼é‡‡æ ·**ï¼šåœ¨åå‘æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­ï¼Œç”¨å¤åˆèƒ½é‡å‡½æ•°`E`çš„æ¢¯åº¦å¼•å¯¼è½¨è¿¹ç”Ÿæˆï¼Œå®æ—¶ä¼˜åŒ–å®‰å…¨æ€§ã€‚<br>3. **åŠ¨ä½œæ··åˆä¸ç»éªŒæ ¡æ­£(SAEC)**ï¼šå°†DALçš„å®‰å…¨åŠ¨ä½œä¸PPOåŠ¨ä½œæ··åˆæ‰§è¡Œï¼Œå¹¶å°†**æ ¡æ­£åçš„çŠ¶æ€-ä¸“å®¶åŠ¨ä½œå¯¹**å­˜å…¥ç¼“å†²åŒºä¾›ç­–ç•¥å­¦ä¹ ã€‚ | **æ”¹å–„**ï¼š**å®ç°äº†é—­ç¯ã€è‡ªé€‚åº”ã€å¯å­¦ä¹ çš„ä¸»åŠ¨å®‰å…¨æœºåˆ¶**ã€‚<br>**å…·ä½“æ”¹åŠ¨**ï¼š<br>- å°†æ‰©æ•£æ¨¡å‹ä»**å¼€ç¯è½¨è¿¹ç”Ÿæˆå™¨**å˜ä¸º**æŒ‰éœ€æ¿€æ´»çš„é—­ç¯å®‰å…¨æ¨¡å—**ï¼Œä¸ä¸»ç­–ç•¥ç´§å¯†è€¦åˆã€‚<br>- è®¾è®¡äº†**é£é™©è‡ªé€‚åº”çš„èƒ½é‡æƒé‡**ï¼ˆå¦‚ `w_lane = w_base_lane * (1 + Î±_hazard * h_t)`ï¼‰ï¼Œä½¿å®‰å…¨çº¦æŸèƒ½éšå±é™©ç¨‹åº¦`h_t`åŠ¨æ€åŠ å¼ºã€‚<br>- æå‡º**SAECæœºåˆ¶**ï¼šDALåœ¨è¿è¡Œæ—¶é˜²æ­¢ç¢°æ’ï¼Œä½†å­˜å‚¨åˆ°PPOç¼“å†²åŒºçš„å´æ˜¯å½“å‰çŠ¶æ€å¯¹åº”çš„**FSMä¸“å®¶åŠ¨ä½œ**ã€‚è¿™ä½¿ç­–ç•¥èƒ½ä»â€œæ¿’ä¸´å¤±è´¥â€çš„è¾¹ç¼˜æ¡ˆä¾‹ä¸­å­¦ä¹ æ­£ç¡®çš„ä¸“å®¶æ¢å¤è¡Œä¸ºï¼Œè€ŒDALä»…ä½œä¸ºâ€œé˜²æŠ¤æ â€ï¼Œå®ç°äº†å®‰å…¨å¹²é¢„å‘ç­–ç•¥èƒ½åŠ›çš„**è’¸é¦**ï¼ˆè§å›¾4ï¼‰ã€‚ |\n| **3. æ„ŸçŸ¥ä¸æ³¨æ„åŠ›** | **é™æ€æˆ–è®¡ç®—é‡å¤§**ï¼š<br>- **æ ‡å‡†CNNç¼–ç å™¨**ï¼šå¹³ç­‰å¤„ç†æ‰€æœ‰å›¾åƒåŒºåŸŸï¼Œæ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€èšç„¦ã€‚<br>- **å¤æ‚è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šè™½èƒ½åŠ¨æ€èšç„¦ï¼Œä½†è®¡ç®—å¼€é”€å¤§ï¼Œéš¾ä»¥è§£é‡Šã€‚ | **å¯å­¦ä¹ è‡ªé€‚åº”æ©ç (LAM)**ï¼š<br>1. **è½»é‡ä¸”å¯è§£é‡Š**ï¼šä»…å¼•å…¥ä¸¤ä¸ªå¯å­¦ä¹ å‚æ•°(`Î±_speed`, `Î±_lidar`)ã€‚<br>2. **ä¸Šä¸‹æ–‡é©±åŠ¨**ï¼šæ ¹æ®è½¦è¾†é€Ÿåº¦(`v_t`)å’Œæ¿€å…‰é›·è¾¾æœ€è¿‘è·ç¦»(`d_min,t`)ç”Ÿæˆä¸€ä¸ª**å‚ç›´æ¢¯åº¦æ©ç **ï¼Œä¸RGBå›¾åƒé€šé“æ‹¼æ¥ã€‚<br>3. **åŠ¨æ€è°ƒåˆ¶**ï¼šé«˜é€Ÿæ—¶å¢å¼ºå¯¹é“è·¯ä¸‹éƒ¨çš„å…³æ³¨ï¼ˆä¿æŒè½¦é“ï¼‰ï¼›é è¿‘éšœç¢ç‰©æ—¶æ•´ä½“å¢å¼ºå…³æ³¨åº¦ï¼ˆç´§æ€¥é¿éšœï¼‰ã€‚ | **æ”¹å–„**ï¼š**å®ç°äº†è½»é‡çº§ã€å¯è§£é‡Šã€ä¸é©¾é©¶ä¸Šä¸‹æ–‡ç´§å¯†è€¦åˆçš„åŠ¨æ€è§†è§‰æ³¨æ„åŠ›**ã€‚<br>**å…·ä½“æ”¹åŠ¨**ï¼š<br>- æ‘’å¼ƒäº†å¤æ‚çš„åŸºäºTransformerçš„è‡ªæ³¨æ„åŠ›ï¼Œè®¾è®¡äº†ä¸€ä¸ª**åŸºäºå…ˆéªŒç‰©ç†è§„åˆ™ï¼ˆé€Ÿåº¦ã€è·ç¦»ï¼‰çš„å‚æ•°åŒ–æ©ç ç”Ÿæˆå™¨**ã€‚<br>- æ©ç ä¸æ˜¯ç§»åŠ¨æ³¨æ„åŠ›ç„¦ç‚¹ï¼Œè€Œæ˜¯**è°ƒåˆ¶ç©ºé—´ç‰¹å¾çš„å¼ºåº¦æ¢¯åº¦**ï¼Œé€šè¿‡ç®€å•çš„é€šé“æ‹¼æ¥èå…¥ç½‘ç»œï¼Œå®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚<br>- ä½¿æ„ŸçŸ¥ç³»ç»Ÿèƒ½ä¾æ®**æ˜ç¡®çš„é©¾é©¶çŠ¶æ€**ï¼ˆè€Œééšå¼å­¦ä¹ ï¼‰æ¥åˆ†é…è®¡ç®—èµ„æºï¼Œåœ¨ä¿è¯å®‰å…¨æ€§çš„åŒæ—¶æå‡äº†æ•ˆç‡ï¼ˆè§å›¾2, 7ï¼‰ã€‚ |\n| **4. æ•´ä½“æ¶æ„** | **æ¨¡å—å †å **ï¼šæ„ŸçŸ¥ã€è§„åˆ’ã€æ§åˆ¶ç­‰æ¨¡å—å¾€å¾€ç‹¬ç«‹è®¾è®¡ã€è®­ç»ƒå’Œéƒ¨ç½²ï¼Œå½¢æˆâ€œå †å â€å¼ç³»ç»Ÿï¼Œå­˜åœ¨æ¥å£ä¸åŒ¹é…å’Œè¯¯å·®ç´¯ç§¯ã€‚ | **ç»Ÿä¸€ååŒæ¡†æ¶**ï¼š<br>LAMã€FSM-Replayã€IRL-PPOã€DAL-SAECå››ä¸ªç»„ä»¶åœ¨ä¸€ä¸ª**ä¸¤é˜¶æ®µè¯¾ç¨‹å­¦ä¹ **ä¸­ååŒå·¥ä½œï¼Œå½¢æˆä¸€ä¸ªä»æ„ŸçŸ¥åˆ°å†³ç­–å†åˆ°å®‰å…¨ç›‘ç£çš„**é—­ç¯å­¦ä¹ ä¸æ‰§è¡Œç³»ç»Ÿ**ã€‚ | **æ”¹å–„**ï¼š**è§£å†³äº†ç ”ç©¶ç©ºç™½ä¸­æŒ‡å‡ºçš„â€œç¼ºä¹ç«¯åˆ°ç«¯ç»Ÿä¸€å¾ªç¯â€é—®é¢˜**ï¼Œæ˜¾è‘—æå‡äº†åœ¨æ‰°åŠ¨ä¸‹çš„é²æ£’æ€§å’Œä»è§„åˆ’åˆ°æ§åˆ¶çš„è¿è´¯æ€§ã€‚<br>**å…·ä½“æ”¹åŠ¨**ï¼š<br>- è®¾è®¡äº†**ç®—æ³•1**æ‰€ç¤ºçš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼Œå°†æ¨¡ä»¿ã€å¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹ ã€ç­–ç•¥ä¼˜åŒ–ã€å®‰å…¨è§„åˆ’ä¸ç»éªŒæ ¡æ­£**æ— ç¼é›†æˆ**åœ¨ä¸€ä¸ªè¯¾ç¨‹ä¸­ã€‚<br>- ç¡®ä¿äº†**å®‰å…¨è§„åˆ’ï¼ˆDALï¼‰çš„è¾“å‡ºèƒ½ç›´æ¥å½±å“ç­–ç•¥ï¼ˆPPOï¼‰çš„ä¼˜åŒ–æ•°æ®ï¼ˆSAECï¼‰**ï¼Œæ‰“ç ´äº†å¼€ç¯è§„åˆ’ä¸é—­ç¯æ§åˆ¶é—´çš„å£å’ã€‚ |\n\n### æ€»ç»“\n\nIRL-DALå¹¶éåœ¨æŸä¸€ç¯‡ç‰¹å®šâ€œå‰ä½œâ€ä¸Šè¿›è¡Œå¢é‡ä¿®æ”¹ï¼Œè€Œæ˜¯**é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’é¢†åŸŸä¸­â€œå­¦ä¹ æ•ˆç‡â€ã€â€œå†³ç­–å®‰å…¨â€å’Œâ€œæ„ŸçŸ¥è‡ªé€‚åº”â€ä¸‰å¤§è€¦åˆæŒ‘æˆ˜ï¼Œè¿›è¡Œçš„ä¸€æ¬¡ç³»ç»Ÿæ€§ã€é›†æˆåŒ–çš„æ¡†æ¶åˆ›æ–°**ã€‚\n\nå…¶æ ¸å¿ƒæ”¹è¿›åœ¨äºï¼š\n1.  **å°†é€†å¼ºåŒ–å­¦ä¹ (IRL)ä½œä¸ºæ¡¥æ¢**ï¼Œè¿æ¥äº†æ¨¡ä»¿å­¦ä¹ (IL)çš„ç¨³å®šæ€§ä¸å¼ºåŒ–å­¦ä¹ (RL)çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œæä¾›äº†é«˜è´¨é‡çš„æ··åˆå¥–åŠ±ä¿¡å·ã€‚\n2.  **å°†æ‰©æ•£æ¨¡å‹é‡æ–°å®šä½ä¸ºâ€œæŒ‰éœ€å®‰å…¨ç›‘ç£å™¨â€**ï¼Œå¹¶é€šè¿‡èƒ½é‡å¼•å¯¼å’ŒSAECæœºåˆ¶ï¼Œä½¿å…¶ä»ç¦»çº¿çš„è½¨è¿¹ç”Ÿæˆå·¥å…·å˜ä¸ºåœ¨çº¿ã€è‡ªé€‚åº”ã€ä¸”èƒ½æå‡ä¸»ç­–ç•¥å®‰å…¨æ€§çš„æ™ºèƒ½ä½“ç»„æˆéƒ¨åˆ†ã€‚\n3.  **è®¾è®¡äº†ç‰©ç†ä¿¡æ¯é©±åŠ¨çš„è½»é‡è‡ªé€‚åº”æ„ŸçŸ¥æ¨¡å—(LAM)**ï¼Œä»¥æä½çš„è®¡ç®—æˆæœ¬å®ç°äº†ç¬¦åˆé©¾é©¶ç›´è§‰çš„åŠ¨æ€æ³¨æ„åŠ›åˆ†é…ã€‚\n\nè¿™äº›æ”¹åŠ¨å…±åŒä½œç”¨ï¼Œä½¿å¾—IRL-DALåœ¨ä»¿çœŸå®éªŒä¸­å–å¾—äº†**96%çš„æˆåŠŸç‡å’Œæ¯åƒæ­¥ä»…0.05æ¬¡ç¢°æ’**çš„ä¼˜å¼‚æ€§èƒ½ï¼Œæ ‘ç«‹äº†å®‰å…¨å¯¼èˆªçš„æ–°åŸºå‡†ã€‚",
            "timestamp": "2026-02-03T16:04:50.759202",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## IRL-DALå…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è§£\n\næœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆé€†å¼ºåŒ–å­¦ä¹ ã€æ‰©æ•£æ¨¡å‹å’Œè‡ªé€‚åº”æ„ŸçŸ¥çš„è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•å¯å½’çº³ä¸ºä»¥ä¸‹å››ä¸ªç›¸äº’å…³è”çš„æ¨¡å—ï¼Œå…·ä½“æŠ€æœ¯ç»†èŠ‚ã€æ•°å­¦åŸç†åŠå‚æ•°å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŠ€æœ¯æ¨¡å— | æ ¸å¿ƒæ€æƒ³ | æŠ€æœ¯ç»†èŠ‚ä¸æ•°å­¦åŸç†æ¨å¯¼ | å…³é”®å‚æ•°ä¸é…ç½® |\n| :--- | :--- | :--- | :--- |\n| **1. æ··åˆIL-IRL-RLè®­ç»ƒè¯¾ç¨‹** | ç»“åˆæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰çš„ç¨³å®šæ€§ã€é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰çš„å¥–åŠ±æ¨æ–­èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¢ç´¢ä¼˜åŒ–èƒ½åŠ›ï¼Œé€šè¿‡åˆ†é˜¶æ®µè¯¾ç¨‹å®ç°é«˜æ•ˆã€ç¨³å®šçš„ç­–ç•¥å­¦ä¹ ã€‚ | **1.1 æ¨¡ä»¿å­¦ä¹ ï¼ˆBCï¼‰é¢„è®­ç»ƒï¼š** <br> ä½¿ç”¨æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ä¸“å®¶ç­–ç•¥ç”Ÿæˆæ•°æ®é›† \\( \\mathcal{D}_{\\text{expert}} \\)ï¼Œé€šè¿‡è¡Œä¸ºå…‹éš†æœ€å°åŒ–ç­–ç•¥è¾“å‡ºä¸ä¸“å®¶åŠ¨ä½œçš„å‡æ–¹è¯¯å·®ï¼š <br> \\( \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(o_t, a_t^*) \\sim \\mathcal{D}_{\\text{expert}}} [\\| \\pi_\\theta(s_t) - a_t^* \\|^2] + \\lambda_{\\text{L2}} \\| \\theta_{\\text{policy}} \\|^2 \\) <br> **1.2 é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰å¥–åŠ±å¡‘é€ ï¼š** <br> åŸºäºGAILæ¡†æ¶è®­ç»ƒåˆ¤åˆ«å™¨ \\( D_\\psi \\)ï¼Œå…¶æŸå¤±å‡½æ•°ä¸ºäºŒå…ƒäº¤å‰ç†µï¼š <br> \\( \\mathcal{L}_{\\text{Disc}}(\\psi) = -\\mathbb{E}_{(s_t, a_t) \\sim \\pi_\\theta} [\\log(1 - D_\\psi(s_t, a_t))] - \\mathbb{E}_{(s_t^*, a_t^*) \\sim \\mathcal{D}_{\\text{expert}}} [\\log(D_\\psi(s_t^*, a_t^*))] \\) <br> å­¦ä¹ åˆ°çš„å¯†é›†å¥–åŠ±ä¸ºï¼š <br> \\( r_{\\text{irl}}(s_t, a_t) = -\\log(1 - D_\\psi(s_t, a_t) + \\varepsilon) \\) <br> **1.3 æ··åˆå¥–åŠ±PPOå¾®è°ƒï¼š** <br> ç­–ç•¥ä½¿ç”¨PPOç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œæ€»å¥–åŠ±ä¸ºç¯å¢ƒå¥–åŠ±ä¸IRLå¥–åŠ±çš„åŠ æƒå’Œï¼š <br> \\( r_t(s_t, a_t) = w_{\\text{env}} r_{\\text{env}}(s_t, a_t) + w_{\\text{irl}} r_{\\text{irl}}(s_t, a_t) \\)ï¼Œ å…¶ä¸­ \\( w_{\\text{env}} + w_{\\text{irl}} = 1 \\)ã€‚PPOç›®æ ‡å‡½æ•°åŒ…å«è£å‰ªç­–ç•¥æŸå¤±ã€ä»·å€¼å‡½æ•°æŸå¤±å’Œç†µæ­£åˆ™é¡¹ã€‚ | - **è®­ç»ƒé˜¶æ®µï¼š** æ¨¡ä»¿é˜¶æ®µ \\( N_{\\text{imitation}} = 20k \\) æ­¥ï¼Œæ··åˆé˜¶æ®µ \\( N_{\\text{mixed}} = 30k \\) æ­¥ã€‚<br>- **å¥–åŠ±æƒé‡ï¼š** \\( w_{\\text{irl}} = 0.3 \\), \\( w_{\\text{env}} = 0.7 \\)ã€‚<br>- **BCæ­£åˆ™åŒ–ï¼š** \\( \\lambda_{\\text{L2}} = 10^{-6} \\)ã€‚<br>- **PPOå‚æ•°ï¼š** æŠ˜æ‰£å› å­ \\( \\gamma = 0.99 \\)ï¼ŒGAEå‚æ•° \\( \\lambda = 0.95 \\)ï¼Œè£å‰ªèŒƒå›´ \\( \\epsilon = 0.2 \\)ï¼Œæ‰¹æ¬¡å¤§å° 64ï¼Œ å­¦ä¹ ç‡ \\( 3 \\times 10^{-4} \\)ã€‚ |\n| **2. å¯å­¦ä¹ è‡ªé€‚åº”æ©ç ï¼ˆLAMï¼‰** | ä¸€ä¸ªè½»é‡çº§æ„ŸçŸ¥æ¨¡å—ï¼Œæ ¹æ®è½¦è¾†è¿åŠ¨å­¦ï¼ˆé€Ÿåº¦ï¼‰å’Œå‘¨å›´å±é™©ç¨‹åº¦ï¼ˆLiDARï¼‰åŠ¨æ€è°ƒæ•´è§†è§‰æ³¨æ„åŠ›ï¼Œå°†å…ˆéªŒå®‰å…¨çŸ¥è¯†ç¼–ç åˆ°è¾“å…¥ä¸­ã€‚ | **2.1 ä¸Šä¸‹æ–‡ä¿¡å·å½’ä¸€åŒ–ï¼š** <br> é€Ÿåº¦ï¼š\\( v_t^{\\text{norm}} = \\operatorname{clamp}(v_t / v_{\\max}, 0, 1) \\) <br> å±é™©ç¨‹åº¦ï¼š\\( h_t = \\operatorname{clamp}((d_{\\text{safe}} - d_{\\min,t}) / d_{\\text{safe}}, 0, 1) \\)ï¼Œå…¶ä¸­ \\( d_{\\min,t} = \\min(\\min(l_t), d_{\\max}) \\)ã€‚ <br> **2.2 è°ƒåˆ¶å› å­è®¡ç®—ï¼š** <br> ä½¿ç”¨ä¸¤ä¸ªå¯å­¦ä¹ æ ‡é‡å‚æ•° \\( \\alpha_{\\text{speed}}, \\alpha_{\\text{lidar}} \\)ï¼š <br> \\( f_{\\text{speed}} = 1 + \\alpha_{\\text{speed}} \\cdot v_t^{\\text{norm}} \\), \\( f_{\\text{hazard}} = 1 + \\alpha_{\\text{lidar}} \\cdot h_t \\)ã€‚ <br> **2.3 å‚ç›´æ¢¯åº¦æ©ç ç”Ÿæˆï¼š** <br> åŠ¨æ€ä¸‹é™å¼ºåº¦ï¼š\\( w_{\\text{lower}} = w_{\\text{base, lower}} \\cdot f_{\\text{speed}} \\cdot f_{\\text{hazard}} \\) ï¼ˆ\\( w_{\\text{base, lower}}=1.0 \\)ï¼Œä¸Šé™å›ºå®šä¸º0.0ï¼‰ã€‚ <br> å¯¹å›¾åƒæ¯ä¸€è¡Œ \\( y \\) ç”Ÿæˆæ©ç å€¼ï¼š\\( M_t(y) = w_{\\text{base, upper}} + (w_{\\text{lower}} - w_{\\text{base, upper}}) \\cdot \\frac{y}{H-1} \\)ã€‚ <br> **2.4 å½’ä¸€åŒ–ä¸æ‹¼æ¥ï¼š** <br> \\( \\hat{M}_t = M_t / (\\max(M_t) + \\varepsilon) \\)ï¼Œä¸å½’ä¸€åŒ–RGBå›¾åƒæ‹¼æ¥ä¸º4é€šé“è¾“å…¥ï¼š\\( I'_t = \\text{concat}(I_t / 255.0, \\hat{M}_t) \\)ã€‚ <br> LAMå‚æ•° \\( \\theta_{\\text{LAM}} = \\{\\alpha_{\\text{speed}}, \\alpha_{\\text{lidar}}\\} \\) éšBCæŸå¤±ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚ | - **LAMå‚æ•°åˆå€¼ï¼š** \\( \\alpha_{\\text{speed}} = \\alpha_{\\text{lidar}} = 0.5 \\)ã€‚<br>- **å®‰å…¨è·ç¦»ï¼š** \\( d_{\\text{safe}} \\) ä¸ºé¢„è®¾é˜ˆå€¼ã€‚<br>- **å›¾åƒå°ºå¯¸ï¼š** \\( H \\times W = 64 \\times 64 \\)ã€‚<br>- **å½’ä¸€åŒ–å¸¸æ•°ï¼š** \\( \\varepsilon = 10^{-6} \\)ã€‚ |\n| **3. åŸºäºèƒ½é‡çš„æ‰©æ•£è§„åˆ’å™¨ï¼ˆDALï¼‰** | ä¸€ä¸ªæŒ‰éœ€æ¿€æ´»çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä½œä¸ºå®‰å…¨ç›‘ç£å™¨ã€‚åœ¨é«˜é£é™©çŠ¶æ€ä¸‹ï¼Œé€šè¿‡èƒ½é‡å¼•å¯¼çš„åå‘æ‰©æ•£è¿‡ç¨‹ç”ŸæˆçŸ­æœŸå®‰å…¨è½¨è¿¹ï¼Œå…¶èƒ½é‡å‡½æ•°ç¼–ç äº†è½¦é“ä¿æŒã€é¿éšœã€å¹³æ»‘æ€§ç­‰å¤šé‡å®‰å…¨çº¦æŸã€‚ | **3.1 è§¦å‘æ¡ä»¶ï¼š** <br> å½“æœ€è¿‘LiDARè·ç¦» \\( d_{\\min,t} < d_{\\text{trigger}} \\) æˆ–æ¨ªå‘è½¦é“è¯¯å·® \\( |d_{\\text{lane},t}| > e_{\\text{lane}} \\) æ—¶æ¿€æ´»ã€‚ <br> **3.2 æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼š** <br> ä½¿ç”¨1D U-Netï¼Œåœ¨æ¨¡ä»¿é˜¶æ®µé€šè¿‡DDPMç›®æ ‡åœ¨ä¸“å®¶åŠ¨ä½œåºåˆ—ä¸Šè®­ç»ƒï¼šå­¦ä¹ å»å™ªè¿‡ç¨‹ \\( p_\\theta(\\mathcal{A}^{k-1} | \\mathcal{A}^k, c_t) \\)ï¼Œå…¶ä¸­ \\( c_t \\in \\mathbb{R}^{64} \\) ä¸ºç¼–ç å½“å‰ä¸Šä¸‹æ–‡ï¼ˆå±é™©ç¨‹åº¦ã€æ¨ªå‘åå·®ç­‰ï¼‰çš„æ¡ä»¶å‘é‡ã€‚ <br> **3.3 èƒ½é‡å¼•å¯¼é‡‡æ ·ï¼š** <br> åœ¨åå‘æ‰©æ•£çš„æ¯ä¸€æ­¥ \\( k \\)ï¼Œç½‘ç»œé¢„æµ‹å»å™ªè½¨è¿¹ \\( \\hat{\\mathcal{A}}^0_k \\)ï¼Œç„¶åæ²¿å¤åˆèƒ½é‡å‡½æ•° \\( E \\) çš„è´Ÿæ¢¯åº¦æ–¹å‘ä¿®æ­£ï¼š <br> \\( \\tilde{\\mathcal{A}}^0_k = \\hat{\\mathcal{A}}^0_k - w_g \\cdot \\frac{\\nabla E(\\hat{\\mathcal{A}}^0_k, o_t)}{\\|\\nabla E\\| + \\varepsilon} \\)ã€‚ <br> **3.4 å¤åˆèƒ½é‡å‡½æ•° \\( E \\)ï¼š** <br> \\( E = E_{\\text{lane}} + E_{\\text{lidar}} + E_{\\text{jerk}} + E_{\\text{stability}} + E_{\\text{expert}} \\) <br> - **è½¦é“ä¿æŒï¼š** \\( E_{\\text{lane}} = w_{\\text{lane}} (d_{\\text{lane},t} / s_{\\text{lane}})^2 \\)ï¼Œæƒé‡ \\( w_{\\text{lane}} = w_{\\text{base}}^{\\text{lane}}(1 + \\alpha_{\\text{hazard}} h_t) \\)ã€‚ <br> - **éšœç¢ç‰©è§„é¿ï¼š** \\( E_{\\text{lidar}} = w_{\\text{lidar}} \\cdot \\max(0, (d_{\\text{safe}}^{\\text{plan}} - d_{\\min,t}) / s_{\\text{lidar}})^2 \\)ï¼Œæƒé‡ \\( w_{\\text{lidar}} = w_{\\text{base}}^{\\text{lidar}}(1 + h_t) \\)ã€‚ <br> - **æ§åˆ¶å¹³æ»‘åº¦ï¼š** \\( E_{\\text{jerk}} = w_{\\text{jerk}} \\cdot \\frac{1}{H-1} \\sum_{i=1}^{H-1} \\| a_i - a_{i-1} \\|",
            "timestamp": "2026-02-03T16:05:58.045074",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ è¯„ä»·æŒ‡æ ‡\nè®ºæ–‡ä½¿ç”¨äº†**å®šé‡æ€§èƒ½æŒ‡æ ‡**å’Œ**å®šæ€§å¯è§†åŒ–åˆ†æ**ç›¸ç»“åˆçš„è¯„ä»·ä½“ç³»ï¼Œå…·ä½“æŒ‡æ ‡å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š\n\n| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | è®¡ç®—å…¬å¼/å®šä¹‰ | ç‰©ç†æ„ä¹‰ä¸è¯„ä»·ç›®çš„ |\n| :--- | :--- | :--- | :--- |\n| **æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡** | **å¹³å‡å¥–åŠ± (Mean Reward)** | \\( \\mathbb{E}_{\\pi}[\\sum_{t} \\gamma^t r_t] \\)ï¼Œè®ºæ–‡ä¸­å½’ä¸€åŒ–åˆ° [0, 200] åŒºé—´ã€‚ | ç»¼åˆè¡¡é‡ç­–ç•¥åœ¨é•¿æœŸä»»åŠ¡ä¸­çš„æ•´ä½“è¡¨ç°ï¼Œå€¼è¶Šé«˜è¯´æ˜ç­–ç•¥åœ¨å¥–åŠ±é©±åŠ¨ä¸‹è¡¨ç°è¶Šå¥½ã€‚ |\n| | **ç¢°æ’ç‡ (Collisions per 1k Steps)** | \\( \\frac{\\text{æ€»ç¢°æ’æ¬¡æ•°}}{\\text{æ€»æ­¥æ•°}} \\times 1000 \\) | **æ ¸å¿ƒå®‰å…¨æŒ‡æ ‡**ã€‚ç›´æ¥åæ˜ ç­–ç•¥çš„é¿éšœèƒ½åŠ›å’Œå®‰å…¨æ€§ï¼Œå€¼è¶Šä½è¶Šå¥½ã€‚è®ºæ–‡åŸºçº¿ä¸º0.63ï¼Œæœ€ç»ˆæ¨¡å‹é™è‡³**0.05**ã€‚ |\n| | **æˆåŠŸç‡ (Success Rate)** | \\( \\frac{\\text{æˆåŠŸåˆ°è¾¾ç»ˆç‚¹çš„å›åˆæ•°}}{\\text{æ€»å›åˆæ•°}} \\times 100\\% \\) | è¡¡é‡ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€‚æˆåŠŸå®šä¹‰ä¸ºæ— ç¢°æ’åˆ°è¾¾ç›®æ ‡ç‚¹ã€‚æœ€ç»ˆæ¨¡å‹è¾¾åˆ°**96%**ã€‚ |\n| **æ¨¡ä»¿å­¦ä¹ è´¨é‡æŒ‡æ ‡** | **BCæŸå¤± (BC Loss)** | \\( \\frac{1}{B}\\sum_{i=1}^{B} \\| \\pi_\\theta(s_t^{(i)}) - a_{expert}^{(i)} \\|^2 \\) | è¡¡é‡å­¦ä¹ ç­–ç•¥ä¸ä¸“å®¶ç­–ç•¥ï¼ˆFSMæ§åˆ¶å™¨ï¼‰è¾“å‡ºåŠ¨ä½œçš„å‡æ–¹è¯¯å·®ã€‚å€¼è¶Šä½è¯´æ˜æ¨¡ä»¿å¾—è¶Šå‡†ç¡®ï¼Œç­–ç•¥åˆå§‹åŒ–è¶Šç¨³å®šã€‚ |\n| | **åŠ¨ä½œç›¸ä¼¼åº¦ (Action Similarity)** | æœªç»™å‡ºæ˜ç¡®å…¬å¼ï¼Œæ¨æµ‹ä¸ºç­–ç•¥åŠ¨ä½œä¸ä¸“å®¶åŠ¨ä½œåœ¨æŸç§åº¦é‡ä¸‹çš„åŒ¹é…ç™¾åˆ†æ¯”ã€‚ | å®šé‡è¯„ä¼°ç­–ç•¥è¡Œä¸ºä¸ä¸“å®¶è¡Œä¸ºçš„æ¥è¿‘ç¨‹åº¦ï¼Œå€¼è¶Šé«˜è¯´æ˜é©¾é©¶é£æ ¼è¶Šâ€œåƒâ€ä¸“å®¶ã€‚ |\n| **è½¨è¿¹é¢„æµ‹è´¨é‡æŒ‡æ ‡** | **å¹³å‡ä½ç§»è¯¯å·® (ADE)** | \\( \\frac{1}{T} \\sum_{t=1}^{T} \\| \\hat{p}_t - p_t^{gt} \\| \\) <br> å…¶ä¸­ \\(\\hat{p}_t\\) ä¸ºé¢„æµ‹ä½ç½®ï¼Œ\\(p_t^{gt}\\) ä¸ºçœŸå®ï¼ˆæˆ–ä¸“å®¶ï¼‰ä½ç½®ã€‚ | è¯„ä¼°æ•´æ¡é¢„æµ‹è½¨è¿¹ä¸å‚è€ƒè½¨è¿¹çš„å¹³å‡åå·®ã€‚å€¼è¶Šä½ï¼Œè¯´æ˜è½¨è¿¹é¢„æµ‹çš„**æ•´ä½“å‡†ç¡®æ€§**è¶Šé«˜ã€‚ |\n| | **æœ€ç»ˆä½ç§»è¯¯å·® (FDE)** | \\( \\| \\hat{p}_T - p_T^{gt} \\| \\) | è¯„ä¼°é¢„æµ‹è½¨è¿¹**ç»ˆç‚¹**ä¸å‚è€ƒè½¨è¿¹ç»ˆç‚¹çš„åå·®ã€‚å€¼è¶Šä½ï¼Œè¯´æ˜**é•¿æœŸé¢„æµ‹çš„å‡†ç¡®æ€§**å’Œ**ç›®æ ‡è¾¾æˆç²¾åº¦**è¶Šé«˜ã€‚ |\n| **è®­ç»ƒè¿‡ç¨‹ç›‘æ§æŒ‡æ ‡** | **DALå¹²é¢„é¢‘ç‡** | åœ¨è®­ç»ƒæ›²çº¿ä¸­å±•ç¤ºï¼Œæœªç»™å‡ºå…·ä½“å…¬å¼ã€‚ | ç›‘æ§é«˜é£é™©çŠ¶æ€ä¸‹ï¼Œæ‰©æ•£å®‰å…¨è§„åˆ’å™¨è¢«è§¦å‘çš„é¢‘ç‡ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œéšç€ç­–ç•¥å­¦ä¹ ï¼Œå¹²é¢„åº”é€æ¸å‡å°‘ï¼Œè¡¨æ˜ç­–ç•¥è‡ªèº«å®‰å…¨æ€§åœ¨æå‡ã€‚ |\n| | **å¥–åŠ±æ›²çº¿** | è®­ç»ƒè¿‡ç¨‹ä¸­å¹³å‡å¥–åŠ±éšæ­¥æ•°çš„å˜åŒ–ã€‚ | ç›‘æ§è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å¹³æ»‘ä¸Šå‡çš„æ›²çº¿è¡¨æ˜å­¦ä¹ è¿‡ç¨‹ç¨³å®šæœ‰æ•ˆã€‚ |\n\n**æŒ‡æ ‡ä½¿ç”¨é€»è¾‘**ï¼š\n1.  **å®‰å…¨ä¸æ€§èƒ½å¹¶é‡**ï¼š`ç¢°æ’ç‡`å’Œ`æˆåŠŸç‡`æ˜¯ç¡¬æ€§å®‰å…¨ä¸ä»»åŠ¡æŒ‡æ ‡ã€‚\n2.  **æ¨¡ä»¿è´¨é‡éªŒè¯**ï¼š`BCæŸå¤±`å’Œ`åŠ¨ä½œç›¸ä¼¼åº¦`ç¡®ä¿ç­–ç•¥ä»ä¸“å®¶é‚£é‡Œå­¦åˆ°äº†å¯é çš„åŸºç¡€è¡Œä¸ºã€‚\n3.  **è½¨è¿¹è´¨é‡è¯„ä¼°**ï¼š`ADE/FDE`ï¼ˆå¸¸ç”¨äºè½¨è¿¹é¢„æµ‹ä»»åŠ¡ï¼‰åœ¨è¿™é‡Œç”¨äºè¯„ä¼°ç­–ç•¥ç”Ÿæˆçš„è¡Œé©¶è½¨è¿¹ä¸ä¸“å®¶è½¨è¿¹çš„å»åˆåº¦ï¼Œé—´æ¥åæ˜ é©¾é©¶çš„å¹³æ»‘æ€§å’Œåˆç†æ€§ã€‚\n4.  **æ¶ˆèç ”ç©¶æ”¯æ’‘**ï¼šæ‰€æœ‰æŒ‡æ ‡åœ¨å››ä¸ªæ¨¡å‹å˜ä½“ï¼ˆBaseline, +FSM Replay, +Diffusion Planner, Full Modelï¼‰ä¸Šè¿›è¡Œå¯¹æ¯”ï¼Œæ¸…æ™°å±•ç¤ºäº†æ¯ä¸ªç»„ä»¶ï¼ˆç»“æ„åŒ–ç»éªŒå›æ”¾ã€æ‰©æ•£è§„åˆ’å™¨ã€LAMã€SAECï¼‰çš„è´¡çŒ®ã€‚\n\n### äºŒã€ æ•°æ®é›†\næœ¬æ–‡**æœªä½¿ç”¨å…¬å¼€çš„çœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®é›†**ï¼Œè€Œæ˜¯é‡‡ç”¨**ä»¿çœŸç¯å¢ƒå†…è‡ªåŠ¨ç”Ÿæˆä¸“å®¶æ•°æ®**çš„ç­–ç•¥ã€‚\n\n| æ•°æ®é›†å±æ€§ | å…·ä½“ç»†èŠ‚ |\n| :--- | :--- |\n| **æ•°æ®æ¥æº** | **Webotsä»¿çœŸå™¨**ä¸­ï¼Œç”±ä¸€ä¸ªç¡®å®šæ€§çš„**æœ‰é™çŠ¶æ€æœºä¸“å®¶ç­–ç•¥**åœ¨çº¿ç”Ÿæˆã€‚ |\n| **ä¸“å®¶ç­–ç•¥** | **FSMæ§åˆ¶å™¨**ï¼ŒåŒ…å«å››ç§è¡Œä¸ºæ¨¡å¼ï¼š<br> 1. **è½¦é“è·Ÿéš** <br> 2. **éšœç¢ç‰©é¿è®©** <br> 3. **ç›´è¡Œ** <br> 4. **è¿”å›è½¦é“** <br> æ¨¡å¼é—´é€šè¿‡ä¼ æ„Ÿå™¨è§„åˆ™ï¼ˆå¦‚LiDARè·ç¦»ã€è½¦é“åç§»ï¼‰è§¦å‘åˆ‡æ¢ã€‚ |\n| **æ•°æ®å½¢å¼** | è½¬ç§»å…ƒç»„ \\((o_t, a_t^*, s_{\\text{FSM}, t})\\)ï¼Œå…¶ä¸­ï¼š<br> â€¢ \\(o_t\\)ï¼šå¤šæ¨¡æ€è§‚æµ‹ï¼ˆå›¾åƒã€LiDARã€è½¦è¾†åŠ¨åŠ›å­¦ï¼‰<br> â€¢ \\(a_t^*\\)ï¼šFSMä¸“å®¶ç”Ÿæˆçš„åŠ¨ä½œï¼ˆè½¬å‘ã€é€Ÿåº¦ï¼‰<br> â€¢ \\(s_{\\text{FSM}, t}\\)ï¼šå½“å‰æ´»è·ƒçš„FSMçŠ¶æ€ï¼ˆæ¨¡å¼ï¼‰ |\n| **æ•°æ®å­˜å‚¨ä¸ç®¡ç†** | **FSMæ„ŸçŸ¥çš„ç»éªŒå›æ”¾ç¼“å†²åŒº**ï¼š<br> â€¢ **æ ¸å¿ƒåˆ›æ–°**ï¼šä¸æ˜¯ç”¨ä¸€ä¸ªç»Ÿä¸€çš„ç¼“å†²åŒºï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªFSMçŠ¶æ€ \\(s\\) ç»´æŠ¤ä¸€ä¸ªç‹¬ç«‹çš„ç¼“å†²åŒº \\(D_s\\)ã€‚<br> â€¢ **æ€»æ•°æ®é›†**ï¼š\\(D_{\\text{expert}} = \\bigcup_s D_s\\)<br> â€¢ **é‡‡æ ·ç­–ç•¥**ï¼šè®­ç»ƒæ—¶ï¼ˆå°¤å…¶æ˜¯BCé˜¶æ®µï¼‰ä»å„çŠ¶æ€ç¼“å†²åŒº**å¹³è¡¡é‡‡æ ·**ï¼Œç¡®ä¿ç½•è§ä½†å…³é”®çš„å±é™©åœºæ™¯ï¼ˆå¦‚ç´§æ€¥é¿éšœï¼‰ä¸å¸¸è§åœºæ™¯ï¼ˆè½¦é“è·Ÿéšï¼‰è·å¾—åŒç­‰å…³æ³¨ã€‚ |\n| **æ•°æ®é›†è§„æ¨¡** | è®ºæ–‡æœªæ˜ç¡®è¯´æ˜ç”Ÿæˆçš„æ€»æ•°æ®é‡ï¼Œä½†æåˆ°äº†è®­ç»ƒè¿‡ç¨‹çš„æ€»æ­¥æ•°ï¼ˆ\\(N_{\\text{total}} = 50,000\\) æ­¥ï¼‰å’Œç¼“å†²åŒºå®¹é‡ï¼ˆ**50,000 transitions**ï¼‰ã€‚ |\n| **ç¯å¢ƒä¸åœºæ™¯** | â€¢ **ä»¿çœŸå™¨**ï¼šWebotsã€‚<br> â€¢ **åœºæ™¯å†…å®¹**ï¼šåŒ…å«å¤šè½¦é“å¼¯é“ã€ç§»åŠ¨éšœç¢ç‰©ã€ä»¥åŠä»ç™½å¤©åˆ°å¤œæ™šçš„ä¸åŒå…‰ç…§æ¡ä»¶çš„è™šæ‹ŸåŸå¸‚é“è·¯ã€‚<br> â€¢ **ä¼ æ„Ÿå™¨é…ç½®**ï¼šå‰è§†RGBæ‘„åƒå¤´ï¼ˆ64x64åˆ†è¾¨ç‡ï¼‰ã€180æŸå‰å‘LiDARã€è½¦è¾†é€Ÿåº¦ä¿¡æ¯ã€‚ |\n\n**æ•°æ®é›†è®¾è®¡çš„ä»·å€¼**ï¼š\n1.  **è§£å†³æ•°æ®ä¸å¹³è¡¡**ï¼šä¼ ç»Ÿé©¾é©¶æ•°æ®é›†ä¸­ï¼Œå®‰å…¨é©¾é©¶çŠ¶æ€å ç»å¤§å¤šæ•°ï¼Œå±é™©è¾¹ç¼˜æ¡ˆä¾‹æå°‘ã€‚FSMåˆ†åŒºå›æ”¾é€šè¿‡**å¹³è¡¡é‡‡æ ·**å¼ºåˆ¶æ¨¡å‹å­¦ä¹ æ‰€æœ‰é©¾é©¶æ¨¡å¼ï¼Œæ˜¾è‘—æå‡äº†åœ¨å±é™©æƒ…å†µä¸‹çš„é²æ£’æ€§ï¼ˆå®éªŒæ˜¾ç¤ºç¢°æ’ç‡å¤§å¹…ä¸‹é™ï¼‰ã€‚\n2.  **è·å¾—é«˜è´¨é‡ã€æ— å™ªå£°æ ‡ç­¾**ï¼šç›¸æ¯”çœŸå®äººç±»é©¾é©¶æ•°æ®å¯èƒ½å­˜åœ¨çš„ä¸ä¸€è‡´å’Œå™ªå£°ï¼ŒFSMä¸“å®¶æä¾›**ç¡®å®šã€æœ€ä¼˜ä¸”ç¬¦åˆè§„åˆ™**çš„åŠ¨ä½œæ ‡ç­¾ï¼Œä¸ºæ¨¡ä»¿å­¦ä¹ æä¾›äº†å¹²å‡€å¯é çš„åŸºç¡€ã€‚\n3.  **æ”¯æŒè¯¾ç¨‹å­¦ä¹ **ï¼šè¯¥æ•°æ®é›†æ˜¯ä¸¤é˜¶æ®µè®­ç»ƒè¯¾ç¨‹ï¼ˆæ¨¡ä»¿é¢„è®­ç»ƒ + åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼‰çš„åŸºç¡€ï¼Œç¡®ä¿äº†ç­–ç•¥ä»å®‰å…¨ã€ç¨³å®šçš„åˆå§‹ç‚¹å¼€å§‹å­¦ä¹ ã€‚\n\n**æ€»ç»“**ï¼šæœ¬æ–‡çš„è¯„ä»·æŒ‡æ ‡ä½“ç³»**å…¨é¢ä¸”å…·æœ‰é’ˆå¯¹æ€§**ï¼Œç´§å¯†å›´ç»•è‡ªåŠ¨é©¾é©¶çš„**å®‰å…¨æ€§ã€ä»»åŠ¡å®Œæˆåº¦ã€æ¨¡ä»¿è´¨é‡**ä¸‰å¤§æ ¸å¿ƒã€‚å…¶æ•°æ®é›†æ„å»ºæ–¹æ³•**å·§å¦™ä¸”å®ç”¨**ï¼Œé€šè¿‡ä»¿çœŸç¯å¢ƒå’Œè§„åˆ™ä¸“å®¶è§£å†³äº†çœŸå®æ•°æ®ç¨€ç¼ºã€å™ªå£°å¤§ã€è¾¹ç¼˜æ¡ˆä¾‹å°‘çš„å…³é”®é—®é¢˜ï¼Œä¸ºæ•´ä¸ªæ¡†æ¶çš„æˆåŠŸè®­ç»ƒå¥ å®šäº†åšå®åŸºç¡€ã€‚",
            "timestamp": "2026-02-03T16:06:54.637774",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒä¸è¯„ä¼°åˆ†æ\n\n### å®éªŒæ•°æ®é›†\nè®ºæ–‡**æœªä½¿ç”¨å…¬å¼€çš„çœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®é›†**è¿›è¡Œå®éªŒã€‚æ‰€æœ‰å®éªŒå‡åœ¨**Webotsä»¿çœŸå™¨**ä¸­æ„å»ºçš„è™šæ‹Ÿé©¾é©¶ç¯å¢ƒä¸­è¿›è¡Œã€‚è¯¥ä»¿çœŸç¯å¢ƒå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n- **åœºæ™¯è®¾ç½®**ï¼šåŒ…å«å¤šè½¦é“å¼¯æ›²é“è·¯ã€ç§»åŠ¨éšœç¢ç‰©ä»¥åŠä»ç™½å¤©åˆ°å¤œæ™šçš„ä¸åŒå…‰ç…§æ¡ä»¶ã€‚\n- **ä¼ æ„Ÿå™¨æ¨¡æ‹Ÿ**ï¼šæä¾›å‰å‘RGBç›¸æœºå›¾åƒå’Œ180æŸLiDARæ‰«ææ•°æ®ã€‚\n- **ä¸“å®¶æ•°æ®ç”Ÿæˆ**ï¼šé€šè¿‡ä¸€ä¸ªç¡®å®šæ€§çš„**æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰æ§åˆ¶å™¨**ç”Ÿæˆä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼Œå¹¶å­˜å‚¨åœ¨FSMåˆ†åŒºçš„ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­ã€‚\n\n### ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨ä»¥ä¸‹é‡åŒ–æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š\n\n| æŒ‡æ ‡ | è¯´æ˜ | æ”¹å–„æ–¹å‘ |\n| :--- | :--- | :--- |\n| **å¹³å‡å¥–åŠ±ï¼ˆMean Rewardï¼‰** | å½’ä¸€åŒ–åˆ°[0, 200]çš„ç´¯ç§¯æŠ˜æ‰£å›æŠ¥ï¼Œåæ˜ æ•´ä½“ä»»åŠ¡å®Œæˆè´¨é‡ã€‚ | â†‘ |\n| **ç¢°æ’ç‡ï¼ˆColl./1k Stepsï¼‰** | æ¯1000æ­¥çš„å¹³å‡ç¢°æ’æ¬¡æ•°ï¼Œç›´æ¥è¡¡é‡å®‰å…¨æ€§ã€‚ | â†“ |\n| **æˆåŠŸç‡ï¼ˆSuccess %ï¼‰** | æ— ç¢°æ’åˆ°è¾¾ç›®æ ‡ç»ˆç‚¹çš„å›åˆæ¯”ä¾‹ã€‚ | â†‘ |\n| **BCæŸå¤±ï¼ˆBC Lossï¼‰** | è¡Œä¸ºå…‹éš†æŸå¤±ï¼Œè¡¡é‡ç­–ç•¥ä¸ä¸“å®¶åŠ¨ä½œçš„åŒ¹é…ç¨‹åº¦ã€‚ | â†“ |\n| **åŠ¨ä½œç›¸ä¼¼åº¦ï¼ˆAction Sim. %ï¼‰** | ç­–ç•¥è¾“å‡ºä¸ä¸“å®¶åŠ¨ä½œçš„ä½™å¼¦ç›¸ä¼¼åº¦ç™¾åˆ†æ¯”ã€‚ | â†‘ |\n| **å¹³å‡ä½ç§»è¯¯å·®ï¼ˆADEï¼‰** | é¢„æµ‹è½¨è¿¹ä¸å‚è€ƒè½¨è¿¹ä¹‹é—´çš„å¹³å‡ç‚¹å¯¹ç‚¹è·ç¦»ã€‚ | â†“ |\n| **æœ€ç»ˆä½ç§»è¯¯å·®ï¼ˆFDEï¼‰** | é¢„æµ‹è½¨è¿¹ç»ˆç‚¹ä¸å‚è€ƒè½¨è¿¹ç»ˆç‚¹ä¹‹é—´çš„è·ç¦»ã€‚ | â†“ |\n\n### æ€§èƒ½æå‡ï¼ˆç»„ä»¶æ¶ˆèå®éªŒï¼‰\né€šè¿‡é€æ­¥æ·»åŠ æ ¸å¿ƒç»„ä»¶ï¼Œè®ºæ–‡å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ˆåŸºäº10æ¬¡å®éªŒçš„å‡å€¼Â±æ ‡å‡†å·®ï¼‰ï¼š\n\n| æ¨¡å‹å˜ä½“ | å¹³å‡å¥–åŠ± | ç¢°æ’ç‡ (/1k steps) | æˆåŠŸç‡ | BCæŸå¤± (Ã—10â»Â²) | åŠ¨ä½œç›¸ä¼¼åº¦ | ADE (m) | FDE (m) |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **åŸºçº¿ (PPO + å‡åŒ€é‡‡æ ·)** | 85.2 Â± 4.1 | 0.63 Â± 0.12 | 78.1% | 17.1 Â± 1.4 | 65.3% | 5.25 Â± 0.31 | 11.8 Â± 0.65 |\n| **+ FSM ç»éªŒå›æ”¾** | 120.4 Â± 3.8 **(+41%)** | 0.30 Â± 0.08 | 88.4% | 12.3 Â± 1.1 | 75.1% | 4.10 Â± 0.27 | 9.5 Â± 0.58 |\n| **+ æ‰©æ•£è§„åˆ’å™¨** | 155.1 Â± 3.2 **(+29%)** | 0.15 Â± 0.05 | 92.0% | 13.0 Â± 1.0 | 80.2% | 3.15 Â± 0.22 | 7.2 Â± 0.49 |\n| **+ LAM + SAEC (å®Œæ•´æ¨¡å‹)** | **180.7 Â± 2.9 (+16%)** | **0.05 Â± 0.03** | **96.3%** | **7.4 Â± 0.8** | **85.7%** | **2.45 Â± 0.18** | **5.1 Â± 0.41** |\n\n### å…³é”®æ€§èƒ½æ€»ç»“\n1.  **æœ€ç»ˆæ€§èƒ½**ï¼šå®Œæ•´IRL-DALæ¡†æ¶è¾¾åˆ°äº†**96.3%çš„æˆåŠŸç‡**ï¼Œå¹¶å°†ç¢°æ’ç‡é™è‡³æä½çš„**0.05æ¬¡/åƒæ­¥**ï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹ç¢°æ’ç‡é™ä½äº†**92%**ã€‚\n2.  **æ ¸å¿ƒç»„ä»¶è´¡çŒ®**ï¼š\n    - **FSMç»éªŒå›æ”¾**ï¼šé€šè¿‡å¹³è¡¡é‡‡æ ·ç¨€æœ‰å®‰å…¨äº‹ä»¶ï¼Œå¸¦æ¥æœ€æ˜¾è‘—çš„åˆå§‹æå‡ï¼ˆå¥–åŠ±+41%ï¼Œç¢°æ’ç‡-52%ï¼‰ã€‚\n    - **æ‰©æ•£è§„åˆ’å™¨ï¼ˆDALï¼‰**ï¼šä½œä¸ºå®‰å…¨ç›‘ç£å™¨ï¼Œè¿›ä¸€æ­¥å°†ç¢°æ’ç‡é™ä½50%ï¼Œå¹¶æå‡è½¨è¿¹å¹³æ»‘åº¦ã€‚\n    - **å¯å­¦ä¹ è‡ªé€‚åº”æ©ç ï¼ˆLAMï¼‰ä¸å®‰å…¨æ„ŸçŸ¥ç»éªŒæ ¡æ­£ï¼ˆSAECï¼‰**ï¼šå®ç°äº†æœ€ç»ˆçš„æ€§èƒ½çªç ´ï¼Œåœ¨å·²æœ‰åŸºç¡€ä¸Šå°†ç¢°æ’ç‡å†é™ä½67%ï¼Œå¹¶æ˜¾è‘—æå‡äº†ä¸ä¸“å®¶è¡Œä¸ºçš„å¯¹é½åº¦ï¼ˆBCæŸå¤±é™ä½43%ï¼‰ã€‚\n3.  **å®é™…ä»·å€¼ä½“ç°**ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡**é›†æˆæ¨¡ä»¿å­¦ä¹ ã€é€†å‘å¼ºåŒ–å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ä¸ç”Ÿæˆå¼å®‰å…¨è§„åˆ’**ï¼Œæœ‰æ•ˆè§£å†³äº†æ ·æœ¬æ•ˆç‡ã€å®‰å…¨æ€§ä¸ä¸“å®¶è¡Œä¸ºå¯¹é½ç­‰å…³é”®æŒ‘æˆ˜ï¼Œä¸ºåœ¨ä»¿çœŸä¸­å®‰å…¨ã€é«˜æ•ˆåœ°è®­ç»ƒè‡ªåŠ¨é©¾é©¶ç­–ç•¥æä¾›äº†æ–°çš„åŸºå‡†æ–¹æ¡ˆã€‚",
            "timestamp": "2026-02-03T16:07:30.291746",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\nåŸºäºå¯¹è®ºæ–‡å†…å®¹çš„åˆ†æï¼ŒIRL-DALæ¡†æ¶è™½ç„¶åœ¨ä»¿çœŸä¸­å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨ä»¥ä¸‹ä¸»è¦å±€é™æ€§ï¼š\n\n### 1. **ä»¿çœŸä¸ç°å®ä¸–ç•Œçš„å·®è·**\n   - **ä»¿çœŸç¯å¢ƒé™åˆ¶**ï¼šæ‰€æœ‰å®éªŒå‡åœ¨Webotsä»¿çœŸå™¨ä¸­è¿›è¡Œï¼Œå…¶ç‰©ç†å¼•æ“ã€ä¼ æ„Ÿå™¨æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯LiDARå’Œç›¸æœºï¼‰ä¸çœŸå®ä¸–ç•Œå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚çœŸå®ä¸–ç•Œçš„å™ªå£°ã€å…‰ç…§å˜åŒ–ã€ä¼ æ„Ÿå™¨æ•…éšœç­‰æœªå……åˆ†å»ºæ¨¡ã€‚\n   - **åœºæ™¯å¤æ‚åº¦æœ‰é™**ï¼šè®ºæ–‡ä¸­çš„æµ‹è¯•åœºæ™¯ï¼ˆå¤šè½¦é“å¼¯é“ã€ç§»åŠ¨éšœç¢ç‰©ï¼‰è™½ç„¶å…·æœ‰ä¸€å®šæŒ‘æˆ˜æ€§ï¼Œä½†ç›¸æ¯”çœŸå®åŸå¸‚äº¤é€šï¼ˆå¦‚å¯†é›†è¡Œäººã€å¤æ‚è·¯å£ã€çªå‘äº‹ä»¶ï¼‰ä»æ˜¾ç®€åŒ–ã€‚\n\n### 2. **è®¡ç®—æ•ˆç‡ä¸å®æ—¶æ€§**\n   - **æ‰©æ•£æ¨¡å‹æ¨ç†å¼€é”€**ï¼šæ‰©æ•£è§„åˆ’å™¨ï¼ˆDALï¼‰åœ¨éœ€è¦æ—¶è¢«è§¦å‘ï¼Œä½†å…¶åå‘æ‰©æ•£è¿‡ç¨‹ï¼ˆ100æ­¥ï¼‰è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œå¯èƒ½å½±å“å®æ—¶å†³ç­–ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶ã€‚\n   - **å¤šæ¨¡å—é›†æˆå¼€é”€**ï¼šæ¡†æ¶åŒ…å«å¤šä¸ªç»„ä»¶ï¼ˆLAMã€æ‰©æ•£è§„åˆ’å™¨ã€GAILåˆ¤åˆ«å™¨ã€PPOï¼‰ï¼Œæ•´ä½“ç³»ç»Ÿå¤æ‚åº¦é«˜ï¼Œè®­ç»ƒå’Œæ¨ç†éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚\n\n### 3. **ä¸“å®¶æ•°æ®çš„ä¾èµ–ä¸è´¨é‡**\n   - **FSMä¸“å®¶ç­–ç•¥çš„å±€é™æ€§**ï¼šä¸“å®¶æ•°æ®å®Œå…¨ç”±æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰æ§åˆ¶å™¨ç”Ÿæˆï¼Œå…¶è§„åˆ™å¯èƒ½æ— æ³•è¦†ç›–æ‰€æœ‰å¤æ‚é©¾é©¶åœºæ™¯ï¼Œé™åˆ¶äº†ç­–ç•¥å­¦ä¹ çš„å¤©èŠ±æ¿ã€‚\n   - **æ•°æ®åˆ†å¸ƒåå·®**ï¼šå°½ç®¡ä½¿ç”¨äº†FSM-aware replayæ¥å¹³è¡¡æ•°æ®ï¼Œä½†ä»¿çœŸç”Ÿæˆçš„ä¸“å®¶æ•°æ®åˆ†å¸ƒå¯èƒ½ä¸çœŸå®é©¾é©¶æ•°æ®åˆ†å¸ƒä¸ä¸€è‡´ï¼Œå½±å“ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n### 4. **å®‰å…¨æœºåˆ¶çš„å‡è®¾ä¸é²æ£’æ€§**\n   - **å®‰å…¨è§¦å‘æ¡ä»¶ä¾èµ–ç²¾ç¡®æ„ŸçŸ¥**ï¼šDALçš„æ¿€æ´»ä¾èµ–äºLiDARæµ‹è·ï¼ˆ`d_min,t`ï¼‰å’Œè½¦é“åç§»ï¼ˆ`d_lane,t`ï¼‰çš„ç²¾ç¡®æµ‹é‡ã€‚åœ¨å®é™…ä¸­ï¼Œä¼ æ„Ÿå™¨å™ªå£°æˆ–æ•…éšœå¯èƒ½å¯¼è‡´è¯¯è§¦å‘æˆ–æ¼è§¦å‘ã€‚\n   - **èƒ½é‡å‡½æ•°æƒé‡éœ€æ‰‹åŠ¨è°ƒæ•´**ï¼šæ‰©æ•£è§„åˆ’å™¨ä½¿ç”¨çš„èƒ½é‡å‡½æ•°ï¼ˆ`E_lane`, `E_lidar`ç­‰ï¼‰åŒ…å«å¤šä¸ªæ‰‹åŠ¨è®¾ç½®çš„æƒé‡å‚æ•°ï¼ˆ`w_lane`, `w_lidar`ç­‰ï¼‰ï¼Œè¿™äº›å‚æ•°å¯èƒ½éš¾ä»¥æ³›åŒ–åˆ°æ–°ç¯å¢ƒã€‚\n\n### 5. **æ³›åŒ–ä¸å¯æ‰©å±•æ€§**\n   - **æœªè§åœºæ™¯çš„é€‚åº”æ€§**ï¼šè®ºæ–‡ä¸»è¦æµ‹è¯•äº†è®­ç»ƒè¿‡çš„åœºæ™¯ç±»å‹ï¼Œå¯¹äºå®Œå…¨æœªè§è¿‡çš„éšœç¢ç‰©ç±»å‹ã€æç«¯å¤©æ°”æˆ–é“è·¯ç»“æ„ï¼Œç³»ç»Ÿçš„è¡¨ç°æœªçŸ¥ã€‚\n   - **å¤šæ™ºèƒ½ä½“äº¤äº’ç®€åŒ–**ï¼šç¯å¢ƒä¸­å…¶ä»–äº¤é€šå‚ä¸è€…ï¼ˆè½¦è¾†ã€è¡Œäººï¼‰çš„è¡Œä¸ºç›¸å¯¹ç®€å•ï¼Œæœªæ¶‰åŠå¤æ‚çš„ç¤¾äº¤äº’åŠ¨æˆ–åšå¼ˆè¡Œä¸ºã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\n### 1. **å‘çœŸå®ä¸–ç•Œè¿ç§»ä¸éƒ¨ç½²**\n   - **å¼€å‘æ›´çœŸå®çš„ä»¿çœŸç¯å¢ƒ**ï¼šä½¿ç”¨é«˜ä¿çœŸä»¿çœŸï¼ˆå¦‚CARLAã€AirSimï¼‰æˆ–æ•°å­—å­ªç”ŸæŠ€æœ¯ï¼Œå¹¶å¼•å…¥ä¼ æ„Ÿå™¨å™ªå£°ã€å¤©æ°”å˜åŒ–ç­‰çœŸå®å› ç´ ã€‚\n   - **è¿›è¡Œå®ç‰©å¹³å°æµ‹è¯•**ï¼šåœ¨å®è½¦æˆ–å°å‹æœºå™¨äººå¹³å°ä¸ŠéªŒè¯æ¡†æ¶ï¼Œé€æ­¥ç¼©å°ä»¿çœŸåˆ°ç°å®çš„å·®è·ã€‚\n   - **è®¾è®¡è½»é‡åŒ–æ‰©æ•£æ¨¡å‹**ï¼šç ”ç©¶æ›´é«˜æ•ˆçš„æ‰©æ•£é‡‡æ ·ç®—æ³•ï¼ˆå¦‚è’¸é¦ã€å‡å°‘æ‰©æ•£æ­¥æ•°ï¼‰æˆ–ä½¿ç”¨å…¶ä»–è½»é‡çº§ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æµæ¨¡å‹ï¼‰æ›¿ä»£ï¼Œä»¥æ»¡è¶³å®æ—¶æ€§è¦æ±‚ã€‚\n\n### 2. **å¢å¼ºå­¦ä¹ æ•ˆç‡ä¸è‡ªåŠ¨åŒ–**\n   - **å‡å°‘å¯¹è§„åˆ™ä¸“å®¶çš„ä¾èµ–**ï¼šæ¢ç´¢ä»çœŸå®äººç±»é©¾é©¶æ•°æ®ä¸­å­¦ä¹ ä¸“å®¶ç­–ç•¥ï¼Œæˆ–ä½¿ç”¨æ›´å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼ï¼‰æ¥äº§ç”Ÿé«˜è´¨é‡çš„ç¤ºèŒƒæ•°æ®ã€‚\n   - **è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¸èƒ½é‡å‡½æ•°è°ƒæ•´**ï¼šå¼•å…¥å…ƒå­¦ä¹ æˆ–è´å¶æ–¯ä¼˜åŒ–æ¥è‡ªåŠ¨è°ƒæ•´æ‰©æ•£èƒ½é‡å‡½æ•°çš„æƒé‡ã€æ··åˆå¥–åŠ±æƒé‡ç­‰å…³é”®è¶…å‚æ•°ã€‚\n   - **æ”¹è¿›è¯¾ç¨‹å­¦ä¹ ç­–ç•¥**ï¼šè®¾è®¡æ›´è‡ªé€‚åº”ã€åŸºäºæ™ºèƒ½ä½“å½“å‰èƒ½åŠ›çš„è¯¾ç¨‹å­¦ä¹ ï¼ŒåŠ¨æ€è°ƒæ•´è®­ç»ƒéš¾åº¦ã€‚\n\n### 3. **æå‡å®‰å…¨ä¸é²æ£’æ€§**\n   - **èåˆå½¢å¼åŒ–éªŒè¯**ï¼šå°†æ‰©æ•£è§„åˆ’å™¨ä¸å½¢å¼åŒ–æ–¹æ³•ï¼ˆå¦‚å¯è¾¾æ€§åˆ†æã€å±éšœå‡½æ•°ï¼‰ç»“åˆï¼Œä¸ºå®‰å…¨å…³é”®çŠ¶æ€æä¾›å¯è¯æ˜çš„ä¿è¯ã€‚\n   - **å¼€å‘æ›´é²æ£’çš„æ„ŸçŸ¥ä¸è§¦å‘æœºåˆ¶**ï¼šåˆ©ç”¨å¤šä¼ æ„Ÿå™¨èåˆï¼ˆç›¸æœºã€é›·è¾¾ã€LiDARï¼‰å’Œä¸ç¡®å®šæ€§ä¼°è®¡æ¥æ›´å¯é åœ°åˆ¤æ–­é£é™©çŠ¶æ€ï¼Œå‡å°‘å¯¹å•ä¸€ä¼ æ„Ÿå™¨çš„ä¾èµ–ã€‚\n   - **ç ”ç©¶å¯¹æŠ—æ€§è®­ç»ƒ**ï¼šåœ¨è®­ç»ƒä¸­å¼•å…¥å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ˆå¯¹å›¾åƒæˆ–LiDARæ•°æ®ï¼‰ï¼Œæå‡ç­–ç•¥å¯¹æ„ŸçŸ¥æ”»å‡»å’Œå¼‚å¸¸è¾“å…¥çš„é²æ£’æ€§ã€‚\n\n### 4. **æ‰©å±•æ¡†æ¶èƒ½åŠ›**\n   - **èå…¥å¤šæ™ºèƒ½ä½“ä¸ç¤¾äº¤æ„è¯†**ï¼šå°†æ¡†æ¶æ‰©å±•è‡³åŒ…å«å…¶ä»–æ™ºèƒ½ä½“çš„ç¯å¢ƒï¼Œé€šè¿‡é›†ä¸­å¼æˆ–åˆ†æ•£å¼è®­ç»ƒï¼Œå­¦ä¹ å…·æœ‰ç¤¾äº¤åˆè§„æ€§çš„é©¾é©¶è¡Œä¸ºã€‚\n   - **æ”¯æŒé•¿æ—¶ç¨‹ä¸åˆ†å±‚è§„åˆ’**ï¼šå°†å½“å‰çŸ­æ—¶ç¨‹çš„æ‰©æ•£è§„åˆ’å™¨ä¸ä¸€ä¸ªé«˜å±‚ä»»åŠ¡è§„åˆ’å™¨ç»“åˆï¼Œå¤„ç†æ›´å¤æ‚çš„ç‚¹å¯¹ç‚¹å¯¼èˆªä»»åŠ¡ã€‚\n   - **æ¢ç´¢å¤šæ¨¡æ€è¾“å‡º**ï¼šä¸ä»…è¾“å‡ºè½¨è¿¹ï¼Œè¿˜å¯ä»¥è¾“å‡ºè§£é‡Šæ€§ä¿¡å·ï¼ˆå¦‚æ³¨æ„åŠ›çƒ­å›¾ã€é£é™©ä¼°è®¡ï¼‰ï¼Œå¢å¼ºç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œäººæœºä¿¡ä»»ã€‚\n\n### 5. **ç®—æ³•ä¸ç†è®ºåˆ›æ–°**\n   - **æ¢ç´¢æ›´ç´§å¯†çš„è§„åˆ’-æ§åˆ¶è€¦åˆ**ï¼šç ”ç©¶å°†æ‰©æ•£è§„åˆ’è¿‡ç¨‹æ›´æ·±åº¦åœ°åµŒå…¥ç­–ç•¥ç½‘ç»œï¼Œå®ç°çœŸæ­£çš„ç«¯åˆ°ç«¯å®‰å…¨ç­–ç•¥å­¦ä¹ ï¼Œè€Œéå½“å‰â€œä¸»ç­–ç•¥+å®‰å…¨å¤‡ä»½â€çš„æ¾è€¦åˆæ¨¡å¼ã€‚\n   - **æ”¹è¿›é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰éƒ¨åˆ†**ï¼šå°è¯•æ›´å…ˆè¿›çš„IRLæ–¹æ³•ï¼Œä»¥æ›´ç²¾ç»†åœ°æ•æ‰ä¸“å®¶ç­–ç•¥ä¸­çš„å¤šç›®æ ‡æƒè¡¡ï¼ˆå¦‚æ•ˆç‡ã€èˆ’é€‚åº¦ã€å®‰å…¨ï¼‰ã€‚\n   - **å¼€æºä¸ç¤¾åŒºåŸºå‡†**ï¼šå°†ä»£ç å’Œä»¿çœŸç¯å¢ƒå®Œå…¨å¼€æºï¼Œå¹¶å»ºç«‹æ ‡å‡†æµ‹è¯•åŸºå‡†ï¼Œä»¥ä¿ƒè¿›æ›´å…¬å¹³çš„æ¯”è¾ƒå’Œæ›´å¿«çš„ç¤¾åŒºè¿­ä»£ã€‚\n\n**æ€»ç»“**ï¼šIRL-DALæ˜¯ä¸€ä¸ªåœ¨ä»¿çœŸä¸­è¡¨ç°ä¼˜å¼‚çš„é›†æˆæ¡†æ¶ï¼Œä½†å…¶è¿ˆå‘å®ç”¨åŒ–ä»éœ€åœ¨**ç°å®ä¸–ç•ŒéªŒè¯**ã€**è®¡ç®—æ•ˆç‡**ã€**å®‰å…¨å¯è¯æ˜æ€§**å’Œ**åœºæ™¯æ³›åŒ–**ç­‰æ–¹é¢å–å¾—çªç ´ã€‚æœªæ¥çš„å·¥ä½œåº”è‡´åŠ›äºæ„å»ºä¸€ä¸ª**æ›´é«˜æ•ˆã€æ›´é²æ£’ã€æ›´å¯è§£é‡Šä¸”æ˜“äºéƒ¨ç½²**çš„è‡ªåŠ¨é©¾é©¶å†³ç­–ç³»ç»Ÿã€‚",
            "timestamp": "2026-02-03T16:08:17.276927",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T18:34:10Z",
    "created_at": "2026-02-03T15:53:11.282071",
    "updated_at": "2026-02-03T15:53:11.282078"
}