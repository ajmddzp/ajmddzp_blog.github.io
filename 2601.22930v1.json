{
    "id": "2601.22930v1",
    "title": "MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving",
    "authors": [
        "Xidong Li",
        "Mingyu Guo",
        "Chenchao Xu",
        "Bailin Li",
        "Wenjing Zhu",
        "Yangang Zou",
        "Rui Chen",
        "Zehuan Wang"
    ],
    "abstract": "è½¨è¿¹è§„åˆ’æ˜¯è‡ªåŠ¨é©¾é©¶çš„æ ¸å¿ƒä»»åŠ¡ï¼Œéœ€åœ¨ä¸åŒåœºæ™¯ä¸‹é¢„æµ‹å®‰å…¨èˆ’é€‚çš„è¡Œé©¶è·¯å¾„ã€‚èåˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•åœ¨åº”å¯¹\"é•¿å°¾\"åœºæ™¯ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å—é™äºå•è½®æ¨ç†ï¼Œéš¾ä»¥å¤„ç†éœ€è¦è¿­ä»£ä¼˜åŒ–çš„å¤æ‚ä»»åŠ¡ã€‚ä¸ºçªç ´è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬æå‡ºMTDriveâ€”â€”ä¸€ä¸ªæ”¯æŒå¤šè½®è¿­ä»£ä¼˜åŒ–çš„æ¡†æ¶ï¼Œä½¿MLLMsèƒ½å¤ŸåŸºäºç¯å¢ƒåé¦ˆæŒç»­ä¼˜åŒ–è½¨è¿¹ã€‚è¯¥æ¡†æ¶å¼•å…¥å¤šè½®åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆmtGRPOï¼‰ï¼Œé€šè¿‡è·¨è½®æ¬¡è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿å€¼ç¼“è§£å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åŸºäºé—­ç¯ä»¿çœŸæ„å»ºäº¤äº’å¼è½¨è¿¹ç†è§£æ•°æ®é›†ä»¥æ”¯æŒå¤šè½®è®­ç»ƒã€‚åœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒéªŒè¯äº†å¤šè½®æ¨ç†èŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç³»ç»Ÿçº§ä¼˜åŒ–é™ä½é«˜åˆ†è¾¨ç‡å›¾åƒä¸å¤šè½®åºåˆ—å¯¼è‡´çš„æ•°æ®ä¼ è¾“å¼€é”€ï¼Œå®ç°äº†2.5å€çš„è®­ç»ƒååé‡æå‡ã€‚ç›¸å…³æ•°æ®ã€æ¨¡å‹ä¸ä»£ç å³å°†å¼€æºã€‚",
    "url": "https://arxiv.org/abs/2601.22930v1",
    "html_url": "https://arxiv.org/html/2601.22930v1",
    "html_content": "1\n1\ninstitutetext:\nLi Auto Inc., Beijing, China\n1\n1\nemail:\n{lixidong,guomingyu,libailin,zouyangang}@lixiang.com\n2\n2\ninstitutetext:\nNVIDIA, Shanghai, China\n2\n2\nemail:\n{chenchaox,wenjingz,charliech,zehuanw}@nvidia.com\nâˆ—\nEqual contribution\nâ€ \nCorresponding author\nMTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving\nXidong Li\nâˆ—\nMingyu Guo\nâˆ—\nChenchao Xu\nâˆ—\nBailin Li\nâ€ \nWenjing Zhu\nâ€ \nYangang Zou\nRui Chen\nZehuan Wang\nAbstract\nTrajectory planning is a core task in autonomous driving,\nrequiring the prediction of safe and comfortable paths across diverse scenarios.\nIntegrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL)\nhas shown promise in addressing â€œlong-tailâ€ scenarios.\nHowever, existing methods are constrained to single-turn reasoning,\nlimiting their ability to handle complex tasks requiring iterative refinement.\nTo overcome this limitation, we present\nMTDrive\n, a multi-turn framework\nthat enables MLLMs to iteratively refine trajectories based on environmental feedback.\nMTDrive introduces\nMulti-Turn Group Relative Policy Optimization (mtGRPO)\n,\nwhich mitigates reward sparsity by computing relative advantages across turns.\nWe further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training.\nExperiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods and the effectiveness of our multi-turn reasoning paradigm.\nAdditionally, we implement system-level optimizations to reduce the data transfer overhead\ncaused by high-resolution images and multi-turn sequences, achieving\n2.5\nÃ—\n2.5\\times\ntraining throughput.\nOur data, models, and code will be made available soon.\n1\nIntroduction\nAutonomous driving (AD) systems need to predict safe and comfortable trajectories\nacross diverse scenarios,\nincluding rare but safety-critical â€œlong-tailâ€ situations\n[\n8\n,\n9\n]\n.\nThese scenarios are underrepresented in training data,\nwhere end-to-end approaches often struggle\ndespite their success in common cases\n[\n10\n]\n.\nTo address this, recent work integrates Vision-Language Models (VLMs)\ninto AD pipelines\n[\n11\n,\n34\n,\n38\n]\n,\nleveraging their broad knowledge for better generalization.\nHowever, VLMs still struggle with fine-grained spatial reasoning:\neven state-of-the-art models frequently misjudge ego-vehicle positioning,\nmiss critical obstacles, or miscount lanes\n[\n22\n,\n35\n]\n.\nReinforcement Learning (RL) has emerged as a promising approach\nto improve VLMs for driving tasks\n[\n12\n,\n30\n]\n.\nBeyond the single-turn setting, where the model must succeed on the first attempt,\nRL can be naturally extended to multi-turn interaction:\nthe model proposes a trajectory, receives feedback on potential issues\n(e.g., collision risks or lane violations),\nand iteratively refines the trajectory.\nHowever, directly extending standard algorithms like PPO\n[\n29\n]\nor GRPO\n[\n31\n]\nto multi-turn settings\nintroduces sparse reward challenges:\nrewards are typically assigned only at the final turn,\nleaving intermediate steps without direct supervision.\nAs the number of turns grows,\nthe model must determine which refinements actually contributed to the outcomeâ€”\na classic credit assignment problem that makes learning inefficient and unstable.\nMoreover, existing trajectory datasets lack support for multi-turn interactive refinement,\nand there is no established data curation pipeline for multi-turn training scenarios.\nBeyond algorithmic challenges,\ninfrastructure support for multimodal multi-turn RL in autonomous driving remains underdeveloped.\nExisting RL frameworks\n[\n23\n,\n32\n,\n33\n,\n40\n]\nare designed for text-based reasoning tasks,\nlacking optimizations for vision-language driving modelsâ€”where\nhigh-resolution images and extended multi-turn sequences\ncreate substantial computational and data transfer overhead.\nTo overcome these challenges, we present MTDrive, a comprehensive RL framework\nfor multi-turn trajectory refinement in autonomous driving,\nencompassing data, algorithms, and system infrastructure.\nOur main contributions are summarized as follows:\nâ€¢\nInteractive trajectory understanding dataset\n:\nWe curate a multi-turn interactive dataset from a closed-loop driving simulator,\nfiltering scenarios based on key safety metrics\n(collision, drivable area compliance, time-to-collision).\nTrajectory understanding is formulated as a reasoning task,\nwith counterfactual questioning introduced to help the model identify critical obstacles.\nWe also design dedicated SFT data to activate the modelâ€™s self-reflection capabilities\nfor iterative trajectory refinement.\nâ€¢\nMulti-turn Group Relative Policy Optimization (mtGRPO)\n:\nWe propose mtGRPO, a novel RL algorithm designed to mitigate the sparse reward problem\nin multi-turn environments.\nBuilding upon GRPO, it ensures training stability\nby maintaining token-level advantage consistency within each turn,\nwhile employing turn-level advantages to distinguish contributions across turns.\nThe algorithm also incorporates a progressive reward mechanism\nthat encourages the model to improve its reasoning accuracy as turns increase.\nâ€¢\nMultimodal multi-turn RL training system\n:\nWe build a dedicated RL post-training system for vision-language autonomous driving\nbased on the veRL framework\n[\n33\n]\n.\nTo address data transfer bottlenecks caused by large image inputs and multi-turn interactions,\nwe implement targeted optimizations achieving\n2.5\nÃ—\n2.5\\times\nthroughput improvement,\nwhich are also applicable to other multimodal RL tasks.\nâ€¢\nComprehensive evaluation on real-world benchmarks\n:\nEvaluated on the NAVSIM benchmark\n[\n3\n]\n, MTDrive achieves a PDMS of 96.2\nwhen using privileged ground-truth perception inputs for planning evaluation,\nand 91.1 under a more realistic setting that relies only on current-frame perception\nwith kinematic modeling for future prediction.\nThis demonstrates the robustness of our multi-turn reasoning framework\nacross different levels of perceptual information availability.\nExtensive experiments validate the effectiveness of our framework\nin handling complex, long-tail driving scenarios\nthat require iterative trajectory refinement.\n2\nRelated Work\n2.1\nVision-Language Models for Autonomous Driving\nVLMs have gained significant traction in AD,\nleveraging cross-modal alignment and zero-shot generalization\nfor high-level decision-making and trajectory prediction.\nExisting VLM-based driving frameworks generally follow two paradigms.\nThe first paradigm utilizes VLMs to generate high-level semantic instructions\nbased on historical trajectories and multi-view images\n[\n34\n,\n35\n]\n.\nThese instructions or latent features then guide a downstream point-to-point planner,\nsuch as a diffusion-based planner or a detection head\n[\n14\n]\n.\nThe second paradigm, which our work follows,\ndirectly employs VLMs to output future driving trajectories\n[\n10\n,\n20\n,\n41\n]\n.\nCompared to the former, this approach is structurally more concise and flexible,\nwhile representing trajectories in natural language\nenhances the modelâ€™s task comprehension and provides superior interpretability.\n2.2\nReinforcement Learning in Autonomous Driving\nDevelopments in RL have significantly reshaped\nthe training paradigms of LLMs and VLMs.\nOptimization techniques such as PPO\n[\n29\n]\n,\nREINFORCE-style variants\n[\n1\n]\n,\nand more efficient methods like ReMax\n[\n18\n]\n, REINFORCE++\n[\n7\n]\n, and GRPO\n[\n31\n]\nhave shown remarkable success in aligning large models\nwith human feedback or complex reasoning tasks.\nIn the AD domain, researchers have increasingly integrated RL\nto enhance policy robustness and generalization.\nFor instance, RAD\n[\n5\n]\ntrains agents in photo-realistic 3DGS environments,\nCarPlanner\n[\n39\n]\nlearns an auto-regressive policy for multimodal trajectories,\nand Alpamayo-R1\n[\n25\n]\nemploys RL to enhance reasoning-action consistency in VLAs.\nNotably, recent works like AlphaDrive\n[\n12\n]\n, TrajHF\n[\n16\n]\n,\nand R2SE\n[\n21\n]\nhave introduced GRPO\nto further boost the generalization of driving policies.\n2.3\nInteractive Multi-turn Reasoning in Reinforcement Learning\nMulti-turn interactive reasoning has emerged as a promising direction\nfor enhancing decision-making in complex tasks.\nIn AD,\nDriveAgent-R1\n[\n42\n]\ndemonstrates this potential\nby adaptively switching between linguistic reasoning and tool-assisted inference,\nachieving improvements over state-of-the-art (SOTA) VLMs.\nFrontier models in general AI (e.g., OpenAIâ€™s o3/o4\n[\n27\n]\n,\nKimi-Researcher\n[\n24\n]\n, and RAGEN\n[\n36\n]\n)\nhave achieved superior intelligence through multi-turn reasoning and iterative refinement.\nHowever, transitioning this capability to AD\npresents challenges in training stability and reward sparsity.\nTo address this, we introduce\nMTDrive\nwith\nmtGRPO\n,\nenabling effective multi-turn policy refinement for autonomous driving.\nFigure 1:\nThe proposed MTDrive framework.\nLeft\n: Multi-turn interaction loopâ€”at each turn\ni\ni\n, the VLM takes front image, ego status, navigation, and historical feedback as input to generate trajectory\nÏ„\ni\n+\n1\n\\tau_{i+1}\n. The Agent evaluates the trajectory and provides per-metric feedback (e.g., collision, drivable area compliance), which is appended for the next turn.\nRight\n: Actor update with mtGRPOâ€”unlike standard GRPO which uses a single sequence-level reward, mtGRPO computes per-turn rewards\nR\ni\nR_{i}\nand advantages\nA\ni\nA_{i}\nacross multiple rollouts, enabling fine-grained credit assignment for each turnâ€™s contribution.\n3\nMethodology\nIn this section, we propose\nMTDrive\n, as illustrated in Fig.\n1\n,\na comprehensive framework that integrates multi-turn interactive data curation,\na novel RL algorithm (mtGRPO), and an effective multimodal RL training system.\nSection\n3.1\nintroduces the PDM Agent,\nwhich leverages collision-related metrics from the NAVSIM simulator\n[\n3\n]\nto provide interactive feedback for trajectory refinement.\nSection\n3.2\ndescribes the data construction for both SFT and RL stages,\nincluding single-turn, multi-turn, and PDM understanding data.\nSection\n3.3\npresents mtGRPO,\na novel RL algorithm that computes advantages separately for each turn\nto address the sparse reward problem in multi-turn settings.\nSection\n3.4\npresents a multimodal multi-turn RL training system\nbuilt on veRL\n[\n33\n]\n, along with 2 optimization strategies for effective training.\n3.1\nPDM Agent\nOur multi-turn framework is designed to be agent-agnosticâ€”it can integrate with any simulation agent that provides trajectory-level feedback.\nIn this work, we instantiate it with the PDM Agent from the NAVSIM benchmark\n[\n3\n]\n,\nwhich offers standardized metrics and enables direct comparison with existing work.\nThe PDM Agent is developed based on the closed-loop benchmark NAVSIM.\nCentral to this framework is the PDM Score (PDMS),\nintroduced in NAVSIM v1 as the primary metric for assessing driving quality.\nAs formulated in Eq.Â (\n1\n),\nthe PDMS is a hybrid metric comprising 5 distinct components\nthat strike a balance between safety constraints, driving comfort, and mission progress.\nPDMS\n=\n(\nâˆ\nm\nâˆˆ\n{\nNC, DAC\n}\nScore\nm\n)\nâŸ\npenalties\nÃ—\n(\nâˆ‘\nw\nâˆˆ\n{\nEP, TTC, C\n}\nWeight\nw\nÃ—\nScore\nw\nâˆ‘\nw\nâˆˆ\n{\nEP, TTC, C\n}\nWeight\nw\n)\nâŸ\nweighted average\n\\text{PDMS}=\\underbrace{\\left(\\prod_{m\\in\\{\\text{NC, DAC}\\}}\\text{Score}_{m}\\right)}_{\\text{penalties}}\\times\\underbrace{\\left(\\frac{\\sum_{w\\in\\{\\text{EP, TTC, C}\\}}\\text{Weight}_{w}\\times\\text{Score}_{w}}{\\sum_{w\\in\\{\\text{EP, TTC, C}\\}}\\text{Weight}_{w}}\\right)}_{\\text{weighted average}}\n(1)\nAfter analyzing the distribution of the metrics,\ncollision-related metricsâ€”No Collisions (NC), Time-to-Collision (TTC),\nand Drivable Area Compliance (DAC)â€”are selected to build the PDM Agent.\nSince the PDM Agent needs to be used in both training and evaluation,\nwe do not include Ego Progress (EP)\nbecause computing EP requires access to ground-truth trajectories.\nThe selected metrics are briefly described below:\nâ€¢\nNC\nmeasures whether the autonomous vehicle (AV) collides with other traffic participants or objects.\nâ€¢\nDAC\nmeasures whether the AV consistently stays within the designated drivable area.\nâ€¢\nTTC\nmeasures whether the AV maintains a sufficient safety margin to other vehicles (typically the lead vehicle) by considering time-to-collision.\nPDM Feedback\nis obtained by summarizing the violated metrics\nidentified by the PDM Agent into textual prompts.\nFor example, with the DAC metric,\ntrajectory points outside the drivable area are extracted and described in text form.\nFor TTC and NC, we similarly extract the coordinates of trajectory points\nthat will result in collisions, as well as the corresponding 3D bounding boxes,\nand present these in textual format.\n3.2\nData Curation\nUnlike traditional multi-turn reasoning SFT (Supervised Fine-Tuning) in LLMs or VLMs,\npretrained models for autonomous driving tasks exhibit significant limitations\nin both trajectory generation quality and instruction-following ability.\nTherefore, our SFT phase not only serves as a cold start\nto enable multi-turn trajectory generation and PDM feedback understanding,\nbut also ensures sufficient trajectory quality for efficient RL sampling.\nThe SFT training datasets are categorized into three types:\nsingle-turn data, multi-turn data, and PDM understanding data, as illustrated in Fig.\n2\n.\nFigure 2:\nOverview of the SFT Data Generation Pipeline.\nTop-left\n: Single-turn data provides the basic trajectory generation ability which takes front-view image, navigation instruction, historical trajectory (2s), and PDM metric description as input to predict future trajectory (4s).\nTop-right\n: PDM understanding data enables the model to interpret PDM feedback through positive/negative QA pairs.\nBottom\n: Multi-turn data is iteratively bootstrapped from single-turnâ€”train on\ni\ni\n-turn data, run inference, obtain PDM feedback, and stack to form\n(\ni\n+\n1\n)\n(i+1)\n-turn samples, enabling feedback-guided trajectory refinement.\nSingle-turn data.\nInitially, single-turn data is included in the SFT training dataset to enable the model to acquire basic trajectory generation capabilities.\nAs shown on the upper left of Fig.\n2\n, we adopt the trajectory dataset from RecogDrive\n[\n15\n]\n, which is built on NAVSIM.\nThe model receives front-view images, navigation instructions,\n2-second historical trajectory, and PDM metric descriptions,\nthen predicts the 4-second future trajectory.\nMulti-turn data.\nTo activate the modelâ€™s multi-turn trajectory reasoning capability,\nwe add multi-turn data to the SFT training dataset.\nAs shown in the second column of Fig.\n2\n,\nthis data is constructed through an iterative bootstrap process.\nStarting from a model trained on last-turn data,\nwe run inference to obtain predicted trajectories, which are then fed to the PDM agent for feedback.\nThe original prompt, model prediction, and PDM feedback are concatenated\nto form the next turnâ€™s input, with the ground-truth trajectory as the target.\nThis process iterates by training on\nk\nk\n-turn data to generate\n(\nk\n+\n1\n)\n(k+1)\n-turn samples.\nSee Appendix\n0.B\nand\n0.A.1\nfor detailed construction steps and examples.\nPDM understanding data.\nTo help the model interpret PDM feedback,\nwe construct question-answering pairs for each PDM metric.\nFor each PDM metric (except Comfort), we design several types of positive and negative samples.\nFor example, regarding the DAC metric,\nthe model is provided with a trajectory point and asked to determine whether it lies within the drivable area, as the examples in the upper right of Fig.\n2\nshow.\nSee Appendix\n0.A.2\nfor detailed examples.\nRL data.\nTo improve training efficiency,\nonly a subset of the NAVSIM training set is used.\nSpecifically, we run evaluations on the training set using the model trained with SFT,\nand categorize the data into three types:\n(1) 2-turn data, where the PDM feedback is not empty after the first inference;\n(2) low-score data, where the PDM score after the first inference\nis below a certain threshold (0.8 in this work);\nand (3) other data.\nTo construct the RL training set, we include all data from categories 1 and 2,\nand randomly sample data from category 3 to balance the distribution.\n3.3\nmtGRPO\nIn RL training for multi-turn reasoning tasks,\ndirectly applying GRPO leads to the sparse reward problem.\nGRPO computes a single reward for each sequence\nand uses this value for advantage calculation across the entire sequence.\nHowever, in multi-turn tasks,\nthe performance may vary significantly across different turns within a sequence.\nFor example, if the first turn performs poorly while the second turn performs well,\nbut the overall sequence reward is positive,\nthe poorly-performing first turn would still be rewarded,\nand the advantage of the well-performing second turn\nwould be diluted by the negative influence of the first turn.\nTo address this issue, we propose a novel reward design\nand the corresponding advantage calculation method for multi-turn reasoning, named mtGRPO.\nDuring a single RL rollout episode,\nthe PDM scorer independently scores the output of each turn\nand provides multiple rewards corresponding to the number of turns in the current rollout.\nUnlike GRPO, where a single reward value is assigned to the whole sequence,\nmtGRPO assigns the reward for each turn to the corresponding tokens of that turn,\ni.e., the reward from the first turn is assigned to the tokens of the first turn,\nthe second turn reward to the tokens of the second turn, and so on.\nWe also incorporate a format score for each turn\nto prevent degradation of format capability during RL training.\nWe provide the detailed reward calculation in Eq.Â (\n2\n),\nwhere\np\ni\n,\nj\np_{i,j}\ndenotes the PDM score of the\nj\nj\n-th turn in the\ni\ni\n-th rollout,\nand\nf\ni\n,\nj\nf_{i,j}\ndenotes the format score of the\nj\nj\n-th turn in the\ni\ni\n-th rollout.\nw\np\n=\n0.8\nw_{p}=0.8\nindicates the weight of the PDM score,\nand\nw\nf\n=\n0.2\nw_{f}=0.2\nindicates the weight of the format score.\nr\ni\n,\nj\n=\nw\np\nâ‹…\np\ni\n,\nj\n+\nw\nf\nâ‹…\nf\ni\n,\nj\nr_{i,j}=w_{p}\\cdot p_{i,j}+w_{f}\\cdot f_{i,j}\n(2)\nAfter the reward assignment,\nadvantage estimation is performed within each turn across the rollout batch,\nfollowing the same normalization method as GRPO.\nThe detailed advantage calculation is provided in Eq.Â (\n3\n),\nwhere\nG\nG\ndenotes the rollout number\nand\nA\n~\ni\n,\nt\n\\tilde{A}_{i,t}\ndenotes the advantage of the\nt\nt\n-th token in the\ni\ni\n-th rollout.\nThe indices\ni\ni\nand\nt\nt\nidentify which turn\nj\nj\nthe current token belongs to.\nA\n~\ni\n,\nt\n=\nr\ni\n,\nj\nâˆ’\n1\nG\nâ€‹\nâˆ‘\ni\n=\n1\nG\nr\ni\n,\nj\nstd\nâ€‹\n(\n{\nr\ni\n,\nj\n}\ni\n=\n1\nG\n)\n\\tilde{A}_{i,t}=\\frac{r_{i,j}-\\frac{1}{G}\\sum_{i=1}^{G}r_{i,j}}{\\mathrm{std}\\!\\left(\\{r_{i,j}\\}_{i=1}^{G}\\right)}\n(3)\nThe mtGRPO objective function is given by:\nJ\nmtGRPO\nâ€‹\n(\nÎ¸\n)\n=\n\\displaystyle\\qquad J_{\\mathrm{mtGRPO}}(\\theta)=\nğ”¼\nq\nâˆ¼\nP\n(\nQ\n)\n,\n{\no\ni\n}\ni\n=\n1\nG\nâˆ¼\nÏ€\nÎ¸\nold\n(\nâ‹…\nâˆ£\nq\n)\n1\nG\nâˆ‘\ni\n=\n1\nG\n1\n|\no\ni\n|\nâˆ‘\nt\n=\n1\n|\no\ni\n|\n{\n\\displaystyle\\;\\mathbb{E}_{\\begin{subarray}{c}q\\sim P(Q),\\;\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\mathrm{old}}}(\\cdot\\mid q)\\end{subarray}}\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\Bigg\\{\n(4)\nmin\nâ¡\n(\nÏ€\nÎ¸\nâ€‹\n(\no\ni\n,\nt\nâˆ£\nq\n,\no\ni\n,\n<\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\no\ni\n,\nt\nâˆ£\nq\n,\no\ni\n,\n<\nt\n)\nâ€‹\nA\n~\ni\n,\nt\n,\nclip\nâ€‹\n(\nÏ€\nÎ¸\nâ€‹\n(\no\ni\n,\nt\nâˆ£\nq\n,\no\ni\n,\n<\nt\n)\nÏ€\nÎ¸\nold\nâ€‹\n(\no\ni\n,\nt\nâˆ£\nq\n,\no\ni\n,\n<\nt\n)\n,\n1\nâˆ’\nÏµ\n,\n1\n+\nÏµ\n)\nâ€‹\nA\n~\ni\n,\nt\n)\n\\displaystyle\\hskip-18.49988pt\\min\\Bigg(\\frac{\\pi_{\\theta}(o_{i,t}\\mid q,o_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t}\\mid q,o_{i,<t})}\\tilde{A}_{i,t},\\mathrm{clip}\\!\\left(\\frac{\\pi_{\\theta}(o_{i,t}\\mid q,o_{i,<t})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,t}\\mid q,o_{i,<t})},1-\\epsilon,1+\\epsilon\\right)\\tilde{A}_{i,t}\\Bigg)\nâˆ’\nÎ²\nğ”»\nKL\n(\nÏ€\nÎ¸\nâˆ¥\nÏ€\nğ‘Ÿğ‘’ğ‘“\n)\n}\n\\displaystyle\\qquad-\\beta\\,\\mathbb{D}_{\\mathrm{KL}}\\!\\left(\\pi_{\\theta}\\,\\|\\,\\pi_{\\mathit{ref}}\\right)\\Bigg\\}\nThe detailed procedure is illustrated in Algorithm\n1\n.\nAlgorithm 1\nMulti-Turn Group Relative Policy Optimization (mtGRPO)\n1:\nInput:\ninitial policy\nÏ€\nÎ¸\ninit\n\\pi_{\\theta}^{\\text{init}}\n, reward models\nr\nÏ•\nr_{\\phi}\n, task prompts\nD\nD\n, hyperparameters\nÏµ\n,\nÎ²\n,\nÎ¼\n\\epsilon,\\beta,\\mu\n, max turn number\nN\nN\n2:\nInitialize policy:\nÏ€\nÎ¸\nâ†\nÏ€\nÎ¸\ninit\n\\pi_{\\theta}\\leftarrow\\pi_{\\theta}^{\\text{init}}\n3:\nfor\niteration\n=\n1\n=1\nto\nI\nI\ndo\n4:\nReference model:\nÏ€\nref\nâ†\nÏ€\nÎ¸\n\\pi_{\\text{ref}}\\leftarrow\\pi_{\\theta}\n5:\nfor\nstep\n=\n1\n=1\nto\nM\nM\ndo\n6:\nSample a batch\nD\nb\nD_{b}\nfrom\nD\nD\n7:\nUpdate the old policy model:\nÏ€\nÎ¸\nold\nâ†\nÏ€\nÎ¸\n\\pi_{\\theta}^{\\text{old}}\\leftarrow\\pi_{\\theta}\n8:\nFor each question\nq\nâˆˆ\nD\nb\nq\\in D_{b}\n, sample\nG\nG\noutputs\n{\no\ni\n}\ni\n=\n1\nG\nâˆ¼\nÏ€\nÎ¸\nold\n(\nâ‹…\n|\nq\n)\n\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta}^{\\text{old}}(\\cdot|q)\n9:\nCompute turn rewards\nr\ni\nâ€‹\nj\nr_{ij}\nfor\ni\n=\n1\n,\nâ€¦\n,\nG\ni=1,\\ldots,G\nand\nj\n=\n1\n,\nâ€¦\n,\nN\nj=1,\\ldots,N\nusing\nr\nÏ•\nr_{\\phi}\n10:\nCompute\nA\n~\ni\n,\nt\n\\tilde{A}_{i,t}\nthrough turn-level group relative advantage estimation\n11:\nfor\nGRPO iteration\n=\n1\n=1\nto\nÎ¼\n\\mu\ndo\n12:\nUpdate policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nby maximizing the GRPO objective (Eq.Â (\n4\n))\n13:\nend\nfor\n14:\nend\nfor\n15:\nend\nfor\n16:\nOutput:\noptimized policy\nÏ€\nÎ¸\n\\pi_{\\theta}\n3.4\nMultimodal Multi-turn RL Training System\nOur training system is built on top of veRL\n[\n33\n]\n,\na flexible RL post-training framework that supports rollout generation,\nactor and reference model inference, and policy optimization.\nIts wide adoption and active community make it well-suited for extension to multimodal AD tasks.\nWe extend veRL to support multi-turn interactions with a PDM-based environment\nfor vision-language autonomous driving.\nThe training pipeline operates iteratively, with each step consisting of:\n(1)\nRollout generation\n:\nthe policy interacts with the PDM agent across multiple turns\nto iteratively refine trajectories and receive PDMS-based rewards;\n(2)\nLog probability recomputation\n:\nforward pass through the actor model to obtain log probabilities;\n(3)\nReference model inference\n:\nforward pass through the reference model for KL regularization;\n(4)\nActor update\n:\nadvantage calculation and actor update via backpropagation.\nThis iterative process introduces data transfer bottlenecks in the multimodal setting.\nIn veRLâ€™s architecture, the rollout generation runs as an independent process,\nrequiring all generated data including image tensors\nto be serialized and transferred through an object store.\nThis leads to two sources of overhead:\n(1)\ninter-process\n: rollout results must wait until the entire batch finishes\nbefore being serialized and dispatched to training workers;\n(2)\nintra-process\n: even when actor and reference model are co-located,\nthe controller dispatches data to each module separately,\ncausing redundant deserialization of the same inputs.\nWe address these challenges with two complementary optimizations,\nas illustrated in Fig.\n3\n.\nInter-Process Streaming Serialization (IPSS).\nIn multi-turn rollout, different samples complete at varying times\ndue to differences in response length.\nThe naive approach waits for all rollouts to finish before serializing the entire batch,\nleaving serialization time as pure overhead.\nWe observe that this serialization can be overlapped with generation:\nas each rollout completes, we immediately offload its multimodal data serialization\nto a dedicated thread pool, overlapping with the generation of remaining rollouts.\nIntra-Process Tensor Cache (IPTC).\nWhen log probability recomputation, reference model inference,\nand actor training are co-located in the same process,\nthe controller still dispatches data to each module separately,\ncausing redundant deserialization.\nTo eliminate this overhead, we implement a simple caching mechanism.\nUpon the first moduleâ€™s execution, IPTC caches shared inputs\n(e.g., tokenized sequences, attention masks, and visual embeddings)\ndirectly in GPU memory.\nTogether, these optimizations reduce per-step training time\nfrom\nâˆ¼\n1250\n{\\sim}1250\ns to\nâˆ¼\n490\n{\\sim}490\ns (\n2.5\nÃ—\n2.5\\times\nspeedup),\nwith IPSS contributing\nâˆ¼\n1.5\nÃ—\n{\\sim}1.5\\times\nand IPTC\ncontributing an additional\nâˆ¼\n1.7\nÃ—\n{\\sim}1.7\\times\nin our experiment settings\n(Section\n4.1\n, Table\n1\n).\nRollout\nRollout Process\nObject Store\nTraining Process\nLogProb\nRef\nActor\nTensor Cache\nIPTC\nIPSS\nBefore:\nr0\nr1\nr2\nr3\nserialize batch\nAfter:\nr0\nr1\nr2\nr3\noverlapped\nrollout\nserialize\n(a) Data Flow Architecture  (b) IPSS Timeline\nFigure 3:\nData Transfer Optimization for Multimodal Multi-turn RL Training.\n(a) Rollout and training workers run in separate processes.\nIPSS streams serialization during rollout;\nIPTC enables tensor sharing among co-located modules via a shared cache.\n(b) IPSS overlaps serialization with rollout generation instead of blocking.\n4\nExperiments\n4.1\nExperimental Setting\nSupervised Finetuning.\nWe finetune Qwen2.5-VL-7B-Instruct for 6-turn dialogue with a maximum token length of 11,500.\nWe use 215,000 data samples in total,\nincluding approximately 80,000 single-turn samples, 50,000 2-turn samples,\n5,000 multi-turn samples, and 80,000 PDM understanding samples, and train for 4 epochs.\nThe learning rate is set to\n4\nÃ—\n10\nâˆ’\n5\n4\\times 10^{-5}\n, and the global batch size is 128.\nIt takes about 1 day for training on a cluster of 64 A800 GPUs.\nReinforcement Learning.\nFor reinforcement learning, we set the group size to 8,\nglobal batch size to 256, with a mini-batch size of 128.\nWe use a constant learning rate of\n1\nÃ—\n10\nâˆ’\n6\n1\\times 10^{-6}\n, set the KL penalty coefficient to 0.01\nwith KL divergence computed using the K3 method.\nWe use 13,000 data samples and train for 300 steps (about 6 epochs)\non a 32-GPU cluster, which takes around 2 days.\nEvaluation.\nDuring inference, we adopt the same sampling configuration\nas in the training rollout stage and set the max reasoning number to 6.\nTable 1:\nPerformance Comparison on NAVSIM Benchmark.\nâ€ \nand\nâ€¡\ndenote supervised fine-tuning on single-turn and multi-turn data, respectively.\nâˆ—\nindicates the kinematic model setting where future agent positions are predicted via constant-velocity motion models,\nsimulating practical deployment scenarios without privileged information.\nâˆ—âˆ—\nindicates the oracle setting with access to ground-truth future agent states,\nproviding an upper-bound assessment of the planning capability.\nMethod\nNC\nâ†‘\n\\uparrow\nDAC\nâ†‘\n\\uparrow\nTTC\nâ†‘\n\\uparrow\nCF\nâ†‘\n\\uparrow\nEP\nâ†‘\n\\uparrow\nPDMS\nâ†‘\n\\uparrow\nTraditional End-to-End methods\nUniAD\n[\n9\n]\n97.8\n91.9\n92.9\n100\n78.8\n83.4\nTransFuser\n[\n28\n]\n97.7\n92.8\n92.8\n100\n84.0\n84.0\nDiffusionDrive\n[\n19\n]\n98.2\n96.2\n94.7\n100\n82.2\n88.1\nHydra-NeXt\n[\n17\n]\n98.1\n97.7\n94.6\n100\n81.8\n88.6\nGoalFlow\n[\n37\n]\n98.4\n98.3\n94.6\n100\n85.0\n90.3\nVLM-Diffusion-based methods\nReCogDrive\n[\n15\n]\n97.9\n97.3\n94.9\n100\n87.3\n90.8\nReflectDrive\nâˆ—\n[\n13\n]\n97.7\n99.3\n93.5\n100\n86.9\n91.1\nReflectDrive\nâˆ—âˆ—\n[\n13\n]\n99.7\n99.5\n99.1\n99.9\n88.9\n94.7\nHuman\n[\n3\n]\n100.0\n100.0\n100.0\n99.9\n87.5\n94.8\nQwenVL2.5-8B\nâ€ \n[\n2\n]\n97.4\n92.5\n92.7\n100\n79.0\n83.7\nMTDrive\nâ€¡\n(Ours)\n99.1\n95.5\n97.5\n99.9\n81.8\n88.1\nMTDrive\nâˆ—\n(Ours)\n97.5\n98.2\n91.8\n99.8\n90.6\n91.1\nMTDrive\nâˆ—âˆ—\n(Ours)\n100.0\n98.2\n99.9\n99.8\n93.5\n96.2\n4.2\nMain Results\nTable\n1\npresents the performance of MTDrive and other models\non the NAVSIM dataset.\nWe report results under two perception settings:\n(1)\nKinematic model\n(\nâˆ—\n),\nwhere surrounding agents are assumed to move at constant velocityâ€”a practical setting for deployment;\n(2)\nGT Oracle\n(\nâˆ—âˆ—\n),\nusing privileged ground-truth boxesâ€”suitable for auto-labeling applications.\nEven with SFT training alone, we achieve a performance of 88.1,\nimproving by 4.4 points compared to the single-turn baseline of 83.7,\nwhich demonstrates the strong capability of the multi-turn reasoning paradigm.\nWith RL training, MTDrive achieves a PDMS of 96.2 when using ground-truth perception inputs,\neven surpassing the human driving benchmark of 94.8.\nSpecifically, under the kinematic setting, MTDrive\nâˆ—\nachieves 91.1,\ndemonstrating strong performance compared to traditional end-to-end methods.\nWith the GT oracle, MTDrive\nâˆ—âˆ—\nachieves 96.2,\ndemonstrating the potential of multi-turn reasoning for high-quality trajectory annotation.\nNotably, for the EP metric, we donâ€™t provide EP information in the multi-turn prompts\n(as supplying EP information would reveal the ground truth trajectory).\nThe model attains this high EP score purely based on reward feedback during RL training.\n4.3\nAblation Study\nTable 2:\nAblation Study of SFT Training\nID\nOne Stage\nPDM Data\nTurns\nPDMS\nâ†‘\n\\uparrow\n1\n-\n-\n1\n83.7\n2\n2\n84.9\n3\nâœ“\n2\n87.3\n4\nâœ“\nâœ“\n2\n87.7\n5\nâœ“\nâœ“\n6\n88.1\nIn this section, we present an extensive ablation study to quantify\nthe contribution of each component in our method.\nThe ablation studies are structured along two dimensions: SFT vs. RL, and data vs. algorithm.\nFor all SFT experiments, we train for a total of 6 epochs\nand select the epoch with the highest score as the final result.\nTable\n2\nshows the impact of dataset volume\nand training methods on SFT training.\nIn Experiment 1, we follow RecogDrive by using only 80,000 single-turn samples\nfor SFT training, serving as our baseline.\nIn Experiment 2, we attempted two-stage SFT training:\nthe first stage uses 80,000 single-turn samples,\nand the second stage uses 50,000 2-turn samples.\nThe results show that 2-turn reasoning improves by 1.2 points\ncompared to the single-turn baseline.\nIn Experiment 3, we performed single-stage training\nwith 80,000 single-turn samples and 50,000 2-turn samples\nand observed an increase of 3.6 points in 2-turn performance\nover single-turn performance.\nIn Experiment 4, we introduced PDM understanding data,\nwhich led to improvements in 2-turn results with 0.4.\nFinally, in Experiment 5, we incorporated multi-turn data\nand increased the number of reasoning turns to 6,\nresulting in further gains in multi-turn PDMS scores with 88.1.\nThe highest score of our best-performing 6-turn SFT model was achieved at epoch 4.\nTable 3:\nAblation Study of RL Strategies\nID\nMethod\nNC\nâ†‘\n\\uparrow\nDAC\nâ†‘\n\\uparrow\nTTC\nâ†‘\n\\uparrow\nCF\nâ†‘\n\\uparrow\nEP\nâ†‘\n\\uparrow\nPDMS\nâ†‘\n\\uparrow\n1\nGRPO (seq-level reward)\n99.9\n96.0\n99.7\n99.9\n92.0\n94.2\n2\nmtGRPO (intra-group norm)\n99.9\n98.9\n99.7\n99.6\n90.6\n95.2\n3\nmtGRPO (cross-turn norm)\n100.0\n98.2\n99.9\n99.8\n93.5\n96.2\nTable\n3\npresents the ablation of different reward\nand advantage calculation strategies.\nFor the RL experiments, we use the 6-turn SFT model as the initial checkpoint\nand train for 300 steps (almost 6 epochs) on the 13,000-sample dataset\ndescribed in Section\n3.2\nand set the format weight to 0.2, as mentioned in Section\n3.3\n.\nIn Experiment 1, we adopt the GRPO,\nby averaging multi-turn rewards to obtain a sequence-level reward.\nExperimental results show that, compared to the baseline SFT model,\nthe PDMS score increased by 6.1 points.\nIn Experiment 2, we employ mtGRPO with intra-group normalization\nfor advantage calculation.\nThis further increases the PDMS score by 1 point.\nFinally, when we further modify mtGRPO to use cross-turn normalization\nwithin the group for advantage calculation,\nthe multi-turn score increases to 96.2 points.\n(a) Front-view images  (b) Multi-turn trajectory\nFigure 4:\nMulti-turn reasoning visualization.\nThe left figures represent images from the left, center, and right front-facing cameras,\nwhile the right figures display the results of one to multi turns of reasoning.\nIn the right images, red crosses indicate trajectory points that violate the DAC metric,\nsolid blue dots indicate obstacles where potential collisions may occur according to the TTC metric,\nand solid yellow dots indicate obstacles where collisions may occur according to the NC metric.\n4.4\nQualitative Results\nTo further demonstrate the ability of multi-turn reasoning to improve trajectory quality,\nFig.\n4\npresents 7 scenarios.\nIt can be observed that, during the first round of reasoning,\nthe model tends to aggressively explore forward trajectories.\nSubsequently, problematic trajectory points are progressively refined based on PDM feedback,\nwhile non-problematic points are preserved.\nIn the first 2 scenarios, the model initially generates trajectory points\noutside the drivable area which violates the DAC metric.\nIn subsequent reasoning turns, the model progressively corrects the problematic trajectories\nwhile preserving valid points near the ego vehicle.\nIn the 4th scenario, the initial trajectory generated in the first reasoning\npasses through the crosswalk, resulting in a potential collision risk with pedestrians.\nAfter receiving PDM feedback indicating potential pedestrian activity,\nit gradually adjusts the trajectory to stop before the crosswalk to avoid the collision.\nIn the 5th and 6th queueing scenarios at intersections,\nthe model first triggers NC and then TTC violations,\nand ultimately maintains a safe distance from the front vehicle by the 3rd turn.\nIn the last scenario, the model encounters a situation\nwhere the following vehicle is approaching at high speed, posing a rear-end collision risk.\nAfter 3 reasoning rounds, the model avoids the risk by accelerating its own speed.\nThese scenarios fully showcase the rationality and superiority\nof the multi-turn reasoning approach in autonomous driving.\n5\nConclusion\nIn this work, we propose MTDrive, a multi-turn VLM framework for autonomous driving.\nA multi-turn data generation pipeline is introduced to elicit the modelâ€™s reflective capabilities.\nBuilding on this foundation,\nmtGRPO is designed to address the sparse-reward challenge in multi-turn environments.\nFinally, for multimodal multi-turn RL training,\nwe implement two effective optimization techniques, IPSS and IPTC,\nto improve training throughput.\n6\nLimitations and Future Work\nPerception Dependency.\nOur current approach is a VLM-based planner that relies on perception ground truth\nor an additional perception module to provide PDM feedback.\nA promising direction for future work is to integrate perception data into the VLM,\nenabling the model to generate PDM feedback solely based on its own reasoning.\nSuch perception tasks are also expected to enhance the modelâ€™s understanding of the environment.\nAuto-Labeling.\nSince our current metrics even surpass those of human drivers,\nwe believe that the multi-turn reasoning framework holds promise for trajectory annotation,\noffering a high-quality alternative to human-driver data.\nThis could help address the lack of multimodal data in imitation learning for autonomous driving.\nGeneralization to Other Agents.\nWhile we instantiate our framework with the rule-based PDM Agent,\nthe multi-turn interaction paradigm can work with any agent that provides trajectory-level feedback.\nFuture work could explore learned reward agents or\nadapt the framework to agents in other simulation environments\n(e.g., CARLA\n[\n4\n]\n, Waymax\n[\n6\n]\n, AlpaSim\n[\n26\n]\n).\nReferences\n[1]\nA. Ahmad\net al.\n(2024)\nBack to basics: revisiting reinforce style optimization for learning from human feedback in llms\n.\narXiv preprint arXiv:2402.14740\n.\nCited by:\nÂ§2.2\n.\n[2]\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang,\net al.\n(2025)\nQwen2.5-vl technical report\n.\narXiv preprint arXiv:2502.13923\n.\nCited by:\nTable 1\n.\n[3]\nD. Dauner, M. Hallgarten, A. Geiger, and K. Chitta\n(2024)\nNAVSIM: data-driven non-reactive autonomous vehicle simulation and benchmarking\n.\nAdvances in Neural Information Processing Systems\n37\n.\nCited by:\n4th item\n,\nÂ§3.1\n,\nÂ§3\n,\nTable 1\n.\n[4]\nA. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun\n(2017)\nCARLA: an open urban driving simulator\n.\nIn\nConference on Robot Learning\n,\npp.Â 1â€“16\n.\nCited by:\nÂ§6\n.\n[5]\nH. Gao\net al.\n(2025)\nRAD: training an end-to-end driving policy via large-scale 3dgs-based reinforcement learning\n.\narXiv preprint\n.\nCited by:\nÂ§2.2\n.\n[6]\nC. Gulino, J. Fu, W. Luo, G. Tucker, E. Bronstein, Y. Lu, J. Harber, X. Pan, Y. Wang, X. Chen,\net al.\n(2024)\nWaymax: an accelerated, data-driven simulator for large-scale autonomous driving research\n.\nAdvances in Neural Information Processing Systems\n36\n.\nCited by:\nÂ§6\n.\n[7]\nJ. Hu\n(2025)\nREINFORCE++: a simple and efficient approach for aligning large language models\n.\narXiv preprint arXiv:2501.03262\n.\nCited by:\nÂ§2.2\n.\n[8]\nS. Hu, L. Chen, P. Wu, H. Li, J. Yan, and D. Tao\n(2022)\nST-p3: end-to-end vision-based autonomous driving via spatial-temporal feature learning\n.\nEuropean Conference on Computer Vision\n,\npp.Â 533â€“549\n.\nCited by:\nÂ§1\n.\n[9]\nY. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang,\net al.\n(2023)\nPlanning-oriented autonomous driving\n.\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 17853â€“17862\n.\nCited by:\nÂ§1\n,\nTable 1\n.\n[10]\nJ. Hwang, R. Xu, H. Lin, W. Hung, J. Ji, K. Choi, D. Huang, T. He, P. Covington, B. Sapp,\net al.\n(2024)\nEmma: end-to-end multimodal model for autonomous driving\n.\narXiv preprint arXiv:2410.23262\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[11]\nB. Jiang, S. Chen, B. Liao, X. Zhang, W. Yin, Q. Zhang, C. Huang, W. Liu, and X. Wang\n(2024)\nSenna: bridging large vision-language models and end-to-end autonomous driving\n.\narXiv preprint arXiv:2410.22313\n.\nCited by:\nÂ§1\n.\n[12]\nB. Jiang, S. Chen, Q. Zhang, W. Liu, and X. Wang\n(2025)\nAlphadrive: unleashing the power of vlms in autonomous driving via reinforcement learning and reasoning\n.\narXiv preprint arXiv:2503.07608\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\n[13]\nP. Li, Y. Zheng, Y. Wang, H. Wang, H. Zhao, J. Liu, X. Zhan, K. Zhan, and X. Lang\n(2025)\nDiscrete diffusion for reflective vision-language-action models in autonomous driving\n.\narXiv preprint arXiv:2509.20109\n.\nCited by:\nTable 1\n,\nTable 1\n.\n[14]\nY. Li\net al.\n(2024)\nRecogDrive: a vlm-based autonomous driving dataset for recognition tasks\n.\narXiv preprint\n.\nCited by:\nÂ§2.1\n.\n[15]\nY. Li, K. Xiong, X. Guo, F. Li, S. Yan, G. Xu, L. Zhou, L. Chen, H. Sun, B. Wang, G. Chen, H. Ye, W. Liu, and X. Wang\n(2025)\nReCogDrive: a reinforced cognitive framework for end-to-end autonomous driving\n.\narXiv preprint arXiv:2506.08052\n.\nCited by:\nÂ§3.2\n,\nTable 1\n.\n[16]\nY. Li\net al.\n(2025)\nTrajHF: trajectory optimization with human feedback for autonomous driving\n.\narXiv preprint\n.\nCited by:\nÂ§2.2\n.\n[17]\nZ. Li, S. Wang, S. Lan, Z. Yu, Z. Wu, and J. M. Alvarez\n(2025)\nHydra-next: robust closed-loop driving with open-loop training\n.\narXiv preprint arXiv:2503.12030\n.\nCited by:\nTable 1\n.\n[18]\nZ. Li, T. Xu, Y. Zhang,\net al.\n(2023)\nReMax: a simple, effective, and efficient reinforcement learning method for aligning large language models\n.\narXiv preprint arXiv:2310.10505\n.\nCited by:\nÂ§2.2\n.\n[19]\nB. Liao, S. Chen, H. Yin, B. Jiang, C. Wang, S. Yan, X. Zhang, X. Li, Y. Zhang, Q. Zhang, and X. Wang\n(2024)\nDiffusionDrive: truncated diffusion model for end-to-end autonomous driving\n.\narXiv preprint arXiv:2411.15139\n.\nCited by:\nTable 1\n.\n[20]\nX. Liu, Z. Zhong, Y. Guo, Y. Liu, Z. Su, Q. Zhang, J. Wang, Y. Gao, Y. Zheng, Q. Lin,\net al.\n(2025)\nReasonPlan: unified scene prediction and decision reasoning for closed-loop autonomous driving\n.\narXiv preprint arXiv:2505.20024\n.\nCited by:\nÂ§2.1\n.\n[21]\nY. Liu\net al.\n(2025)\nR2SE: reward-to-score estimation for grpo-based autonomous driving\n.\narXiv preprint\n.\nCited by:\nÂ§2.2\n.\n[22]\nA. Marcu, L. Chen, J. HÃ¼nermann, A. Karnsund, B. Hanotte, P. Chidananda, S. Nair, V. Badrinarayanan, A. Kendall, J. Shotton,\net al.\n(2024)\nLingoqa: visual question answering for autonomous driving\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 252â€“269\n.\nCited by:\nÂ§1\n.\n[23]\nZ. Mei, C. Wei, Y. Zhang, Y. Yao,\net al.\n(2025)\nAReaL: scaling reinforcement learning for language reasoning through fully asynchronous training\n.\narXiv preprint arXiv:2505.24298\n.\nCited by:\nÂ§1\n.\n[24]\nMoonshot\n(2025)\nKimi-researcher: multi-turn tool calling for research agents\n.\nTechnical Report\n.\nCited by:\nÂ§2.3\n.\n[25]\nNVIDIA, :, Y. Wang, W. Luo, J. Bai, Y. Cao, T. Che, K. Chen, Y. Chen, J. Diamond, Y. Ding, W. Ding, L. Feng, G. Heinrich, J. Huang, P. Karkus, B. Li, P. Li, T. Lin, D. Liu, M. Liu, L. Liu, Z. Liu, J. Lu, Y. Mao, P. Molchanov, L. Pavao, Z. Peng, M. Ranzinger, E. Schmerling, S. Shen, Y. Shi, S. Tariq, R. Tian, T. Wekel, X. Weng, T. Xiao, E. Yang, X. Yang, Y. You, X. Zeng, W. Zhang, B. Ivanovic, and M. Pavone\n(2026)\nAlpamayo-r1: bridging reasoning and action prediction for generalizable autonomous driving in the long tail\n.\nExternal Links:\n2511.00088\n,\nLink\nCited by:\nÂ§2.2\n.\n[26]\nNVIDIA, Y. Cao, R. de Lutio, S. Fidler, G. G. Cobo, Z. Gojcic, M. Igl, B. Ivanovic, P. Karkus, J. M. Esturo, M. Pavone, A. Smith, E. Tanimura, M. Tyszkiewicz, M. Watson, Q. Wu, and L. Zhang\n(2025)\nAlpaSim: a modular, lightweight, and data-driven research simulator for autonomous driving\n.\nNote:\nhttps://github.com/NVlabs/alpasim\nCited by:\nÂ§6\n.\n[27]\nOpenAI\n(2025)\nIntroducing o3 and o4-mini\n.\nNote:\nhttps://openai.com/index/introducing-o3-and-o4-mini/\nCited by:\nÂ§2.3\n.\n[28]\nA. Prakash, K. Chitta, and A. Geiger\n(2021)\nMulti-modal fusion transformer for end-to-end autonomous driving\n.\narXiv preprint arXiv:2104.09224\n.\nCited by:\nTable 1\n.\n[29]\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov\n(2017)\nProximal policy optimization algorithms\n.\narXiv preprint arXiv:1707.06347\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\n[30]\nH. Shao, Y. Hu, L. Wang, G. Song, S. L. Waslander, Y. Liu, and H. Li\n(2024)\nLmdrive: closed-loop end-to-end driving with large language models\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 15120â€“15130\n.\nCited by:\nÂ§1\n.\n[31]\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang,\net al.\n(2024)\nDeepSeekMath: pushing the limits of mathematical reasoning in open language models\n.\narXiv preprint arXiv:2402.03300\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\n[32]\nG. Shen, Z. Wang, S. Balachandran,\net al.\n(2024)\nNeMo-aligner: scalable toolkit for efficient model alignment\n.\narXiv preprint arXiv:2405.01481\n.\nCited by:\nÂ§1\n.\n[33]\nG. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu\n(2024)\nHybridFlow: a flexible and efficient rlhf framework\n.\narXiv preprint arXiv:2409.19256\n.\nCited by:\n3rd item\n,\nÂ§1\n,\nÂ§3.4\n,\nÂ§3\n.\n[34]\nX. Tian, J. Gu, B. Li, Y. Liu, Y. Wang, Z. Zhao, K. Zhan, P. Jia, X. Lang, and H. Zhao\n(2024)\nDrivevlm: the convergence of autonomous driving and large vision-language models\n.\narXiv preprint arXiv:2402.12289\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[35]\nS. Wang, Z. Yu, X. Jiang, S. Lan, M. Shi, N. Chang, J. Kautz, Y. Li, and J. M. Alvarez\n(2025)\nOmnidrive: a holistic vision-language dataset for autonomous driving with counterfactual reasoning\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 22442â€“22452\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[36]\nZ. Wang\net al.\n(2025)\nRAGEN: understanding self-evolution in llm agents via multi-turn reinforcement learning\n.\narXiv preprint arXiv:2504.20073\n.\nCited by:\nÂ§2.3\n.\n[37]\nZ. Xing, X. Zhang, Y. Hu, B. Jiang, T. He, Q. Zhang, X. Long, and W. Yin\n(2025)\nGoalFlow: goal-driven flow matching for multimodal trajectories generation in end-to-end autonomous driving\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 1602â€“1611\n.\nCited by:\nTable 1\n.\n[38]\nZ. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K. K. Wong, Z. Li, and H. Zhao\n(2024)\nDrivegpt4: interpretable end-to-end autonomous driving via large language model\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\nÂ§1\n.\n[39]\nD. Zhang\net al.\n(2025)\nCarPlanner: consistent auto-regressive trajectory planning for large-scale reinforcement learning in autonomous driving\n.\narXiv preprint\n.\nCited by:\nÂ§2.2\n.\n[40]\nY. Zhang, X. Liu, Y. Dong, J. Tang,\net al.\n(2025)\nSlime: a simple yet efficient rl framework for llm reasoning\n.\narXiv preprint\n.\nCited by:\nÂ§1\n.\n[41]\nR. Zhao, Q. Yuan, J. Li, H. Hu, Y. Li, C. Zheng, and F. Gao\n(2025)\nSce2drivex: a generalized mllm framework for scene-to-drive learning\n.\narXiv preprint arXiv:2502.14917\n.\nCited by:\nÂ§2.1\n.\n[42]\nW. Zheng, X. Mao, N. Ye, P. Li, K. Zhan, X. Lang, and H. Zhao\n(2025)\nDriveAgent-r1: advancing vlm-based autonomous driving with active perception and hybrid thinking\n.\nExternal Links:\n2507.20879\n,\nLink\nCited by:\nÂ§2.3\n.\nAppendix\nAppendix 0.A\nPrompt Example\n0.A.1\nmulti-turn data\nQuestion\n: <image>\\nAs an autonomous driving system, predict the vehicleâ€™s trajectory based on:\\n1. Visual perception from front camera view\\n2. Historical motion context (last 4 timesteps): - t-3: (-9.44, +0.02, -0.02) - t-2: (-6.21, -0.02, 0.0) - t-1: (-3.05, -0.03, 0.0) - t-0: (0.0, 0.0, 0.0)\\n3. Active navigation command: [GO STRAIGHT]\\nOutput requirements:\\n- Predict 8 future trajectory points\\n- Each point format: (x:float, y:float, heading:float)\\n- Use [PT, â€¦] to encapsulate the trajectory\\n- Maintain numerical precision to 2 decimal places<|im_end|>\\n<|im_start|>assistant\\nHere is the planning trajectory [PT, (+3.1, 0.0, 0.0), (+6.21, 0.0, 0.0), (+9.31, 0.0, 0.0), (+12.42, 0.0, 0.0), (+15.52, 0.0, 0.0), (+18.62, 0.0, 0.0), (+21.73, 0.0, 0.0), (+24.83, 0.0, 0.0)].<|im_end|><|im_start|>user\\nThis was your previous trajectory prediction. Analyze this trajectory data and propose a revised prediction. Consider the following identified issues:\\n1. The TTC metric reveals that certain trajectory points fail to maintain a safe distance from surrounding objects. The format for describing these objects is: (x, y, z, l, w, h, heading, name). Where x, y, z are the center coordinates of the object in ego-coordinate system, l, w, h are length, width, height of the bounding box, heading is the object heading, and name is the object class name. The specific trajectory points and their corresponding objects are as follows:\\nTrajectory Point 1: (+21.73, 0.0, 0.0) with Objects: (34.14, -0.85, 1.59, 10.26, 2.95, 4.39, -0.02, vehicle).\\nBased on these issues, provide an improved trajectory prediction.\nAnswer\n: Here is the planning trajectory [PT, (+2.97, +0.02, 0.0), (+5.82, +0.02, 0.0), (+8.54, 0.0, -0.03), (+11.17, -0.07, -0.05), (+13.73, -0.22, -0.09), (+16.21, -0.56, -0.16), (+18.62, -1.10, -0.24), (+20.94, -1.77, -0.28)].\n0.A.2\npdm understanding data\nQuestion\n: <image>\\nYou are a vehicle trajectory prediction model for an autonomous driving system. One of your key metrics is Drivable Area Compliance (DAC), which assesses whether trajectory points remain within the drivable area. Given a trajectory prediction point and a front-view image, where trajectory point is formatted as (x: float, y: float, heading: float), your task is to identify if the given trajectory point stays within the drivable area.\\nTrajectory point is (+24.87, 0.0, 0.0).\\nOutput Requirements:\\nFormat: Yes or No\\nMaintain numerical precision to two decimal places.\nAnswer\n: No.\nAppendix 0.B\nMulti-turn Data Curation\nWe first train Qwen2.5VL on single-turn data,\nthis model is referred to as\nQwen2.5VL_sft1\n,\nto obtain a model with basic trajectory generation abilities.\nWe then use\nQwen2.5VL_sft1\nto perform inference on the single-turn dataset,\nobtaining sequences of predicted trajectories.\nBy feeding the generated trajectories into the PDM agent,\nwe obtain the corresponding PDM feedback.\nThen we concatenate the prompts from the single-turn data,\nthe model-generated trajectories, and the corresponding PDM feedback,\nto construct the prompt for the second turn data,\nusing the same ground-truth from the single-turn data as the second turnâ€™s ground-truth.\nWe further employ the constant velocity model provided by NAVSIM to follow the above steps and generate additional 2-turn data, because 2-turn samples from\nQwen2.5VL_sft1\nare far fewer than single-turn ones.\nThen we follow the above steps with 2-turn data to get\nQwen2.5VL_sft2\nand 3-turn data. For the subsequent turn data, we did not continue the above process due to computational resource constraints. Instead, we directly use the three-turn model to construct data for subsequent turns.\nSpecifically, we run the\nQwen2.5VL_sft2\nmodel multiple times on the same data\nto produce different 3-turn sequences, and stack these sequences together,\nwhich we refer to as mock multi-turn data.\nSince our mock multi-turn data is not entirely derived from real agent inference results,\nits distribution does not match real multi-turn reasoning,\nwe only use a subset of it to stimulate the modelâ€™s multi-turn trajectory reasoning capability.\nNow we have obtained all multi-turn data,\nincluding 2-turn, 3-turn, and mock multi-turn data.\nAppendix 0.C\nAdditional Ablation\nWe also study the impact of dataset size on RL performance.\nThis ablation was performed using an earlier version of our approach; due to resource constraints, we are unable to rerun it with the latest version.\nTherefore, we present indicative conclusions instead of reporting precise quantitative results.\nWhen training with only 7,000 hard samples described in Section\n3.2\n, the PDM score exhibits a minor increase within the first 30 steps, but then quickly declines.\nEvaluation on the 7,000 training set reveals clear signs of overfitting.\nThen we increase the dataset size to 13,000, 26,000, and 39,000 samples; the overall trend remains similar to that observed with 13,000 samples: convergence is slightly slower, but the performance metrics at 300 steps are very close.",
    "preview_text": "Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing \"long-tail\" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.\n\n1\n1\ninstitutetext:\nLi Auto Inc., Beijing, China\n1\n1\nemail:\n{lixidong,guomingyu,libailin,zouyangang}@lixiang.com\n2\n2\ninstitutetext:\nNVIDIA, Shanghai, China\n2\n2\nemail:\n{chenchaox,wenjingz,charliech,zehuanw}@nvidia.com\nâˆ—\nEqual contribution\nâ€ \nCorresponding author\nMTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving\nXidong Li\nâˆ—\nMingyu Guo\nâˆ—\nChenchao Xu\nâˆ—\nBailin Li\nâ€ \nWenjing Zhu\nâ€ \nYangang Zou\nRui Chen\nZehuan Wang\nAbstract\nTrajectory planning is a core task in autonomous driving,\nrequiring the prediction of safe and comfortable paths across diverse scenarios.\nIntegrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL)\nhas shown promise in addressing â€œlong-tailâ€ scenarios.",
    "is_relevant": true,
    "relevance_score": 7.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "VLA",
        "locomotion",
        "VLM",
        "whole body control",
        "Vision-Language-Action Model",
        "VLA for Robotics",
        "Flow Matching",
        "Diffusion",
        "Inference Acceleration",
        "Edge Deployment",
        "fine tune",
        "offline Reinforcement Learning",
        "constrained reinforcement learning",
        "safe reinforcement learning",
        "goal conditioned reinforcement learning",
        "world model"
    ],
    "one_line_summary": "MTDriveæå‡ºäº†ä¸€ç§å¤šè½®äº¤äº’å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„è½¨è¿¹è§„åˆ’ï¼Œä»¥å¤„ç†å¤æ‚åœºæ™¯å’Œé•¿å°¾é—®é¢˜ã€‚",
    "detailed_summary": "## MTDrive è®ºæ–‡è¯¦ç»†æ‘˜è¦\n\n**1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº**\nè‡ªåŠ¨é©¾é©¶ä¸­çš„è½¨è¿¹è§„åˆ’éœ€è¦å¤„ç†å¤šæ ·åŒ–çš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯æ•°æ®ç¨€ç¼ºä½†å®‰å…¨å…³é”®çš„â€œé•¿å°¾â€åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œä½†å¤§å¤šå±€é™äºå•è½®æ¨ç†ï¼Œéš¾ä»¥å¤„ç†éœ€è¦è¿­ä»£ä¼˜åŒ–çš„å¤æ‚ä»»åŠ¡ã€‚ç›´æ¥æ‰©å±•æ ‡å‡†å¼ºåŒ–å­¦ä¹ ç®—æ³•åˆ°å¤šè½®äº¤äº’åœºæ™¯ä¼šé¢ä¸´**å¥–åŠ±ç¨€ç–**å’Œ**ä¿¡ç”¨åˆ†é…**çš„éš¾é¢˜ï¼ŒåŒæ—¶ï¼Œç°æœ‰çš„æ•°æ®é›†å’Œè®­ç»ƒç³»ç»Ÿä¹Ÿç¼ºä¹å¯¹å¤šè½®äº¤äº’å¼è½¨è¿¹ä¼˜åŒ–çš„æ”¯æŒã€‚\n\n**2. æ ¸å¿ƒæ–¹æ³•å’ŒæŠ€æœ¯åˆ›æ–°**\næœ¬æ–‡æå‡ºäº† **MTDrive**ï¼Œä¸€ä¸ªç”¨äºè‡ªåŠ¨é©¾é©¶çš„å¤šè½®äº¤äº’å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š\n- **æ•°æ®æ„å»º**ï¼šä»é—­ç¯ä»¿çœŸä¸­æ„å»ºäº†ä¸€ä¸ª**å¤šè½®äº¤äº’å¼è½¨è¿¹ç†è§£æ•°æ®é›†**ï¼ŒåŒ…å«å•è½®ã€å¤šè½®å’ŒPDMï¼ˆè§„åˆ’å†³ç­–æŒ‡æ ‡ï¼‰ç†è§£æ•°æ®ï¼Œä»¥æ¿€æ´»æ¨¡å‹çš„è‡ªæˆ‘åæ€å’Œè¿­ä»£ä¼˜åŒ–èƒ½åŠ›ã€‚\n- **ç®—æ³•åˆ›æ–°**ï¼šæå‡ºäº† **å¤šè½®åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•**ï¼Œé€šè¿‡ä¸ºæ¯ä¸€è½®ç‹¬ç«‹è®¡ç®—å¥–åŠ±å’Œä¼˜åŠ¿ï¼Œæœ‰æ•ˆç¼“è§£äº†å¤šè½®ç¯å¢ƒä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ï¼Œå®ç°äº†æ›´ç²¾ç»†çš„è·¨è½®ä¿¡ç”¨åˆ†é…ã€‚\n- **ç³»ç»Ÿä¼˜åŒ–**ï¼šæ„å»ºäº†åŸºäºveRLçš„å¤šæ¨¡æ€å¤šè½®RLè®­ç»ƒç³»ç»Ÿï¼Œå¹¶å®æ–½äº†**è¿›ç¨‹é—´æµå¼åºåˆ—åŒ–**å’Œ**è¿›ç¨‹å†…å¼ é‡ç¼“å­˜**ä¸¤é¡¹ä¼˜åŒ–ï¼Œå°†è®­ç»ƒååé‡æå‡äº† **2.5å€**ã€‚\n\n**3. ä¸»è¦å®éªŒç»“æœ**\nåœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼š\n- ä»…ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ŒMTDriveå°±è¾¾åˆ°äº†88.1çš„PDMSï¼Œæ¯”å•è½®åŸºçº¿ï¼ˆ83.7ï¼‰æå‡äº†4.4åˆ†ã€‚\n- ç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒåï¼Œåœ¨ä½¿ç”¨çœŸå®æœªæ¥çŠ¶æ€ï¼ˆGT Oracleï¼‰çš„è®¾å®šä¸‹ï¼ŒPDMSè¾¾åˆ°**96.2**ï¼Œç”šè‡³è¶…è¿‡äº†äººç±»é©¾é©¶åŸºå‡†ï¼ˆ94.8ï¼‰ã€‚\n- åœ¨æ›´è´´è¿‘å®é™…éƒ¨ç½²çš„**è¿åŠ¨å­¦æ¨¡å‹**è®¾å®šä¸‹ï¼ŒPDMSè¾¾åˆ°91.1ï¼Œå±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚\n- æ¶ˆèå®éªŒéªŒè¯äº†å¤šè½®æ•°æ®ã€PDMç†è§£æ•°æ®ä»¥åŠmtGRPOç®—æ³•å„è‡ªçš„æœ‰æ•ˆæ€§ã€‚\n\n**4. ç ”ç©¶æ„ä¹‰å’Œä»·å€¼**\nMTDriveé¦–æ¬¡ç³»ç»Ÿæ€§åœ°è§£å†³äº†è‡ªåŠ¨é©¾é©¶ä¸­å¤šè½®äº¤äº’å¼è½¨è¿¹ä¼˜åŒ–çš„æ•°æ®ã€ç®—æ³•å’Œç³»ç»Ÿç“¶é¢ˆã€‚å…¶**å¤šè½®æ¨ç†èŒƒå¼**æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚é•¿å°¾åœºæ™¯ä¸‹çš„å†³ç­–è´¨é‡å’Œå®‰å…¨æ€§ã€‚æ‰€æå‡ºçš„**mtGRPOç®—æ³•**ä¸ºå¤šè½®å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜æä¾›äº†æ–°æ€è·¯ã€‚é«˜æ•ˆçš„**è®­ç»ƒç³»ç»Ÿä¼˜åŒ–**å…·æœ‰é€šç”¨æ€§ï¼Œå¯æƒ åŠå…¶ä»–å¤šæ¨¡æ€RLä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è½¨è¿¹è‡ªåŠ¨æ ‡æ³¨ç­‰é¢†åŸŸä¹Ÿå±•ç°å‡ºå·¨å¤§åº”ç”¨æ½œåŠ›ï¼Œæœ‰åŠ©äºè§£å†³è‡ªåŠ¨é©¾é©¶æ¨¡ä»¿å­¦ä¹ ä¸­é«˜è´¨é‡æ•°æ®åŒ®ä¹çš„é—®é¢˜ã€‚",
    "qa_pairs": [
        {
            "question": "è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»–æƒ³è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œæ€ä¹ˆè§£å†³çš„ï¼Ÿ",
            "answer": "## MTDrive è®ºæ–‡æ ¸å¿ƒåˆ†æ\n\n### **è®ºæ–‡æ‹Ÿè§£å†³çš„æ ¸å¿ƒé—®é¢˜**\n1.  **é•¿å°¾åœºæ™¯å¤„ç†èƒ½åŠ›ä¸è¶³**ï¼šä¼ ç»Ÿç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸­ç½•è§çš„ã€ä½†å®‰å…¨å…³é”®çš„é•¿å°¾åœºæ™¯ï¼ˆå¦‚å¤æ‚äº¤äº’ã€çªå‘å±é™©ï¼‰ä¸­è¡¨ç°ä¸ä½³ã€‚\n2.  **å•è½®æ¨ç†çš„å±€é™æ€§**ï¼šç°æœ‰åŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLM/VLMï¼‰çš„é©¾é©¶æ–¹æ³•é€šå¸¸åªè¿›è¡Œå•è½®è½¨è¿¹é¢„æµ‹ï¼Œç¼ºä¹æ ¹æ®ç¯å¢ƒåé¦ˆè¿›è¡Œ**è¿­ä»£ä¼˜åŒ–**çš„èƒ½åŠ›ï¼Œéš¾ä»¥å¤„ç†éœ€è¦å¤šæ­¥å†³ç­–çš„å¤æ‚ä»»åŠ¡ã€‚\n3.  **å¤šè½®å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ç¨€ç–æ€§**ï¼šå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ‰©å±•åˆ°å¤šè½®äº¤äº’æ—¶ï¼Œå¥–åŠ±é€šå¸¸åªåœ¨æœ€ç»ˆè½®æ¬¡ç»™å‡ºï¼Œå¯¼è‡´**ä¿¡ç”¨åˆ†é…é—®é¢˜**â€”â€”æ¨¡å‹éš¾ä»¥åˆ¤æ–­ä¸­é—´è½®æ¬¡çš„æ”¹è¿›å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼Œä½¿å¾—è®­ç»ƒä½æ•ˆä¸”ä¸ç¨³å®šã€‚\n4.  **å¤šæ¨¡æ€å¤šè½®RLçš„è®­ç»ƒæ•ˆç‡ç“¶é¢ˆ**ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤šè½®åºåˆ—æ•°æ®åœ¨è®­ç»ƒç³»ç»Ÿä¸­é€ æˆå·¨å¤§çš„**æ•°æ®ä¼ è¾“å’Œè®¡ç®—å¼€é”€**ï¼Œç¼ºä¹é’ˆå¯¹æ€§çš„ç³»ç»Ÿä¼˜åŒ–ã€‚\n\n### **æ ¸å¿ƒè§£å†³æ–¹æ¡ˆï¼šMTDrive æ¡†æ¶**\nè®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ¶µç›–**æ•°æ®ã€ç®—æ³•ã€ç³»ç»Ÿ**ä¸‰ä¸ªå±‚é¢çš„å®Œæ•´å¤šè½®äº¤äº’å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚\n\n#### **1. æŠ€æœ¯åˆ›æ–°ç‚¹**\n- **å¤šè½®äº¤äº’å¼è½¨è¿¹ç†è§£æ•°æ®é›†**ï¼š\n    - ä»é—­ç¯ä»¿çœŸï¼ˆNAVSIMï¼‰ä¸­æ„å»ºæ•°æ®ï¼ŒåŸºäºç¢°æ’ã€å¯è¡Œé©¶åŒºåŸŸåˆè§„æ€§ã€ç¢°æ’æ—¶é—´ç­‰å®‰å…¨æŒ‡æ ‡ç­›é€‰åœºæ™¯ã€‚\n    - åŒ…å«**å•è½®æ•°æ®**ï¼ˆåŸºç¡€è½¨è¿¹ç”Ÿæˆï¼‰ã€**å¤šè½®æ•°æ®**ï¼ˆé€šè¿‡æ¨¡å‹è‡ªä¸¾è¿­ä»£ç”Ÿæˆï¼Œå®ç°åé¦ˆå¼•å¯¼çš„è½¨è¿¹ä¿®æ­£ï¼‰å’Œ**PDMç†è§£æ•°æ®**ï¼ˆé€šè¿‡é—®ç­”å¯¹è®©æ¨¡å‹ç†è§£è¯„ä¼°æŒ‡æ ‡çš„åé¦ˆï¼‰ã€‚\n    - å¼•å…¥**åäº‹å®æé—®**ï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«å…³é”®éšœç¢ç‰©ã€‚\n\n- **æ–°é¢–çš„RLç®—æ³•ï¼šå¤šè½®ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆmtGRPOï¼‰**ï¼š\n    - **æ ¸å¿ƒæ”¹è¿›**ï¼šé’ˆå¯¹æ ‡å‡†GRPOåœ¨å¤šè½®ä»»åŠ¡ä¸­**å•åºåˆ—çº§å¥–åŠ±**å¯¼è‡´çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚\n    - **æŒ‰è½®æ¬¡å¥–åŠ±åˆ†é…**ï¼šä¸ºæ¯ä¸€è½®ç”Ÿæˆçš„è½¨è¿¹ç‹¬ç«‹è®¡ç®—PDMåˆ†æ•°å’Œæ ¼å¼åˆ†æ•°ï¼Œå¹¶å°†è¯¥è½®å¥–åŠ±ä»…åˆ†é…ç»™å¯¹åº”è½®æ¬¡çš„tokenã€‚\n    - **è·¨è½®æ¬¡çš„ä¼˜åŠ¿åº¦è®¡ç®—**ï¼šåœ¨ä¼˜åŠ¿åº¦ä¼°è®¡æ—¶ï¼Œåœ¨æ‰¹æ¬¡å†…å¯¹**åŒä¸€è½®æ¬¡**çš„å¥–åŠ±è¿›è¡Œç»„å†…å½’ä¸€åŒ–ï¼Œä»è€Œæ¸…æ™°åŒºåˆ†ä¸åŒè½®æ¬¡å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ã€‚\n    - **æ¸è¿›å¼å¥–åŠ±æœºåˆ¶**ï¼šé¼“åŠ±æ¨¡å‹éšç€è½®æ¬¡å¢åŠ ï¼Œæå‡å…¶æ¨ç†å‡†ç¡®æ€§ã€‚\n\n- **å¤šæ¨¡æ€å¤šè½®RLè®­ç»ƒç³»ç»Ÿä¼˜åŒ–**ï¼š\n    - åŸºäºveRLæ¡†æ¶æ„å»ºï¼Œé’ˆå¯¹å›¾åƒå’Œå¤šè½®åºåˆ—çš„æ•°æ®ä¼ è¾“ç“¶é¢ˆè¿›è¡Œäº†ä¸¤é¡¹å…³é”®ä¼˜åŒ–ï¼š\n        1.  **è¿›ç¨‹é—´æµå¼åºåˆ—åŒ–ï¼ˆIPSSï¼‰**ï¼šå°†æ•°æ®åºåˆ—åŒ–ä¸ rollout ç”Ÿæˆè¿‡ç¨‹é‡å ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´ã€‚\n        2.  **è¿›ç¨‹å†…å¼ é‡ç¼“å­˜ï¼ˆIPTCï¼‰**ï¼šåœ¨å…±å€çš„æ¨¡å—é—´ç¼“å­˜ååºåˆ—åŒ–åçš„è¾“å…¥å¼ é‡ï¼ˆå¦‚è§†è§‰åµŒå…¥ï¼‰ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚\n    - **æ•ˆæœ**ï¼šå®ç°äº† **2.5å€** çš„è®­ç»ƒååé‡æå‡ã€‚\n\n#### **2. å®é™…ä»·å€¼ä¸æ•ˆæœ**\n- **æ€§èƒ½æå‡**ï¼šåœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMTDriveåœ¨ä½¿ç”¨çœŸå®æ„ŸçŸ¥ï¼ˆOracleï¼‰è®¾ç½®ä¸‹å–å¾—äº† **96.2çš„PDMS**ï¼Œè¶…è¶Šäº†äººç±»é©¾é©¶åŸºå‡†ï¼ˆ94.8ï¼‰ã€‚åœ¨æ›´ç°å®çš„è¿åŠ¨å­¦æ¨¡å‹è®¾ç½®ä¸‹ä¹Ÿè¾¾åˆ°91.1ï¼Œä¼˜äºå¤šæ•°ä¼ ç»Ÿæ–¹æ³•ã€‚\n- **éªŒè¯å¤šè½®æ¨ç†çš„æœ‰æ•ˆæ€§**ï¼šä»…é€šè¿‡SFTè®­ç»ƒï¼Œå¤šè½®æ¨¡å‹ï¼ˆ88.1ï¼‰å°±æ¯”å•è½®åŸºçº¿ï¼ˆ83.7ï¼‰æå‡äº†4.4åˆ†ï¼Œè¯æ˜äº†è¿­ä»£ä¿®æ­£èŒƒå¼çš„æ½œåŠ›ã€‚\n- **è§£å†³å¤æ‚åœºæ™¯èƒ½åŠ›**ï¼šå®šæ€§å®éªŒæ˜¾ç¤ºï¼Œæ¨¡å‹èƒ½æ ¹æ®PDMåé¦ˆï¼ˆå¦‚DACè¿è§„ã€TTCé£é™©ï¼‰åœ¨å¤šè½®ä¸­é€æ­¥ä¿®æ­£è½¨è¿¹ï¼Œä¾‹å¦‚åœ¨è¡Œäººæ¨ªç©¿ã€è·¯å£æ’é˜Ÿã€åè½¦å¿«é€Ÿæ¥è¿‘ç­‰é•¿å°¾åœºæ™¯ä¸­åšå‡ºåˆç†è°ƒæ•´ã€‚\n- **ä¸ºè‡ªåŠ¨æ ‡æ³¨æä¾›å¯èƒ½**ï¼šç”±äºå…¶è§„åˆ’è´¨é‡é«˜ï¼Œè¯¥æ¡†æ¶æœ‰æœ›ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„è½¨è¿¹æ ‡æ³¨æ•°æ®ï¼Œç¼“è§£è‡ªåŠ¨é©¾é©¶æ¨¡ä»¿å­¦ä¹ ä¸­å¤šæ¨¡æ€æ•°æ®çš„åŒ®ä¹é—®é¢˜ã€‚\n\n### **æ€»ç»“**\nMTDriveçš„æ ¸å¿ƒåˆ›æ–°åœ¨äº**å°†å¤§æ¨¡å‹çš„â€œå¤šè½®æ€è€ƒâ€èƒ½åŠ›ç³»ç»Ÿæ€§åœ°å¼•å…¥è‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’**ã€‚å®ƒé€šè¿‡æ„å»ºä¸“ç”¨çš„äº¤äº’æ•°æ®é›†ã€è®¾è®¡è§£å†³å¥–åŠ±ç¨€ç–æ€§çš„mtGRPOç®—æ³•ï¼Œä»¥åŠä¼˜åŒ–è®­ç»ƒç³»ç»Ÿï¼ŒæˆåŠŸä½¿VLMèƒ½å¤Ÿæ ¹æ®ç¯å¢ƒåé¦ˆè¿­ä»£ä¼˜åŒ–è½¨è¿¹ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚ã€é•¿å°¾é©¾é©¶åœºæ™¯ä¸­çš„è§„åˆ’å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºæ„å»ºæ›´æ™ºèƒ½ã€æ›´ç±»äººçš„è‡ªåŠ¨é©¾é©¶å†³ç­–ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ–¹æ³•è®ºå’Œå·¥ç¨‹å®è·µå‚è€ƒã€‚",
            "timestamp": "2026-02-03T15:54:23.826758",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "åŸºäºä»–çš„å‰ä½œï¼Œæ¢³ç†è¿™ä¸ªæ–¹å‘çš„æ•´ä¸ªå‘å±•è„‰ç»œï¼Œæ¯ä¸€æ­¥ç›¸æ¯”äºä¹‹å‰çš„å·¥ä½œéƒ½æ”¹è¿›äº†ä»€ä¹ˆï¼Œç€é‡äºå‡ ä¸ªä¸åŒçš„å‘å±•æ–¹å‘ã€‚",
            "answer": "## è‡ªåŠ¨é©¾é©¶ä¸­å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ èåˆçš„å‘å±•è„‰ç»œåˆ†æ\n\næœ¬æ–‡æå‡ºçš„ **MTDrive** æ˜¯â€œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLM/VLMï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆç”¨äºè‡ªåŠ¨é©¾é©¶è½¨è¿¹è§„åˆ’â€è¿™ä¸€æŠ€æœ¯æ–¹å‘çš„æœ€æ–°è¿›å±•ã€‚å…¶å‘å±•è„‰ç»œå¯ä»¥æ¢³ç†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹å‰ä½œçš„å±€é™æ€§è¿›è¡Œäº†é’ˆå¯¹æ€§æ”¹è¿›ï¼Œå¹¶åˆ†åŒ–å‡ºä¸åŒçš„æŠ€æœ¯å‘å±•æ–¹å‘ã€‚\n\n### å‘å±•è„‰ç»œä¸æ ¸å¿ƒæ”¹è¿›\n\n#### ç¬¬ä¸€é˜¶æ®µï¼šVLM ä½œä¸ºé«˜çº§æŒ‡ä»¤ç”Ÿæˆå™¨ï¼ˆè¯­ä¹‰è§„åˆ’ï¼‰\n*   **ä»£è¡¨å·¥ä½œ**ï¼šDriveVLM, Omnidrive ç­‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨ VLM ç†è§£åœºæ™¯ï¼Œç”Ÿæˆé«˜çº§è¯­ä¹‰æŒ‡ä»¤ï¼ˆå¦‚â€œå‡é€Ÿâ€ã€â€œæ¢é“â€ï¼‰ï¼Œå†ç”¨ä¸€ä¸ª**ç‹¬ç«‹çš„ã€ä¼ ç»Ÿçš„è§„åˆ’å™¨**ï¼ˆå¦‚åŸºäºæ‰©æ•£æ¨¡å‹æˆ–æ£€æµ‹å¤´çš„è§„åˆ’å™¨ï¼‰æ¥ç”Ÿæˆå…·ä½“è½¨è¿¹ã€‚\n*   **ç›¸æ¯”äºçº¯ç«¯åˆ°ç«¯æ–¹æ³•çš„æ”¹è¿›**ï¼š\n    *   **å¼•å…¥å…ˆéªŒçŸ¥è¯†**ï¼šåˆ©ç”¨ VLM çš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›å¤„ç†**é•¿å°¾åœºæ™¯**ï¼Œå¼¥è¡¥äº†çº¯æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨ç½•è§åœºæ™¯ä¸‹æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚\n    *   **å¯è§£é‡Šæ€§**ï¼šç”Ÿæˆçš„è¯­ä¹‰æŒ‡ä»¤ä¸ºå†³ç­–æä¾›äº†å¯è§£é‡Šçš„ä¸­é—´å±‚ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **ç»“æ„å¤æ‚**ï¼šéœ€è¦ç»´æŠ¤ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å—ï¼ˆVLM å’Œè§„åˆ’å™¨ï¼‰ï¼Œæµç¨‹å†—é•¿ã€‚\n    *   **è¯¯å·®ç´¯ç§¯**ï¼šVLM çš„è¯­ä¹‰æŒ‡ä»¤è‹¥å­˜åœ¨åå·®ï¼Œä¼šç›´æ¥å½±å“ä¸‹æ¸¸è§„åˆ’å™¨çš„æ€§èƒ½ã€‚\n    *   **ç©ºé—´æ¨ç†å¼±**ï¼šVLM åœ¨**ç»†ç²’åº¦ç©ºé—´æ¨ç†**ï¼ˆå¦‚ç²¾ç¡®çš„è½¦è¾†ä½ç½®ã€éšœç¢ç‰©è·ç¦»åˆ¤æ–­ï¼‰ä¸Šè¡¨ç°ä¸ä½³ã€‚\n\n#### ç¬¬äºŒé˜¶æ®µï¼šVLM ä½œä¸ºç«¯åˆ°ç«¯è½¨è¿¹ç”Ÿæˆå™¨ï¼ˆç›´æ¥è¾“å‡ºï¼‰\n*   **ä»£è¡¨å·¥ä½œ**ï¼šEmma, LMDrive, Sce2DriveX, ReasonPlan ç­‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼š**ç®€åŒ–æ¶æ„**ï¼Œè®© VLM **ç›´æ¥è¾“å‡ºæœªæ¥è½¨è¿¹åæ ‡**ã€‚å°†è½¨è¿¹ç¼–ç ä¸ºè‡ªç„¶è¯­è¨€åºåˆ—ï¼Œä½¿æ¨¡å‹èƒ½ç›´æ¥ç†è§£å¹¶æ‰§è¡Œâ€œè§„åˆ’â€ä»»åŠ¡ã€‚\n*   **ç›¸æ¯”äºç¬¬ä¸€é˜¶æ®µçš„æ”¹è¿›**ï¼š\n    *   **ç»“æ„ç®€æ´**ï¼šç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé¿å…äº†æ¨¡å—é—´æ¥å£å’Œè¯¯å·®ä¼ é€’ã€‚\n    *   **çµæ´»æ€§ä¸æ³›åŒ–**ï¼šç›´æ¥è¾“å‡ºè½¨è¿¹æ›´çµæ´»ï¼Œä¸”åˆ©ç”¨è‡ªç„¶è¯­è¨€è¡¨ç¤ºå¢å¼ºäº†ä»»åŠ¡çš„**å¯ç†è§£æ€§**å’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **è§„åˆ’è´¨é‡ä¾èµ–æ¨¡å‹èƒ½åŠ›**ï¼šè½¨è¿¹çš„èˆ’é€‚æ€§ã€å®‰å…¨æ€§å®Œå…¨ä¾èµ–äº VLM æœ¬èº«çš„è®­ç»ƒå’Œå¾®è°ƒã€‚\n    *   **ç¼ºä¹åœ¨çº¿ä¼˜åŒ–**ï¼šç”Ÿæˆè½¨è¿¹æ˜¯â€œå•æ¬¡é€šè¿‡â€çš„ï¼Œæ— æ³•æ ¹æ®ç¯å¢ƒåé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œåœ¨å¤„ç†å¤æ‚ã€åŠ¨æ€åœºæ™¯æ—¶å¯èƒ½ä¸€æ¬¡è§„åˆ’å¤±è´¥å³å¯¼è‡´ä¸è‰¯åæœã€‚\n\n#### ç¬¬ä¸‰é˜¶æ®µï¼šå¼•å…¥å¼ºåŒ–å­¦ä¹ å¯¹é½ä¸ä¼˜åŒ–\n*   **ä»£è¡¨å·¥ä½œ**ï¼šAlphaDrive, TrajHF, R2SE, Alpamayo-R1 ç­‰ã€‚\n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå€Ÿé‰´å¤§è¯­è¨€æ¨¡å‹å¯¹é½çš„æˆåŠŸç»éªŒï¼Œä½¿ç”¨ **RLï¼ˆç‰¹åˆ«æ˜¯é«˜æ•ˆçš„ GRPO ç®—æ³•ï¼‰** å¯¹ VLM è§„åˆ’å™¨è¿›è¡Œ**åè®­ç»ƒ**ï¼Œä»¥ä¼˜åŒ–éš¾ä»¥é€šè¿‡ç›‘ç£å­¦ä¹ ç›´æ¥è·å¾—çš„æŒ‡æ ‡ï¼ˆå¦‚å®‰å…¨æ€§ã€èˆ’é€‚åº¦ï¼‰ã€‚\n*   **ç›¸æ¯”äºç¬¬äºŒé˜¶æ®µçš„æ”¹è¿›**ï¼š\n    *   **ä¼˜åŒ–å¤æ‚ç›®æ ‡**ï¼šèƒ½å¤Ÿé€šè¿‡å¥–åŠ±å‡½æ•°ç›´æ¥ä¼˜åŒ–å¤šç›®æ ‡æƒè¡¡ï¼ˆå¦‚å®‰å…¨ vs. æ•ˆç‡ï¼‰ï¼Œä½¿ç­–ç•¥æ›´**é²æ£’**ï¼Œ**æ³›åŒ–èƒ½åŠ›**æ›´å¼ºã€‚\n    *   **è¶…è¶Šæ¨¡ä»¿**ï¼šä¸å†å±€é™äºæ¨¡ä»¿äººç±»é©¾é©¶æ•°æ®ï¼Œå¯ä»¥æ¢ç´¢æ›´ä¼˜æˆ–æ›´å®‰å…¨çš„é©¾é©¶ç­–ç•¥ã€‚\n*   **å±€é™æ€§**ï¼š\n    *   **å•è½®æ¨ç†é™åˆ¶**ï¼šRL è®­ç»ƒå’Œæ¨ç†ä»å±€é™äº**å•è½®äº¤äº’**ã€‚æ¨¡å‹å¿…é¡»â€œä¸€æ¬¡æˆåŠŸâ€ï¼Œæ— æ³•æ¨¡æ‹Ÿäººç±»â€œè§‚å¯Ÿ-æ€è€ƒ-ä¿®æ­£â€çš„è¿­ä»£æ¨ç†è¿‡ç¨‹ã€‚\n    *   **ç¨€ç–å¥–åŠ±é—®é¢˜**ï¼šåœ¨éœ€è¦å¤šæ­¥å†³ç­–çš„ä»»åŠ¡ä¸­ï¼Œæœ€ç»ˆçš„æˆåŠŸ/å¤±è´¥å¥–åŠ±éš¾ä»¥æœ‰æ•ˆåˆ†é…ï¼ˆ**ä¿¡ç”¨åˆ†é…é—®é¢˜**ï¼‰ï¼Œå¯¼è‡´å­¦ä¹ ä½æ•ˆã€‚\n\n#### ç¬¬å››é˜¶æ®µï¼ˆæœ¬æ–‡ MTDriveï¼‰ï¼šå¤šè½®äº¤äº’å¼å¼ºåŒ–å­¦ä¹ \n*   **æ ¸å¿ƒæ€æƒ³**ï¼šå°† **VLM çš„ç›´æ¥è½¨è¿¹ç”Ÿæˆèƒ½åŠ›** ä¸ **æ”¯æŒå¤šè½®è¿­ä»£çš„ RL æ¡†æ¶** æ·±åº¦èåˆï¼Œå½¢æˆä¸€ä¸ª**é—­ç¯åæ€ä¸ä¼˜åŒ–**ç³»ç»Ÿã€‚\n*   **ç›¸æ¯”äºç¬¬ä¸‰é˜¶æ®µå·¥ä½œçš„é©å‘½æ€§æ”¹è¿›**ï¼š\n    1.  **èŒƒå¼åˆ›æ–°ï¼šä»å•è½®åˆ°å¤šè½®**\n        *   **ä¹‹å‰**ï¼šæ¨¡å‹è¾“å‡ºè½¨è¿¹ï¼Œä»»åŠ¡ç»“æŸã€‚\n        *   **MTDrive**ï¼šæ¨¡å‹è¾“å‡ºè½¨è¿¹ â†’ ä»¿çœŸå™¨æä¾›å…·ä½“åé¦ˆï¼ˆå¦‚â€œç¬¬Xç‚¹å¯èƒ½ç¢°æ’Aè½¦â€ï¼‰ â†’ æ¨¡å‹ç»“åˆåé¦ˆé‡æ–°è§„åˆ’ â†’ è¿­ä»£ç›´è‡³æ»¡æ„æˆ–è¾¾åˆ°è½®æ¬¡ä¸Šé™ã€‚è¿™æ›´è´´è¿‘å¤æ‚ä»»åŠ¡çš„è§£å†³é€»è¾‘ã€‚\n    2.  **ç®—æ³•åˆ›æ–°ï¼šè§£å†³å¤šè½®RLçš„ç¨€ç–å¥–åŠ±é—®é¢˜**\n        *   **æå‡º mtGRPO**ï¼šåœ¨ GRPO çš„åŸºç¡€ä¸Šï¼Œ**æŒ‰è½®æ¬¡è®¡ç®—å¥–åŠ±å’Œä¼˜åŠ¿å‡½æ•°**ï¼Œè€Œéå¯¹æ•´ä¸ªåºåˆ—ä½¿ç”¨å•ä¸€å¥–åŠ±ã€‚è¿™ç²¾å‡†åœ°è¯„ä¼°äº†æ¯ä¸€è½®æ”¹è¿›çš„è´¡çŒ®ï¼Œæœ‰æ•ˆè§£å†³äº†å¤šè½®äº¤äº’ä¸­çš„ä¿¡ç”¨åˆ†é…éš¾é¢˜ã€‚\n    3.  **æ•°æ®åˆ›æ–°ï¼šæ„å»ºå¤šè½®äº¤äº’æ•°æ®é›†**\n        *   æ­¤å‰ç¼ºä¹æ”¯æŒå¤šè½®åæ€çš„è®­ç»ƒæ•°æ®ã€‚MTDrive è®¾è®¡äº†ä¸€å¥—ä»é—­ç¯ä»¿çœŸä¸­**è‡ªä¸¾ç”Ÿæˆå¤šè½®æ•°æ®**çš„ç®¡é“ï¼ŒåŒ…æ‹¬å•è½®æ•°æ®ã€å¤šè½®æ•°æ®å’Œ PDM ç†è§£æ•°æ®ï¼Œä¸ºæ¨¡å‹æ¿€æ´»äº†â€œè‡ªæˆ‘åæ€â€èƒ½åŠ›ã€‚\n    4.  **ç³»ç»Ÿåˆ›æ–°ï¼šä¼˜åŒ–å¤šæ¨¡æ€å¤šè½®è®­ç»ƒæ•ˆç‡**\n        *   é’ˆå¯¹é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤šè½®åºåˆ—å¯¼è‡´çš„æ•°æ®ä¼ è¾“ç“¶é¢ˆï¼Œæå‡ºäº† **IPSSï¼ˆè¿›ç¨‹é—´æµå¼åºåˆ—åŒ–ï¼‰** å’Œ **IPTCï¼ˆè¿›ç¨‹å†…å¼ é‡ç¼“å­˜ï¼‰** ä¼˜åŒ–ï¼Œå°†è®­ç»ƒååé‡æå‡ **2.5å€**ï¼Œä½¿å¤§è§„æ¨¡å¤šè½® RL è®­ç»ƒå˜å¾—å¯è¡Œã€‚\n\n### ç€é‡å‘å±•çš„å‡ ä¸ªä¸åŒæ–¹å‘\n\nåŸºäºä¸Šè¿°è„‰ç»œï¼Œè¯¥é¢†åŸŸä¸»è¦å‘ä¸‰ä¸ªæ–¹å‘æ·±åŒ–ï¼š\n\n1.  **æ¶æ„ç®€åŒ–ä¸èƒ½åŠ›ç»Ÿä¸€æ–¹å‘**\n    *   **ç›®æ ‡**ï¼šæ„å»ºæ›´ç®€æ´ã€æ›´å¼ºå¤§çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚\n    *   **æ¼”è¿›**ï¼šä» `VLMï¼ˆæŒ‡ä»¤ï¼‰ + ä¼ ç»Ÿè§„åˆ’å™¨` â†’ `VLMï¼ˆç›´æ¥è½¨è¿¹è¾“å‡ºï¼‰`ã€‚**MTDrive å±äºæ­¤æ–¹å‘çš„æ·±åŒ–**ï¼Œå®ƒä¿æŒäº†ç«¯åˆ°ç«¯è¾“å‡ºçš„ç®€æ´æ€§ï¼Œå¹¶é€šè¿‡å¤šè½® RL æå¤§æå‡äº†è¾“å‡ºè´¨é‡ã€‚\n\n2.  **ä¼˜åŒ–ä¸å¯¹é½æ–¹å‘**\n    *   **ç›®æ ‡**ï¼šä½¿æ¨¡å‹è¡Œä¸ºæ›´å¥½åœ°å¯¹é½å®‰å…¨ã€èˆ’é€‚ã€é«˜æ•ˆç­‰å¤æ‚ã€éšå«çš„ç›®æ ‡ã€‚\n    *   **æ¼”è¿›**ï¼šä» `ç›‘ç£å¼å¾®è°ƒ (SFT)` â†’ `åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF)` / `åŸºäºè§„åˆ™å¥–åŠ±çš„RLï¼ˆå¦‚GRPOï¼‰` â†’ **`å¤šè½®äº¤äº’å¼RLï¼ˆmtGRPOï¼‰`**ã€‚MTDrive ä»£è¡¨äº†æ­¤æ–¹å‘çš„æœ€å‰æ²¿ï¼Œè§£å†³äº†å¤šæ­¥å†³ç­–ä¸­çš„æ ¸å¿ƒä¼˜åŒ–éš¾é¢˜ã€‚\n\n3.  **æ¨ç†ä¸äº¤äº’èƒ½åŠ›æ–¹å‘**\n    *   **ç›®æ ‡**ï¼šèµ‹äºˆæ¨¡å‹ç±»äººçš„é€æ­¥æ¨ç†ã€åæ€å’Œä¿®æ­£èƒ½åŠ›ï¼Œä»¥å¤„ç†æç«¯å¤æ‚åœºæ™¯ã€‚\n    *   **æ¼”è¿›**ï¼šä» `é™æ€ã€å•æ¬¡æ¨ç†` â†’ `å·¥å…·è°ƒç”¨ä¸æ··åˆæ¨ç†ï¼ˆå¦‚DriveAgent-R1ï¼‰` â†’ **`é—­ç¯ã€è¿­ä»£å¼è½¨è¿¹ refinement`**ã€‚MTDrive çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå°†å¤šè½®æ¨ç†èŒƒå¼**ç³»ç»Ÿæ€§åœ°**åº”ç”¨äºè‡ªåŠ¨é©¾é©¶çš„**è¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆè½¨è¿¹ï¼‰è§„åˆ’**é—®é¢˜ä¸­ï¼Œå¹¶æä¾›äº†å®Œæ•´çš„è®­ç»ƒæ¡†æ¶ã€‚\n\n**æ€»ç»“**ï¼šMTDrive çš„å‘å±•è„‰ç»œæ¸…æ™°åœ°å±•ç¤ºäº†ä» **â€œåˆ©ç”¨VLMçŸ¥è¯†â€** åˆ° **â€œèµ‹äºˆVLMè§„åˆ’èƒ½åŠ›â€**ï¼Œå†åˆ° **â€œä¼˜åŒ–è§„åˆ’ç›®æ ‡â€**ï¼Œæœ€ç»ˆå®ç° **â€œèµ‹äºˆè¿­ä»£åæ€ä¸ä¼˜åŒ–èƒ½åŠ›â€** çš„æŠ€æœ¯æ¼”è¿›ã€‚å®ƒå¹¶éå•ä¸€æŠ€æœ¯çš„æ”¹è¿›ï¼Œè€Œæ˜¯åœ¨**æ•°æ®æ„å»ºã€ç®—æ³•è®¾è®¡ã€ç³»ç»Ÿä¼˜åŒ–**ä¸‰ä¸ªå±‚é¢è¿›è¡Œäº†ååŒåˆ›æ–°ï¼Œä»£è¡¨äº†å½“å‰å°†å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸å¼ºåŒ–å­¦ä¹ åœ¨çº¿ä¼˜åŒ–ç›¸ç»“åˆï¼Œç”¨äºè§£å†³è‡ªåŠ¨é©¾é©¶é•¿å°¾è§„åˆ’é—®é¢˜çš„æœ€æ–°ä¸”æœ€å…¨é¢çš„æ¢ç´¢ã€‚",
            "timestamp": "2026-02-03T15:55:23.890699",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–çš„å‰ä½œæœ‰å“ªäº›ï¼Ÿä½¿ç”¨è¡¨æ ¼ä»”ç»†è®²è®²ä»–çš„æ¯ç¯‡å‰ä½œï¼Œä»–å’Œå‰ä½œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Œä¸»è¦æ”¹å–„æ˜¯ä»€ä¹ˆï¼Ÿç€é‡äºå…·ä½“ç›¸æ¯”äºä¹‹å‰æ–‡ç« çš„æ”¹åŠ¨",
            "answer": "## MTDrive çš„å‰ä½œåˆ†æ\n\næ ¹æ®è®ºæ–‡å†…å®¹ï¼ŒMTDrive çš„ç ”ç©¶å»ºç«‹åœ¨å¤šç¯‡ç›¸å…³å·¥ä½œçš„åŸºç¡€ä¸Šï¼Œä¸»è¦é›†ä¸­åœ¨**è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”¨äºè‡ªåŠ¨é©¾é©¶**å’Œ**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºè‡ªåŠ¨é©¾é©¶**è¿™ä¸¤ä¸ªé¢†åŸŸã€‚ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒå‰ä½œçš„æ¢³ç†ï¼Œä»¥åŠä¸ MTDrive çš„ä¸»è¦åŒºåˆ«å’Œæ”¹è¿›ã€‚\n\n### å‰ä½œæ¢³ç†ä¸å¯¹æ¯”\n\n| å‰ä½œç±»åˆ« | è®ºæ–‡/æ–¹æ³•åç§° | æ ¸å¿ƒæ€æƒ³/è´¡çŒ® | ä¸ MTDrive çš„ä¸»è¦åŒºåˆ« | MTDrive çš„ä¸»è¦æ”¹è¿›/åˆ›æ–°ç‚¹ |\n| :--- | :--- | :--- | :--- | :--- |\n| **VLM for AD (èŒƒå¼ä¸€)** | DriveVLM [34], Omnidrive [35] | ä½¿ç”¨ VLM ç”Ÿæˆ**é«˜çº§è¯­ä¹‰æŒ‡ä»¤**ï¼ˆå¦‚â€œå·¦è½¬â€ã€â€œè·Ÿè½¦â€ï¼‰ï¼Œå†ç”±ä¸‹æ¸¸è§„åˆ’å™¨ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ç”Ÿæˆå…·ä½“è½¨è¿¹ã€‚ | **æ¶æ„ä¸åŒ**ï¼šé‡‡ç”¨â€œVLMï¼ˆæŒ‡ä»¤ï¼‰â†’ ä¸“ç”¨è§„åˆ’å™¨â€çš„ä¸¤é˜¶æ®µèŒƒå¼ã€‚VLM ä¸ç›´æ¥è¾“å‡ºè½¨è¿¹ã€‚ | **ç«¯åˆ°ç«¯è½¨è¿¹ç”Ÿæˆ**ï¼šMTDrive å±äºç¬¬äºŒèŒƒå¼ï¼ŒVLM **ç›´æ¥è¾“å‡ºæœªæ¥è½¨è¿¹åæ ‡**ã€‚è¿™ä½¿å¾—ç»“æ„æ›´ç®€æ´ï¼Œä¸”ç”¨è‡ªç„¶è¯­è¨€æè¿°è½¨è¿¹å¢å¼ºäº†æ¨¡å‹çš„ä»»åŠ¡ç†è§£å’Œå¯è§£é‡Šæ€§ã€‚ |\n| **VLM for AD (èŒƒå¼äºŒ)** | EMMA [10], ReasonPlan [20], Sce2DriveX [41] | VLM **ç›´æ¥è¾“å‡ºæœªæ¥é©¾é©¶è½¨è¿¹**ã€‚è¿™æ˜¯ MTDrive æ‰€éµå¾ªçš„èŒƒå¼ã€‚ | **æ¨ç†æ¨¡å¼ä¸åŒ**ï¼šè¿™äº›å·¥ä½œé€šå¸¸æ˜¯**å•æ¬¡æ¨ç†**ï¼ˆsingle-turnï¼‰ï¼Œæ¨¡å‹æ ¹æ®å½“å‰è§‚å¯Ÿä¸€æ¬¡æ€§ç”Ÿæˆæœ€ç»ˆè½¨è¿¹ã€‚ | **å¤šè½®äº¤äº’å¼æ¨ç†**ï¼šMTDrive çš„æ ¸å¿ƒåˆ›æ–°æ˜¯å¼•å…¥äº†**å¤šè½®è¿­ä»£ç²¾ä¿®**æœºåˆ¶ã€‚æ¨¡å‹é¦–è½®ç”Ÿæˆè½¨è¿¹åï¼Œæ¥æ”¶ç¯å¢ƒåé¦ˆï¼ˆå¦‚ç¢°æ’é£é™©ï¼‰ï¼Œå¹¶åœ¨åç»­è½®æ¬¡ä¸­è¿­ä»£ä¼˜åŒ–è½¨è¿¹ï¼Œä»¥å¤„ç†æ›´å¤æ‚çš„é•¿å°¾åœºæ™¯ã€‚ |\n| **RL for AD (åŸºäºGRPO)** | AlphaDrive [12], TrajHF [16], R2SE [21] | å°† **Group Relative Policy Optimization (GRPO)** ç®—æ³•å¼•å…¥è‡ªåŠ¨é©¾é©¶ï¼Œåˆ©ç”¨RLæå‡VLMç­–ç•¥çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ | **å¥–åŠ±è®¾è®¡ä¸åŒ**ï¼šè¿™äº›æ–¹æ³•åœ¨**å•è½®æ¨ç†**è®¾å®šä¸‹åº”ç”¨GRPOï¼Œä¸ºæ•´ä¸ªè¾“å‡ºåºåˆ—è®¡ç®—ä¸€ä¸ª**åºåˆ—çº§å¥–åŠ±**ã€‚ | **å¤šè½®GRPO (mtGRPO)**ï¼šé’ˆå¯¹å¤šè½®ç¯å¢ƒä¸‹çš„**ç¨€ç–å¥–åŠ±**é—®é¢˜ï¼ŒMTDrive æå‡ºäº† **mtGRPO**ã€‚å…³é”®æ”¹è¿›åœ¨äºï¼š1) **è½®çº§å¥–åŠ±åˆ†é…**ï¼šä¸ºæ¯ä¸€è½®ç”Ÿæˆçš„è½¨è¿¹ç‹¬ç«‹è®¡ç®—å¥–åŠ±ã€‚2) **è½®çº§ä¼˜åŠ¿ä¼°è®¡**ï¼šåœ¨ç»„å†…è¿›è¡Œè½®çº§å½’ä¸€åŒ–æ¥è®¡ç®—ä¼˜åŠ¿å€¼ï¼Œä»è€Œæ›´ç²¾ç»†åœ°åˆ†é…å„è½®è´¡çŒ®çš„ä¿¡ç”¨ï¼Œè§£å†³äº†å¤šè½®ä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…éš¾é¢˜ã€‚ |\n| **äº¤äº’å¼å¤šè½®æ¨ç†** | DriveAgent-R1 [42] | å±•ç¤ºäº†åœ¨ADä¸­é€šè¿‡å¤šè½®äº¤äº’ï¼ˆåœ¨è¯­è¨€æ¨ç†å’Œå·¥å…·è°ƒç”¨é—´åˆ‡æ¢ï¼‰æ¥æå‡å†³ç­–çš„æ½œåŠ›ã€‚ | **ä¼˜åŒ–ç›®æ ‡ä¸æ–¹æ³•ä¸åŒ**ï¼šDriveAgent-R1 ä¾§é‡äºé€šè¿‡äº¤äº’æå‡æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œæœªæ˜ç¡®é‡‡ç”¨é’ˆå¯¹å¤šè½®è½¨è¿¹ç²¾ä¿®çš„RLç®—æ³•è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚ | **ä¸“ä¸ºè½¨è¿¹ç²¾ä¿®è®¾è®¡çš„RLæ¡†æ¶**ï¼šMTDrive æ„å»ºäº†**å®Œæ•´çš„æ•°æ®-ç®—æ³•-ç³»ç»Ÿæ¡†æ¶**ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–å¤šè½®è½¨è¿¹ç²¾ä¿®ï¼š1) æ„å»ºäº†**å¤šè½®äº¤äº’è½¨è¿¹ç†è§£æ•°æ®é›†**ï¼›2) æå‡ºäº†é’ˆå¯¹æ€§çš„ **mtGRPO ç®—æ³•**ï¼›3) å¼€å‘äº†**å¤šæ¨¡æ€å¤šè½®RLè®­ç»ƒç³»ç»Ÿ**å¹¶è¿›è¡Œäº†ç³»ç»Ÿçº§ä¼˜åŒ–ã€‚ |\n| **SOTA å¯¹æ¯”åŸºçº¿** | ReCogDrive [15], ReflectDrive [13] | å…ˆè¿›çš„VLMé©±åŠ¨è‡ªåŠ¨é©¾é©¶æ–¹æ³•ã€‚ReCogDrive æ˜¯VLM-æ‰©æ•£æ··åˆæ¨¡å‹ï¼›ReflectDrive ä½¿ç”¨äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚ | **æ¨ç†æœºåˆ¶**ï¼šå®ƒä»¬è™½å¯èƒ½æ¶‰åŠåæ€æˆ–è¿­ä»£ï¼Œä½†MTDriveè®ºæ–‡æŒ‡å‡ºï¼Œç°æœ‰æ–¹æ³•å¤§å¤šå—é™äº**å•è½®æ¨ç†**ï¼Œæ— æ³•è¿›è¡ŒåŸºäºåé¦ˆçš„è¿­ä»£å¼è½¨è¿¹ä¼˜åŒ–ã€‚ | **è¿­ä»£ç²¾ä¿®èƒ½åŠ›**ï¼šMTDrive æ˜ç¡®è®¾è®¡äº†**å¤šè½®äº¤äº’å¾ªç¯**ï¼Œå…¶æ¨¡å‹èƒ½å¤Ÿæ ¹æ®PDM Agentæä¾›çš„å…·ä½“ã€å¯æ“ä½œçš„åé¦ˆï¼ˆå¦‚â€œè½¨è¿¹ç‚¹Xåœ¨å¯è¡Œé©¶åŒºåŸŸå¤–â€ï¼‰æ¥ä¿®æ­£è½¨è¿¹ï¼Œè€Œä¸ä»…ä»…æ˜¯ç”Ÿæˆä¸€ä¸ªæ›´å¥½çš„åˆå§‹çŒœæµ‹ã€‚ |\n\n### æ€»ç»“ï¼šMTDrive ç›¸å¯¹äºå‰ä½œçš„æ ¸å¿ƒæ”¹è¿›\n\n1.  **èŒƒå¼åˆ›æ–°ï¼šä»å•è½®åˆ°å¤šè½®äº¤äº’å¼è½¨è¿¹è§„åˆ’**\n    *   **å‰ä½œå±€é™**ï¼šæ— è®ºæ˜¯VLMç›´æ¥è§„åˆ’è¿˜æ˜¯RLä¼˜åŒ–çš„æ–¹æ³•ï¼Œå¤§å¤šåœç•™åœ¨â€œè§‚å¯Ÿ-ä¸€æ¬¡è¾“å‡ºâ€çš„å•è½®æ¨¡å¼ï¼Œå¯¹äºéœ€è¦é€æ­¥è°ƒæ•´çš„å¤æ‚åœºæ™¯å¤„ç†èƒ½åŠ›æœ‰é™ã€‚\n    *   **MTDrive æ”¹è¿›**ï¼šå¼•å…¥äº†ä¸€ä¸ª**é—­ç¯äº¤äº’æ¡†æ¶**ï¼Œæ¨¡æ‹Ÿäº†äººç±»é©¾é©¶å‘˜â€œè§„åˆ’-è¯„ä¼°-è°ƒæ•´â€çš„å†³ç­–è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹å…·å¤‡äº†**è¿­ä»£ä¼˜åŒ–**çš„èƒ½åŠ›ã€‚\n\n2.  **ç®—æ³•åˆ›æ–°ï¼šé’ˆå¯¹å¤šè½®ä»»åŠ¡çš„ mtGRPO**\n    *   **å‰ä½œå±€é™**ï¼šç›´æ¥å°†GRPOç­‰RLç®—æ³•ç”¨äºå¤šè½®ä»»åŠ¡ä¼šå¯¼è‡´ç¨€ç–å¥–åŠ±å’Œä¿¡ç”¨åˆ†é…é—®é¢˜ï¼Œå› ä¸ºåªæœ‰æœ€ç»ˆè½®æ¬¡æœ‰æ˜¾è‘—å¥–åŠ±ï¼Œä¸­é—´è½®æ¬¡çš„è´¡çŒ®éš¾ä»¥è¡¡é‡ã€‚\n    *   **MTDrive æ”¹è¿›**ï¼šæå‡º **mtGRPO**ï¼Œé€šè¿‡**è½®çº§å¥–åŠ±åˆ†é…**å’Œ**è½®çº§ä¼˜åŠ¿ä¼°è®¡**ï¼Œä½¿RLè®­ç»ƒèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ¯ä¸€è½®æ”¹è¿›çš„ä»·å€¼ï¼Œæ˜¾è‘—æå‡äº†å¤šè½®åœºæ™¯ä¸‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡ã€‚\n\n3.  **æ•°æ®ä¸ç³»ç»ŸååŒåˆ›æ–°**\n    *   **æ•°æ®**ï¼šä¸“é—¨æ„å»ºäº†åŒ…å«**å•è½®ã€å¤šè½®å’ŒPDMç†è§£æ•°æ®**çš„SFTæ•°æ®é›†ï¼Œä»¥åŠç”¨äºRLçš„**äº¤äº’å¼è½¨è¿¹ç²¾ä¿®æ•°æ®é›†**ï¼Œä¸ºå¤šè½®èƒ½åŠ›æä¾›äº†æ•°æ®åŸºç¡€ã€‚\n    *   **ç³»ç»Ÿ**ï¼šåŸºäºveRLæ¡†æ¶æ„å»ºäº†å¤šæ¨¡æ€å¤šè½®RLè®­ç»ƒç³»ç»Ÿï¼Œå¹¶å®æ–½äº† **IPSSï¼ˆè¿›ç¨‹é—´æµå¼åºåˆ—åŒ–ï¼‰** å’Œ **IPTCï¼ˆè¿›ç¨‹å†…å¼ é‡ç¼“å­˜ï¼‰** ä¼˜åŒ–ï¼Œå°†è®­ç»ƒååé‡æå‡äº† **2.5å€**ï¼Œè§£å†³äº†é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤šè½®åºåˆ—å¸¦æ¥çš„æ•°æ®ä¼ è¾“ç“¶é¢ˆã€‚è¿™æ˜¯æ­¤å‰å·¥ä½œä¸­æœªå……åˆ†æ¶‰åŠçš„**å·¥ç¨‹æ€§é‡å¤§æ”¹è¿›**ã€‚\n\n**æ€»è€Œè¨€ä¹‹ï¼ŒMTDrive å¹¶éç®€å•åœ°å°†ç°æœ‰VLMæˆ–RLæŠ€æœ¯åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ï¼Œè€Œæ˜¯é’ˆå¯¹â€œå¤æ‚åœºæ™¯éœ€è¦å¤šæ­¥æ¨ç†ç²¾ä¿®â€è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€å¥—æ¶µç›–æ–°é¢–äº¤äº’èŒƒå¼ã€ä¸“ç”¨RLç®—æ³•ä»¥åŠé«˜æ•ˆè®­ç»ƒç³»ç»Ÿçš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚å…¶ä¸å‰ä½œæœ€æ ¹æœ¬çš„åŒºåˆ«åœ¨äºèµ‹äºˆäº†è‡ªåŠ¨é©¾é©¶æ¨¡å‹â€œåå¤æ€è€ƒå¹¶ä¿®æ­£â€çš„èƒ½åŠ›ã€‚**",
            "timestamp": "2026-02-03T15:56:12.847152",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡æå‡ºäº†å“ªäº›å…³é”®æŠ€æœ¯æ–¹æ³•ï¼Œè¯·åˆ—è¡¨æ ¼å…·ä½“è¯¦ç»†è¯´æ˜æŠ€æœ¯ç»†èŠ‚ï¼Œéœ€è¦åŒ…å«å…·ä½“çš„æ•°å­¦åŸç†æ¨å¯¼ï¼Œä»¥åŠå…·ä½“å‚æ•°ã€‚",
            "answer": "## MTDriveå…³é”®æŠ€æœ¯æ–¹æ³•è¯¦è§£\n\nä»¥ä¸‹æ˜¯è®ºæ–‡æå‡ºçš„å››é¡¹å…³é”®æŠ€æœ¯æ–¹æ³•åŠå…¶è¯¦ç»†æŠ€æœ¯ç»†èŠ‚ï¼š\n\n### 1. å¤šè½®äº¤äº’å¼è½¨è¿¹ç†è§£æ•°æ®é›†æ„å»º\n\n| æ–¹æ³•ç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ | å…·ä½“å‚æ•°/æ•°å­¦åŸç† |\n| :--- | :--- | :--- |\n| **æ•°æ®æ¥æºä¸ç­›é€‰** | åŸºäºNAVSIMé—­ç¯é©¾é©¶æ¨¡æ‹Ÿå™¨æ„å»ºæ•°æ®é›†ï¼Œä½¿ç”¨PDMè¯„åˆ†ï¼ˆå…¬å¼1ï¼‰ç­›é€‰åœºæ™¯ã€‚é‡ç‚¹å…³æ³¨ä¸å®‰å…¨ç›¸å…³çš„ä¸‰ä¸ªæŒ‡æ ‡ï¼š<br>â€¢ **NCï¼ˆæ— ç¢°æ’ï¼‰**ï¼šäºŒå…ƒæŒ‡æ ‡ï¼Œæ£€æŸ¥æ˜¯å¦å‘ç”Ÿç¢°æ’<br>â€¢ **DACï¼ˆå¯è¡Œé©¶åŒºåŸŸåˆè§„æ€§ï¼‰**ï¼šæ£€æŸ¥è½¨è¿¹ç‚¹æ˜¯å¦åœ¨å¯è¡Œé©¶åŒºåŸŸå†…<br>â€¢ **TTCï¼ˆç¢°æ’æ—¶é—´ï¼‰**ï¼šè®¡ç®—ä¸å‰æ–¹è½¦è¾†çš„æ—¶é—´é—´éš”ï¼Œç¡®ä¿å®‰å…¨è·ç¦» | **PDMè¯„åˆ†å…¬å¼**ï¼š<br>`PDMS = (âˆ_{mâˆˆ{NC, DAC}} Score_m) Ã— (âˆ‘_{wâˆˆ{EP, TTC, C}} Weight_w Ã— Score_w / âˆ‘_{w} Weight_w)`<br>**ç­›é€‰é˜ˆå€¼**ï¼šåœ¨RLè®­ç»ƒä¸­ï¼Œå°†é¦–æ¬¡æ¨ç†PDMåˆ†æ•°ä½äº0.8çš„æ ·æœ¬å½’ç±»ä¸ºâ€œä½åˆ†æ•°æ®â€ç”¨äºé‡ç‚¹è®­ç»ƒ |\n| **å•è½®æ•°æ®** | åŸºç¡€è½¨è¿¹ç”Ÿæˆèƒ½åŠ›è®­ç»ƒã€‚è¾“å…¥ï¼šå‰è§†å›¾åƒã€å¯¼èˆªæŒ‡ä»¤ã€2ç§’å†å²è½¨è¿¹ã€PDMæŒ‡æ ‡æè¿°ã€‚è¾“å‡ºï¼š4ç§’æœªæ¥è½¨è¿¹ï¼ˆ8ä¸ªç‚¹ï¼‰ã€‚ | **æ•°æ®é‡**ï¼š~80,000ä¸ªæ ·æœ¬<br>**è½¨è¿¹æ ¼å¼**ï¼šæ¯ä¸ªç‚¹`(x: float, y: float, heading: float)`ï¼Œå…±8ä¸ªç‚¹ï¼Œæ—¶é—´é—´éš”0.5ç§’ |\n| **å¤šè½®æ•°æ®** | é€šè¿‡è¿­ä»£è‡ªä¸¾è¿‡ç¨‹æ„å»ºï¼Œæ¿€æ´»æ¨¡å‹çš„å¤šè½®æ¨ç†èƒ½åŠ›ã€‚æµç¨‹ï¼šè®­ç»ƒkè½®æ¨¡å‹ â†’ æ¨ç†è·å¾—è½¨è¿¹ â†’ PDMæ™ºèƒ½ä½“æä¾›åé¦ˆ â†’ æ„å»º(k+1)è½®æ ·æœ¬ã€‚ | **æ„å»ºæ­¥éª¤**ï¼š<br>1. ç”¨å•è½®æ¨¡å‹`Qwen2.5VL_sft1`åœ¨å•è½®æ•°æ®ä¸Šæ¨ç†<br>2. å°†é¢„æµ‹è½¨è¿¹è¾“å…¥PDMæ™ºèƒ½ä½“è·å¾—åé¦ˆ<br>3. æ‹¼æ¥åŸå§‹æç¤ºã€æ¨¡å‹é¢„æµ‹å’ŒPDMåé¦ˆä½œä¸ºä¸‹ä¸€è½®è¾“å…¥ï¼ŒçœŸå€¼è½¨è¿¹ä½œä¸ºç›®æ ‡<br>**æ•°æ®é‡**ï¼š2è½®~50,000ï¼Œ3è½®åŠæ¨¡æ‹Ÿå¤šè½®~5,000 |\n| **PDMç†è§£æ•°æ®** | å¸®åŠ©æ¨¡å‹ç†è§£PDMåé¦ˆçš„é—®ç­”å¯¹æ•°æ®ã€‚é’ˆå¯¹æ¯ä¸ªPDMæŒ‡æ ‡è®¾è®¡æ­£è´Ÿæ ·æœ¬ã€‚ | **ç¤ºä¾‹ï¼ˆDACæŒ‡æ ‡ï¼‰**ï¼š<br>è¾“å…¥ï¼šè½¨è¿¹ç‚¹`(+24.87, 0.0, 0.0)`å’Œå‰è§†å›¾åƒ<br>é—®é¢˜ï¼šè¯¥ç‚¹æ˜¯å¦åœ¨å¯è¡Œé©¶åŒºåŸŸå†…ï¼Ÿ<br>è¾“å‡ºï¼š`Yes`æˆ–`No`<br>**æ•°æ®é‡**ï¼š~80,000ä¸ªæ ·æœ¬ |\n| **RLè®­ç»ƒæ•°æ®æ„å»º** | ä»NAVSIMè®­ç»ƒé›†ä¸­ç­›é€‰ä¸‰ç±»æ•°æ®ç”¨äºé«˜æ•ˆRLè®­ç»ƒï¼š<br>1. **2è½®æ•°æ®**ï¼šé¦–æ¬¡æ¨ç†åPDMåé¦ˆéç©º<br>2. **ä½åˆ†æ•°æ®**ï¼šé¦–æ¬¡æ¨ç†PDMåˆ†æ•° < 0.8<br>3. **å…¶ä»–æ•°æ®**ï¼šéšæœºé‡‡æ ·ä»¥å¹³è¡¡åˆ†å¸ƒ | **æœ€ç»ˆRLæ•°æ®é›†**ï¼š13,000ä¸ªæ ·æœ¬ï¼ˆåŒ…å«æ‰€æœ‰ç¬¬1ã€2ç±»æ•°æ®åŠéƒ¨åˆ†ç¬¬3ç±»ï¼‰ |\n\n### 2. å¤šè½®ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•\n\n| æ–¹æ³•ç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ | å…·ä½“å‚æ•°/æ•°å­¦åŸç† |\n| :--- | :--- | :--- |\n| **é—®é¢˜å®šä¹‰** | æ ‡å‡†GRPOåœ¨**å¤šè½®ä»»åŠ¡**ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜ï¼šGRPOä¸ºæ•´ä¸ªåºåˆ—è®¡ç®—å•ä¸€å¥–åŠ±ï¼Œåœ¨å¤šè½®ä»»åŠ¡ä¸­ï¼Œä¸åŒè½®æ¬¡æ€§èƒ½å·®å¼‚å¤§ï¼Œå¯¼è‡´**ä¿¡ç”¨åˆ†é…**å›°éš¾ã€‚ | **ç¤ºä¾‹**ï¼šç¬¬ä¸€è½®è¡¨ç°å·®ï¼Œç¬¬äºŒè½®è¡¨ç°å¥½ï¼Œä½†æ•´ä½“åºåˆ—å¥–åŠ±ä¸ºæ­£æ—¶ï¼ŒGRPOä¼šé”™è¯¯åœ°å¥–åŠ±ç¬¬ä¸€è½®ï¼Œå¹¶ç¨€é‡Šç¬¬äºŒè½®çš„ä¼˜åŠ¿ã€‚ |\n| **å¥–åŠ±è®¾è®¡** | ä¸ºæ¯ä¸€è½®ç‹¬ç«‹è®¡ç®—å¥–åŠ±ï¼Œå¹¶åˆ†é…ç»™å¯¹åº”è½®æ¬¡çš„æ‰€æœ‰tokenã€‚å¥–åŠ±ç”±PDMåˆ†æ•°å’Œæ ¼å¼åˆ†æ•°åŠ æƒç»„æˆã€‚ | **å¥–åŠ±è®¡ç®—å…¬å¼**ï¼š<br>`r_{i,j} = w_p Â· p_{i,j} + w_f Â· f_{i,j}`<br>**å‚æ•°**ï¼š<br>`w_p = 0.8` (PDMåˆ†æ•°æƒé‡)<br>`w_f = 0.2` (æ ¼å¼åˆ†æ•°æƒé‡)<br>`p_{i,j}`ï¼šç¬¬iæ¬¡rolloutä¸­ç¬¬jè½®çš„PDMåˆ†æ•°<br>`f_{i,j}`ï¼šç¬¬iæ¬¡rolloutä¸­ç¬¬jè½®çš„æ ¼å¼åˆ†æ•° |\n| **ä¼˜åŠ¿ä¼°è®¡** | åœ¨rolloutæ‰¹æ¬¡ä¸­ï¼Œ**ä¸ºæ¯ä¸€è½®ç‹¬ç«‹è¿›è¡Œä¼˜åŠ¿ä¼°è®¡**ï¼Œä½¿ç”¨ç»„å†…å½’ä¸€åŒ–ã€‚å…³é”®æ”¹è¿›ï¼šä½¿ç”¨**è·¨è½®å½’ä¸€åŒ–**ï¼ˆå®éªŒ3ï¼‰è€Œéç»„å†…å½’ä¸€åŒ–ï¼ˆå®éªŒ2ï¼‰ã€‚ | **ä¼˜åŠ¿è®¡ç®—å…¬å¼**ï¼š<br>`Ãƒ_{i,t} = (r_{i,j} - (1/G)âˆ‘_{i=1}^G r_{i,j}) / std({r_{i,j}}_{i=1}^G)`<br>**å…¶ä¸­**ï¼š<br>`G`ï¼šrolloutæ•°é‡ï¼ˆç»„å¤§å°ï¼‰<br>`Ãƒ_{i,t}`ï¼šç¬¬iæ¬¡rolloutä¸­ç¬¬tä¸ªtokençš„ä¼˜åŠ¿å€¼<br>`j`ï¼šè¯¥tokenæ‰€å±çš„è½®æ¬¡ç´¢å¼• |\n| **ç›®æ ‡å‡½æ•°** | åŸºäºGRPOç›®æ ‡å‡½æ•°æ‰©å±•ï¼Œä½†ä¼˜åŠ¿è®¡ç®—åŸºäºæ¯è½®ç‹¬ç«‹å¥–åŠ±ã€‚ | **mtGRPOç›®æ ‡å‡½æ•°**ï¼š<br>`J_mtGRPO(Î¸) = E_{q,{o_i}} [ (1/G)âˆ‘_{i=1}^G (1/|o_i|)âˆ‘_{t=1}^{|o_i|} { min( (Ï€_Î¸/Ï€_Î¸_old)Ãƒ_{i,t}, clip(Ï€_Î¸/Ï€_Î¸_old, 1-Îµ, 1+Îµ)Ãƒ_{i,t} ) - Î² D_KL(Ï€_Î¸ || Ï€_ref) } ]`<br>**å‚æ•°**ï¼š<br>`Îµ = 0.2` (PPOè£å‰ªèŒƒå›´)<br>`Î² = 0.01` (KLæƒ©ç½šç³»æ•°)<br>`Ï€_ref`ï¼šå‚è€ƒæ¨¡å‹ï¼ˆå½“å‰ç­–ç•¥çš„å‰¯æœ¬ï¼‰ |\n| **è®­ç»ƒæµç¨‹** | ç®—æ³•1è¯¦ç»†æè¿°äº†mtGRPOçš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç­–ç•¥é‡‡æ ·ã€å¥–åŠ±è®¡ç®—ã€ä¼˜åŠ¿ä¼°è®¡å’Œå¤šæ­¥ä¼˜åŒ–ã€‚ | **å…³é”®è¶…å‚æ•°**ï¼š<br>â€¢ ç»„å¤§å° `G = 8`<br>â€¢ æœ€å¤§è½®æ•° `N = 6`<br>â€¢ å…¨å±€æ‰¹æ¬¡å¤§å° `256`ï¼Œå°æ‰¹æ¬¡å¤§å° `128`<br>â€¢ å­¦ä¹ ç‡ `1Ã—10^{-6}`<br>â€¢ KLæ•£åº¦è®¡ç®—ä½¿ç”¨K3æ–¹æ³• |\n\n### 3. å¤šæ¨¡æ€å¤šè½®RLè®­ç»ƒç³»ç»Ÿä¼˜åŒ–\n\n| æ–¹æ³•ç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ | å…·ä½“å‚æ•°/æ•°å­¦åŸç† |\n| :--- | :--- | :--- |\n| **ç³»ç»ŸåŸºç¡€** | åŸºäºveRLæ¡†æ¶æ‰©å±•ï¼Œæ”¯æŒä¸PDMç¯å¢ƒçš„å¤šè½®äº¤äº’ã€‚è®­ç»ƒæµç¨‹è¿­ä»£è¿›è¡Œï¼š<br>1. **Rolloutç”Ÿæˆ**ï¼šç­–ç•¥ä¸PDMæ™ºèƒ½ä½“å¤šè½®äº¤äº’ï¼Œè¿­ä»£ä¼˜åŒ–è½¨è¿¹å¹¶è·å¾—å¥–åŠ±<br>2. **å¯¹æ•°æ¦‚ç‡é‡è®¡ç®—**ï¼šå‰å‘ä¼ æ’­æ¼”å‘˜æ¨¡å‹è·å–å¯¹æ•°æ¦‚ç‡<br>3. **å‚è€ƒæ¨¡å‹æ¨ç†**ï¼šå‰å‘ä¼ æ’­å‚è€ƒæ¨¡å‹ç”¨äºKLæ­£åˆ™åŒ–<br>4. **æ¼”å‘˜æ›´æ–°**ï¼šä¼˜åŠ¿è®¡ç®—å’Œåå‘ä¼ æ’­æ›´æ–°æ¼”å‘˜ | **ç“¶é¢ˆåˆ†æ**ï¼šé«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤šè½®åºåˆ—å¯¼è‡´**æ•°æ®ä¼ é€’å¼€é”€å·¨å¤§**ï¼Œæˆä¸ºä¸»è¦æ€§èƒ½ç“¶é¢ˆã€‚ |\n| **è¿›ç¨‹é—´æµå¼åºåˆ—åŒ–** | **é—®é¢˜**ï¼šä¼ ç»Ÿæ–¹æ³•ç­‰å¾…æ‰€æœ‰rolloutå®Œæˆåæ‰åºåˆ—åŒ–æ•´ä¸ªæ‰¹æ¬¡ï¼Œåºåˆ—åŒ–æ—¶é—´ä¸ºçº¯å¼€é”€ã€‚<br>**è§£å†³æ–¹æ¡ˆ**ï¼šæ¯å®Œæˆä¸€ä¸ªrolloutï¼Œç«‹å³å°†å…¶å¤šæ¨¡æ€æ•°æ®åºåˆ—åŒ–å¸è½½åˆ°ä¸“ç”¨çº¿ç¨‹æ± ï¼Œä¸å‰©ä½™rolloutçš„ç”Ÿæˆ**é‡å è¿›è¡Œ**ã€‚ | **æ€§èƒ½æå‡**ï¼šè´¡çŒ®çº¦ **1.5å€** åŠ é€Ÿæ¯”ã€‚<br>**å®ç°æœºåˆ¶**ï¼šå¼‚æ­¥çº¿ç¨‹æ± å¤„ç†å·²å®Œæˆrolloutçš„åºåˆ—åŒ–ï¼Œä¸»çº¿ç¨‹ç»§ç»­ç”Ÿæˆæœªå®Œæˆçš„rolloutã€‚ |\n| **è¿›ç¨‹å†…å¼ é‡ç¼“å­˜** | **é—®é¢˜**ï¼šå½“å¯¹æ•°æ¦‚ç‡é‡è®¡ç®—ã€å‚è€ƒæ¨¡å‹æ¨ç†å’Œæ¼”å‘˜è®­ç»ƒä½äºåŒä¸€è¿›ç¨‹æ—¶ï¼Œæ§åˆ¶å™¨ä»å°†æ•°æ®åˆ†å‘ç»™æ¯ä¸ªæ¨¡å—ï¼Œå¯¼è‡´**é‡å¤ååºåˆ—åŒ–**ã€‚<br>**è§£å†³æ–¹æ¡ˆ**ï¼šå®ç°ç¼“å­˜æœºåˆ¶ï¼Œåœ¨ç¬¬ä¸€ä¸ªæ¨¡å—æ‰§è¡Œæ—¶ï¼Œå°†å…±äº«è¾“å…¥ï¼ˆå¦‚tokenåŒ–åºåˆ—ã€æ³¨æ„åŠ›æ©ç ã€è§†è§‰åµŒå…¥ï¼‰**ç›´æ¥ç¼“å­˜åˆ°GPUå†…å­˜**ä¸­ã€‚ | **æ€§èƒ½æå‡**ï¼šè´¡çŒ®çº¦ **1.7å€** åŠ é€Ÿæ¯”ã€‚<br>**ç¼“å­˜å†…å®¹**ï¼š`tokenized_sequences`, `attention_masks`, `visual_embeddings` |\n| **æ•´ä½“ä¼˜åŒ–æ•ˆæœ** | ä¸¤é¡¹ä¼˜åŒ–ç»“åˆï¼Œæ˜¾è‘—å‡å°‘æ¯æ­¥è®­ç»ƒæ—¶é—´ã€‚ | **é‡åŒ–ç»“æœ**ï¼š<br>æ¯æ­¥è®­ç»ƒæ—¶é—´ä» **~1250ç§’** å‡å°‘åˆ° **~490ç§’**ï¼Œæ•´ä½“è¾¾åˆ° **2.5å€** è®­ç»ƒååé‡æå‡ã€‚ |\n\n### 4. åŸºäºPDMæ™ºèƒ½ä½“çš„äº¤äº’åé¦ˆæœºåˆ¶\n\n| æ–¹æ³•ç»„ä»¶ | æŠ€æœ¯ç»†èŠ‚ | å…·ä½“å‚æ•°/æ•°å­¦åŸç† |\n| :--- | :--- | :--- |\n| **æ™ºèƒ½ä½“å®ä¾‹åŒ–** | æ¡†æ¶è®¾è®¡ä¸ºæ™ºèƒ½ä½“æ— å…³ï¼Œä½†æœ¬æ–‡å®ä¾‹åŒ–ä¸ºNAVSIMåŸºå‡†ä¸­çš„**PDMæ™ºèƒ½ä½“**ï¼Œä»¥æä¾›æ ‡å‡†åŒ–è¯„ä¼°å’Œåé¦ˆã€‚ | **åé¦ˆæ¥æº**ï¼š",
            "timestamp": "2026-02-03T15:57:21.769245",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "ä»–ä½¿ç”¨äº†å“ªäº›è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†ï¼Œåˆ—è¡¨æ ¼å…·ä½“è®²è®²ä»–çš„è¯„ä»·æŒ‡æ ‡çš„ç»†èŠ‚ä¸æ•°æ®é›†çš„ç»†èŠ‚",
            "answer": "## è®ºæ–‡è¯„ä»·æŒ‡æ ‡ä¸æ•°æ®é›†åˆ†æ\n\n### ä¸€ã€ ä¸»è¦è¯„ä»·æŒ‡æ ‡ï¼ˆåŸºäºNAVSIMåŸºå‡†ï¼‰\n\nè®ºæ–‡ä½¿ç”¨NAVSIMåŸºå‡†ä¸­çš„**PDM Score (PDMS)** ä½œä¸ºæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ã€‚PDMSæ˜¯ä¸€ä¸ªå¤åˆæŒ‡æ ‡ï¼Œç”±5ä¸ªå­æŒ‡æ ‡åŠ æƒè®¡ç®—å¾—å‡ºï¼Œæ—¨åœ¨å¹³è¡¡å®‰å…¨æ€§ã€èˆ’é€‚æ€§å’Œä»»åŠ¡è¿›åº¦ã€‚\n\n| æŒ‡æ ‡ç¼©å†™ | å…¨ç§° | æ ¸å¿ƒå®šä¹‰ä¸è®¡ç®—ç»†èŠ‚ | åœ¨è®ºæ–‡ä¸­çš„è§’è‰² |\n| :--- | :--- | :--- | :--- |\n| **NC** | No Collisions (æ— ç¢°æ’) | è¡¡é‡è‡ªè½¦æ˜¯å¦ä¸å…¶ä»–äº¤é€šå‚ä¸è€…æˆ–ç‰©ä½“å‘ç”Ÿç¢°æ’ã€‚æ˜¯**äºŒå…ƒå®‰å…¨æŒ‡æ ‡**ï¼ˆå‘ç”Ÿç¢°æ’åˆ™å¾—åˆ†ä¸¥é‡ä¸‹é™ï¼‰ã€‚ | **æ ¸å¿ƒå®‰å…¨æŒ‡æ ‡**ã€‚PDM Agentåé¦ˆçš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºåœ¨å¤šè½®äº¤äº’ä¸­è¯†åˆ«ç¢°æ’é£é™©ç‚¹ã€‚ |\n| **DAC** | Drivable Area Compliance (å¯è¡Œé©¶åŒºåŸŸåˆè§„æ€§) | è¡¡é‡è‡ªè½¦è½¨è¿¹ç‚¹æ˜¯å¦å§‹ç»ˆä¿æŒåœ¨æŒ‡å®šçš„å¯è¡Œé©¶åŒºåŸŸï¼ˆå¦‚è½¦é“çº¿å†…ï¼‰å†…ã€‚**è¿è§„çš„è½¨è¿¹ç‚¹ä¼šè¢«æå–å¹¶æ–‡æœ¬åŒ–æè¿°**ã€‚ | **æ ¸å¿ƒå®‰å…¨ä¸åˆè§„æ€§æŒ‡æ ‡**ã€‚ç”¨äºæä¾›å…·ä½“çš„ã€ç©ºé—´åŒ–çš„åé¦ˆï¼ŒæŒ‡å¯¼æ¨¡å‹ä¿®æ­£è½¨è¿¹ã€‚ |\n| **TTC** | Time-to-Collision (ç¢°æ’æ—¶é—´) | è¡¡é‡è‡ªè½¦ä¸å‰è½¦ï¼ˆé€šå¸¸æ˜¯å¼•å¯¼è½¦ï¼‰ä¹‹é—´æ˜¯å¦ä¿æŒäº†è¶³å¤Ÿçš„**å®‰å…¨æ—¶é—´è£•åº¦**ã€‚è€ƒè™‘ç›¸å¯¹é€Ÿåº¦å’Œè·ç¦»ã€‚ | **æ ¸å¿ƒå‰ç»æ€§å®‰å…¨æŒ‡æ ‡**ã€‚ç”¨äºè¯†åˆ«æ½œåœ¨çš„ã€æœªæ¥çš„ç¢°æ’é£é™©ï¼Œè€Œä¸ä»…ä»…æ˜¯å³æ—¶ç¢°æ’ã€‚ |\n| **CF** | Comfort (èˆ’é€‚åº¦) | è¡¡é‡é©¾é©¶è¡Œä¸ºçš„å¹³é¡ºæ€§ï¼Œé€šå¸¸ä¸åŠ é€Ÿåº¦ã€åŠ åŠ é€Ÿåº¦ï¼ˆjerkï¼‰ç›¸å…³ã€‚åœ¨PDMSå…¬å¼ä¸­ä½œä¸ºåŠ æƒå¹³å‡çš„ä¸€éƒ¨åˆ†ã€‚ | **èˆ’é€‚æ€§æŒ‡æ ‡**ã€‚åœ¨è®ºæ–‡çš„PDM Agentåé¦ˆç”Ÿæˆä¸­**æœªè¢«ä½¿ç”¨**ï¼Œå› å…¶è®¡ç®—ä¸ä¾èµ–äºçœŸå®è½¨è¿¹ã€‚ |\n| **EP** | Ego Progress (è‡ªè½¦è¿›åº¦) | è¡¡é‡è‡ªè½¦æ²¿ç€è§„åˆ’è·¯å¾„å‰è¿›çš„è·ç¦»æˆ–è¿›åº¦ã€‚åœ¨PDMSå…¬å¼ä¸­ä½œä¸ºåŠ æƒå¹³å‡çš„ä¸€éƒ¨åˆ†ã€‚ | **ä»»åŠ¡å®Œæˆåº¦æŒ‡æ ‡**ã€‚åœ¨**å¤šè½®æ¨ç†æç¤ºä¸­ä¸æä¾›EPä¿¡æ¯**ï¼ˆä»¥é¿å…æ³„éœ²çœŸå®è½¨è¿¹ï¼‰ï¼Œæ¨¡å‹çš„é«˜EPå¾—åˆ†å®Œå…¨é€šè¿‡RLè®­ç»ƒä¸­çš„å¥–åŠ±åé¦ˆä¹ å¾—ã€‚ |\n\n**PDMSè®¡ç®—å…¬å¼**ï¼š\n`PDMS = (NCå¾—åˆ† Ã— DACå¾—åˆ†) Ã— [ (Weight_EPÃ—EPå¾—åˆ† + Weight_TTCÃ—TTCå¾—åˆ† + Weight_CÃ—CFå¾—åˆ†) / æƒé‡å’Œ ]`\n- **å‰åŠéƒ¨åˆ†ï¼ˆä¹˜ç§¯ï¼‰**ï¼šNCå’ŒDACä½œä¸º**æƒ©ç½šé¡¹**ï¼Œä»»ä¸€æŒ‡æ ‡å¤±è´¥ä¼šæ˜¾è‘—æ‹‰ä½æ€»åˆ†ã€‚\n- **ååŠéƒ¨åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰**ï¼šEPã€TTCã€CFå…±åŒåæ˜ é©¾é©¶è´¨é‡ã€‚\n\n### äºŒã€ ä½¿ç”¨çš„ä¸»è¦æ•°æ®é›†\n\nè®ºæ–‡æ„å»ºå¹¶ä½¿ç”¨äº†å¤šä¸ªå®šåˆ¶æ•°æ®é›†ï¼Œç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µã€‚\n\n| æ•°æ®é›†ç±»å‹ | æ•°æ®æ¥æºä¸æ„å»ºæ–¹æ³• | æ•°æ®è§„æ¨¡ä¸ç»†èŠ‚ | æ ¸å¿ƒç›®çš„ä¸ä½œç”¨ |\n| :--- | :--- | :--- | :--- |\n| **å•è½®æ•°æ® (Single-turn)** | åŸºäºNAVSIMï¼Œæºè‡ª**RecogDrive**æ•°æ®é›†ã€‚ | çº¦ **80,000** ä¸ªæ ·æœ¬ã€‚ **è¾“å…¥**ï¼šå‰è§†å›¾åƒã€å¯¼èˆªæŒ‡ä»¤ã€2ç§’å†å²è½¨è¿¹ã€PDMæŒ‡æ ‡æè¿°ã€‚ **è¾“å‡º**ï¼š4ç§’æœªæ¥è½¨è¿¹ã€‚ | **å†·å¯åŠ¨**ã€‚èµ‹äºˆæ¨¡å‹åŸºç¡€çš„è½¨è¿¹ç”Ÿæˆèƒ½åŠ›ï¼Œæ˜¯åç»­å¤šè½®æ•°æ®æ„å»ºçš„èµ·ç‚¹ã€‚ |\n| **å¤šè½®æ•°æ® (Multi-turn)** | **è¿­ä»£è‡ªä¸¾æ³•**æ„å»ºã€‚ 1. ç”¨ç¬¬ `k` è½®æ¨¡å‹æ¨ç†å¾—åˆ°è½¨è¿¹ã€‚ 2. é€å…¥PDM Agentè·å¾—åé¦ˆã€‚ 3. å°†åŸå§‹æç¤ºã€æ¨¡å‹è¾“å‡ºã€PDMåé¦ˆæ‹¼æ¥ï¼Œä½œä¸ºç¬¬ `k+1` è½®è¾“å…¥ï¼ŒçœŸå®è½¨è¿¹ä½œä¸ºç›®æ ‡ã€‚ | åŒ…å«2è½®ã€3è½®å’Œâ€œæ¨¡æ‹Ÿâ€å¤šè½®æ•°æ®ï¼Œæ€»è®¡çº¦ **55,000** ä¸ªæ ·æœ¬ï¼ˆ5ä¸‡2è½® + 5åƒå¤šè½®ï¼‰ã€‚ â€œæ¨¡æ‹Ÿâ€å¤šè½®æ•°æ®ï¼šç”¨3è½®æ¨¡å‹å¯¹åŒä¸€æ•°æ®å¤šæ¬¡æ¨ç†ï¼Œå°†ç»“æœå †å è€Œæˆï¼Œç”¨äºæ¿€å‘æ¨¡å‹èƒ½åŠ›ã€‚ | **æ¿€æ´»å¤šè½®æ¨ç†èƒ½åŠ›**ã€‚ä½¿æ¨¡å‹å­¦ä¼šæ ¹æ®å†å²åé¦ˆï¼ˆæ–‡æœ¬å½¢å¼çš„NC/DAC/TTCé—®é¢˜æè¿°ï¼‰è¿­ä»£ä¼˜åŒ–è½¨è¿¹ã€‚ |\n| **PDMç†è§£æ•°æ® (PDM Understanding)** | é’ˆå¯¹æ¯ä¸ªPDMæŒ‡æ ‡ï¼ˆNC, DAC, TTCï¼‰æ‰‹å·¥æ„å»ºçš„**é—®ç­”å¯¹**ã€‚ | çº¦ **80,000** ä¸ªæ ·æœ¬ã€‚ åŒ…å«æ­£ä¾‹å’Œåä¾‹ã€‚ä¾‹å¦‚ï¼Œç»™å®šä¸€ä¸ªè½¨è¿¹ç‚¹åæ ‡å’Œå‰è§†å›¾åƒï¼Œåˆ¤æ–­è¯¥ç‚¹æ˜¯å¦åœ¨å¯è¡Œé©¶åŒºåŸŸå†…ï¼ˆDACï¼‰ã€‚ | **æå‡åé¦ˆç†è§£èƒ½åŠ›**ã€‚å°†PDM Agentè¾“å‡ºçš„ç»“æ„åŒ–æŒ‡æ ‡ï¼ˆå¦‚åæ ‡ã€è¾¹ç•Œæ¡†ï¼‰è½¬åŒ–ä¸ºæ¨¡å‹èƒ½ç²¾ç¡®ç†è§£çš„è¯­ä¹‰ï¼Œæ˜¯æœ‰æ•ˆå¤šè½®äº¤äº’çš„**å…³é”®å‰æ**ã€‚ |\n| **RLè®­ç»ƒæ•°æ®** | ä»NAVSIMè®­ç»ƒé›†ä¸­ç­›é€‰ã€‚ 1. **2è½®æ•°æ®**ï¼šç¬¬ä¸€è½®æ¨ç†åPDMåé¦ˆéç©ºã€‚ 2. **ä½åˆ†æ•°æ®**ï¼šç¬¬ä¸€è½®æ¨ç†åPDMS < 0.8ã€‚ 3. **å…¶ä»–æ•°æ®**ï¼šéšæœºé‡‡æ ·ä»¥å¹³è¡¡åˆ†å¸ƒã€‚ | å…± **13,000** ä¸ªæ ·æœ¬ï¼ˆè®ºæ–‡ä¸»è¦å®éªŒè§„æ¨¡ï¼‰ã€‚ åŒ…å«æ‰€æœ‰ç¬¬1ã€2ç±»æ•°æ®ï¼Œå¹¶é‡‡æ ·ç¬¬3ç±»æ•°æ®ã€‚ æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ­¤è§„æ¨¡è¶³ä»¥æœ‰æ•ˆè®­ç»ƒï¼Œæ›´å¤§è§„æ¨¡ï¼ˆ26k, 39kï¼‰æ”¶ç›Šé€’å‡ã€‚ | **é«˜æ•ˆRLè®­ç»ƒ**ã€‚ä¸“æ³¨äº**æœ‰æ”¹è¿›ç©ºé—´**å’Œ**å·²å‡ºç°é—®é¢˜**çš„åœºæ™¯ï¼Œæå‡è®­ç»ƒæ ·æœ¬çš„â€œä»·å€¼å¯†åº¦â€ï¼ŒåŠ é€Ÿç­–ç•¥ä¼˜åŒ–ã€‚ |\n\n### ä¸‰ã€ è¯„ä¼°è®¾ç½®ä¸åŸºå‡†ç»“æœ\n\nè®ºæ–‡åœ¨**NAVSIMåŸºå‡†**ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶åŒºåˆ†äº†ä¸¤ç§æ„ŸçŸ¥è¾“å…¥è®¾ç½®ï¼Œä»¥ä½“ç°æ–¹æ³•çš„é²æ£’æ€§ï¼š\n\n| è¯„ä¼°è®¾ç½® | æ„ŸçŸ¥è¾“å…¥è¯´æ˜ | å¯¹åº”ç»“æœæ ‡è¯† | ä½“ç°çš„ä»·å€¼ |\n| :--- | :--- | :--- | :--- |\n| **è¿åŠ¨å­¦æ¨¡å‹** | **æ›´è´´è¿‘å®é™…éƒ¨ç½²**ã€‚å‡è®¾å‘¨å›´è½¦è¾†ä»¥æ’å®šé€Ÿåº¦è¿åŠ¨ï¼Œä½¿ç”¨å½“å‰å¸§æ„ŸçŸ¥å¹¶é€šè¿‡è¿åŠ¨å­¦æ¨¡å‹é¢„æµ‹æœªæ¥çŠ¶æ€ã€‚ | `MTDrive*` | è¯æ˜äº†åœ¨**ä¸ä¾èµ–å®Œç¾æ„ŸçŸ¥**çš„å®ç”¨æ¡ä»¶ä¸‹ï¼Œå¤šè½®æ¨ç†æ¡†æ¶ä¾ç„¶æœ‰æ•ˆï¼ˆPDMS: 91.1ï¼‰ã€‚ |\n| **GT Oracle** | **è§„åˆ’èƒ½åŠ›ä¸Šé™è¯„ä¼°**ã€‚ä½¿ç”¨**ç‰¹æƒä¿¡æ¯**ï¼ˆçœŸå®æœªæ¥éšœç¢ç‰©çŠ¶æ€ï¼‰ã€‚é€‚ç”¨äºè‡ªåŠ¨æ ‡æ³¨ç­‰åœºæ™¯ã€‚ | `MTDrive**` | å±•ç¤ºäº†æ–¹æ³•**çº¯ç²¹çš„è§„åˆ’ä¸æ¨ç†èƒ½åŠ›**ï¼Œåœ¨NAVSIMä¸Šå–å¾—äº†SOTAç»“æœï¼ˆPDMS: 96.2ï¼‰ï¼Œç”šè‡³è¶…è¿‡äº†äººç±»é©¾é©¶å‘˜åŸºå‡†ï¼ˆ94.8ï¼‰ã€‚ |\n\n**æ ¸å¿ƒç»“è®º**ï¼šMTDriveé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„**å¤šè½®äº¤äº’æ•°æ®é›†**å’Œ**PDMç†è§£æ•°æ®**ï¼Œè§£å†³äº†æ¨¡å‹â€œå¬æ‡‚â€åé¦ˆçš„é—®é¢˜ï¼›å†åˆ©ç”¨åŸºäº**NCã€DACã€TTC**ç­‰å®‰å…¨æŒ‡æ ‡çš„**å¤šè½®RLä¼˜åŒ–ï¼ˆmtGRPOï¼‰**ï¼Œä½¿æ¨¡å‹å­¦ä¼šâ€œæ‰§è¡Œâ€ä¿®æ­£ã€‚è¿™å¥—ç»„åˆæ‹³åœ¨æ ‡å‡†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨å®ç”¨åŒ–è®¾ç½®ä¸‹çš„æ½œåŠ›ã€‚",
            "timestamp": "2026-02-03T15:58:14.100599",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡åœ¨å“ªäº›æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Ÿä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡å’Œæ€§èƒ½æå‡æ˜¯å¤šå°‘ï¼Ÿ",
            "answer": "## è®ºæ–‡å®éªŒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡ä¸æ€§èƒ½æå‡åˆ†æ\n\n### ä¸€ã€ å®éªŒæ•°æ®é›†\nè®ºæ–‡ä¸»è¦åœ¨ä»¥ä¸‹æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼š\n1.  **æ ¸å¿ƒè®­ç»ƒä¸è¯„ä¼°æ•°æ®é›†ï¼šNAVSIM**\n    -   **æ¥æº**ï¼šä¸€ä¸ªæ•°æ®é©±åŠ¨çš„ã€éååº”å¼è‡ªåŠ¨é©¾é©¶ä»¿çœŸä¸åŸºå‡†æµ‹è¯•å¹³å°ã€‚\n    -   **ç”¨é€”**ï¼š\n        -   **è®­ç»ƒæ•°æ®æ¥æº**ï¼šä»NAVSIMçš„é—­ç¯ä»¿çœŸä¸­æ„å»º**äº¤äº’å¼è½¨è¿¹ç†è§£æ•°æ®é›†**ï¼Œç”¨äºSFTå’ŒRLè®­ç»ƒã€‚\n        -   **è¯„ä¼°åŸºå‡†**ï¼šåœ¨NAVSIMåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œä¸»è¦æ€§èƒ½è¯„ä¼°å’Œå¯¹æ¯”ã€‚\n\n2.  **è¾…åŠ©è®­ç»ƒæ•°æ®é›†ï¼šRecogDrive**\n    -   **æ¥æº**ï¼šåŸºäºNAVSIMæ„å»ºçš„è½¨è¿¹æ•°æ®é›†ã€‚\n    -   **ç”¨é€”**ï¼šç”¨äºç”Ÿæˆ**å•å›åˆï¼ˆSingle-turnï¼‰** çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®ï¼Œä¸ºæ¨¡å‹æä¾›åŸºç¡€çš„è½¨è¿¹ç”Ÿæˆèƒ½åŠ›ã€‚\n\n### äºŒã€ ä¸»è¦è¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡é‡‡ç”¨NAVSIMåŸºå‡†å®šä¹‰çš„**PDM Score (PDMS)** ä½œä¸ºæ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ã€‚PDMSæ˜¯ä¸€ä¸ªç»¼åˆæŒ‡æ ‡ï¼Œå¹³è¡¡äº†å®‰å…¨æ€§ã€èˆ’é€‚æ€§å’Œä»»åŠ¡è¿›åº¦ã€‚\n\n**PDMSè®¡ç®—å…¬å¼**ï¼š\n`PDMS = (æƒ©ç½šé¡¹) Ã— (åŠ æƒå¹³å‡é¡¹)`\n-   **æƒ©ç½šé¡¹**ï¼š`No Collisions (NC)` å’Œ `Drivable Area Compliance (DAC)` å¾—åˆ†çš„ä¹˜ç§¯ã€‚ä»»ä½•ä¸€é¡¹ä¸ºé›¶å°†å¯¼è‡´æ€»åˆ†å½’é›¶ï¼Œå¼ºè°ƒå®‰å…¨åº•çº¿ã€‚\n-   **åŠ æƒå¹³å‡é¡¹**ï¼š`Ego Progress (EP)`ã€`Time-to-Collision (TTC)` å’Œ `Comfort (CF)` å¾—åˆ†çš„åŠ æƒå¹³å‡ã€‚\n\n**å…·ä½“æŒ‡æ ‡è¯´æ˜**ï¼š\n-   **NC (æ— ç¢°æ’)**ï¼šè¯„ä¼°è‡ªè½¦æ˜¯å¦ä¸å…¶ä»–äº¤é€šå‚ä¸è€…æˆ–ç‰©ä½“å‘ç”Ÿç¢°æ’ã€‚\n-   **DAC (å¯è¡Œé©¶åŒºåŸŸåˆè§„)**ï¼šè¯„ä¼°è‡ªè½¦æ˜¯å¦å§‹ç»ˆä¿æŒåœ¨æŒ‡å®šçš„å¯è¡Œé©¶åŒºåŸŸå†…ã€‚\n-   **TTC (ç¢°æ’æ—¶é—´)**ï¼šè¯„ä¼°è‡ªè½¦ä¸å‰è½¦ï¼ˆé€šå¸¸æ˜¯å¼•å¯¼è½¦ï¼‰æ˜¯å¦ä¿æŒäº†è¶³å¤Ÿçš„ç¢°æ’æ—¶é—´å®‰å…¨è£•åº¦ã€‚\n-   **EP (è‡ªè½¦è¿›åº¦)**ï¼šè¯„ä¼°ä»»åŠ¡å®Œæˆè¿›åº¦ã€‚\n-   **CF (èˆ’é€‚åº¦)**ï¼šè¯„ä¼°é©¾é©¶èˆ’é€‚æ€§ã€‚\n\n### ä¸‰ã€ æ€§èƒ½æå‡ä¸ä¸»è¦ç»“æœ\nè®ºæ–‡åœ¨ä¸¤ç§æ„ŸçŸ¥è®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ï¼Œä»¥å±•ç¤ºå…¶é²æ£’æ€§ï¼š\n1.  **è¿åŠ¨å­¦æ¨¡å‹è®¾ç½® (`*`)**ï¼šä½¿ç”¨æ’å®šé€Ÿåº¦æ¨¡å‹é¢„æµ‹å‘¨å›´æ™ºèƒ½ä½“çš„æœªæ¥çŠ¶æ€ï¼Œæ¨¡æ‹Ÿå®é™…éƒ¨ç½²åœºæ™¯ï¼ˆæ— ç‰¹æƒä¿¡æ¯ï¼‰ã€‚\n2.  **GT Oracle è®¾ç½® (`**`)**ï¼šä½¿ç”¨çœŸå®æœªæ¥æ™ºèƒ½ä½“çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œè¯„ä¼°è§„åˆ’èƒ½åŠ›çš„ç†è®ºä¸Šé™ã€‚\n\n**å…³é”®æ€§èƒ½æå‡å¯¹æ¯”å¦‚ä¸‹**ï¼š\n\n| æ–¹æ³• / è®¾ç½® | PDMS å¾—åˆ† | æ ¸å¿ƒæå‡ä¸å¯¹æ¯” |\n| :--- | :--- | :--- |\n| **å•å›åˆåŸºçº¿ (QwenVL2.5-8Bâ€ )** | 83.7 | **åŸºå‡†çº¿** |\n| **MTDrive (ä»…SFTï¼Œå¤šå›åˆ)** | 88.1 | **+4.4**ï¼Œè¯æ˜äº†å¤šå›åˆæ¨ç†èŒƒå¼çš„æœ‰æ•ˆæ€§ |\n| **MTDrive* (è¿åŠ¨å­¦æ¨¡å‹ + RL)** | 91.1 | åœ¨æ›´ç°å®çš„è®¾å®šä¸‹ï¼Œæ€§èƒ½ä¼˜äºå¤šæ•°ä¼ ç»Ÿç«¯åˆ°ç«¯æ–¹æ³• |\n| **MTDrive** (GT Oracle + RL)** | **96.2** | **+12.5 (ç›¸æ¯”åŸºçº¿)**ï¼Œ**ç”šè‡³è¶…è¶Šäº†äººç±»é©¾é©¶åŸºå‡† (94.8)** |\n\n**ç»“è®ºæ€§æå‡**ï¼š\n-   **æŠ€æœ¯åˆ›æ–°ä»·å€¼**ï¼šé€šè¿‡å¼•å…¥**å¤šå›åˆäº¤äº’å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ (MTDrive)** å’Œ **mtGRPOç®—æ³•**ï¼Œæœ‰æ•ˆè§£å†³äº†é•¿å°¾åœºæ™¯å’Œç¨€ç–å¥–åŠ±é—®é¢˜ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½é£è·ƒã€‚\n-   **å®é™…åº”ç”¨ä»·å€¼**ï¼šåœ¨æ— éœ€æä¾›çœŸå®è½¨è¿¹ä¿¡æ¯ï¼ˆEPæŒ‡æ ‡ï¼‰çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹é€šè¿‡RLå¥–åŠ±åé¦ˆè‡ªä¸»å­¦ä¹ å®ç°äº†é«˜è¿›åº¦å¾—åˆ†ï¼ˆ93.5ï¼‰ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„ä»äº¤äº’ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¶…è¶Šäººç±»æ°´å¹³çš„æ€§èƒ½è¡¨æ˜è¯¥æ¡†æ¶åœ¨**é«˜è´¨é‡è½¨è¿¹è‡ªåŠ¨æ ‡æ³¨**æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚",
            "timestamp": "2026-02-03T15:58:43.627350",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "è®ºæ–‡çš„ä¸»è¦å±€é™æ€§æœ‰å“ªäº›ï¼Ÿæœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "## è®ºæ–‡ä¸»è¦å±€é™æ€§åˆ†æ\n\næ ¹æ®è®ºæ–‡ç¬¬6èŠ‚ï¼ˆLimitations and Future Workï¼‰åŠå…¨æ–‡å†…å®¹ï¼ŒMTDriveæ¡†æ¶çš„ä¸»è¦å±€é™æ€§å¯å½’çº³ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š\n\n### ä¸»è¦å±€é™æ€§\n\n- **æ„ŸçŸ¥æ¨¡å—ä¾èµ–æ€§å¼º**\n  - å½“å‰æ¡†æ¶æ˜¯ä¸€ä¸ª**åŸºäºVLMçš„è§„åˆ’å™¨**ï¼Œå…¶**PDMåé¦ˆ**ä¾èµ–äº**å¤–éƒ¨æ„ŸçŸ¥æ¨¡å—**æä¾›çš„**åœ°é¢çœŸå€¼**æˆ–æ£€æµ‹ç»“æœï¼ˆå¦‚è½¦è¾†ã€è¡Œäººçš„3Dè¾¹ç•Œæ¡†ï¼‰ã€‚\n  - æ¨¡å‹è‡ªèº«**ä¸å…·å¤‡ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰ä¸­ç›´æ¥ç†è§£ç¯å¢ƒã€è¯†åˆ«éšœç¢ç‰©å¹¶ç”ŸæˆPDMåé¦ˆçš„èƒ½åŠ›**ã€‚è¿™é™åˆ¶äº†ç³»ç»Ÿçš„ç«¯åˆ°ç«¯è‡ªä¸»æ€§å’Œåœ¨æ„ŸçŸ¥ç»“æœä¸å¯é æ—¶çš„é²æ£’æ€§ã€‚\n\n- **æ•°æ®ç”Ÿæˆä¸ä»¿çœŸç¯å¢ƒç»‘å®š**\n  - è®­ç»ƒæ•°æ®ï¼ˆå°¤å…¶æ˜¯å¤šè½®äº¤äº’æ•°æ®ï¼‰ä¸¥é‡ä¾èµ–äº**NAVSIMä»¿çœŸå™¨**åŠå…¶å†…ç½®çš„**è§„åˆ™å‹PDMæ™ºèƒ½ä½“**æ¥æä¾›åé¦ˆã€‚\n  - è™½ç„¶è®ºæ–‡æŒ‡å‡ºæ¡†æ¶æ˜¯â€œæ™ºèƒ½ä½“æ— å…³çš„â€ï¼Œä½†å®é™…æ€§èƒ½**é«˜åº¦ä¾èµ–äºä»¿çœŸç¯å¢ƒæä¾›çš„åé¦ˆè´¨é‡å’ŒçœŸå®æ€§**ã€‚åœ¨æ›´å¤æ‚ã€éç»“æ„åŒ–çš„ç°å®ä¸–ç•Œåœºæ™¯ä¸­ï¼Œè§„åˆ™å‹æ™ºèƒ½ä½“çš„åé¦ˆå¯èƒ½ä¸å‡†ç¡®æˆ–ä¸å…¨é¢ã€‚\n\n- **è®¡ç®—ä¸å·¥ç¨‹å¤æ‚åº¦é«˜**\n  - å¤šè½®æ¨ç†ã€é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ä»¥åŠRLè®­ç»ƒæµç¨‹å¯¼è‡´äº†æ˜¾è‘—çš„**è®¡ç®—å¼€é”€å’Œæ•°æ®ä¼ è¾“ç“¶é¢ˆ**ã€‚\n  - å°½ç®¡é€šè¿‡IPSSå’ŒIPTCä¼˜åŒ–å®ç°äº†2.5å€ååé‡æå‡ï¼Œä½†æ•´ä¸ªæ¡†æ¶ï¼ˆSFT + RLï¼‰ä»éœ€åœ¨å¤§å‹GPUé›†ç¾¤ä¸Šè®­ç»ƒæ•°å¤©ï¼Œ**éƒ¨ç½²å’Œæ‰©å±•æˆæœ¬è¾ƒé«˜**ã€‚\n\n- **å¯¹â€œå®Œç¾åé¦ˆâ€çš„æ½œåœ¨è¿‡æ‹Ÿåˆ**\n  - åœ¨RLè®­ç»ƒä¸­ï¼Œæ¨¡å‹ä¾èµ–äºPDMæ™ºèƒ½ä½“æä¾›çš„ã€åŸºäºæ˜ç¡®è§„åˆ™çš„â€œå®Œç¾â€åé¦ˆã€‚è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹**è¿‡åº¦é€‚åº”è¿™ç§ç†æƒ³åŒ–çš„åé¦ˆæœºåˆ¶**ï¼Œè€Œåœ¨ç°å®ä¸–ç•Œä¸­é¢å¯¹å™ªå£°æ„ŸçŸ¥ã€ä¸ç¡®å®šè¡Œä¸ºæˆ–å…¶ä»–æ™ºèƒ½ä½“æ—¶æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚\n\n---\n\n## æœªæ¥å¯èƒ½çš„æ”¹è¿›æ–¹å‘\n\nè®ºæ–‡æ˜ç¡®æå‡ºäº†ä»¥ä¸‹å‡ ä¸ªæœªæ¥å·¥ä½œæ–¹å‘ï¼š\n\n### 1. æ„ŸçŸ¥ä¸è§„åˆ’çš„æ·±åº¦é›†æˆ\n- **æ–¹å‘**ï¼šå°†**æ„ŸçŸ¥ä»»åŠ¡ç›´æ¥é›†æˆåˆ°VLMä¸­**ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºè‡ªèº«çš„å¤šæ¨¡æ€ç†è§£ï¼ˆå¦‚å›¾åƒã€æ¿€å…‰é›·è¾¾ç‚¹äº‘ï¼‰æ¥**ç”ŸæˆPDMåé¦ˆ**ï¼Œè€Œæ— éœ€ä¾èµ–å¤–éƒ¨æ„ŸçŸ¥æ¨¡å—ã€‚\n- **ä»·å€¼**ï¼šå®ç°çœŸæ­£çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œæå‡åœ¨æ„ŸçŸ¥ç»“æœä¸å®Œç¾æ—¶çš„é²æ£’æ€§ï¼Œå¹¶å¯èƒ½é€šè¿‡è”åˆä¼˜åŒ–æ„ŸçŸ¥ä¸è§„åˆ’æ¥æå‡æ•´ä½“æ€§èƒ½ã€‚\n\n### 2. åº”ç”¨äºé«˜è´¨é‡è½¨è¿¹è‡ªåŠ¨æ ‡æ³¨\n- **æ–¹å‘**ï¼šåˆ©ç”¨MTDriveåœ¨å¤šè½®æ¨ç†å**è¶…è¶Šäººç±»é©¾é©¶å‘˜**çš„è§„åˆ’æŒ‡æ ‡ï¼ˆPDMS 96.2 > äººç±» 94.8ï¼‰ï¼Œå°†å…¶ä½œä¸º**è‡ªåŠ¨æ ‡æ³¨å·¥å…·**ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è½¨è¿¹æ•°æ®ã€‚\n- **ä»·å€¼**ï¼šè§£å†³è‡ªåŠ¨é©¾é©¶æ¨¡ä»¿å­¦ä¹ ä¸­**é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®ç¨€ç¼º**çš„é—®é¢˜ï¼Œé™ä½å¯¹æ˜‚è´µäººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼ŒåŠ é€Ÿæ•°æ®é›†çš„æ„å»ºã€‚\n\n### 3. æ¡†æ¶æ³›åŒ–ä¸æ™ºèƒ½ä½“å¤šæ ·åŒ–\n- **æ–¹å‘**ï¼šå°†å¤šè½®äº¤äº’èŒƒå¼**æ¨å¹¿åˆ°å…¶ä»–ä»¿çœŸç¯å¢ƒ**ï¼ˆå¦‚CARLA, Waymax, AlpaSimï¼‰å’Œ**ä¸åŒç±»å‹çš„åé¦ˆæ™ºèƒ½ä½“**ã€‚\n  - æ¢ç´¢ä½¿ç”¨**å­¦ä¹ å‹å¥–åŠ±æ¨¡å‹**æ›¿ä»£è§„åˆ™å‹PDMæ™ºèƒ½ä½“ã€‚\n  - é€‚é…æ›´å¤æ‚çš„äº¤é€šå‚ä¸è€…å’ŒåŠ¨æ€åœºæ™¯ã€‚\n- **ä»·å€¼**ï¼šéªŒè¯æ¡†æ¶çš„é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¯„ä¼°æ ‡å‡†å’Œç¯å¢ƒï¼Œæå‡ç ”ç©¶çš„å¹¿åº¦å’Œå®ç”¨ä»·å€¼ã€‚\n\n### 4. ç®—æ³•ä¸ç³»ç»Ÿçš„è¿›ä¸€æ­¥ä¼˜åŒ–\n- **éšå«æ–¹å‘**ï¼ˆå¯ä»æŒ‘æˆ˜ä¸­æ¨å¯¼ï¼‰ï¼š\n  - **å¥–åŠ±è®¾è®¡**ï¼šæ¢ç´¢æ›´ç²¾ç»†ã€æ›´ç¨ å¯†çš„å¥–åŠ±ä¿¡å·è®¾è®¡ï¼Œä»¥ç¼“è§£å¤šè½®åœºæ™¯ä¸‹æ›´å¤æ‚çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚\n  - **è®­ç»ƒæ•ˆç‡**ï¼šç»§ç»­ä¼˜åŒ–å¤šæ¨¡æ€å¤šè½®RLçš„è®­ç»ƒç³»ç»Ÿï¼Œé™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼Œä½¿ç ”ç©¶æ›´æ˜“äºå¤ç°å’Œæ‰©å±•ã€‚\n  - **å®‰å…¨æ€§ä¸å¯è§£é‡Šæ€§**ï¼šå¢å¼ºå¤šè½®æ¨ç†è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼Œç¡®ä¿æ¯è½®ä¿®æ­£çš„å†³ç­–ä¾æ®æ¸…æ™°ï¼Œè¿™å¯¹äºå®‰å…¨å…³é”®çš„ç³»ç»Ÿè‡³å…³é‡è¦ã€‚\n\n### æ€»ç»“\nMTDriveçš„æ ¸å¿ƒå±€é™æ€§åœ¨äºå…¶**å¯¹ä»¿çœŸç¯å¢ƒå’Œå¤–éƒ¨æ„ŸçŸ¥çš„ä¾èµ–**ï¼Œä»¥åŠéšä¹‹è€Œæ¥çš„**å·¥ç¨‹å¤æ‚æ€§**ã€‚æœªæ¥çš„æ”¹è¿›å°†å›´ç»•**æ„å»ºæ›´è‡ªä¸»ã€æ›´é€šç”¨ã€æ›´é«˜æ•ˆçš„ç«¯åˆ°ç«¯ç³»ç»Ÿ**å±•å¼€ï¼Œç‰¹åˆ«æ˜¯å‘**æ„ŸçŸ¥-è§„åˆ’ä¸€ä½“åŒ–**å’Œ**é«˜è´¨é‡æ•°æ®ç”Ÿæˆ**è¿™ä¸¤ä¸ªæå…·å®é™…ä»·å€¼çš„æ–¹å‘è¿ˆè¿›ã€‚",
            "timestamp": "2026-02-03T15:59:17.282207",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-30T12:47:55Z",
    "created_at": "2026-02-03T15:53:08.382523",
    "updated_at": "2026-02-03T15:53:08.382531"
}