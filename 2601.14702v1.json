{
    "id": "2601.14702v1",
    "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving",
    "authors": [
        "Zecong Tang",
        "Zixu Wang",
        "Yifei Wang",
        "Weitong Lian",
        "Tianjian Gao",
        "Haoran Li",
        "Tengju Ru",
        "Lingyi Meng",
        "Zhejun Cui",
        "Yichen Zhu",
        "Qi Kang",
        "Kaixuan Wang",
        "Yu Zhang"
    ],
    "abstract": "自动驾驶是一个极具挑战性的领域，需要在复杂场景中实现可靠的感知与安全的决策。近期涌现的视觉语言模型展现出推理与泛化能力，为自动驾驶开辟了新的可能性；然而现有基准测试与评估指标过度强调感知能力，未能充分评估决策过程。本研究提出AutoDriDM——一个以决策为核心、包含三个维度（物体、场景与决策）共6,650道问题的渐进式基准测试体系。我们通过评估主流视觉语言模型，界定了自动驾驶中从感知到决策的能力边界，相关性分析揭示了感知性能与决策性能之间的弱关联性。进一步通过模型推理过程的可解释性分析，识别出逻辑推理错误等关键失效模式，并引入分析器模型实现大规模自动化标注。AutoDriDM弥合了以感知为中心和以决策为中心的评估体系之间的鸿沟，为构建更安全可靠的现实世界自动驾驶视觉语言模型提供指引。",
    "url": "https://arxiv.org/abs/2601.14702v1",
    "html_url": "https://arxiv.org/html/2601.14702v1",
    "html_content": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving\nZecong Tang\n1,∗\nZixu Wang\n1,∗\nYifei Wang\n1,∗\nWeitong Lian\n1,∗\nTianjian Gao\n1\nHaoran Li\n1\nTengju Ru\n1\nLingyi Meng\n1\nZhejun Cui\n1\nYichen Zhu\n1\nQi Kang\n1\nKaixuan Wang\n2\nYu Zhang\n1,†\n1\nZhejiang University, Hangzhou, China\n2\nThe University of Hong Kong, Hong Kong, China\n∗\nEqual contribution\n†\nCorresponding author\nAbstract\nAutonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present\nAutoDriDM\n, a decision-centric, progressive benchmark with 6,650 questions across three dimensions—Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models’ reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.\nAutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving\nZecong Tang\n1,∗\nZixu Wang\n1,∗\nYifei Wang\n1,∗\nWeitong Lian\n1,∗\nTianjian Gao\n1\nHaoran Li\n1\nTengju Ru\n1\nLingyi Meng\n1\nZhejun Cui\n1\nYichen Zhu\n1\nQi Kang\n1\nKaixuan Wang\n2\nYu Zhang\n1,†\n1\nZhejiang University, Hangzhou, China\n2\nThe University of Hong Kong, Hong Kong, China\n∗\nEqual contribution\n†\nCorresponding author\nFigure 1:\nOverview of AutoDriDM. The framework is organized into three stages: (Left) Data Source & Filtering, (Middle) Progressive Evaluation Protocol, and (Right) Explainability & Automated Analysis.\n1\nIntroduction\nAutonomous driving is a highly challenging domain operating under diverse and safety-critical conditions. Research has progressed along two main paradigms. The first, a modular pipeline, separates perception, prediction/planning, and control\n(Schwarting\net al.\n,\n2018\n; Badue\net al.\n,\n2021\n; Paden\net al.\n,\n2016\n)\n. The second, end-to-end learning, maps raw sensor inputs directly to waypoints or control commands\n(Tampuu\net al.\n,\n2022\n; Chen\net al.\n,\n2024a\n; Codevilla\net al.\n,\n2018\n)\n. While both approaches have achieved notable success, modular systems remain brittle due to error propagation, whereas end-to-end methods often lack explainability and robust reasoning about causal structure\n(Badue\net al.\n,\n2021\n; Chib and Singh,\n2023\n)\n.\nThe development of large language models (LLMs) has enabled significant advances in instruction following and reasoning\n(OpenAI\net al.\n,\n2024\n; Touvron\net al.\n,\n2023\n; DeepSeek-AI\net al.\n,\n2025\n)\n. In parallel, vision–language models (VLMs) have emerged with capabilities in zero-shot transfer and language-based reasoning\n(Zhang\net al.\n,\n2024\n; Shen\net al.\n,\n2024\n; Lu\net al.\n,\n2025\n)\n. These properties highlight the potential of VLMs for autonomous driving; domain-specific VLM-based systems for autonomous driving have been proposed\n(Zhou\net al.\n,\n2024b\n; Tian\net al.\n,\n2024\n; Fu\net al.\n,\n2024b\n; You\net al.\n,\n2025\n)\n, demonstrating increasing research activity and diverse approaches.\nDespite these advancements, current approaches face several limitations. Benchmarks serve as a bridge between methodological advances and practical deployment by revealing model failures. First, existing VLM training primarily focuses on perception, overlooking the decision-making requirements of driving scenarios\n(Zhou\net al.\n,\n2024a\n)\n, which is a core component for fully autonomous driving. Second, many benchmarks define independent metrics that decouple perception from the decisions they should support, limiting the assessment of models’ true capability boundaries\n(Xie\net al.\n,\n2025\n)\n. Third, evaluations often consider only final answers, making reasoning error a black box, which constrains analysis of performance gaps\n(Li\net al.\n,\n2025\n; Guo\net al.\n,\n2024\n)\n.\nTo address these limitations, we introduce AutoDriDM (Figure\n1\n), a decision-centric benchmark for evaluating VLMs in autonomous driving. First, AutoDriDM is constructed from nuScenes, KITTI, and BDD100K and follows a three-level protocol with increasing task complexity — Object, Scene, and Decision. The benchmark comprises 6.65K questions across six metrics, with evaluations stratified by scenario risk. To further assess robustness, we curate 60 pairs of visually similar scenarios. Second, we evaluate mainstream open- and closed-source VLMs to characterize their capability boundaries under zero- and few-shot settings, and quantify cross-level dependencies between perception and decision-making through correlation analysis. Moreover, we investigate the scaling behavior of the Internvl models, identifying key inflection points. Third, we collect and annotate reasoning traces from multiple VLMs to conduct systematic analyses of their reasoning and failure patterns. Based on this analysis, we fine-tune a lightweight analyzer model to automatically tag new traces, enabling scalable evaluation and providing insights for model optimization.\nOur work makes three key contributions:\n•\nWe introduce AutoDriDM, a decision-centric benchmark with a progressive three-level protocol—Object, Scene, and Decision—covering 6.65K questions with six evaluation metrics and risk-aware splits.\n•\nWe evaluate mainstream VLMs, analyze perception–decision dependencies, study robustness on visually similar scenarios, and investigate the scaling behavior of Internvl models to identify key inflection points.\n•\nWe collect and annotate reasoning traces from VLMs, analyze these traces to reveal reasoning and failure patterns, and fine-tune a model to automate reasoning trace labeling, enabling large-scale explainability.\n2\nRelated Work\n2.1\nVLMs for Autonomous Driving\nRecent advances in LLMs have accelerated the development of VLMs\nOpenAI\net al.\n(\n2024\n); Touvron\net al.\n(\n2023\n); DeepSeek-AI\net al.\n(\n2025\n); Zhao\net al.\n(\n2023\n)\n.\nVLMs leverage large-scale image–text pretraining and instruction tuning, demonstrating robust instruction-following, zero-shot generalization, and contextual reasoning capabilities\nLi\net al.\n(\n2023\n); Wang\net al.\n(\n2024a\n); Lu\net al.\n(\n2025\n)\n.\nThey have been extensively investigated in autonomous driving for scene understanding, interactive reasoning, and end-to-end planning\nZhou\net al.\n(\n2024b\n); Tian\net al.\n(\n2024\n); Xu\net al.\n(\n2024\n); Sima\net al.\n(\n2025\n); You\net al.\n(\n2025\n); Fu\net al.\n(\n2024b\n); Gopalkrishnan\net al.\n(\n2024\n)\n.\nHowever, current VLMs still suffer from hallucinations, brittle reasoning, and omissions of critical visual cues in complex scenes\nGuan\net al.\n(\n2024\n); Yu\net al.\n(\n2024\n); Bai\net al.\n(\n2025\n)\n.\nFigure 2:\nThe word cloud about autonomous driving collected from our QA pairs in AutoDriDM.\n2.2\nBenchmarks for LLMs/VLMs\nFor LLMs, mainstream benchmark suites evaluate knowledge, reasoning, and math/coding abilities using more challenging protocols and adversarial design\nWang\net al.\n(\n2024b\n); Rein\net al.\n(\n2024\n); Sakaguchi\net al.\n(\n2021\n); Clark\net al.\n(\n2018\n); Chen\net al.\n(\n2021\n); Patel\net al.\n(\n2024\n)\n.\nFor VLMs, general-purpose evaluations aim to assess comprehensive capabilities across perception, grounding, OCR/text reasoning, and hallucination robustness\nFu\net al.\n(\n2024a\n); Liu\net al.\n(\n2024a\n); Yu\net al.\n(\n2024\n); Yue\net al.\n(\n2024\n,\n2025\n); Guan\net al.\n(\n2024\n); Liu\net al.\n(\n2024b\n); Tang\net al.\n(\n2025\n); Mathew\net al.\n(\n2021\n); Zhang\net al.\n(\n2025\n)\n.\nIn autonomous driving, recent VLM-oriented benchmarks provide driving scenes and tasks for perception–planning evaluation\nGuo\net al.\n(\n2024\n); Li\net al.\n(\n2024\n,\n2025\n); Sima\net al.\n(\n2025\n)\n; however, most employ independent metrics, with insufficient analysis of model reasoning traces.\n3\nAutoDriDM\nThis section introduces our comprehensive decision-making–centered benchmark framework for evaluating VLMs in autonomous driving.\n3.1\nData Source and Filtering\nData source.\nWe construct our benchmark from three widely used driving datasets—nuScenes, KITTI, and BDD100K\nCaesar\net al.\n(\n2020\n); Geiger\net al.\n(\n2013\n); Yu\net al.\n(\n2020\n)\n—focusing on front-facing images. The integration of these datasets spans multiple regions and diverse driving environments. This diversity mitigates dataset-specific bias and enhances the coverage of real-world driving conditions. Since we attach multiple task-specific questions to the same driving scene, a single image may correspond to multiple questions across different tasks; Appendix\nA.1\nsummarizes the dataset profiles (regions, scene types, and task coverage) together with the image / QA usage statistics.\nData filtering.\nFor quality control, we employ a similarity-based filtering procedure. We compute image embeddings using a pretrained ResNet-50\nHe\net al.\n(\n2016\n)\nwith global average pooled representations and construct candidate pairs based on cosine similarity. Pairs exceeding a similarity threshold of 0.9 are treated as near-duplicate cases and removed to reduce redundancy.\n3.2\nBenchmark Construction\nWe adopt a three-level protocol, comprising Object, Scene, and Decision, which is progressively structured rather than independent: perception supports scene understanding, and both stages jointly inform decision-making.\nOverview.\nWe define two metrics at each level, thereby establishing an Object–Scene–Decision evaluation pipeline. All metrics are selection questions: Object and Decision are single-choice, while Scene is multiple-choice. The benchmark contains\n6.65K\nquestions covering the three levels. Figure\n2\nshows a word cloud for our benchmark. Detailed annotation templates for all six tasks are provided in Appendix\nC.5\n.\nObject level\nevaluates perception: (Object-1) identify the object in the image that most influences the decision; (Object-2) determine the state of a designated object.\nScene level\naims to evaluate scene-level understanding: (Scene-1) recognize weather and illumination; (Scene-2) identify special scene factors that potentially influence driving decisions (e.g., roadworks, accidents).\nDecision level\naims to evaluate decision-making capability: (Decision-1) select the optimal action for the ego vehicle in the scenario; (Decision-2) evaluate the safety of specified and potentially suboptimal actions. Both tasks are informed by critical perceptual and contextual signals from Object-1/2 and Scene-1/2, providing the basis and constraints for decision-making. We summarize the design intentions of the six tasks in Appendix\nA.2\n.\nAnnotation protocol.\nTo ensure annotation accuracy, all QA pairs are manually annotated by human annotators. To ensure annotation consistency, each item is independently labeled by two experienced drivers. In cases of disagreement, a third human arbitrator adjudicates, and the final annotation is assigned accordingly.\nHigh-risk scenario design.\nDue to the scarcity of high-risk scenarios, collecting sufficient data for these cases can be challenging and may limit the pretraining of VLMs. However, these scenarios are critical for safe driving, and thus we evaluate them to assess the generalization capability of VLMs. Each scenario is rated by two expert annotators using a five-point risk scale (1: minimal, 5: severe, see Appendix\nA.3\n). The average score is used as the final rating, and scenarios with a score of 4 or higher are designated as high-risk scenarios, resulting in a total of\n1.6K\nquestions for analysis.\nSimilar-scene robustness.\nHumans can identify task-relevant information and ignore redundant cues, enabling accurate decision-making in visually similar scenarios. To test whether models exhibit this fine-grained capability, we construct\n60\nnear-duplicate pairs based on similarity filtering in Sec. 3.1. We evaluate model performance on Decision-1 using both the single-image accuracy and the joint accuracy (the probability that both images in a pair are answered correctly). Comparison with the squared baseline of individual accuracies evaluates whether decisions reflect causal understanding rather than superficial feature associations, thereby assessing VLMs’ robustness in causal reasoning. Representative near-duplicate examples are provided in Appendix\nA.4\n.\nCorrelation analysis.\nBenefiting from our three-level design (Object, Scene, and Decision), which establishes a progressive rather than independent structure, we explore the internal consistency of model behavior across perception and decision dimensions by computing pairwise Pearson correlations among all task scores for each model. This analysis reveals whether enhancements in one capability co-occur with gains or trade-offs in others. The resulting correlation matrices provide evidence for the multimodal reasoning across cognitive stages within VLMs.\nFigure 3:\nGPT-4.1 Failure Cases. The six subfigures illustrate GPT-4.1 failure cases across different tasks. Each subfigure consists of the given image, the question, the model’s answer, the ground truth, and the model’s explanation.\nTable 1:\nEvaluation on All Scenarios (0-shot)\nModel\nSize\nOpen\nObject-1\nObject-2\nScene-1\nScene-2\nDecision-1\nDecision-2\nAvg Score\nGemini-2.5Pro\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n62.90\n74.35\n76.73\n25.61\n49.47\n55.11\n57.36\nGPT-4.1\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n67.48\n73.74\n92.54\n46.44\n56.95\n51.91\n64.84\nGemma\n4B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n49.86\n50.07\n27.34\n0\n0.89\n45.78\n30.66\n34.10\nGemma\n27B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n64.46\n63.55\n63.45\n0\n7.20\n50.40\n31.65\n46.78\nLlama\n11B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n58.67\n53.45\n87.00\n18.37\n41.84\n37.99\n49.55\nLlama\n90B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n51.39\n63.48\n91.75\n36.78\n45.14\n49.55\n56.35\nLlava\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n26.23\n52.95\n87.15\n32.49\n41.40\n29.87\n45.01\nLlava\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n45.88\n66.72\n85.66\n35.42\n51.60\n34.89\n53.36\nPhi\n6B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n60.23\n63.10\n91.01\n21.67\n44.58\n30.72\n51.89\nQwen\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n65.58\n59.34\n65.16\n33.60\n40.87\n35.48\n50.00\nQwen\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n68.06\n68.24\n77.19\n37.62\n54.69\n38.20\n57.33\n3.3\nExplainability Settings\nWe analyze the reasoning process of VLMs by classifying errors and fine-tune a lightweight model to scale up analysis for automated evaluation.\nModel reasoning.\nChain-of-thought (CoT) reasoning has been shown to improve problem-solving capability\n(Wei\net al.\n,\n2022\n)\nand reveal intermediate dependencies that determine the final choice. To leverage these advantages, we instruct the model to generate a structured CoT rationale enclosed within\n<think>…</think>\n, followed by the decision enclosed within\n<answer>…</answer>\nin a single-turn interaction. This configuration facilitates fine-grained analysis across the Object, Scene, and Decision levels, and allows a more precise assessment of the model’s upper-bound capability.\nError categories.\nWe define nine error categories (E1–E9) to characterize failures across the decision-making process, following the taxonomy in Appendix\nB.1\n. The three primary categories are (E1) Logical Reasoning Error (the reasoning chain violates causality), (E2) Semantic Feature Omission (the model overlooks or misjudges semantic/visual cues such as turn signals), and (E3) Model Hallucination (the model invents non-existent objects, attributes, or relations). The remaining categories are (E4) Modality Imbalance, (E5) Spatial Relation Misjudgment, (E6) Limited Logical Inference, (E7) Generalization Deficit, (E8) Decision Boundary Instability, and (E9) Target Priority Misjudgment.\nAutomated reasoning-mode tagging.\nAnalyzing the explainability process manually is resource-intensive and constrains scalability. To enable automated large-scale evaluation, we train a lightweight analyzer model with 7B parameters that identifies error modes. The model takes as input: (1) the image, question, options, and ground-truth answer; (2) the tested model’s reasoning and final answer. It outputs one or more error-mode labels from the nine defined error categories, or a no-error label otherwise. This approach enables scalable error-type identification, and supports systematic evaluation of model reasoning. The analyzer input/output format and the prompt templates are provided in Appendix\nB.2\n.\n4\nExperiments\n4.1\nAll Scenarios Results (0-shot)\nWe analyzed the overall results to evaluate the general capability strengths. GPT-4.1 ranks first in overall performance, while among open-source models, Qwen (72B) yields the highest average performance. Although closed-source models continue to demonstrate superior generalization and stability, recent open-source systems show growing competitiveness. Representative failure cases of GPT-4.1 are shown in Figure\n3\n. Complete 0-shot results are presented in Table\n1\n. Experimental setup is shown in Appendix\nC.1\n.\nThe results exhibit a clear stratification: larger-scale models generally achieve higher accuracy on Object-1/2 and perform more consistently on Scene-1, while models under 10B parameters exhibit higher performance variance—some achieving high Scene-1 scores but inconsistent results across other tasks—highlighting the importance of model scale for multi-task stability. On Scene-2, most models exhibit substantially lower accuracy, suggesting that current VLMs struggle with robust scene understanding and often fail to accurately interpret unrepresentative environments that should constrain downstream decisions. For Decision-1 and Decision-2, most VLMs still lack the stability observed in perception tasks and exhibit high variability under ambiguous conditions, indicating that current models remain inadequate for complex driving-scene decision-making.\n4.2\nHigh-Risk Scenario Results (0-shot)\nWe further analyze high-risk scenarios under the 0-shot setting. Although high-risk scenarios are relatively scarce and may limit VLM pretraining, the average scores are higher than those on all scenarios, mainly because leading models improve substantially on decision tasks. Meanwhile, compared with all scenarios, object-level recognition tends to improve under high-risk conditions, but Scene-2 remains a significant challenge across models. Complete 0-shot high-risk results are reported in Appendix\nD.1\n.\nRelative to all scenarios, high-risk scenarios further widen the performance gap between large and small models. This is primarily because hazardous scenes impose a more challenging test of the model’s generalization. Large models exhibit more robust decision-making: salient objects are easier to identify, and decisions focus on common safety strategies. In contrast, small models fail to exploit these scenario-specific advantages and show limited cross-task integration. Although large models improve under high-risk conditions, current capabilities remain insufficient for direct deployment in safety-critical settings.\n4.3\nFew-Shot Learning Results\nWe analyze the impact of few-shot prompting on overall performance and individual task dimensions. Considering average performance, some models exhibit small improvements, whereas others exhibit performance decline or a non-monotonic trend, with initial declines at 1-shot followed by partial recovery at higher shot counts. Performance gains are predominantly observed in larger models, while smaller models generally show a performance decline or variability. At the task level, certain tasks maintain relatively high performance, whereas tasks requiring cross-granularity integration—such as Scene-2 and Decision-2—tend to lead to more errors and may result in substantial performance degradation. An overview of all-scenario / high-risk trends under 0/1/2/5-shot prompting is shown in Appendix\nD.2\n(Figures\n9\nand\n10\n); comprehensive per-shot results are summarized in Appendix Table\n8\nand\n9\n.\nFew-shot prompting produces different outcomes across models, indicating that decision-making under uncertainty may not consistently benefit from examples. Theoretically, a small number of examples can provide attention anchors and decision boundaries, potentially enabling modest gains for models. However, when alignment is fragile or prior biases are strong, few-shot examples may lead to cross-task instability or performance declines. Using Decision-2 as an example, few-shot prompting can bias the decision distribution toward examples priors. Compared with 0-shot prompting, selection of representative examples is critical for enhancing cross-task capabilities.\n4.4\nPerception–Decision Correlation Analysis\nTo investigate the internal consistency of model behavior across levels, we computed Pearson correlation coefficients between Object (O), Scene (S), and Decision (D) dimensions under both all and high-risk scenarios. Figure\n4\nillustrates the correlation matrices for Qwen-7B and -72B models. Extended correlation matrices covering all prompt settings and scenario types are provided in Appendix\nD.3\n(Figure\n15\n).\nCorrelations across tasks are generally weak (mostly\n−\n0.2\n-0.2\nto\n0.2\n0.2\n), with only decision-related tasks (Decision-1/2) exhibiting mild positive associations with Object recognition, suggesting limited reliance on object-level cues rather than scene-level understanding. Compared to 7B, 72B exhibits slightly stronger correlations under normal conditions. In high-risk settings, 7B shows stronger negative correlations (e.g., Scene–Decision down to\nr\n=\n−\n0.23\nr=-0.23\n), indicating that performance degradation in one task corresponds to worse decision-making. In contrast, 72B maintains more balanced correlations with fewer negatives.\nOverall, these results suggest that current VLMs struggle to translate perceptual information into decisions. While scaling improves stability and reduces negative interactions between tasks, it does not substantially strengthen cross-task coupling, implying that tighter integration of perception and decision may require changes beyond model scale.\nFigure 4:\nCorrelation Matrices of Qwen (7B) and Qwen (72B)\n4.5\nRobustness under Similar-Scene Pairs\nFor each model, we evaluate accuracy on individual decision tasks (Decision-1) and the probability that both images in a closely related pair are answered correctly (Decision-1 (both)). If a model relies on task-relevant semantics, the joint accuracy should approximate the product of the individual accuracies; substantially lower joint performance suggests that shared but irrelevant cues influence decisions. Detailed results are provided in Appendix\nD.5\n.\nAnalysis reveals the following patterns: For almost all models, the joint correct rate is lower than the expected squared baseline. Models such as Llama (11B) and Llava (7B) show significant drops (\np\n<\n0.05\np<0.05\nor\np\n<\n0.10\np<0.10\n). This pattern confirms that small-scale models tend to recombine perceptual elements without truly assessing which visual components are decision-relevant. Larger models demonstrate limited robustness, with Llava (72B) achieving the highest performance on both individual and paired tests.\nThe pronounced gap between individual and joint accuracies, particularly in smaller models, indicates that most systems fail to identify the objects or scenes that causally drive decisions. This reflects inherent inductive biases and reliance on compositional rather than causal reasoning. While increased scale partially improves robustness, current architectures lack effective mechanisms to suppress irrelevant perceptual priors, limiting reliable visual decision-making.\nFigure 5:\n”Scaling behavior” of Internvl models\nFigure 6:\nResults of Explainability Analysis for Qwen (7B) and Qwen (72B)\nTable 2:\nPerformance of Analyzer Model and Baselines (%)\nModel\nSize\nOpen\nExact Match\n↑\n\\uparrow\nPartial Match\nMismatch\n↓\n\\downarrow\nAvg Score\n↑\n\\uparrow\nGPT-4.1\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n48.09\n23.02\n28.88\n59.60\nQwen\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n48.09\n14.39\n37.52\n55.29\nQwen\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n50.25\n21.58\n28.17\n61.04\nLlama\n11B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n17.16\n20.86\n61.98\n27.59\nLlama\n90B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n40.18\n25.18\n34.64\n52.77\nOurs\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n65.36\n11.51\n23.13\n71.11\n4.6\nValidation of the “Scaling Behavior”\nWe evaluate the scaling behavior of parameter size in the Internvl family. In traditional scaling laws, model performance increases with parameter size and typically follows an approximately logarithmic trend; however, on our benchmark, the 38B model exhibits a pronounced performance drop (Figure\n5\n). This anomaly persists under both 0-/1-shot settings and across all-scenario and high-risk splits.\nThe 38B model exhibits deviations in reasoning process: compared with smaller models, it demonstrates more structured multi-step reasoning, but relative to the larger model, its reasoning chain remains incomplete. For instance, in the Scene-2 task, the 38B model categorizes options by factors such as lighting and weather conditions, but assumes that one option must be selected from each category, even when no correct answer exists. This excessively constrained reasoning leads to decision-making errors and performance degradation.\nThese observations have important implications for VLM evaluation. Model performance does not scale monotonically with parameter size, as intermediate model sizes may exhibit a mismatch between reasoning capacity and task demands. Moreover, analyzing reasoning chains is necessary to identify systematic failure modes.\n4.7\nExplainability Analysis Results\nWe conducted an explainability analysis on the Qwen-7B and Qwen-72B models. Results are presented in Figure\n6\n. The results indicate that most of the models’ reasoning processes contain varying degrees of flaws. Although the overall accuracy remains acceptable, the reasoning process is often inconsistent or erroneous.\nFigure\n6\nreveals that while the relative ranking of error-weight distributions slightly differs between models of varying sizes, logical reasoning errors, semantic feature omissions, and model hallucinations consistently occupy the top three error categories—with logical reasoning errors being predominant, indicating that current VLMs still struggle to perform coherent multi-modal reasoning.\nA comparison of the 7B and 72B models shows that the 72B model does not exhibit errors related to decision boundary instability, whereas the 7B model does, indicating that smaller models are limited in handling boundary conditions (e.g., distinguishing lane changes from turns). While logical reasoning errors remain predominant, their reduced frequency in the 72B model suggests that increased model scale improves reasoning capacity.\n4.8\nEvaluation Results of Analyzer Model\nFor model training and evaluation, we collected approximately 1,680 reasoning traces from the previously assessed models, metrics, and shot settings. 1,500 traces were used for training the analyzer model, while 180 traces were reserved for evaluation. We employ Qwen (7B) as the base model, and perform supervised fine-tuning to classify reasoning errors according to predefined error modes.\nWe evaluate the fine-tuned 7B analyzer against mainstream baselines, as summarized in Table\n2\n. Our model achieves the highest Exact Match and the lowest Mismatch, demonstrating superior performance relative to larger general-purpose models, including Qwen (72B) and GPT-4.1. The Average Score is also the best among all systems, indicating consistent tag quality across diverse cases. Consequently, the analyzer’s outputs are suitable for large-scale auxiliary assessment and can support automated reasoning-trace labeling.\n5\nConclusion\nThis paper presents AutoDriDM, a benchmark for evaluating the decision-making capabilities of VLMs in autonomous driving scenarios. We conduct systematic experiments on multiple VLMs and draw the following conclusions: (1) VLMs excel at object perception, but this does not translate into decision-making ability, as reflected by low correlation and poor robustness, suggesting current decision capability is insufficient for real-world deployment. (2) Although high-risk data are scarce and may limit VLM pretraining, VLMs still show stronger decision-making performance in high-risk scenarios. (3) While performance generally scales with parameter size, constrained reasoning can also degrade model capability. (4) Even when the selected answer is correct, the underlying reasoning is often flawed; the main failure modes are logical errors, missing semantic features, and hallucinations. (5) Our fine-tuned analyzer model can support large-scale annotation of reasoning traces. This benchmark provides a systematic foundation for studying the capability boundaries and reasoning patterns of VLMs, and offers insights for building more reliable autonomous driving systems.\nLimitations\nOur study, benchmark, and analyses have several limitations.(1) Data scope: AutoDriDM is constructed from front-facing camera images in nuScenes, KITTI, and BDD100K; it does not include multi-camera setups, multi-sensor inputs (e.g., LiDAR, radar), or video temporal information, which constrains temporal reasoning and occlusion handling. (2) Task format: We employ only single-/multiple-choice questions to reduce ambiguity and enable objective scoring, which may underestimate models capable of producing richer free-form explanations and may induce answer priors. (3) Model coverage: We evaluate representative open- and closed-source models as well as Internvl scales, yet vendor updates and inference settings (e.g., temperature, generation length) can affect the results.\nEthical Considerations\nWe use only public datasets under their research licenses. The data may contain pedestrians, vehicles, and buildings in public spaces; no attempts were made to identify individuals, and no new personal data were collected. Annotators were team members informed of the task requirements. The benchmark is intended solely for offline research and is not to be used to determine the roadworthiness of any driving system; prominent disclaimers will be provided. To mitigate misuse risks, such as selective citation or overfitting, we will release the full protocol and open-source the relevant code.\nReferences\nM. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann,\net al.\n(2024)\nPhi-4 technical report\n.\narXiv preprint arXiv:2412.08905\n.\nCited by:\nAppendix C\n.\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,\net al.\n(2023)\nGpt-4 technical report\n.\narXiv preprint arXiv:2303.08774\n.\nCited by:\nAppendix C\n.\nC. Badue, R. Guidolini, R. V. Carneiro, P. Azevedo, V. B. Cardoso, A. Forechi, L. Jesus, R. Berriel, T. M. Paixão, F. Mutz, L. De Paula Veronese, T. Oliveira-Santos, and A. F. De Souza (2021)\nSelf-driving cars: A survey\n.\nExpert Systems with Applications\n165\n,\npp. 113816\n(\nen\n).\nExternal Links:\nISSN 09574174\n,\nLink\n,\nDocument\nCited by:\n§1\n.\nZ. Bai, P. Wang, T. Xiao, T. He, Z. Han, Z. Zhang, and M. Z. Shou (2025)\nHallucination of Multimodal Large Language Models: A Survey\n.\narXiv\n.\nNote:\narXiv:2404.18930 [cs]Comment: 228 references\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.1\n.\nH. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom (2020)\nNuscenes: a multimodal dataset for autonomous driving\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp. 11621–11631\n.\nCited by:\n§3.1\n.\nL. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li (2024a)\nEnd-to-end Autonomous Driving: Challenges and Frontiers\n.\narXiv\n(\nen\n).\nNote:\narXiv:2306.16927 [cs]Comment: Accepted by IEEE TPAMI\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba (2021)\nEvaluating Large Language Models Trained on Code\n.\narXiv\n.\nNote:\narXiv:2107.03374 [cs]Comment: corrected typos, added references, added authors, added acknowledgements\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.2\n.\nZ. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu,\net al.\n(2024b)\nInternvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp. 24185–24198\n.\nCited by:\nAppendix C\n.\nP. S. Chib and P. Singh (2023)\nRecent Advancements in End-to-End Autonomous Driving using Deep Learning: A Survey\n.\narXiv\n.\nNote:\narXiv:2307.04370 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord (2018)\nThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\n.\narXiv\n.\nNote:\narXiv:1803.05457 [cs]Comment: 10 pages, 7 tables, 2 figures\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.2\n.\nF. Codevilla, M. Müller, A. López, V. Koltun, and A. Dosovitskiy (2018)\nEnd-to-end Driving via Conditional Imitation Learning\n.\narXiv\n.\nNote:\narXiv:1710.02410 [cs]Comment: Published at the International Conference on Robotics and Automation (ICRA), 2018\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n.\nG. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen,\net al.\n(2025)\nGemini 2.5: pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities\n.\narXiv preprint arXiv:2507.06261\n.\nCited by:\nAppendix C\n.\nDeepSeek-AI, A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Wang, J. Chen, J. Chen, J. Yuan, J. Qiu, J. Li, J. Song, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Wang, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Wang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Zhang, R. Pan, R. Wang, R. Xu, R. Zhang, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ye, S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Pan, T. Wang, T. Yun, T. Pei, T. Sun, W. L. Xiao, W. Zeng, W. Zhao, W. An, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen, X. Chen, X. Zhang, X. Chen, X. Nie, X. Sun, X. Wang, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yu, X. Song, X. Shan, X. Zhou, X. Yang, X. Li, X. Su, X. Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Y. Zhang, Y. Xu, Y. Xu, Y. Huang, Y. Li, Y. Zhao, Y. Sun, Y. Li, Y. Wang, Y. Yu, Y. Zheng, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Tang, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Wu, Y. Ou, Y. Zhu, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Zha, Y. Xiong, Y. Ma, Y. Yan, Y. Luo, Y. You, Y. Liu, Y. Zhou, Z. F. Wu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Huang, Z. Zhang, Z. Xie, Z. Zhang, Z. Hao, Z. Gou, Z. Ma, Z. Yan, Z. Shao, Z. Xu, Z. Wu, Z. Zhang, Z. Li, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Gao, and Z. Pan (2025)\nDeepSeek-V3 Technical Report\n.\narXiv\n.\nNote:\narXiv:2412.19437 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nC. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu, and R. Ji (2024a)\nMME: a comprehensive evaluation benchmark for multimodal large language models\n.\nExternal Links:\n2306.13394\n,\nLink\nCited by:\n§2.2\n.\nY. Fu, A. Jain, X. Chen, Z. Mo, and X. Di (2024b)\nDriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving\n.\nIn\n2024 IEEE International Automated Vehicle Validation Conference (IAVVC)\n,\npp. 1–6\n.\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nA. Geiger, P. Lenz, C. Stiller, and R. Urtasun (2013)\nVision meets robotics: the kitti dataset\n.\nThe international journal of robotics research\n32\n(\n11\n),\npp. 1231–1237\n.\nCited by:\n§3.1\n.\nA. Gopalkrishnan, R. Greer, and M. Trivedi (2024)\nMulti-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving\n.\narXiv\n.\nNote:\narXiv:2403.19838 [cs]Comment: 9 pages, 3 figures, Accepted at CVPR 2024 Vision and Language for Autonomous Driving and Robotics Workshop\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.1\n.\nA. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Roziere, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. Canton Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Wyatt, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Guzmán, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Thattai, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. Arrieta Ibarra, I. Kloumann, I. Misra, I. Evtimov, J. Zhang, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. Vasuden Alwala, K. Prasad, K. Upasani, K. Plawiak, K. Li, K. Heafield, K. Stone, K. El-Arini, K. Iyer, K. Malik, K. Chiu, K. Bhalla, K. Lakhotia, L. Rantala-Yeary, L. van der Maaten, L. Chen, L. Tan, L. Jenkins, L. Martin, L. Madaan, L. Malo, L. Blecher, L. Landzaat, L. de Oliveira, M. Muzzi, M. Pasupuleti, M. Singh, M. Paluri, M. Kardas, M. Tsimpoukelli, M. Oldham, M. Rita, M. Pavlova, M. Kambadur, M. Lewis, M. Si, M. K. Singh, M. Hassan, N. Goyal, N. Torabi, N. Bashlykov, N. Bogoychev, N. Chatterji, N. Zhang, O. Duchenne, O. Çelebi, P. Alrassy, P. Zhang, P. Li, P. Vasic, P. Weng, P. Bhargava, P. Dubal, P. Krishnan, P. Singh Koura, P. Xu, Q. He, Q. Dong, R. Srinivasan, R. Ganapathy, R. Calderer, R. Silveira Cabral, R. Stojnic, R. Raileanu, R. Maheswari, R. Girdhar, R. Patel, R. Sauvestre, R. Polidoro, R. Sumbaly, R. Taylor, R. Silva, R. Hou, R. Wang, S. Hosseini, S. Chennabasappa, S. Singh, S. Bell, S. S. Kim, S. Edunov, S. Nie, S. Narang, S. Raparthy, S. Shen, S. Wan, S. Bhosale, S. Zhang, S. Vandenhende, S. Batra, S. Whitman, S. Sootla, S. Collot, S. Gururangan, S. Borodinsky, T. Herman, T. Fowler, T. Sheasha, T. Georgiou, T. Scialom, and T. Speckbacher (2024)\nThe Llama 3 Herd of Models\n.\narXiv e-prints\n,\npp. arXiv:2407.21783\n.\nExternal Links:\nDocument\n,\n2407.21783\nCited by:\nAppendix C\n.\nT. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob, D. Manocha, and T. Zhou (2024)\nHallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models\n.\nIn\n2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nSeattle, WA, USA\n,\npp. 14375–14385\n(\nen\n).\nExternal Links:\nISBN 979-8-3503-5300-6\n,\nLink\n,\nDocument\nCited by:\n§2.1\n,\n§2.2\n.\nX. Guo, R. Zhang, Y. Duan, Y. He, C. Zhang, S. Liu, and L. Chen (2024)\nDriveMLLM: a benchmark for spatial understanding with multimodal large language models in autonomous driving\n.\nExternal Links:\n2411.13112\n,\nLink\nCited by:\n§1\n,\n§2.2\n.\nK. He, X. Zhang, S. Ren, and J. Sun (2016)\nDeep residual learning for image recognition\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp. 770–778\n.\nCited by:\n§3.1\n.\nJ. Li, X. Liu, S. Wei, Z. Chen, B. Li, Q. Guo, X. Yang, Y. Pu, and J. Wang (2025)\nTowards benchmarking and assessing the safety and robustness of autonomous driving on safety-critical scenarios\n.\nExternal Links:\n2503.23708\n,\nLink\nCited by:\n§1\n,\n§2.2\n.\nJ. Li, D. Li, S. Savarese, and S. Hoi (2023)\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\n.\nIn\nProceedings of the 40th International Conference on Machine Learning\n,\npp. 19730–19742\n(\nen\n).\nNote:\nISSN: 2640-3498\nExternal Links:\nLink\nCited by:\n§2.1\n.\nY. Li, W. Zhang, K. Chen, Y. Liu, P. Li, R. Gao, L. Hong, M. Tian, X. Zhao, Z. Li,\net al.\n(2024)\nAutomated evaluation of large vision-language models on self-driving corner cases\n.\narXiv preprint arXiv:2404.10595\n.\nCited by:\n§2.2\n.\nH. Liu, C. Li, Y. Li, and Y. J. Lee (2023)\nImproved baselines with visual instruction tuning\n.\nExternal Links:\n2310.03744\nCited by:\nAppendix C\n.\nY. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu,\net al.\n(2024a)\nMmbench: is your multi-modal model an all-around player?\n.\nIn\nEuropean conference on computer vision\n,\npp. 216–233\n.\nCited by:\n§2.2\n.\nY. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X. Yin, C. Liu, L. Jin, and X. Bai (2024b)\nOCRBench: on the hidden mystery of OCR in large multimodal models\n.\nScience China Information Sciences\n67\n(\n12\n),\npp. 220102\n.\nExternal Links:\nISSN 1869-1919\n,\nLink\n,\nDocument\nCited by:\n§2.2\n.\nD. Lu, Y. Sun, Z. Zhang, L. Huang, J. Zeng, M. Shu, and H. Cao (2025)\nInternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression\n.\narXiv\n.\nNote:\narXiv:2503.21307 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nM. Mathew, D. Karatzas, and C. V. Jawahar (2021)\nDocVQA: A Dataset for VQA on Document Images\n.\nIn\n2021 IEEE Winter Conference on Applications of Computer Vision (WACV)\n,\nWaikoloa, HI, USA\n,\npp. 2199–2208\n(\nen\n).\nExternal Links:\nISBN 978-1-6654-0477-8\n,\nLink\n,\nDocument\nCited by:\n§2.2\n.\nOpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A. Brakman, G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti, T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray, R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kaftan, Ł. Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, Ł. Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mély, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O’Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. d. A. B. Peres, M. Petrov, H. P. d. O. Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman, D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such, N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian, E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. J. Weinmann, A. Welihinda, P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, and B. Zoph (2024)\nGPT-4 Technical Report\n.\narXiv\n.\nNote:\narXiv:2303.08774 [cs]\nversion: 6Comment: 100 pages; updated authors list; fixed author names and added citation\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nB. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli (2016)\nA Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles\n.\nIEEE Transactions on Intelligent Vehicles\n1\n(\n1\n),\npp. 33–55\n.\nExternal Links:\nISSN 2379-8904\n,\nLink\n,\nDocument\nCited by:\n§1\n.\nB. Patel, S. Chakraborty, W. A. Suttle, M. Wang, A. S. Bedi, and D. Manocha (2024)\nAIME: AI System Optimization via Multiple LLM Evaluators\n.\narXiv\n.\nNote:\narXiv:2410.03131 [cs]Comment: 21 pages, 10 Figures, 4 Tables\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.2\n.\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman (2024)\nGPQA: A Graduate-Level Google-Proof Q&A Benchmark\n.\n(\nen\n).\nCited by:\n§2.2\n.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi (2021)\nWinoGrande: an adversarial winograd schema challenge at scale\n.\nCommunications of the ACM\n64\n(\n9\n),\npp. 99–106\n(\nen\n).\nExternal Links:\nISSN 0001-0782, 1557-7317\n,\nLink\n,\nDocument\nCited by:\n§2.2\n.\nW. Schwarting, J. Alonso-Mora, and D. Rus (2018)\nPlanning and Decision-Making for Autonomous Vehicles\n.\nAnnual Review of Control, Robotics, and Autonomous Systems\n1\n(\n1\n),\npp. 187–210\n(\nen\n).\nExternal Links:\nISSN 2573-5144, 2573-5144\n,\nLink\n,\nDocument\nCited by:\n§1\n.\nY. Shen, C. Fu, P. Chen, M. Zhang, K. Li, X. Sun, Y. Wu, S. Lin, and R. Ji (2024)\nAligning and Prompting Everything All at Once for Universal Visual Perception\n.\nIn\n2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nSeattle, WA, USA\n,\npp. 13193–13203\n(\nen\n).\nExternal Links:\nISBN 979-8-3503-5300-6\n,\nLink\n,\nDocument\nCited by:\n§1\n.\nC. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie, J. Beißwenger, P. Luo, A. Geiger, and H. Li (2025)\nDriveLM: Driving with Graph Visual Question Answering\n.\nIn\nComputer Vision – ECCV 2024\n,\nA. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol (Eds.)\n,\nCham\n,\npp. 256–274\n.\nExternal Links:\nISBN 978-3-031-72943-0\nCited by:\n§2.1\n,\n§2.2\n.\nA. Tampuu, T. Matiisen, M. Semikin, D. Fishman, and N. Muhammad (2022)\nA Survey of End-to-End Driving: Architectures and Training Methods\n.\nIEEE Transactions on Neural Networks and Learning Systems\n33\n(\n4\n),\npp. 1364–1384\n.\nExternal Links:\nISSN 2162-2388\n,\nLink\n,\nDocument\nCited by:\n§1\n.\nJ. Tang, Q. Liu, Y. Ye, J. Lu, S. Wei, C. Lin, W. Li, M. F. F. B. Mahmood, H. Feng, Z. Zhao, Y. He, K. Lu, Y. Wang, Y. Liu, H. Liu, X. Bai, and C. Huang (2025)\nMTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering\n.\narXiv\n.\nNote:\narXiv:2405.11985 [cs]Comment: Accepted by ACL 2025 findings\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.2\n.\nG. Team (2025a)\nGemma 3\n.\nExternal Links:\nLink\nCited by:\nAppendix C\n.\nQ. Team (2025b)\nQwen2.5-vl\n.\nExternal Links:\nLink\nCited by:\nAppendix C\n.\nX. Tian, J. Gu, B. Li, Y. Liu, Y. Wang, Z. Zhao, K. Zhan, P. Jia, X. Lang, and H. Zhao (2024)\nDriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models\n.\narXiv\n.\nNote:\narXiv:2402.12289 [cs]Comment: Project Page: https://tsinghua-mars-lab.github.io/DriveVLM/\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample (2023)\nLLaMA: Open and Efficient Foundation Language Models\n.\narXiv\n.\nNote:\narXiv:2302.13971 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nP. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin (2024a)\nQwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution\n.\narXiv\n.\nNote:\narXiv:2409.12191 [cs]Comment: Code is available at https://github.com/QwenLM/Qwen2-VL. arXiv admin note: text overlap with arXiv:2408.15262 by other authors\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.1\n.\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen (2024b)\nMMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nA. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (Eds.)\n,\nVol.\n37\n,\npp. 95266–95290\n.\nExternal Links:\nLink\nCited by:\n§2.2\n.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou,\net al.\n(2022)\nChain-of-thought prompting elicits reasoning in large language models\n.\nAdvances in neural information processing systems\n35\n,\npp. 24824–24837\n.\nCited by:\n§3.3\n.\nS. Xie, L. Kong, Y. Dong, C. Sima, W. Zhang, Q. A. Chen, Z. Liu, and L. Pan (2025)\nAre vlms ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives\n.\nExternal Links:\n2501.04003\n,\nLink\nCited by:\n§1\n.\nZ. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K. K. Wong, Z. Li, and H. Zhao (2024)\nDrivegpt4: interpretable end-to-end autonomous driving via large language model\n.\nIEEE Robotics and Automation Letters\n.\nCited by:\n§2.1\n.\nJ. You, H. Shi, Z. Jiang, Z. Huang, R. Gan, K. Wu, X. Cheng, X. Li, and B. Ran (2025)\nV2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models\n.\narXiv\n.\nNote:\narXiv:2408.09251 [cs]\nExternal Links:\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nF. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell (2020)\nBdd100k: a diverse driving dataset for heterogeneous multitask learning\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp. 2636–2645\n.\nCited by:\n§3.1\n.\nW. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang (2024)\nMM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities\n.\narXiv\n.\nNote:\narXiv:2308.02490 [cs]Comment: ICML 2024. Code, data and leaderboard: https://github.com/yuweihao/MM-Vet\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.1\n,\n§2.2\n.\nX. Yue, Y. Ni, T. Zheng, K. Zhang, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen (2024)\nMMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\n.\nIn\n2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nSeattle, WA, USA\n,\npp. 9556–9567\n(\nen\n).\nExternal Links:\nISBN 979-8-3503-5300-6\n,\nLink\n,\nDocument\nCited by:\n§2.2\n.\nX. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, Y. Su, W. Chen, and G. Neubig (2025)\nMMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark\n.\narXiv\n.\nNote:\narXiv:2409.02813 [cs]Comment: ACL 2025 Main\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.2\n.\nJ. Zhang, J. Huang, S. Jin, and S. Lu (2024)\nVision-Language Models for Vision Tasks: A Survey\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n46\n(\n8\n),\npp. 5625–5644\n.\nExternal Links:\nISSN 1939-3539\n,\nLink\n,\nDocument\nCited by:\n§1\n.\nT. Zhang, S. Wang, L. Li, G. Zhang, P. Taslakian, S. Rajeswar, J. Fu, B. Liu, and Y. Bengio (2025)\nVCR: A Task for Pixel-Level Complex Reasoning in Vision Language Models via Restoring Occluded Text\n.\narXiv\n.\nNote:\narXiv:2406.06462 [cs]Comment: Accepted at ICLR 2025. Original paper name: VCR: Visual Caption Restoration\nExternal Links:\nLink\n,\nDocument\nCited by:\n§2.2\n.\nW. Zhao, K. Zhou, L. Junyi, T. Tianyi, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, and J. Wen (2023)\nA Survey of Large Language Models\n.\nExternal Links:\nDocument\nCited by:\n§2.1\n.\nX. Zhou, M. Liu, E. Yurtsever, B. L. Zagar, W. Zimmer, H. Cao, and A. C. Knoll (2024a)\nVision language models in autonomous driving: a survey and outlook\n.\nIEEE Transactions on Intelligent Vehicles\n(\n),\npp. 1–20\n.\nExternal Links:\nDocument\nCited by:\n§1\n.\nX. Zhou, M. Liu, E. Yurtsever, B. L. Zagar, W. Zimmer, H. Cao, and A. C. Knoll (2024b)\nVision Language Models in Autonomous Driving: A Survey and Outlook\n.\nIEEE Transactions on Intelligent Vehicles\n,\npp. 1–20\n.\nExternal Links:\nISSN 2379-8904\n,\nLink\n,\nDocument\nCited by:\n§1\n,\n§2.1\n.\nAppendix\nAppendix A\nDataset Summary\nA.1\nSource Datasets and Usage Statistics\nTable\n3\nsummarizes the three source datasets used in AutoDriDM, including their regions, scene types, task coverage, and the corresponding image/QA usage statistics.\nTable 3:\nSource datasets and our usage in AutoDriDM.\nDataset\nCountry / Region\nScene Types\nCovered Tasks\nUsed Images\nQA Items\nnuScenes\nBoston; Singapore\nUrban streets, dense traffic, diverse maneuvers, diverse weather\nObject-1\nObject-2\nDecision-1\nDecision-2\n465\n1908\nBDD100K\nUnited States\nResidential areas, highways, city streets, parking lots, gas stations, tunnels\nObject-1\nObject-2\nScene-1\nScene-2\nDecision-1\nDecision-2\n450\n3218\nKITTI\nKarlsruhe, Germany and surrounding areas\nUrban streets, residential areas, campus, rural roads, highways\nObject-1\nObject-2\nDecision-1\nDecision-2\n380\n1520\nTotal images used:\n1295.\nTotal QA items:\n6650.\nA.2\nDesign Intentions of the Six Tasks\nTable\n4\nsummarizes the design intention of each task in AutoDriDM.\nTable 4:\nDesign intentions of the six tasks in AutoDriDM.\nTask\nDesign intention\nObject-1\nIdentifies the key object, serving as the starting point of the model’s reasoning process, thereby testing its ability to capture task-relevant information and filter out irrelevant information.\nObject-2\nEvaluates the model’s inferential capacity, requiring it to infer the object’s state from visual observations, thereby constraining the feasible subsequent actions.\nScene-1\nCaptures the overall environmental context, providing an overview of general driving conditions rather than frame-specific operational requirements (e.g., rainy or low-light conditions that affect safe driving speed).\nScene-2\nExtends critical information extraction from object-level perception to the broader scene context, imposing additional constraints to support downstream decision-making.\nDecision-1\nRepresents the terminal point of the reasoning process, measuring the model’s core ability to make correct decisions.\nDecision-2\nComplements Decision-1 by evaluating responses to suboptimal options, providing a more comprehensive evaluation of VLMs’ capability boundaries.\nA.3\nRisk Rating Rules\nWe define scenario risk as an ordinal scale that measures the short-term likelihood of a collision and the potential severity of its consequences under a reasonable human-driver policy. The rating is based on safety-relevant cues visible in a single image (e.g., relative positions, right-of-way, vulnerable road users, and abnormal road conditions).\nTwo annotators independently assign an integer level\nr\n∈\n{\n1\n,\n2\n,\n3\n,\n4\n,\n5\n}\nr\\in\\{1,2,3,4,5\\}\naccording to the rules in Table\n5\n. We take the mean score\nr\n¯\n\\bar{r}\nas the final risk score and define the high-risk split as\nr\n¯\n≥\n4.0\n\\bar{r}\\geq 4.0\n.\nTable 5:\nFive-level risk rules for scenario annotation.\nLevel\nOperational definition (single-image, near-term driving risk)\n1\nMinimal:\nNo safety-critical risk is observed; no immediate action is required beyond lane keeping and maintaining speed.\n2\nLow:\nThe scene shows mild complexity or weak risk cues; a cautious driver may reduce speed or increase headway, but immediate intervention is typically unnecessary.\n3\nModerate:\nA conflict may arise in the near term; the driver should increase attention and be ready to brake or yield when needed, and the situation is likely to be resolved safely with timely response.\n4\nHigh:\nStrong risk cues are present and an unsafe outcome is likely without timely action; the driver should take an immediate safety action (e.g., braking or yielding).\n5\nSevere:\nA safety-critical situation with near-collision cues; the likelihood of a collision is very high (e.g., a pedestrian suddenly appears within the field of view at close range).\nA.4\nSimilarity Illustration\nThis section illustrates the curated near-duplicate scene pairs used to evaluate robustness. The pairs are selected by high embedding similarity and are intended to test whether models maintain causally consistent decisions under small visual changes. Figure\n7\npresents representative pairs.\nFigure 7:\nExamples of similar-scene robustness test.\nAppendix B\nExplainability Error Taxonomy & Examples & Analyzer Model\nB.1\nExplainability Error Taxonomy and Examples\nWe use the following taxonomy to analyze reasoning traces and characterize failure modes.\nExplainability Error Taxonomy.\n1.\nLogical Reasoning Error\n: Perception may be correct, but the subsequent reasoning is logically invalid or contradicts traffic rules or causality.\n2.\nSemantic Feature Omission\n: The model overlooks or misjudges safety-critical semantic/visual cues (e.g., brake lights, turn signals, pedestrian gestures) or other discriminative attributes necessary for correct recognition and categorization.\n3.\nModel Hallucination\n: The model produces information inconsistent with the actual input, e.g., inventing non-existent objects, attributes, or relations.\n4.\nModality Imbalance\n: The model over-relies on one modality while neglecting information from another (e.g., text vs. image).\n5.\nSpatial Relation Misjudgment\n: The model makes incorrect judgments about spatial relations (e.g., distance, relative position, depth).\n6.\nLimited Logical Inference\n: The model draws decisions from insufficient local evidence without integrating broader scene context.\n7.\nGeneralization Deficit\n: The model generalizes poorly to rare or out-of-distribution scenarios (e.g., construction zones, traffic incidents, temporary lane closures).\n8.\nDecision Boundary Instability\n: Near critical operating points, small input perturbations cause large changes in the chosen action.\n9.\nTarget Priority Misjudgment\n: The model fails to correctly prioritize among competing objectives (e.g., yielding to pedestrians vs. maintaining speed).\nFigure\n8\nprovides representative, annotated examples for some categories.\nB.2\nExplainability Model I/O Schematic\nThe analyzer consumes an image, question, options, the ground-truth answer, and a model’s rationale/answer, and outputs one or more predicted error labels from the taxonomy. See Figure\n14\nfor the system instruction and user input template used in our analysis.\nAppendix C\nExperimental Details\nThis section lists full model names, the repetition protocol, and prompt we used. The models evaluated in this work include the Internvl2.5 family\n(Chen\net al.\n,\n2024b\n)\n, GPT-4.1\n(Achiam\net al.\n,\n2023\n)\n, Gemini-2.5-Pro\n(Comanici\net al.\n,\n2025\n)\n, Gemma-3\n(Team,\n2025a\n)\n, Llama-3.2\n(Grattafiori\net al.\n,\n2024\n)\n, Llava\n(Liu\net al.\n,\n2023\n)\n, Phi-4\n(Abdin\net al.\n,\n2024\n)\n, and Qwen2.5-VL\n(Team,\n2025b\n)\n.\nC.1\nExperimental Setup\nThe benchmark assesses VLMs via a format that includes an image, a question and answer options. Models are required to respond with the specified format. For multiple-choice questions, full credit is awarded for exact matches, partial credit for subsets. Selection of any incorrect options results in a zero score. We abbreviate model names by capitalizing only the first letter, and the full-name mapping is provided in Appendix\nC.2\n.\nC.2\nModel List and Full Names\n•\nGemma (4B)\n→\n\\rightarrow\ngemma-3-4b-it\n•\nGemma (27B)\n→\n\\rightarrow\ngemma-3-27b-it\n•\nLlama (11B)\n→\n\\rightarrow\nLlama-3.2-11B-Vision-Instruct\n•\nLlama (90B)\n→\n\\rightarrow\nLlama-3.2-90B-Vision-Instruct\n•\nLlava (7B)\n→\n\\rightarrow\nLLaVA-v1.6-Mistral-7B-hf\n•\nLlava (72B)\n→\n\\rightarrow\nLLaVA-NeXT-72B-hf\n•\nPhi (6B)\n→\n\\rightarrow\nPhi-4-multimodal-instruct\n•\nQwen (7B)\n→\n\\rightarrow\nQwen2.5-VL-7B-Instruct\n•\nQwen (72B)\n→\n\\rightarrow\nQwen2.5-VL-72B-Instruct\n•\nInternvl (1B)\n→\n\\rightarrow\nInternVL2.5-1B-MPO\n•\nInternvl (2B)\n→\n\\rightarrow\nInternVL2.5-2B-MPO\n•\nInternvl (4B)\n→\n\\rightarrow\nInternVL2.5-4B-MPO\n•\nInternvl (8B)\n→\n\\rightarrow\nInternVL2.5-8B-MPO\n•\nInternvl (26B)\n→\n\\rightarrow\nInternVL2.5-26B-MPO\n•\nInternvl (38B)\n→\n\\rightarrow\nInternVL2.5-38B-MPO\n•\nInternvl (78B)\n→\n\\rightarrow\nInternVL2.5-78B-MPO\nC.3\nRepetition Protocol\nWe run each configuration twice and report the average across runs. For open-source models, inference was performed on NVIDIA A100 GPUs; for closed-source models, inference was conducted via the vendors’ official API endpoints.\nC.4\nBenchmark Prompts (System/User)\nFigure\n13\nshows the system and user prompt templates for the single-choice and multiple-choice Chain-of-Thought prompts used in our experiments.\nFigure 8:\nRepresentative Error Examples in Explainability Analysis\nFigure 9:\nModel performance on all-scenarios under 0/1/2/5-shot prompting\nFigure 10:\nModel performance on high-risk-scenarios under 0/1/2/5-shot prompting\nC.5\nAnnotation Templates for the Six Tasks\nTo ensure consistent annotation across items, we provide the unified labeling template for the six tasks in AutoDriDM.\nFigure\n11\nshows the example image used for illustration.\nBased on this image, Figure\n12\npresents the corresponding scenario-level risk label (\ndanger_score\n) and the question–option–answer templates for all six tasks.\nAll\nunderlined\nfields indicate placeholders that annotators replace according to the actual objects, states, and scene conditions in the image.\nExample Image\nFigure 11:\nExample image used for the six-task annotation templates.\nDanger Score Label\ndanger_score\n:\n2\nObject-1: Key object influencing the driving decision\nQUESTION: When driving a car, what is the most important object in the image that influences your driving decision? Please select only one option.\nOPTIONS:\nA.Building; B.Vehicle; C.Traffic light; D.Pedestrian; E.Street lamp\nANSWER:\nC\nObject-2: State of the designated key object\nQUESTION: What is the state of the\ntraffic light\n? Please select only one option.\nOPTIONS:\nA.Green; B.Yellow; C.Red; D.There is no traffic light\nANSWER:\nA\nScene-1: Weather / illumination conditions\nQUESTION: What is the weather condition in the image? Please select one or more options.\nOPTIONS: A.Daytime; B.Nighttime; C.Rain; D.Snow; E.Heavy fog\nANSWER:\nA\nScene-2: Special scene factors affecting decisions\nQUESTION: What special scenes in the image could potentially affect the decision-making of the host vehicle? Please select one or more options.\nOPTIONS:\nA.Heavy Traffic; B.Accident Scene; C.Construction Zone; D.Vehicle breakdown; E.School road section; F.Rain or Snow Weather; G.None of the above\nANSWER:\nA\nDecision-1: Optimal driving action\nQUESTION: When driving a car, which decision do you think is the best choice in the current situation? Please select only one option.\nOPTIONS:\nA.Drive straight; B.Slow down/make an emergency stop; C.Turn left; D.Turn right; E.Change lanes; F.Stay still\nANSWER:\nA\nDecision-2: Risk of a specified action\nQUESTION: If you\nspeed up\n, what level of risk will be faced? Please select only one option.\nOPTIONS: A.Absolutely safe; B.Moderate risk; C.Extreme risk\nANSWER:\nB\nFigure 12:\nAnnotation templates instantiated on an example image. Underlined fields (\n…\n) are placeholders that annotators replace according to the image.\nPrompt 1: Single-choice question — System Instruction\nYou are an AI assistant specializing in automatic driving scene judgment. Your primary task is to analyze a given traffic scene and make a decision or evaluation based on it.\nInput:\nYou will be provided with an image, a corresponding question, and a set of possible options.\nAnswering Guidelines:\n• Your reasoning must be based on the visual evidence in the image and fundamental principles of safe driving.\n• Analyze the situation step-by-step before making a final decision.\nInstruction:\nYou should choose one from the given options, outputting only a single letter corresponding to the selected option with no extra output.\nOutput Format:\nYour response must be structured in a specific two-part format within a single turn. First, generate a structured Chain-of-Thought rationale enclosed within <think>...</think> tags; second, provide the final decision enclosed within <answer>...</answer> tags. The structure must be exactly:\n<think>Your detailed reasoning process here.</think><answer>Your chosen option letter(s) here.</answer>\nPrompt 1: Single-choice question — User Input Template\nIMAGE: {image}\nQUESTION: {question}\nOPTIONS: {options}\nPrompt 2: Multiple-choice question — System Instruction\nYou are an AI assistant specializing in automatic driving scene judgment. Your primary task is to analyze a given traffic scene and make a decision or evaluation based on it.\nInput:\nYou will be provided with an image, a corresponding question, and a set of possible options.\nAnswering Guidelines:\n• Your reasoning must be based on the visual evidence in the image and fundamental principles of safe driving.\n• Analyze the situation step-by-step before making a final decision.\nINSTRUCTION:\nYou should select one or more options from the given choices, outputting only the corresponding letters separated by commas.\nOutput Format:\nYour response must be structured in a specific two-part format within a single turn. First, generate a structured Chain-of-Thought rationale enclosed within <think>...</think> tags; second, provide the final decision enclosed within <answer>...</answer> tags. The structure must be exactly:\n<think>Your detailed reasoning process here.</think><answer>Your chosen option letter(s) here (comma-separated).</answer>\nPrompt 2: Multiple-choice question — User Input Template\nIMAGE: {image}\nQUESTION: {question}\nOPTIONS: {options}\nFigure 13:\nSystem instruction and user input templates for the single-choice and multiple-choice Chain-of-Thought prompts.\nPrompt 3: Error Tagging (Explainability) — System Instruction\nYou are an autonomous-driving evaluation assistant for multimodal (image + text) questions. Your job is to analyze a model’s final answer and its detailed reasoning (chain-of-thought), detect which error patterns apply, and map them to the integer labels defined below. The user input will always contain: (1) the original question, (2) the question options, (3) the correct answer, (4) the model’s final answer, (5) the model’s detailed reasoning, and (6) an image associated with the question. Use both the image and the textual inputs (final answer + reasoning) to decide which label(s) apply.\nLabel definitions (0--9):\n0 = Correct — The model’s final answer is correct and its reasoning is sound and sufficient.\n1 = Logical Reasoning Error — Perception may be correct, but the subsequent reasoning is logically invalid or contradicts traffic rules or causality.\n2 = Semantic Feature Omission — The model overlooks or misjudges safety-critical semantic/visual cues (e.g., brake lights, turn signals, pedestrian gestures) or other discriminative attributes necessary for correct recognition and categorization.\n3 = Model Hallucination — The model produces information inconsistent with the actual input (e.g., invents non-existent objects, attributes, or relations).\n4 = Modality Imbalance — The model over-relies on one modality (text or image) while neglecting the other, causing an error.\n5 = Spatial Relation Misjudgment — The model makes incorrect judgments about spatial relations (e.g., distance, relative position, depth).\n6 = Limited Logical Inference — The model derives a decision from insufficient local evidence without integrating broader scene context.\n7 = Generalization Deficit — The model generalizes poorly to rare or out-of-distribution scenarios (e.g., construction zones, traffic incidents, temporary lane closures).\n8 = Decision Boundary Instability — Near critical operating points, small input perturbations cause large changes in the chosen action.\n9 = Target Priority Misjudgment — The model fails to correctly prioritize among competing objectives (e.g., yielding to pedestrians vs. maintaining speed).\nClassification rules:\n• Use both the image and the provided text (final answer + reasoning) to decide labels.\n• If the final answer is correct but the reasoning is flawed in any way, do NOT output 0; instead output the label(s) for the flawed aspects (you may include multiple labels).\nOutput format:\nYour output must be ONLY a comma-separated list of integers (no words, no punctuation except commas and digits). Examples:\n0\n7\n1,3\n2,6,9\nPrompt 3: Error Tagging (Explainability) — User Input Template\nIMAGE: {image}\nQUESTION: {question}\nOPTIONS: {options}\nANSWER: {answer}\nMODEL FINAL ANSWER: {model_answer}\nMODEL REASONING: {reasoning}\nFigure 14:\nSystem instruction and user input template for the explainability error-tagging prompt.\nTable 6:\nEvaluation on High-Risk Scenarios (0-shot)\nModel\nSize\nOpen\nObject-1\nObject-2\nScene-1\nScene-2\nDecision-1\nDecision-2\nAvg Score\nGemini-2.5Pro\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n77.40\n70.62\n75.00\n23.68\n63.84\n77.40\n64.66\nGPT-4.1\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n80.23\n68.93\n92.76\n48.90\n71.75\n68.36\n71.82\nGemma\n4B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n67.62\n37.05\n24.14\n0\n0.00\n55.88\n36.34\n36.84\nGemma\n27B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n67.74\n56.11\n64.37\n0\n2.64\n58.78\n38.43\n48.01\nLlama\n11B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n63.37\n42.08\n86.82\n16.13\n37.70\n51.04\n49.52\nLlama\n90B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n53.04\n54.33\n90.25\n28.83\n59.71\n69.26\n59.24\nLlava\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n9.37\n48.37\n86.02\n19.78\n21.37\n27.37\n35.38\nLlava\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n42.57\n60.19\n86.42\n27.37\n52.28\n50.56\n53.23\nPhi\n6B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n55.89\n51.88\n91.13\n19.87\n26.36\n35.48\n46.77\nQwen\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n59.57\n49.97\n68.55\n40.27\n33.42\n44.63\n49.40\nQwen\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n66.69\n59.01\n84.09\n36.78\n57.06\n45.87\n58.25\nAppendix D\nSupplemental Figures and Tables\nWe provide correlation heatmaps, radar plots, and per-shot score tables to complement the main results and support fine-grained inspection.\nD.1\nHigh-Risk Scenario Results (0-shot)\nTable\n6\nreports the 0-shot performance of all evaluated models on the high-risk split.\nD.2\nRadar Charts\nFigures\n9\nand\n10\nsummarize shot-wise performance trends and non-monotonic behaviors under few-shot prompting, for the all-scenario and high-risk splits, respectively.\nD.3\nCorrelation Matrices (Qwen Total)\nFigure\n15\nreports Pearson correlations among all tasks across shot settings for Qwen 7B/72B under both all and high-risk splits.\nFigure 15:\nCorrelation matrices across the six evaluation tasks—for Qwen (7B) and Qwen (72B). For each model, four panels correspond to prompt settings (0/1/2/5-shot). Rows and columns appear in the fixed order [O1, O2, S1, S2, D1, D2]; each cell reports the Pearson\nϕ\n\\phi\ncorrelation between the binary correctness indicators of the two tasks, computed over the set of images common to both tasks (diagonal entries = 1). Colors encode correlation on a fixed range\n[\n−\n1\n,\n1\n]\n[-1,1]\n(blue\n→\n\\to\nnegative, white\n≈\n0\n\\approx 0\n, red\n→\n\\to\npositive), with the numeric value overlaid in each cell.\nD.4\nAll-Scenario Evaluation\nTables\n8\nand\n9\nlist per-shot scores for all models.\nD.5\nSimilar-Scene Pair Robustness\nWe report single-image accuracy (Decision-1) and the joint accuracy that both images in a similar pair are answered correctly (Decision-1 (both)). Under an independence assumption, the expected baseline for the joint accuracy equals the squared single-image accuracy. Blue superscript stars indicate statistically significant drops of the joint accuracy below that squared baseline (one star for\n|\nz\n|\n∈\n(\n1.65\n,\n1.96\n]\n|z|\\in(1.65,1.96]\n, two stars for\n|\nz\n|\n>\n1.96\n|z|>1.96\n; two-sided normal approximation).\nTable 7:\nDecision-1 accuracy on single images and on similar-scene pairs\nModel\nSize\nDecision-1\nDecision-1 (both)\nGemma\n4B\n37.50\n0\n5.36\n∗\n{}^{*\\phantom{*}}\nGemma\n27B\n42.86\n10.71\nLlama\n11B\n32.14\n0\n1.79\n∗∗\nLlama\n90B\n32.14\n12.50\nLlava\n72B\n48.21\n21.43\nLlava\n7B\n44.64\n0\n0.00\n∗∗\nPhi\n6B\n43.75\n0\n1.79\n∗∗\nQwen\n7B\n39.29\n0\n5.36\n∗\n{}^{*\\phantom{*}}\nQwen\n72B\n44.64\n10.71\nTable 8:\nEvaluation on All Scenarios\nModel\nSize\nOpen\nShots\nObject-1\nObject-2\nScene-1\nScene-2\nDecision-1\nDecision-2\nAvg Score\nGemini-2.5Pro\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n0\n62.90\n74.35\n76.73\n25.61\n49.47\n55.11\n57.36\n1\n65.04\n71.45\n76.28\n30.96\n55.11\n58.78\n59.60\n2\n62.14\n71.76\n81.18\n39.12\n51.30\n45.80\n58.55\n5\n63.05\n72.98\n80.29\n38.68\n51.45\n53.44\n59.98\nGPT-4.1\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n0\n67.48\n73.74\n92.54\n46.44\n56.95\n51.91\n64.84\n1\n68.09\n72.52\n92.76\n45.36\n54.05\n46.87\n63.28\n2\n69.16\n73.89\n93.21\n45.36\n54.81\n46.87\n63.88\n5\n67.94\n73.59\n92.76\n43.65\n55.42\n47.33\n63.45\nGemma\n4B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n49.86\n50.07\n27.34\n0\n0.89\n45.78\n30.66\n34.10\n1\n50.24\n49.93\n34.17\n21.25\n46.85\n30.81\n38.87\n2\n50.99\n52.26\n0\n7.28\n17.14\n45.64\n37.68\n35.17\n5\n51.52\n52.55\n12.04\n18.70\n44.97\n46.14\n37.65\nGemma\n27B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n64.46\n63.55\n63.45\n0\n7.20\n50.40\n31.65\n46.78\n1\n64.15\n63.10\n64.56\n0\n8.17\n50.55\n30.74\n46.88\n2\n64.22\n63.62\n64.94\n10.11\n50.02\n30.43\n47.22\n5\n64.67\n62.95\n64.71\n12.73\n50.10\n30.13\n47.55\nLlama\n11B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n58.67\n53.45\n87.00\n18.37\n41.84\n37.99\n49.55\n1\n45.86\n46.65\n22.29\n0\n0.07\n34.12\n39.18\n31.36\n2\n23.54\n49.36\n26.67\n0\n2.73\n31.62\n39.37\n28.88\n5\n40.59\n42.78\n20.95\n0\n2.92\n26.94\n37.40\n28.60\nLlama\n90B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n51.39\n63.48\n91.75\n36.78\n45.14\n49.55\n56.35\n1\n58.62\n61.50\n66.64\n21.57\n41.16\n45.64\n49.19\n2\n56.61\n65.58\n83.06\n30.79\n37.48\n43.05\n52.76\n5\n59.62\n64.83\n83.13\n35.42\n40.57\n47.28\n55.14\nLlava\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n26.23\n52.95\n87.15\n32.49\n41.40\n29.87\n45.01\n1\n26.59\n47.59\n72.51\n0\n7.31\n34.34\n42.17\n38.42\n2\n25.18\n46.76\n82.76\n10.08\n46.24\n35.04\n41.01\n5\n25.69\n50.19\n85.88\n16.19\n49.90\n29.20\n42.84\nLlava\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n45.88\n66.72\n85.66\n35.42\n51.60\n34.89\n53.36\n1\n55.79\n64.82\n82.62\n22.71\n44.38\n41.97\n52.05\n2\n47.17\n66.04\n86.41\n35.86\n45.84\n34.21\n52.59\n5\n55.53\n66.04\n84.70\n34.22\n52.23\n34.28\n54.50\nPhi\n6B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n60.23\n63.10\n91.01\n21.67\n44.58\n30.72\n51.89\n1\n62.71\n53.80\n82.09\n13.35\n39.37\n39.07\n48.40\n2\n53.52\n60.91\n90.12\n18.25\n46.25\n39.34\n51.40\n5\n62.02\n59.70\n85.51\n24.15\n34.34\n46.09\n51.97\nQwen\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n65.58\n59.34\n65.16\n33.60\n40.87\n35.48\n50.00\n1\n59.03\n61.69\n41.38\n14.07\n45.28\n36.02\n42.91\n2\n60.97\n62.20\n53.34\n22.66\n42.22\n35.56\n46.16\n5\n60.37\n62.36\n54.16\n30.04\n43.81\n32.45\n47.20\nQwen\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n68.06\n68.24\n77.19\n37.62\n54.69\n38.20\n57.33\n1\n68.53\n68.76\n77.05\n45.07\n54.17\n42.31\n59.31\n2\n68.67\n68.69\n86.85\n47.82\n52.87\n36.62\n60.25\n5\n68.67\n68.38\n87.59\n48.98\n53.47\n38.13\n60.87\nTable 9:\nEvaluation on High-Risk Scenarios\nModel\nSize\nOpen\nShots\nObject-1\nObject-2\nScene-1\nScene-2\nDecision-1\nDecision-2\nAvg Score\nGemini-2.5Pro\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n0\n77.40\n70.62\n75.00\n23.68\n63.84\n77.40\n64.66\n1\n70.62\n67.23\n80.26\n26.32\n65.54\n77.40\n64.56\n2\n73.45\n64.41\n81.58\n36.18\n55.93\n60.45\n62.00\n5\n71.75\n65.54\n84.21\n30.48\n61.58\n64.97\n63.09\nGPT-4.1\n-\n×\n\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\times\n0\n80.23\n68.93\n92.76\n48.90\n71.75\n68.36\n71.82\n1\n80.23\n65.54\n93.42\n46.49\n59.32\n58.76\n67.29\n2\n83.05\n66.10\n93.42\n45.18\n62.15\n58.19\n68.02\n5\n81.92\n66.10\n94.74\n45.83\n63.84\n59.89\n68.72\nGemma\n4B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n67.62\n37.05\n24.14\n0\n0.00\n55.88\n36.34\n36.84\n1\n67.93\n37.33\n32.70\n13.35\n59.23\n34.66\n40.87\n2\n67.90\n41.52\n0\n7.21\n11.68\n59.57\n50.28\n39.69\n5\n68.78\n43.67\n12.66\n13.35\n60.11\n65.68\n44.04\nGemma\n27B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n67.74\n56.11\n64.37\n0\n2.64\n58.78\n38.43\n48.01\n1\n67.99\n56.67\n59.71\n0\n8.52\n58.19\n38.23\n48.22\n2\n68.89\n56.73\n62.26\n10.28\n58.19\n35.36\n48.62\n5\n68.61\n56.73\n60.15\n12.91\n57.26\n35.67\n48.56\nLlama\n11B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n63.37\n42.08\n86.82\n16.13\n37.70\n51.04\n49.52\n1\n52.50\n40.49\n23.09\n0\n0.00\n65.34\n54.30\n39.29\n2\n24.97\n42.63\n29.30\n0\n4.80\n54.58\n52.73\n34.84\n5\n44.00\n36.49\n28.47\n0\n5.33\n51.15\n54.98\n36.74\nLlama\n90B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n53.04\n54.33\n90.25\n28.83\n59.71\n69.26\n59.24\n1\n61.43\n52.14\n71.24\n21.36\n66.58\n63.82\n56.10\n2\n58.45\n58.81\n88.93\n25.14\n60.70\n58.98\n58.50\n5\n61.29\n55.88\n88.58\n29.31\n65.73\n63.77\n60.76\nLlava\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n0\n9.37\n48.37\n86.02\n19.78\n21.37\n27.37\n35.38\n1\n10.64\n36.32\n83.13\n0\n2.34\n65.34\n61.82\n43.26\n2\n0\n7.86\n39.05\n81.72\n0\n1.61\n39.08\n50.87\n36.70\n5\n0\n7.91\n43.78\n85.99\n11.01\n50.25\n27.99\n37.82\nLlava\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n42.57\n60.19\n86.42\n27.37\n52.28\n50.56\n53.23\n1\n51.75\n57.26\n78.19\n17.99\n62.89\n62.16\n55.04\n2\n43.41\n59.32\n87.70\n29.04\n63.25\n50.11\n55.47\n5\n51.72\n58.39\n86.86\n27.45\n60.47\n49.27\n55.69\nPhi\n6B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n55.89\n51.88\n91.13\n19.87\n26.36\n35.48\n46.77\n1\n61.63\n42.46\n86.11\n12.10\n64.19\n53.07\n53.26\n2\n46.40\n49.69\n90.69\n16.31\n53.40\n55.21\n51.95\n5\n55.18\n47.72\n85.11\n23.16\n64.78\n65.14\n56.85\nQwen\n7B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n59.57\n49.97\n68.55\n40.27\n33.42\n44.63\n49.40\n1\n50.87\n51.83\n34.68\n11.12\n58.33\n45.02\n41.98\n2\n54.28\n51.40\n48.69\n15.74\n44.40\n43.39\n42.98\n5\n51.41\n55.21\n48.37\n22.83\n45.84\n39.53\n43.86\nQwen\n72B\n\\color[rgb]{0,0.7,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0.7,0}\\sqrt{}\n0\n66.69\n59.01\n84.09\n36.78\n57.06\n45.87\n58.25\n1\n67.80\n58.19\n76.16\n45.91\n60.19\n51.74\n60.00\n2\n67.82\n59.31\n89.10\n51.34\n53.07\n40.66\n60.22\n5\n67.09\n58.28\n89.19\n47.21\n55.21\n40.46\n59.57",
    "preview_text": "Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.\n\nAutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving\nZecong Tang\n1,∗\nZixu Wang\n1,∗\nYifei Wang\n1,∗\nWeitong Lian\n1,∗\nTianjian Gao\n1\nHaoran Li\n1\nTengju Ru\n1\nLingyi Meng\n1\nZhejun Cui\n1\nYichen Zhu\n1\nQi Kang\n1\nKaixuan Wang\n2\nYu Zhang\n1,†\n1\nZhejiang University, Hangzhou, China\n2\nThe University of Hong Kong, Hong Kong, China\n∗\nEqual contribution\n†\nCorresponding author\nAbstract\nAutonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, ",
    "is_relevant": true,
    "relevance_score": 6.0,
    "extracted_keywords": [
        "VLM",
        "decision-making",
        "autonomous driving",
        "benchmark",
        "explainability"
    ],
    "one_line_summary": "这篇论文提出了一个名为AutoDriDM的基准测试，专注于评估视觉语言模型在自动驾驶中的决策能力，并强调可解释性分析。",
    "detailed_summary": "## 论文摘要：AutoDriDM\n\n**1. 研究背景和动机**\n自动驾驶领域高度复杂，需要系统在复杂场景下进行可靠的感知与安全的决策。近年来，视觉-语言模型展现出强大的推理与泛化能力，为自动驾驶带来了新的可能性。然而，现有的评估基准和指标过度强调模型的感知能力，未能充分评估其决策过程。这导致模型在感知与决策之间存在能力鸿沟，且其推理过程如同“黑箱”，难以分析失败原因，阻碍了安全可靠的自动驾驶系统的发展。\n\n**2. 核心方法和技术创新**\n本文提出了 **AutoDriDM**，一个以决策为中心、可解释的自动驾驶VLM评估基准。其核心创新在于：\n- **渐进式三层评估协议**：构建了从**对象**（感知）、**场景**（理解）到**决策**的递进式评估框架，包含6个任务共计6,650个问题，旨在揭示从感知到决策的能力边界。\n- **风险感知与鲁棒性评估**：引入了**高风险场景**（1.6K个问题）和**视觉相似场景对**（60对）的专项评估，以测试模型在安全关键和易混淆情况下的表现。\n- **可解释性分析与自动化标注**：定义了九类推理错误（如逻辑错误、语义特征遗漏、幻觉等），并基于人工标注的推理轨迹，微调了一个轻量级（7B）的**分析器模型**，实现了对模型推理过程错误模式的大规模自动化标注与分析。\n\n**3. 主要实验结果**\n对主流开源与闭源VLM的评估表明：\n- **感知与决策脱节**：模型在对象感知（如Object-1/2）上表现尚可，但在场景理解（Scene-2）和决策任务上表现不佳，且感知与决策性能之间的相关性很弱。\n- **规模并非万能**：模型性能虽随参数规模增大而总体提升，但存在异常点（如InternVL-38B性能骤降），且即使是大模型，其决策也常基于有缺陷的推理逻辑。\n- **高风险场景表现矛盾**：尽管高风险数据稀缺，但主流模型在高风险场景下的决策得分反而高于普通场景，突显了当前评估的不足。\n- **鲁棒性不足**：在视觉相似场景对上，模型的联合决策正确率显著低于理论基线，表明其决策易受无关视觉线索干扰，缺乏因果推理能力。\n\n**4. 研究意义和价值**\nAutoDriDM填补了以感知为中心和以决策为中心的评估之间的空白。它首次系统性地评估并揭示了VLMs在自动驾驶决策任务上的能力边界与核心缺陷（如逻辑推理薄弱）。所提出的可解释性分析框架和自动化分析器，为模型优化提供了具体的改进方向。该基准为推动开发更安全、可靠、可解释的自动驾驶VLM系统提供了重要的评估工具和理论指导。",
    "qa_pairs": [
        {
            "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
            "answer": "## 论文核心分析：AutoDriDM\n\n### **一、 论文旨在解决的核心问题**\n当前自动驾驶领域对视觉-语言模型（VLMs）的评估存在三个主要**局限性**：\n1.  **评估重心偏颇**：现有基准和指标过度强调**感知能力**（如物体检测），而未能充分、系统地评估**决策制定过程**，而决策是自动驾驶的核心。\n2.  **评估维度割裂**：多数基准将感知、场景理解、决策等任务作为独立指标进行评估，**割裂了它们之间的内在依赖关系**，无法揭示模型从“看到”到“决定”的真实能力边界。\n3.  **评估过程黑箱化**：评估通常只关注最终答案的对错，**缺乏对模型推理过程的可解释性分析**，使得性能差距的根源（如逻辑错误、幻觉）难以诊断，限制了模型的改进。\n\n### **二、 论文的核心创新点**\n论文提出了 **AutoDriDM** 这一全新的基准测试框架，其创新性体现在以下三个层面：\n\n#### **1. 基准设计理念的创新：决策中心化与渐进式评估**\n- **决策中心化**：将评估焦点从传统的感知任务转向**安全决策能力**，直接针对自动驾驶的核心挑战。\n- **渐进式三级协议**：设计了 **Object（物体）→ Scene（场景）→ Decision（决策）** 的递进式评估层次。这模拟了人类驾驶的认知链条：先感知关键物体和状态，再理解整体场景上下文，最后综合信息做出驾驶决策。这种设计能**系统性探查模型从感知到决策的能力边界与断层**。\n- **丰富且有针对性的评估维度**：包含6个具体任务（每级2个），共计6,650个问题。例如：\n    - **Object-1**：识别对决策影响最大的物体。\n    - **Decision-2**：评估指定（可能次优）动作的安全性。\n\n#### **2. 评估深度与可解释性分析的创新**\n- **多维深度评估**：\n    - **风险场景分层**：引入五级风险评分，专门评估模型在**高风险（安全关键）场景**下的表现。\n    - **相似场景鲁棒性测试**：构建60对视觉相似但决策可能不同的场景，测试模型是否基于**因果理解**而非表面特征做决策。\n    - **感知-决策相关性分析**：利用三级协议，定量分析模型在感知任务和决策任务上表现的相关性，**实证揭示了当前VLMs感知能力强但决策能力弱、且两者关联度不高**的关键发现。\n- **系统化的可解释性分析**：\n    - **定义九类错误模式**：对模型的推理链（Chain-of-Thought）进行人工标注，归纳出如**逻辑推理错误（E1）、语义特征遗漏（E2）、模型幻觉（E3）** 等九大失败根源。\n    - **自动化分析器模型**：为了将可解释性分析规模化，论文**微调了一个轻量级（7B参数）的“分析器模型”**，能够自动对新的模型推理过程进行错误分类。该分析器性能优于包括GPT-4.1在内的通用大模型，为实现大规模、自动化的模型诊断提供了工具。\n\n#### **3. 实证研究的重要发现（方法带来的新见解）**\n论文通过在其基准上评估主流VLMs，得出了一系列超越传统基准的深刻结论，这些发现本身就是其方法价值的体现：\n- **关键发现1**：VLMs的**感知性能与决策性能相关性很弱**。模型在物体识别上得分高，并不意味着能做出安全决策。\n- **关键发现2**：模型在**高风险场景下的决策表现反而优于普通场景**，说明当前模型更依赖显著的危险信号，而非复杂的综合推理。\n- **关键发现3**：模型性能**并非随参数规模单调增长**（如InternVL-38B出现性能洼地），表明架构和训练对齐的重要性。\n- **关键发现4**：**即使答案正确，推理过程也常存在缺陷**，逻辑错误是最主要的失败模式。\n- **关键发现5**：**小样本学习对决策任务提升不稳定甚至有害**，提示需要更精巧的提示工程。\n\n### **三、 解决方案总结**\n论文通过构建一个**以决策为中心、具有渐进结构、包含多维度细粒度评估指标，并深度融合可解释性分析**的基准测试框架（AutoDriDM）来解决上述问题。具体路径为：\n1.  **构建新基准**：从多源真实驾驶数据集（nuScenes, KITTI, BDD100K）构建结构化评估集。\n2.  **执行系统评估**：在零样本/少样本下测试主流VLMs，不仅看准确率，更通过相关性分析、鲁棒性测试、尺度定律分析揭示深层能力边界。\n3.  **引入可解释性**：通过分析推理链定义错误类型，并开发自动化工具，将评估从“结果黑箱”推向“过程白盒”，为模型改进提供明确方向。\n\n### **实际价值**\n- **对学术界/工业界**：提供了一个**更全面、更严格、更贴近自动驾驶核心需求**的VLM评估工具，有助于引导研究方向，避免只在感知任务上“刷分”。\n- **对模型开发者**：提供的**错误分类和自动化分析器**能帮助快速定位模型弱点（如逻辑缺陷、幻觉），进行有针对性的优化。\n- **对自动驾驶安全**：强调了**决策能力评估和可解释性**的极端重要性，为推动研发更可靠、更透明的自动驾驶系统提供了基准和洞见。\n\n**总之，AutoDriDM的创新不在于提出新模型，而在于构建了一个能深刻“诊断”现有模型决策能力与缺陷的“体检系统”，填补了从感知评估到决策评估、从结果评估到过程评估的关键空白。**",
            "timestamp": "2026-01-27T16:01:54.175719",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
            "answer": "## AutoDriDM 论文分析：自动驾驶中视觉语言模型决策评估的发展脉络\n\n基于论文《AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving》，我们可以梳理出该研究方向的发展脉络。该领域的发展主要围绕 **“评估基准的演进”** 和 **“模型能力的边界探索”** 两条主线展开，每一步工作都试图解决前作的局限性。\n\n### 发展脉络与核心改进\n\n#### 1. 初期：感知为中心的基准（Pre-AutoDriDM）\n*   **代表工作**：早期的通用VLM基准（如MME, MMBench）及部分驾驶场景数据集。\n*   **核心焦点**：评估模型在**物体识别、场景分类、OCR**等**感知任务**上的能力。\n*   **主要局限性**：\n    *   **任务脱节**：评估指标与最终的**驾驶决策**关联性弱，高分感知能力未必能转化为安全决策。\n    *   **黑箱评估**：只关注最终答案对错，缺乏对模型**推理过程**的分析，无法定位失败根源。\n    *   **缺乏风险意识**：未专门针对**高风险、安全关键**场景进行系统评估。\n\n#### 2. 演进：引入决策任务，但分析不足\n*   **代表工作**：DriveLM、DriveMLLM等专注于自动驾驶的VLM基准。\n*   **核心改进**：在基准中开始纳入**规划、决策类任务**，试图连接感知与行动。\n*   **仍然存在的局限**（AutoDriDM论文指出）：\n    *   **指标独立性**：感知、场景理解、决策等任务的评估指标往往是**独立设计和平行评估**的，未能显式建模其**渐进依赖关系**（即感知为场景理解服务，二者共同支撑决策）。\n    *   **解释性缺失**：虽然有了决策任务，但依然缺乏对模型**产生该决策的推理链**进行大规模、系统化的错误归因分析。\n    *   **评估维度单一**：对模型在**相似场景下的鲁棒性**、**不同风险等级下的表现差异**、**模型规模扩展规律**等深层次能力边界探索不足。\n\n#### 3. 当前：AutoDriDM——决策中心化、可解释、多维度的基准\nAutoDriDM 针对上述所有局限性进行了系统性的改进，代表了该方向的最新进展。\n\n*   **改进1：设计渐进式、决策中心的评估协议**\n    *   **内容**：构建了 **Object -> Scene -> Decision** 三层渐进式评估框架。上层任务依赖于下层的正确感知/理解，从而**强制评估从感知到决策的转化能力**。\n    *   **价值**：首次明确且量化地评估了VLM的“感知-决策”链路，通过**相关性分析**揭示了二者弱关联的关键发现。\n\n*   **改进2：系统化的可解释性与失败模式分析**\n    *   **内容**：要求模型输出链式思考（CoT）过程，并定义了 **9类错误分类法**（如逻辑错误、语义特征遗漏、幻觉等），对推理过程进行人工和自动化标注分析。\n    *   **价值**：将评估从“黑箱”变为“白箱”，不仅能知道模型错了，还能知道**为什么错**（如主要失败模式是逻辑推理错误），为模型改进提供了直接方向。\n\n*   **改进3：多维度的能力边界探测**\n    *   **高风险场景评估**：专门标注和评估高风险场景（风险等级≥4），检验模型在安全关键情况下的表现。\n    *   **相似场景鲁棒性测试**：构建60对视觉相似但决策可能不同的场景，用“联合准确率”检验模型是基于**因果理解**还是**表面特征关联**做决策。\n    *   **模型缩放行为研究**：在InternVL模型族上发现性能并非随参数规模单调增长（如38B模型出现异常下降），并通过分析推理链揭示了**结构化但受限的推理**是导致性能倒退的原因，这对模型开发具有警示意义。\n\n*   **改进4：自动化分析工具的引入**\n    *   **内容**：微调了一个轻量级的 **7B“分析器模型”**，用于自动化、大规模地对模型推理过程进行错误分类。\n    *   **价值**：解决了人工分析成本高、难以扩展的问题，使得对海量测试结果进行深入的错误模式分析成为可能，**极大提升了评估的深度和效率**。\n\n### 着重发展的几个不同方向\n\n基于AutoDriDM的工作和启示，该领域未来可能朝以下几个重点方向发展：\n\n1.  **从静态到动态，从单帧到时序**：\n    *   **当前局限**：AutoDriDM基于单张前视图像，缺乏时序信息和多传感器（激光雷达、雷达）融合。\n    *   **发展方向**：下一代基准将必然纳入**视频序列**，评估VLM对运动预测、遮挡处理、交互意图理解等**动态决策**能力。多模态传感器融合的评估也将是关键。\n\n2.  **从封闭选择到开放生成**：\n    *   **当前局限**：使用选择题形式虽利于标准化评分，但限制了模型自由生成规划路径、解释说明的能力。\n    *   **发展方向**：设计结合**开放式生成任务**（如生成轨迹点、安全解释）与**基于规则或学习型评估器**的基准，以评估更贴近实际应用的端到端规划能力。\n\n3.  **从模型评估到模型训练与优化**：\n    *   **当前价值**：AutoDriDM的核心价值是**诊断**。\n    *   **发展方向**：其提供的细粒度错误分析（特别是通过分析器模型自动化产生）可以直接用于**构建高质量的训练数据**或设计**针对性的训练目标**（如减少逻辑错误、增强因果推理），从而形成“评估-诊断-改进”的闭环。\n\n4.  **从算法研究到系统安全验证**：\n    *   **当前定位**：目前仍是研究性基准。\n    *   **发展方向**：随着VLM在自动驾驶系统中集成度加深，此类基准可能发展为**系统安全验证流程**的一部分，用于对部署前的VLM模块进行合规性、安全性和鲁棒性的标准化测试。\n\n**总结**：AutoDriDM 通过其**决策中心化、渐进式结构、可解释性分析和多维度探测**的设计，标志着自动驾驶VLM评估从粗粒度的能力测评，进入了细粒度的**能力诊断与边界测绘**的新阶段。它不仅揭示了当前模型的根本缺陷（如感知-决策脱节），更为后续研究指明了数据、任务和评估方法上的具体改进方向。",
            "timestamp": "2026-01-27T16:02:47.655595",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
            "answer": "## AutoDriDM 论文分析：与前作的对比及主要改进\n\n根据论文内容，AutoDriDM 是针对自动驾驶中视觉-语言模型决策能力评估的**首个以决策为中心、可解释的基准测试**。其“前作”主要指该领域已有的其他评估基准或方法。论文在“Related Work”（第2节）和“Introduction”（第1节）中明确指出了现有工作的局限性，这些局限性正是 AutoDriDM 旨在解决的问题。\n\n以下表格详细梳理了 AutoDriDM 与代表性前作的区别及核心改进：\n\n| 对比维度 | **前作 (Existing Benchmarks)** | **AutoDriDM (本文工作)** | **核心区别与主要改进** |\n| :--- | :--- | :--- | :--- |\n| **1. 评估核心** | **以感知为中心**。多数基准（如通用VLM评测MME, MMBench；部分驾驶场景理解评测）侧重于评估模型的对象识别、场景分类、OCR等**感知能力**。 | **以决策为中心**。明确设计用于评估从感知到**驾驶决策**的完整推理链。 | **根本性转变**：从“模型看到了什么”转向“模型根据所看到的内容应该做什么”。这是为了填补现有评估与自动驾驶核心需求（安全决策）之间的关键空白。 |\n| **2. 任务与协议设计** | **任务独立或扁平化**。各项任务（如物体检测、场景描述、问答）通常被**独立评估**，指标之间缺乏关联。 | **渐进式三级协议**。设计了 **Object -> Scene -> Decision** 三级渐进式评估框架：<br>1. **Object级**：识别关键物体及其状态。<br>2. **Scene级**：理解天气、光照及特殊场景因素。<br>3. **Decision级**：做出最优驾驶动作并评估动作风险。 | **结构化与因果关联**：<br>- **创新点**：首次在驾驶基准中明确构建了“感知支持场景理解，二者共同支撑决策”的**渐进式依赖关系**。<br>- **改善**：使得评估能够分析模型如何将低级感知信息整合为高级决策，而非孤立地测试单项能力。 |\n| **3. 评估维度与指标** | **指标单一或解耦**。通常使用准确率等单一指标，且**感知指标与决策结果脱钩**，无法判断好的感知是否导向了安全的决策。 | **多维度综合指标与关联分析**。<br>- **6个具体任务指标**（Object-1/2, Scene-1/2, Decision-1/2）。<br>- **风险场景划分**：引入5级风险标注，专门评估高风险场景（>4级）下的表现。<br>- **相关性分析**：计算感知任务与决策任务得分的**皮尔逊相关系数**，量化“感知-决策”对齐程度。 | **深度诊断与安全聚焦**：<br>- **创新点**：系统性引入**风险感知评估**和**跨任务相关性分析**。<br>- **改善**：不仅能给出总分，还能揭示模型在安全关键场景下的泛化能力，以及其感知能力与决策能力是否弱相关（论文发现确实弱相关），这对实际部署至关重要。 |\n| **4. 可解释性与鲁棒性分析** | **黑箱评估居多**。大多只关注**最终答案的对错**，缺乏对模型推理过程的深入分析。对模型在相似场景下的决策一致性关注不足。 | **深度可解释性与鲁棒性测试**。<br>1. **推理过程分析**：要求模型生成链式思考（CoT），并定义了 **9类错误模式**（如逻辑错误、语义特征遗漏、幻觉等）进行人工和自动分析。<br>2. **相似场景鲁棒性测试**：构建60对视觉相似但决策可能不同的场景，评估模型是依赖因果特征还是表面特征。<br>3. **自动化分析器**：微调了一个7B的轻量级模型，用于**自动标注**推理过程中的错误类型，实现大规模可解释性评估。 | **从结果评估到过程诊断**：<br>- **重大创新**：将VLM评估从“判断对错”推进到“**诊断为何出错**”。<br>- **改善**：首次在驾驶基准中大规模、结构化地分析VLM的推理失败模式，并提供了自动化工具。鲁棒性测试直接验证了模型决策的因果性。 |\n| **5. 数据构建与规模** | **数据来源可能单一**，或问题设计未紧密贴合驾驶决策链。 | **多源、高质量、决策导向**。<br>- **数据源**：融合nuScenes, KITTI, BDD100K三大数据集，覆盖多样地域和场景。<br>- **规模**：6,650个问题，涵盖三级协议。<br>- **构建**：严格的人工标注与仲裁流程，确保质量。问题设计直接对应驾驶决策的关键环节（如“什么物体最关键？”“此时最佳动作是什么？”）。 | **规模与针对性提升**：提供了目前**规模最大、设计最系统**的驾驶VLM决策评估数据集。问题设计具有明确的驾驶语义和决策导向性。 |\n| **6. 模型行为洞察** | 通常止步于性能排名，对模型规模缩放规律、少样本学习的影响缺乏深入探索。 | **扩展性分析与少样本研究**。<br>- **缩放规律**：评估InternVL模型族（1B-78B），发现性能**并非随参数单调增长**（38B模型出现异常下降），并通过推理分析揭示了其“过度结构化但约束错误”的原因。<br>- **少样本学习**：系统测试了0/1/2/5-shot，发现少样本提示对决策任务的提升不稳定，甚至可能因偏差导致性能下降。 | **超越基准的洞察**：不仅是一个测试集，更是一个**分析工具**。揭示了VLM在驾驶决策任务上独特的缩放律和提示学习敏感性，为模型开发提供了具体指导（例如，盲目增大参数或添加示例可能无效）。 |\n\n### **总结：AutoDriDM 的核心技术创新与实际价值**\n\n1.  **理念创新**：首次明确提出并实现了面向自动驾驶的 **“决策中心化”评估范式**，将研究焦点从感知能力引向更核心、更困难的决策与推理能力。\n2.  **方法创新**：\n    - **渐进式三级评估协议**：建立了感知到决策的评估桥梁。\n    - **可解释性分析框架**：包含详细的错误分类学和自动化分析工具，打开了VLM驾驶决策的“黑箱”。\n    - **综合诊断套件**：集成了风险评估、相似场景鲁棒性测试、感知-决策相关性分析、缩放律研究等。\n3.  **实际价值**：\n    - **对学术界**：提供了一个强大的基准和分析平台，用于公平比较不同VLM的驾驶决策能力，并深入理解其失败模式，引导未来研究（如如何加强感知与决策的关联、减少逻辑幻觉）。\n    - **对工业界**：指出了当前最先进的VLM（包括GPT-4.1）的决策能力仍不足且不可靠，**感知好不等于决策好**。这为将VLM安全地集成到真实自动驾驶系统中提供了关键的安全边界认知和评估方法论。\n    - **推动方向**：论文结论明确指出，构建可靠的自动驾驶VLM需要超越单纯的规模缩放，关注**推理过程的可靠性、对因果特征的理解以及高风险场景下的泛化能力**。AutoDriDM 正是衡量这些关键维度的标尺。\n\n**简言之，AutoDriDM 不仅仅是一个新的“排行榜”，它是一套从评估理念、数据集构建、评估方法到深度分析工具的完整体系，旨在系统性地诊断和推动VLM在自动驾驶中的决策能力向更安全、更可靠、更可解释的方向发展。**",
            "timestamp": "2026-01-27T16:03:51.570304",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
            "answer": "好的，作为一名专业的学术论文分析助手，我将对《AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving》这篇论文进行深入分析，并详细阐述其提出的关键技术方法。\n\n## 论文核心技术创新点分析\n\n该论文的核心贡献在于构建了一个**以决策为中心、可解释、渐进式**的自动驾驶视觉语言模型（VLM）评测基准。其技术方法不仅体现在基准构建本身，还延伸至评估协议、可解释性分析和自动化工具。\n\n### 关键技术方法详述\n\n以下表格详细说明了论文提出的关键技术方法、其技术细节、数学原理及具体参数。\n\n| 方法类别 | 具体技术方法 | 技术细节与数学原理 | 具体参数与说明 |\n| :--- | :--- | :--- | :--- |\n| **1. 基准构建与数据结构** | **渐进式三级评估协议** | 设计了 **Object -> Scene -> Decision** 三级递进的任务结构。其数学原理基于**条件概率链式法则**的抽象：理想的决策 `P(Decision)` 应建立在正确的场景理解 `P(Scene|Object)` 和物体感知 `P(Object)` 之上。基准通过分层任务量化模型在每一级的性能，并分析级间相关性。 | - **Object级 (感知)**：2个任务（关键物体识别Object-1、物体状态判断Object-2），**单**选题。<br>- **Scene级 (理解)**：2个任务（天气光照Scene-1、特殊场景因素Scene-2），**多**选题。<br>- **Decision级 (决策)**：2个任务（最优动作Decision-1、指定动作风险评估Decision-2），**单**选题。<br>- **总题量**：6,650 道题。 |\n| | **高风险场景划分** | 引入**序数尺度风险评分**。由两名标注员根据预定义规则（见表5）独立对每个场景的短期碰撞可能性和严重性进行1-5分打分。最终风险分数为平均值 `r̄`。**数学上**，这定义了一个基于专家知识的离散风险函数：`R(image) = (r1 + r2) / 2`。 | - **评分规则**：1（最小风险）到 5（严重风险）。<br>- **高风险划分阈值**：`r̄ ≥ 4.0`。<br>- **结果**：筛选出 **1.6K** 个高风险问题用于专项评估。 |\n| | **视觉相似场景对鲁棒性测试** | 构建 **60对** 视觉相似但决策逻辑可能不同的场景。使用预训练的ResNet-50提取图像全局特征向量 **v**，计算**余弦相似度**筛选相似对：`sim(A, B) = (v_A · v_B) / (||v_A|| * ||v_B||)`。评估时计算**联合准确率**，并与**独立假设下的基线**（即单图准确率的平方）进行统计检验。**数学原理**：若模型依赖因果特征而非表面特征，则联合准确率应接近基线；显著低于基线（`p < 0.05`或`0.10`）则表明模型被无关的共享视觉线索误导。 | - **特征模型**：ResNet-50 (ImageNet预训练)，输出池化后的 **2048维** 特征向量。<br>- **相似度阈值**：`cosine_similarity > 0.9`。<br>- **评估指标**：`Acc_single` (单图决策准确率)， `Acc_joint` (配对双图均正确的概率)， `Baseline = (Acc_single)^2`。<br>- **显著性检验**：使用正态近似计算z值，`*` 表示 `p<0.10`, `**` 表示 `p<0.05`。 |\n| **2. 评估与可解释性框架** | **链式思维（CoT）推理收集** | 要求模型在输出最终答案前，生成结构化的推理过程。**提示工程**上，强制模型将推理放在 `<think>...</think>` 标签内，答案放在 `<answer>...</answer>` 标签内。这使得可以分离并分析模型的“思维过程”，即使最终答案正确，也可能发现推理缺陷。 | - **输出格式强制**：`<think>[详细推理步骤]</think><answer>[选项字母]</answer>`。<br>- **作用**：用于后续的**错误模式分析**和**自动化标注器**的训练数据收集。 |\n| | **错误分类学与可解释性分析** | 定义了 **9类错误** (E1-E9)，对CoT推理进行人工标注，形成分类体系。这是一个**多标签分类问题**，因为一次推理可能犯多种错误。主要类别包括：<br>- **E1: 逻辑推理错误**（违反因果或交通规则）。<br>- **E2: 语义特征遗漏**（忽略关键视觉线索）。<br>- **E3: 模型幻觉**（生成不存在的内容）。<br>**数学上**，这为每个模型的推理轨迹 `T` 分配了一个错误标签向量 `L(T) ∈ {0,1}^9`，其中 `L_i(T)=1` 表示犯第i类错误。 | **9类错误完整列表**：<br>1. 逻辑推理错误<br>2. 语义特征遗漏<br>3. 模型幻觉<br>4. 模态不平衡<br>5. 空间关系误判<br>6. 有限逻辑推断<br>7. 泛化缺陷<br>8. 决策边界不稳定<br>9. 目标优先级误判<br>**标注数据量**：约 **1,680** 条推理轨迹（1,500训练，180测试）。 |\n| | **自动化推理分析器** | 训练一个**轻量级VLM（7B参数）作为分类器**，自动为新的推理轨迹打上错误标签。**模型输入**是元组 `(I, Q, O, A_gt, A_pred, T_pred)`，其中 `I`为图像，`Q`为问题，`O`为选项，`A_gt`为真实答案，`A_pred`为模型答案，`T_pred`为模型推理。**模型输出**是错误标签集合（如“1,3”）。这是一个**序列到序列的多标签分类任务**，通过监督微调实现。 | - **基座模型**：Qwen2.5-VL-7B-Instruct。<br>- **训练数据**：1,500条人工标注的 `(输入, 输出标签)` 对。<br>- **评估指标**：<br>  - **精确匹配率**：预测标签集与真实标签集完全一致的比例。<br>  - **部分匹配率**：预测与真实有交集但不完全一致的比例。<br>  - **不匹配率**：预测与真实无交集的比例。<br>  - **平均分数**：综合评分（论文未给出具体公式，可能为F1或自定义分数）。<br>- **性能**：在180条测试数据上，精确匹配率达 **65.36%**，优于GPT-4.1 (48.09%) 和 Qwen-72B (50.25%)。 |\n| **3. 实验分析与度量** | **感知-决策相关性分析** | 计算**皮尔逊相关系数**，分析模型在**不同任务**上表现的一致性。对于每个模型，将其在所有样本上的6个任务得分（二进制正确与否，或连续分数）两两计算相关系数 `r`。公式为：<br>`r_{XY} = Σ[(x_i - μ_x)(y_i - μ_y)] / (n * σ_x * σ_y)`<br>其中 `X, Y` 代表两个任务（如Object-1和Decision-1），`x_i, y_i` 是模型在第i个样本上在该任务的表现，`μ`为均值，`σ`为标准差。**价值**：揭示模型能力是否连贯（感知好则决策好），结果发现相关性很弱 (`-0.2` 到 `0.2`)，说明当前VLM的感知优势**并未有效转化为**决策能力。 | - **分析维度**：在“所有场景”和“高风险场景”下，分别计算0/1/2/5-shot设置下的6x6相关矩阵。<br>- **关键发现**：<br>  - 任务间相关性普遍较弱。<br>  - 决策任务与物体感知任务呈**微弱正相关**，与场景理解任务相关性更弱。<br>  - 高风险场景下，小模型（7B）出现**负相关**（如Scene-Decision的 `r ≈ -0.23`），表明其能力不协调。 |\n| | **模型缩放行为验证** | 对InternVL模型族（1B, 2B, 4B, 8B, 26B, **38B**, 78B）进行性能评估。**挑战了传统的缩放定律**（性能随参数对数单调增长）。发现**38B模型出现性能异常下降**。通过分析其CoT，发现其推理过程存在**自相矛盾的约束**（如在Scene-2任务",
            "timestamp": "2026-01-27T16:05:04.289140",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
            "answer": "## AutoDriDM 评价指标与数据集分析\n\n### 一、 使用的数据集\n论文从三个公开的自动驾驶数据集中构建了 AutoDriDM 基准测试集。具体细节如下表所示：\n\n| 数据集 | 国家/地区 | 场景类型 | 覆盖的任务 | 使用的图像数 | 生成的QA条目数 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **nuScenes** | 美国波士顿；新加坡 | 城市街道、密集交通、多样化驾驶操作、多样化天气 | Object-1, Object-2, Decision-1, Decision-2 | 465 | 1908 |\n| **BDD100K** | 美国 | 住宅区、高速公路、城市街道、停车场、加油站、隧道 | **全部六个任务** (Object-1/2, Scene-1/2, Decision-1/2) | 450 | 3218 |\n| **KITTI** | 德国卡尔斯鲁厄及周边 | 城市街道、住宅区、校园、乡村道路、高速公路 | Object-1, Object-2, Decision-1, Decision-2 | 380 | 1520 |\n| **总计** | - | - | - | **1295** | **6650** |\n\n**数据集构建关键点：**\n- **多样性**：整合了来自北美、欧洲、亚洲的数据，覆盖城市、高速、乡村、不同天气/光照条件，以增强泛化性。\n- **去重**：使用 ResNet-50 提取图像特征，通过余弦相似度（阈值 0.9）过滤近重复图像，减少冗余。\n- **任务附着**：同一张驾驶场景图像可能对应多个不同层级的任务问题。\n\n### 二、 评价指标（六项任务）\nAutoDriDM 采用**渐进式三级评估协议**（Object -> Scene -> Decision），共定义了六项具体任务，每级两项。所有任务均为选择题形式。\n\n下表详细说明了六项评价指标的设计意图、任务描述和格式：\n\n| 层级 | 任务名称 | 任务描述与设计意图 | 问题格式 | 选项类型 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Object (感知)** | **Object-1** | **识别关键影响对象**：从图像中找出对当前驾驶决策**影响最大**的物体。旨在测试模型捕捉任务相关信息、过滤无关信息的能力，是推理链的起点。 | “当驾驶汽车时，图像中哪个物体对你的驾驶决策影响最大？” | 单选题 |\n| | **Object-2** | **推断对象状态**：针对指定的关键对象（如交通灯、车辆），推断其**当前状态**（如红绿灯颜色、车辆是否转向）。旨在评估模型从视觉观察进行推理的能力，为后续决策提供约束。 | “[指定对象] 的状态是什么？” | 单选题 |\n| **Scene (场景理解)** | **Scene-1** | **识别天气/光照条件**：判断图像中的整体环境条件（如白天、夜晚、雨天、雪天）。提供驾驶的**通用环境背景**，而非具体操作指令。 | “图像中的天气条件是什么？” | 多选题 |\n| | **Scene-2** | **识别特殊场景因素**：找出可能影响驾驶决策的**特殊场景元素**（如交通拥堵、事故现场、施工区域、学校路段）。旨在将关键信息提取从物体层面扩展到更广泛的场景上下文，为下游决策施加额外约束。 | “图像中哪些特殊场景可能影响主车的决策？” | 多选题 |\n| **Decision (决策)** | **Decision-1** | **选择最优驾驶动作**：在给定场景下，为自车选择**最佳**的驾驶行为（如直行、减速/急刹、左转、右转、变道、静止）。这是推理过程的终点，衡量模型的**核心决策能力**。 | “在当前情况下，你认为哪个决策是最好的选择？” | 单选题 |\n| | **Decision-2** | **评估指定动作的风险**：评估一个给定的（可能是次优的）动作（如“加速”）所面临的**风险等级**（绝对安全、中等风险、极高风险）。作为 Decision-1 的补充，更全面地评估模型的能力边界和安全意识。 | “如果你执行 [指定动作]，将面临什么等级的风险？” | 单选题 |\n\n### 三、 评价指标的创新性与实际价值\n1.  **决策中心化**：与现有侧重感知的基准不同，AutoDriDM 的核心是 **Decision-1** 和 **Decision-2**，直接评估驾驶的终极目标——**安全决策**。\n2.  **渐进式依赖**：三个层级并非独立，而是**层层递进**。Object 和 Scene 层级的答案共同为 Decision 层提供依据，这使得分析“感知-决策”的关联成为可能（通过相关性分析）。\n3.  **细粒度评估**：\n    - **风险分层**：根据专家标注的5级风险分数（1-5分），将场景分为常规和高风险（≥4分），专门评估模型在**安全关键场景**下的表现。\n    - **相似场景鲁棒性测试**：构建60对视觉相似的场景，通过比较**单图准确率**和**配对联合准确率**，检验模型是基于**因果理解**还是**表面特征关联**做决策。\n    - **可解释性分析**：要求模型输出链式思考（CoT）过程，并定义了**9类错误**（如逻辑错误、语义特征遗漏、幻觉等），用于深度分析失败模式。\n4.  **自动化分析工具**：论文微调了一个 **7B 参数的分析器模型**，能够自动对模型的推理过程进行错误分类，实现了大规模可解释性评估的自动化。\n\n**总结**：AutoDriDM 通过精心设计的**三级六任务指标体系**，结合**多源数据集**、**风险分层**和**可解释性框架**，首次为VLM在自动驾驶中的**决策能力**提供了一个系统、深入、可解释的评估基准，直指当前VLM“感知强、决策弱”的核心痛点，对推动可靠自动驾驶系统的研发具有重要指导意义。",
            "timestamp": "2026-01-27T16:05:52.379578",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
            "answer": "## 论文实验数据集、评估指标与性能分析\n\n### 一、 实验数据集\n论文构建的 **AutoDriDM** 基准测试集主要整合了三个广泛使用的自动驾驶公开数据集：\n- **nuScenes**：包含波士顿和新加坡的城市场景，交通密集，机动多样，天气多变。\n- **BDD100K**：美国场景，覆盖住宅区、高速公路、城市街道、停车场、加油站、隧道等。\n- **KITTI**：德国卡尔斯鲁厄及周边地区，包含城市街道、住宅区、校园、乡村道路、高速公路。\n\n**数据统计**：\n- 总计使用 **1,295张** 图像。\n- 生成 **6,650个** 问答对（覆盖6个任务）。\n- 此外，专门构建了 **60对** 视觉相似场景用于鲁棒性测试，并定义了 **1.6K个** 高风险场景问题。\n\n### 二、 主要评估指标\n评估采用**三级渐进式协议**，共**六个具体任务**，均为选择题形式：\n\n| 级别 | 任务 | 评估目标 | 题型 |\n| :--- | :--- | :--- | :--- |\n| **对象级 (Object)** | Object-1 | 识别对决策**影响最大的关键物体** | 单选 |\n| | Object-2 | 判断指定物体的**状态** | 单选 |\n| **场景级 (Scene)** | Scene-1 | 识别**天气和光照条件** | 多选 |\n| | Scene-2 | 识别可能影响决策的**特殊场景因素**（如施工、事故） | 多选 |\n| **决策级 (Decision)** | Decision-1 | 为自车选择**最优驾驶动作** | 单选 |\n| | Decision-2 | 评估指定（可能次优）动作的**安全风险等级** | 单选 |\n\n**核心评估维度**：\n1.  **零样本/少样本性能**：在0-shot、1-shot、2-shot、5-shot设置下测试。\n2.  **高风险场景性能**：针对风险评分≥4的场景单独评估。\n3.  **鲁棒性**：通过**视觉相似场景对**的“联合准确率”评估模型是否依赖因果推理而非表面特征。\n4.  **可解释性分析**：定义9类错误（如逻辑错误、特征遗漏、幻觉等），分析模型推理链的缺陷。\n5.  **感知-决策相关性分析**：计算不同任务得分间的皮尔逊相关系数，评估能力迁移。\n6.  **模型缩放规律**：分析模型参数量与性能的关系。\n\n### 三、 主要性能结果与结论\n#### 1. **整体性能排名 (0-shot)**\n- **最佳模型**：闭源模型 **GPT-4.1** 在平均分上领先（64.84%），其次是 **Gemini-2.5Pro**（57.36%）。\n- **最佳开源模型**：**Qwen2.5-VL-72B** 平均分为57.33%，与顶级闭源模型差距缩小。\n- **关键发现**：模型在**感知任务**（Object-1/2, Scene-1）上表现相对较好，但在需要复杂理解的**Scene-2**和**决策任务**（Decision-1/2）上表现显著下降，表明**感知能力强不代表决策能力强**。\n\n#### 2. **高风险场景表现**\n- 与全场景相比，主流模型在高风险场景下的**决策任务（Decision-1）准确率有显著提升**（例如GPT-4.1从56.95%提升至71.75%）。\n- 这表明**高风险场景中的关键物体更突出，决策逻辑更明确**，但同时也凸显了当前模型对普通复杂场景的决策能力不足。\n\n#### 3. **核心性能洞见与“提升”**\n论文的价值不在于单一指标的“提升”，而在于通过系统性评估揭示了以下关键洞见，为未来模型**优化指明了方向**：\n\n| 评估维度 | 核心发现与启示 | “提升”含义 |\n| :--- | :--- | :--- |\n| **感知-决策关联** | 相关性分析显示，**感知与决策得分间相关性很弱**（大多在-0.2~0.2）。模型规模增大（如Qwen 7B→72B）能**提升任务间稳定性**，但并未增强跨任务耦合。 | **揭示了当前VLM的核心缺陷**：感知能力无法有效转化为决策能力。未来工作需设计**促进感知-决策对齐的架构或训练目标**。 |\n| **鲁棒性** | 在视觉相似场景对上，几乎所有模型的**联合准确率都显著低于其单图准确率的平方**。这表明模型决策依赖于**表面的视觉关联而非因果理解**。 | **明确了鲁棒性不足的问题**。为提升鲁棒性，需要增强模型对**任务相关语义特征的提取和因果推理能力**。 |\n| **缩放规律** | 对InternVL模型族的测试发现，**性能并非随参数单调增长**。38B模型出现性能异常下降，分析发现其推理过程存在“过度结构化但约束错误”的问题。 | **挑战了“越大越好”的简单假设**。指出**模型推理能力与参数规模的匹配度**至关重要，为模型开发中的“能力对齐”提供了新视角。 |\n| **可解释性** | 即使答案正确，**推理链也常存在错误**。主要错误模式是：**逻辑推理错误(E1)** > **语义特征遗漏(E2)** > **模型幻觉(E3)**。更大模型（72B）能减少“决策边界不稳定”等错误。 | **深度诊断了模型失败根源**。为针对性地**修复模型推理缺陷**（如通过强化逻辑推理训练、减少幻觉）提供了直接依据。 |\n| **自动化分析** | 论文微调的 **7B 分析器模型**在错误分类任务上，**Exact Match (65.36%) 和 Avg Score (71.11%) 均超过了GPT-4.1等更大基线模型**。 | **实现了高效、可扩展的推理过程评估**，为大规模自动化模型诊断提供了实用工具。 |\n\n### 总结\n**AutoDriDM** 的核心贡献是提供了一个**系统性评估框架**，其“性能提升”体现在：\n1.  **诊断深度**：首次大规模、细粒度地揭示了VLM在自动驾驶决策任务上的**能力边界与失败模式**。\n2.  **评估维度**：超越了传统仅看最终准确率的评估，涵盖了**渐进能力、鲁棒性、相关性、可解释性**等多维度。\n3.  **指导价值**：所有发现（如感知-决策脱节、因果推理弱、缩放非单调）都为未来开发**更安全、更可靠**的自动驾驶VLM提供了明确的技术改进方向。论文本身并未追求在某个指标上超越前作，而是填补了该领域**决策中心化评估基准的空白**。",
            "timestamp": "2026-01-27T16:06:47.363706",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
            "answer": "## 论文主要局限性分析\n\n基于对《AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving》的深入分析，该研究存在以下主要局限性：\n\n### 1. **数据范围有限**\n   - **传感器单一性**：仅使用**前向单目摄像头图像**（来自nuScenes、KITTI、BDD100K），缺乏：\n     - **多摄像头系统**（环视感知）\n     - **多模态传感器融合**（如LiDAR、雷达数据）\n     - **时序信息**（视频序列），限制了模型对动态场景和遮挡处理的评估能力\n\n### 2. **任务形式简化**\n   - **仅支持选择题**：采用单/多选题形式以降低歧义、便于客观评分，但可能：\n     - **低估模型能力**：无法评估模型生成自由形式解释和复杂推理的能力\n     - **引入答案先验偏差**：选项设计可能影响模型决策分布\n\n### 3. **模型覆盖与评估设置**\n   - **模型代表性有限**：虽然评估了主流开源/闭源模型及Internvl系列，但：\n     - **商业模型更新快**：API版本变化可能影响结果可复现性\n     - **推理参数未充分探索**：如温度、生成长度等超参数的影响未系统研究\n   - **场景复杂度不足**：缺乏极端天气、强烈光照变化等挑战性场景\n\n### 4. **评估维度缺失**\n   - **实时性要求未考虑**：自动驾驶决策需要低延迟，但基准未评估推理速度或实时性能\n   - **多智能体交互**：缺乏对交通参与者（其他车辆、行人）意图预测和交互决策的评估\n\n---\n\n## 未来可能的改进方向\n\n### 1. **数据与任务扩展**\n   ```markdown\n   - **多模态数据融合**：集成LiDAR点云、雷达数据、GPS信息，构建更接近真实系统的多传感器基准\n   - **时序任务设计**：引入视频片段，评估模型对动态场景、轨迹预测和时序推理的能力\n   - **自由形式问答**：增加开放式问题，评估模型生成自然语言解释和复杂规划的能力\n   ```\n\n### 2. **评估体系深化**\n   - **实时性能指标**：增加**推理延迟**、**吞吐量**等效率指标，更贴近实际部署需求\n   - **交互与安全评估**：\n     - 设计**对抗性场景**（如遮挡、罕见物体）\n     - 引入**安全边界测试**（决策在临界条件下的稳定性）\n   - **因果推理强化**：构建更多“相似场景对”，更系统评估模型对因果特征的识别能力\n\n### 3. **模型与架构创新**\n   - **专有架构探索**：推动针对自动驾驶的**VLM专用架构**设计，例如：\n     - 引入**空间注意力机制**强化对关键区域的感知\n     - 设计**分层推理模块**显式建模“感知→场景理解→决策”的递进关系\n   - **多任务联合训练**：探索在统一框架下联合优化感知、预测、决策任务，减少级联误差\n\n### 4. **可解释性与可靠性提升**\n   - **自动化分析工具扩展**：将训练的**分析器模型**（7B）扩展为更通用的推理诊断工具，支持：\n     - **错误根因追溯**\n     - **不确定性量化**\n     - **决策置信度评估**\n   - **人类对齐研究**：将模型决策过程与**人类专家决策轨迹**对齐，提升决策的可解释性和可信度\n\n### 5. **基准生态建设**\n   - **动态更新机制**：建立基准的**持续迭代机制**，定期纳入新场景、新任务\n   - **开源与标准化**：全面开源基准数据、评估代码和分析工具，推动社区标准化评估流程\n   - **跨领域迁移评估**：探索模型在**其他具身智能领域**（如机器人、无人机）的泛化能力\n\n---\n\n## 总结\nAutoDriDM在填补自动驾驶VLM**决策评估空白**方面做出了重要贡献，但其局限性也为未来研究指明了方向。**核心挑战**在于如何构建更**逼真**、更**全面**、更**可靠**的评估体系，从而推动VLM从“感知能力强”向“决策可信任”的实质跨越。未来的工作应重点关注**多模态时序数据整合**、**因果推理能力评估**以及**面向实际部署的效率与安全约束**。",
            "timestamp": "2026-01-27T16:07:24.855837",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T06:29:09Z",
    "created_at": "2026-01-27T15:53:18.261526",
    "updated_at": "2026-01-27T15:53:18.261534"
}