{
    "id": "2601.14352v1",
    "title": "RoboBrain 2.5: Depth in Sight, Time in Mind",
    "authors": [
        "Huajie Tan",
        "Enshen Zhou",
        "Zhiyu Li",
        "Yijie Xu",
        "Yuheng Ji",
        "Xiansheng Chen",
        "Cheng Chi",
        "Pengwei Wang",
        "Huizhu Jia",
        "Yulong Ao",
        "Mingyu Cao",
        "Sixiang Chen",
        "Zhe Li",
        "Mengzhen Liu",
        "Zixiao Wang",
        "Shanyu Rong",
        "Yaoxu Lyu",
        "Zhongxia Zhao",
        "Peterson Co",
        "Yibo Li",
        "Yi Han",
        "Shaoxuan Xie",
        "Guocai Yao",
        "Songjing Wang",
        "Leiduo Zhang",
        "Xi Yang",
        "Yance Jiao",
        "Donghai Shi",
        "Kunchang Xie",
        "Shaokai Nie",
        "Chunlei Men",
        "Yonghua Lin",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Shanghang Zhang"
    ],
    "abstract": "æˆ‘ä»¬æ¨å‡ºæ–°ä¸€ä»£å…·èº«äººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹RoboBrain 2.5ï¼Œè¯¥æ¨¡å‹é€šè¿‡é«˜è´¨é‡æ—¶ç©ºç›‘ç£çš„å¤§è§„æ¨¡è®­ç»ƒï¼Œåœ¨é€šç”¨æ„ŸçŸ¥ã€ç©ºé—´æ¨ç†ä¸æ—¶é—´å»ºæ¨¡æ–¹é¢å–å¾—çªç ´æ€§è¿›å±•ã€‚åœ¨ç»§æ‰¿å‰ä»£æ¶æ„çš„åŸºç¡€ä¸Šï¼ŒRoboBrain 2.5å®ç°äº†ä¸¤å¤§æ ¸å¿ƒèƒ½åŠ›å‡çº§ï¼šå…¶ä¸€æ˜¯å¼€å¯ç²¾ç¡®ä¸‰ç»´ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ä»äºŒç»´åƒç´ ç›¸å¯¹å®šä½è½¬å‘æ·±åº¦æ„ŸçŸ¥åæ ‡é¢„æµ‹ä¸ç»å¯¹åº¦é‡çº¦æŸç†è§£ï¼Œåœ¨ç‰©ç†çº¦æŸä¸‹ç”Ÿæˆå®Œæ•´çš„ä¸‰ç»´æ“ä½œè½¨è¿¹ä½œä¸ºæœ‰åºå…³é”®ç‚¹åºåˆ—ï¼›å…¶äºŒæ˜¯å»ºç«‹ç¨ å¯†æ—¶é—´ä»·å€¼ä¼°è®¡æœºåˆ¶ï¼Œæä¾›è·¨å¤šè§†è§’çš„ç¨ å¯†æ­¥è¿›æ„ŸçŸ¥è¿›åº¦é¢„æµ‹ä¸æ‰§è¡ŒçŠ¶æ€ç†è§£ï¼Œä¸ºä¸‹æ¸¸å­¦ä¹ ç”Ÿæˆç¨³å®šçš„åé¦ˆä¿¡å·ã€‚è¿™ä¸¤é¡¹å‡çº§å…±åŒæ¨åŠ¨è¯¥æ¡†æ¶å‘æ›´å…·ç‰©ç†åŸºç¡€å’Œæ‰§è¡Œæ„ŸçŸ¥èƒ½åŠ›çš„å…·èº«æ™ºèƒ½æ¼”è¿›ï¼Œä»¥åº”å¯¹å¤æ‚ç²¾ç»†çš„æ“ä½œä»»åŠ¡ã€‚ç›¸å…³ä»£ç ä¸æ¨¡å‹æƒé‡å·²åœ¨é¡¹ç›®ç½‘ç«™å‘å¸ƒï¼šhttps://superrobobrain.github.io",
    "url": "https://arxiv.org/abs/2601.14352v1",
    "html_url": "https://arxiv.org/html/2601.14352v1",
    "html_content": "\\contribution\nPlease see\nContributions and Author List\nfor more author details.\nRoboBrain 2.5: Depth in Sight, Time in Mind.\nBAAI RoboBrain Team\nAbstract\nWe introduce\nRoboBrain 2.5\n, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks\nPrecise 3D Spatial Reasoning\nby shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes\nDense Temporal Value Estimation\nthat provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website:\nhttps://superrobobrain.github.io\n.\nFigure 1\n:\nNew Features of RoboBrain 2.5.\nTop: Precise 3D spatial reasoning with depth-aware grounding, metric measuring, and full manipulation trace generation under physical constraints. Bottom: Dense temporal value estimation for step-aware progress/regress prediction from state transitions across viewpoints and tasks; radar plots summarize performance gains on 2D/3D spatial and temporal benchmarks.\nContents\n1\nIntroduction\n2\nNew Feature\n2.1\nPrecise 3D Spatial Reasoning\n2.1.1\n3D Spatial Referring, Measuring, and Tracing\n2.1.2\n3D Task Formulation\n2.2\nDense Temporal Value Estimation\n2.2.1\nHop-wise Progress Construction\n2.2.2\nMulti-Perspective Progress Fusion\n2.2.3\nBi-directional Consistency Checking\n3\nTraining Data\n3.1\nGeneral MLLM Data\n3.2\nSpatial Reasoning Data\n3.3\nTemporal Prediction Data\n4\nTraining Strategy\n4.1\nStage 1: Foundational Spatiotemporal Learning\n4.2\nStage 2: Specific Spatiotemporal Enhancement\n5\nInfrastructure\n5.1\nHybrid Parallelism\n5.2\nDynamic pre-Allocated Memory\n5.3\nCross-Accelerator Training and Inference\n6\nEvaluation Results\n6.1\n2D Spatial Reasoning Capability\n6.2\n3D Spatial Reasoning Capability\n6.3\nTemporal Value Estimation\n7\nConclusion and Future Works\n8\nContributions and Author List\n9\nQualitative examples\n9.1\nExamples on 3D Spatial Reasoning\n9.2\nExamples on Temporal Value Estimation\n10\nProof of Bounded Global Progress\n1\nIntroduction\nEmbodied AI foundation models have rapidly advanced in bridging language, vision, and action, enabling the generation of actionable plans from natural language instructions and visual observations\n[\n7\n,\n73\n,\n32\n]\n. However, a critical gap persists. While these models often succeed in curated demonstrations, they frequently falter during rigorous real-world deployments. This reliability issue stems from the challenge of translating high-level semantic reasoning into physically grounded manipulation. Real-world tasks are unforgiving. They demand that robots respect absolute metric constraints, operate robustly under occlusions and viewpoint shifts, and continuously self-correct in a closed loop. Unfortunately, these precise physical capabilities remain beyond the reach of current semantic planners.\nThese requirements expose two fundamental limitations in current generalist models.\nFirst, on the spatial dimension\n, models suffer from â€œmetric blindness.â€ Grounding is typically restricted to 2D pixel coordinates or weak topological representations\n[\n9\n,\n13\n,\n85\n]\n. Lacking absolute depth and scale information, such outputs inherently fail to ensure physical compliance. Specifically, they cannot guarantee millimeter-level clearance or generate collision-free 3D trajectories which are critical for precise interaction.\nSecond, on the temporal dimension\n, models usually operate as â€œopen-loopâ€ predictors. They treat action generation as a static sequence prediction task without an intrinsic mechanism to monitor execution progress. Relying on sparse external supervision such as success labels\n[\n53\n,\n2\n]\n, the agent remains oblivious to intermediate failures like slippage or regression. This limitation makes adaptive recovery impossible in long-horizon tasks.\nTo bridge this gap, embodied foundation models must undergo a paradigm shift from semantic reasoners to physically-grounded agents. This evolution requires two precise upgrades. Spatial reasoning must advance from 2D pointing to precise 3D planning to satisfy metric constraints. Simultaneously, temporal modeling must shift from open-loop generation to dense value estimation to ensure closed-loop reliability.\nTo realize this vision, we present\nRoboBrain 2.5\n. Building upon the robust general perception and reasoning capabilities of its predecessor\n[\n33\n,\n72\n]\n, this next-generation model introduces critical upgrades to align internal representations with physical reality. Through large-scale training on high-quality spatiotemporal data, RoboBrain 2.5 achieves a comprehensive upgrade in core capabilities:\nâ€¢\nSpatial: Depth in Sight (Precise 3D Spatial Reasoning).\nWe extend the spatial interface from 2D grounding to depth-aware coordinate prediction and full manipulation trace generation. Instead of predicting a single target point, the model learns to output an ordered sequence of keypoints that describes the complete manipulation procedure, thereby naturally encoding spatial planning. This capability is built via a curriculum of three complementary skills:\n(1) 3D Spatial Referring\nto localize objects;\n(2) 3D Spatial Measuring\nto estimate absolute metric quantities (\ne.g\n.\n, distance, clearance) required by physical constraints; and\n(3) 3D Spatial Trace Generation\nto produce collision-free keypoint traces. Crucially, this is achieved by standardizing supervision into a decoupled\n(\nu\n,\nv\n,\nd\n)\n(u,v,d)\nrepresentation convertible to 3D via camera intrinsics, leveraging large-scale, high-quality 3D supervision across diverse scenes.\nâ€¢\nTemporal: Time in Mind (Dense Temporal Value Estimation).\nIn parallel, we establish a breakthrough in temporal modeling that provides immediate, step-aware feedback robust to viewpoint variations. The objective is to estimate the execution state (progress, stagnation, regression, or error) using only visual observations. We implement this by modeling general reward on multi-view expert trajectories using\nhop-normalized temporal transition labels\n. This formulation normalizes progress by the remaining distance to the goal, producing bounded and stable supervision signals even with dense sampling. Furthermore, we employ multi-perspective fusion to aggregate value predictions, significantly improving robustness under occlusion. Consequently, RoboBrain 2.5 provides dense progress tracking that serves as a high-fidelity reward signal for downstream reinforcement learning.\nâ€¢\nSynergy and Impact.\nCrucially, RoboBrain 2.5 integrates these physical capabilities without sacrificing the general interactive reasoning of the original architecture. By imparting â€œDepth in Sightâ€ to ensure kinematic feasibility and â€œTime in Mindâ€ to ensure execution robustness, our model successfully bridges the reliability gap. Extensive experiments on serious benchmarks demonstrate state-of-the-art performance. Furthermore, real-world evaluations confirm superior zero-shot robustness in contact-rich tasks, effectively translating demo-level success into deployment-level reliability.\n2\nNew Feature\nBuilding upon the foundation of RoboBrain 2.0\n[\n72\n]\nand utilizing the Qwen3-VL architecture\n[\n8\n]\n, RoboBrain 2.5 introduces two core enhancements that further advance physical intelligence.\nSpecifically, we first detail the concept of\nPrecise 3D Spatial Reasoning\n(\nSection\nËœ\n2.1\n), which encompasses three metric-grounded competencies,-spatial referring, measuring, and tracingâ€”derived solely from monocular RGB inputs\n[\n86\n]\n.\nWe then describe\nDense Temporal Value Estimation\n(\nSection\nËœ\n2.2\n), which learns a general-purpose, step-aware process modeling from multi-view RGB-only observations\n[\n67\n]\n.\n2.1\nPrecise 3D Spatial Reasoning\nFor embodied agents to interact effectively with the physical world, they must accurately interpret and act upon spatial information. This necessitates a deep understanding of object locations, inter-object relationships, and precise metric quantities from visual observations. To address these fundamental requirements, we introduce a robust framework for\nPrecise 3D Spatial Reasoning\n.\n2.1.1\n3D Spatial Referring, Measuring, and Tracing\nEmbodied robots usually have to execute actions based on increasingly complex, spatially constrained instructions\n[\n85\n,\n68\n,\n71\n,\n1\n,\n34\n,\n70\n,\n10\n,\n11\n]\n, such as â€œ\nWater flowers from left to right with watering can hovering 1â€“5 cm above each one\nâ€ in\nFigure\nËœ\n1\n, where recent data-scarce Vision-Language-Action (VLA) models fail to master.\nIn this case, it would be beneficial to generate a 3D positional sequence, named as\n3D spatial trace\n, as an intuitive bridge to interpret the instruction following procedure in 3D space and guide the generation of actual action trajectories for robots.\nHowever, this surrogate task (\ni.e\n.\n,\n3D spatial tracing\n) is inherently challenging as it requires multi-step, metric-grounded reasoning in complex 3D scenes.\nTo be specific, each reasoning step requires two key components:\n(1)\n3D spatial referring\nto resolve spatial relationships and accurately localize objects involved in the trace generation (\ne.g\n.\n, identifying flowers with their from left to right order and locating them).\n(2)\n3D spatial measuring\nto understand absolute, real-world metric quantities related to the trace in captured scene (\ne.g\n.\n, quantifying each flowerâ€™s physical height and 1â€“5 cm height above each).\nTo this end, we equip RoboBrain 2.5 with these three key capabilities, enabling it to directly predict metric-grounded outputs from monocular images under spatial constraints for direct interaction with the 3D physical world.\n2.1.2\n3D Task Formulation\nWe formalize 3D spatial tracing as the process of predicting an ordered sequence of 3D points\nÏ„\n=\n{\np\nt\n}\nt\n=\n1\nT\n\\tau=\\{p_{t}\\}_{t=1}^{T}\nâ€”each point\np\nt\n=\n(\nu\nt\n,\nv\nt\n,\nd\nt\n)\np_{t}=(u_{t},v_{t},d_{t})\ncomprising image-plane coordinates\n(\nu\nt\n,\nv\nt\n)\n(u_{t},v_{t})\nand absolute depth\nd\nt\nd_{t}\nâ€”from visual inputs (\ne.g\n.\n, RGB images) and textual instructions via vision-language models.\nThe resulting trace\nÏ„\n\\tau\nfunctions as a spatial plan for guiding entities (\ne.g\n.\n, a robot end-effector or an object) to execute instructions.\nCrucially, these instructions typically encode both 3D spatial referring and 3D spatial measuring, often requiring multi-step compositional reasoning.\nFor instance, in\nFigure\nËœ\n1\n, the instruction â€œ\nWater flowers from left to right with watering can hovering 1-5 cm above each flower\nâ€ necessitates determining the 3D positions and heights of all flowers in the scene.\nAlthough intermediate spatial cues (\ne.g\n.\n, points identified through 3D spatial referring) may not coincide with the final keypoints used in the spatial trace, they provide essential evidence for multi-step reasoningâ€”thereby enabling precise trace generation under spatial constraints at the start, the end, and along the trajectory.\nAt the core of our approach lies a task formulation designed to facilitate training and to leverage diverse data sources effectively.\nRather than predicting 3D coordinates in the form\n(\nx\n,\ny\n,\nz\n)\n(x,y,z)\nin camera or world frame, we adopt a decoupled\n(\nu\n,\nv\n,\nd\n)\n(u,v,d)\nrepresentation, which can be trivially projected to 3D coordinates using known camera intrinsics.\nThis formulation is especially advantageous in embodied scenarios where camera parameters are readily accessible, as it obviates the need for vision-language models to learn camera geometry implicitly. Such an approach streamlines training and enhances accuracy.\nFurthermore, the\n(\nu\n,\nv\n,\nd\n)\n(u,v,d)\nrepresentation can be straightforwardly projected into lower-dimensional subspaces.\nFor instance, omitting\nd\nd\nyields a 2D visual trace (\ni.e\n.\n, a sequence of points in the image plane), while retaining only the start and end points produces 3D or 2D spatial referring data (if depth is further removed).\nThis flexibility not only promotes data reusability but also ensures compatibility with existing 2D datasets\n[\n23\n,\n85\n]\n, thereby boosting multi-task learning performance through co-training across complementary tasks and modalities.\n2.2\nDense Temporal Value Estimation\nEffective execution of long-horizon manipulation tasks demands more than just a final success signal; it requires continuous, granular feedback to guide the agent through complex intermediate states\n[\n52\n,\n3\n,\n54\n,\n80\n,\n15\n]\n. To address the limitations of sparse feedback, we introduce\nDense Temporal Value Estimation\n, a vision-based mechanism that provides real-time, step-aware progress assessments as temporal value feedback, enabling robust closed-loop control and efficient RL.\n2.2.1\nHop-wise Progress Construction\nCentral to our approach is the formulation of value estimation as task progress; thus, our model functions as a vision-language estimator designed to infer fine-grained, real-time progress from visual inputs. To guarantee generalizability across diverse embodiments and task families, we implement a three-stage data curation pipeline handling diverse data origins. This process spans from raw video segmentation to a systematic, hop-based labeling strategy, as detailed below:\nStep-wise task progress discretization.\nGiven raw multi-view video trajectories, we first segment each expert trajectory into sub-tasks using human-annotated multi-view keyframes\n{\nK\n0\n,\nK\n1\n,\nâ€¦\n,\nK\nN\n}\n\\{K_{0},K_{1},\\dots,K_{N}\\}\n, where\nK\n0\nK_{0}\nis the initial observation,\nK\nN\nK_{N}\nis the final success observation, and each\nK\nj\nK_{j}\nis a set of synchronized multi-view keyframes. To obtain dense supervision, we perform adaptive sampling within each segment. For a trajectory with\nL\nL\nframes per view, we set a chunk size\nC\nC\nto determine the total number of sampled points and distribute them uniformly across the\nN\nN\nsegments. The number of intermediate points\nm\nm\nwithin segment\n[\nK\nj\n,\nK\nj\n+\n1\n]\n[K_{j},K_{j+1}]\nis:\nm\n=\nâŒŠ\n1\nN\nâ€‹\nâŒŠ\nL\nC\nâŒ‹\nâŒ‹\n.\nm=\\left\\lfloor\\frac{1}{N}\\left\\lfloor\\frac{L}{C}\\right\\rfloor\\right\\rfloor.\n(1)\nThis yields a sequence of states\nğ’®\n=\n{\ns\n0\n,\ns\n1\n,\nâ€¦\n,\ns\nM\n}\n\\mathcal{S}=\\{s_{0},s_{1},\\dots,s_{M}\\}\n,\nwhere each state\ns\ni\ns_{i}\nis a set of synchronous multi-view visual observations.\nWe then define the ground-truth global progress as\nÎ¦\nâ€‹\n(\ns\ni\n)\n=\ni\n/\nM\n\\Phi(s_{i})=i/M\n.\nHop-based relative progress normalization.\nA naive choice is to regress the progress gain\nÎ¦\nÎ´\nâ€‹\n(\ns\np\n,\ns\nq\n)\n=\nÎ¦\nâ€‹\n(\ns\nq\n)\nâˆ’\nÎ¦\nâ€‹\n(\ns\np\n)\n\\Phi_{\\delta}(s_{p},s_{q})=\\Phi(s_{q})-\\Phi(s_{p})\nbetween two states, but iterating such predictions accumulates error and can push the reconstructed\nÎ¦\nâ‹†\nâ€‹\n(\ns\n)\n\\Phi^{\\star}(s)\noutside\n[\n0\n,\n1\n]\n[0,1]\n. Instead, we introduce a hop-based formulation that learns\nrelative-relative progress\nand naturally supports dense temporal value estimation. Each training sample is a tuple\nğ’Ÿ\n\\mathcal{D}\ncontaining a task description\nd\ntask\nd_{\\text{task}}\n, the initial state\ns\n0\ns_{0}\n, the goal state\ns\nM\ns_{M}\n, a\nâ€œBEFOREâ€\nstate\ns\np\ns_{p}\n, an\nâ€œAFTERâ€\nstate\ns\nq\ns_{q}\n, and a hop label\nâ„‹\nâ€‹\n(\ns\np\n,\ns\nq\n)\n\\mathcal{H}(s_{p},s_{q})\nthat normalizes the progress from\ns\np\ns_{p}\nto\ns\nq\ns_{q}\nrelative to the full task span from\ns\n0\ns_{0}\nto\ns\nM\ns_{M}\n. Given\nÎ¦\nâ€‹\n(\ns\np\n)\n\\Phi(s_{p})\nand\nÎ¦\nâ€‹\n(\ns\nq\n)\n\\Phi(s_{q})\n, we define:\nâ„‹\nâ€‹\n(\ns\np\n,\ns\nq\n)\n=\n{\nÎ¦\nâ€‹\n(\ns\nq\n)\nâˆ’\nÎ¦\nâ€‹\n(\ns\np\n)\nÎ¦\nâ€‹\n(\ns\nM\n)\nâˆ’\nÎ¦\nâ€‹\n(\ns\np\n)\nif\nâ€‹\nq\nâ‰¥\np\nâ€‹\n(progress)\nÎ¦\nâ€‹\n(\ns\nq\n)\nâˆ’\nÎ¦\nâ€‹\n(\ns\np\n)\nÎ¦\nâ€‹\n(\ns\np\n)\nâˆ’\nÎ¦\nâ€‹\n(\ns\n0\n)\nif\nâ€‹\nq\n<\np\nâ€‹\n(regress)\n.\n\\mathcal{H}(s_{p},s_{q})=\\begin{cases}\\dfrac{\\Phi(s_{q})-\\Phi(s_{p})}{\\Phi(s_{M})-\\Phi(s_{p})}&\\text{if }q\\geq p\\textsc{ (progress)}\\\\\n\\\\\n\\dfrac{\\Phi(s_{q})-\\Phi(s_{p})}{\\Phi(s_{p})-\\Phi(s_{0})}&\\text{if }q<p\\textsc{ (regress)}.\\end{cases}\n(2)\nThis dynamically scales the supervision into\n[\nâˆ’\n1\n,\n1\n]\n[-1,1]\n: for forward progress, the change is normalized by the remaining distance to the goal; for regression, by the distance already covered from the initial state. A key theoretical advantage is that, when global progress is reconstructed by iteratively applying predicted hops, the resulting\nÎ¦\nâ‹†\nâ€‹\n(\ns\n)\n\\Phi^{\\star}(s)\nis guaranteed to remain strictly within\n[\n0\n,\n1\n]\n[0,1]\n. Please refer to\nSection\nËœ\n10\nfor the proof.\nSampling strategy and data balancing.\nFor each trajectory, we construct a balanced set of hop-based training samples. Continuous hop values are first discretized into\nN\nhop\nN_{\\text{hop}}\nhop bins. The temporal distance between the\nâ€œBEFOREâ€\nstate\ns\np\ns_{p}\nand\nâ€œAFTERâ€\nstate\ns\nq\ns_{q}\nin each pair is then chosen from\nN\ndis\nN_{\\text{dis}}\ndistance bins within each hop bin, yielding in total\nN\nhop\nÃ—\nN\ndis\nN_{\\text{hop}}\\times N_{\\text{dis}}\nnon-trivial transitions. To reduce bias toward static segments, we further introduce an additional fraction\nÎ±\n\\alpha\nof samples explicitly labeled as zero-hop (i.e.,\nâ„‹\nâ€‹\n(\ns\np\n,\ns\nq\n)\n=\n0\n\\mathcal{H}(s_{p},s_{q})=0\n), constructed by selecting pairs\n(\ns\np\n,\ns\nq\n)\n(s_{p},s_{q})\nwhose progress change is below a small threshold\nÏµ\n\\epsilon\n:\n|\nÎ¦\nâ€‹\n(\ns\nq\n)\nâˆ’\nÎ¦\nâ€‹\n(\ns\np\n)\n|\nâ‰¤\nÏµ\n.\n|\\Phi(s_{q})-\\Phi(s_{p})|\\leq\\epsilon.\n(3)\n2.2.2\nMulti-Perspective Progress Fusion\nTo mitigate error accumulation and ensure consistent accuracy, we fuse dense temporal value estimates from three complementary perspectives: incremental prediction, forward-anchored prediction, and backward-anchored prediction.\nIncremental Prediction\noffers a fine-grained, step-by-step assessment. Refer to\nEquation\nËœ\n2\n, the predicted global progress\nÎ¦\nI\nâ‹†\nâ€‹\n(\ns\nt\n)\n\\Phi_{I}^{\\star}(s_{t})\nis recursively computed from the preceding stateâ€™s progress\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n\\Phi^{\\star}(s_{t-1})\nand the predicted hop\nâ„‹\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n,\ns\nt\n)\n\\mathcal{H}^{\\star}(s_{t-1},s_{t})\n. Let\nÎ”\nâ€‹\nÎ¦\nt\nâˆ’\n1\n,\nt\nâ‹†\n\\Delta\\Phi^{\\star}_{t-1,t}\nbe the estimated progress hop:\nÎ”\nâ€‹\nÎ¦\nt\nâˆ’\n1\n,\nt\nâ‹†\n=\n{\n[\n1\nâˆ’\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n]\nâ‹…\nâ„‹\nâ‹†\nif\nâ€‹\nâ„‹\nâ‹†\nâ‰¥\n0\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\nâ‹…\nâ„‹\nâ‹†\nif\nâ€‹\nâ„‹\nâ‹†\n<\n0\n.\n\\Delta\\Phi^{\\star}_{t-1,t}=\\begin{cases}[1-\\Phi^{\\star}(s_{t-1})]\\cdot\\mathcal{H}^{\\star}&\\text{if }\\mathcal{H}^{\\star}\\geq 0\\\\\n\\Phi^{\\star}(s_{t-1})\\cdot\\mathcal{H}^{\\star}&\\text{if }\\mathcal{H}^{\\star}<0.\\end{cases}\n(4)\nThe incremental progress is then calculated as follow:\nÎ¦\nI\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n+\nÎ”\nâ€‹\nÎ¦\nt\nâˆ’\n1\n,\nt\nâ‹†\n,\n\\Phi_{I}^{\\star}(s_{t})=\\Phi^{\\star}(s_{t-1})+\\Delta\\Phi^{\\star}_{t-1,t},\n(5)\nwhere\nÎ¦\nI\nâ‹†\nâ€‹\n(\ns\nt\n)\n\\Phi_{I}^{\\star}(s_{t})\nis accumulated along the trajectory, initialized with\nÎ¦\nâ‹†\nâ€‹\n(\ns\n0\n)\n=\n0\n\\Phi^{\\star}(s_{0})=0\n. While this method excels at capturing local dynamics, it is susceptible to the accumulation of prediction errors over long trajectories.\nTo counteract this drift, we introduce two global perspectives.\nForward-Anchored Prediction\nprovides a stable global reference by anchoring to the initial state\ns\ninit\ns_{\\text{init}}\n, where progress is zero:\nÎ¦\nF\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\nâ„‹\nâ‹†\nâ€‹\n(\ns\ninit\n,\ns\nt\n)\n.\n\\Phi_{F}^{\\star}(s_{t})=\\mathcal{H^{\\star}}(s_{\\text{init}},s_{t}).\n(6)\nConversely,\nBackward-Anchored Prediction\nis anchored to the goal state\ns\ngoal\ns_{\\text{goal}}\n, where progress is one. This approach offers high sensitivity near task completion:\nÎ¦\nB\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\n1\n+\nâ„‹\nâ‹†\nâ€‹\n(\ns\ngoal\n,\ns\nt\n)\n.\n\\Phi_{B}^{\\star}(s_{t})=1+\\mathcal{H^{\\star}}(s_{\\text{goal}},s_{t}).\n(7)\nThese three methods offer complementary strengths: local precision (incremental), initial stability (forward), and goal sensitivity (backward). We fuse them via averaging to obtain a robust final progress estimate:\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\n1\n3\nâ€‹\n(\nÎ¦\nI\nâ‹†\nâ€‹\n(\ns\nt\n)\n+\nÎ¦\nF\nâ‹†\nâ€‹\n(\ns\nt\n)\n+\nÎ¦\nB\nâ‹†\nâ€‹\n(\ns\nt\n)\n)\n.\n\\Phi^{\\star}(s_{t})=\\frac{1}{3}\\left(\\Phi_{I}^{\\star}(s_{t})+\\Phi_{F}^{\\star}(s_{t})+\\Phi_{B}^{\\star}(s_{t})\\right).\n(8)\nThis fusion yields a more accurate and drift-resistant value signal. Please also refer to\n[\n67\n]\nfor how to apply this kind of value signal for RL process.\n2.2.3\nBi-directional Consistency Checking\nWhile the multi-perspective fusion via averaging (\nEquation\nËœ\n8\n) serves as a baseline, its naive application in online RL faces the risk of Out-of-Distribution (OOD) hallucination. Due to the inherent limitations of data coverage, it is impossible for the training set to encompass every corner of the state space. During RL, the policy inevitably explores unseen regions where dense temporal value estimation may yield spurious high signals, leading to â€œreward hacking.â€\nTo address these, we propose a bi-directional consistency checking strategy that leverages consistency as a proxy for reliability. This design is motivated by the observation that forward\nÎ¦\nF\nâˆ—\n\\Phi^{*}_{F}\nand backward\nÎ¦\nB\nâˆ—\n\\Phi^{*}_{B}\npredictions tend to diverge significantly under OOD observations, whereas they remain consistent in familiar states.\nConsistency-Aware Weighting.\nWe first define the mean estimated progress\nÎ¦\nÂ¯\nâˆ—\nâ€‹\n(\ns\nt\n)\n=\n(\nÎ¦\nF\nâˆ—\nâ€‹\n(\ns\nt\n)\n+\nÎ¦\nB\nâˆ—\nâ€‹\n(\ns\nt\n)\n)\n/\n2\n\\bar{\\Phi}^{*}(s_{t})=(\\Phi^{*}_{F}(s_{t})+\\Phi^{*}_{B}(s_{t}))/2\n. To quantify uncertainty, we calculate a normalized discrepancy metric:\nÎ”\nnorm\nâ€‹\n(\ns\nt\n)\n=\n|\nÎ¦\nB\nâˆ—\nâ€‹\n(\ns\nt\n)\nâˆ’\nÎ¦\nF\nâˆ—\nâ€‹\n(\ns\nt\n)\n|\nÎ¦\nÂ¯\nâˆ—\nâ€‹\n(\ns\nt\n)\n+\nÏµ\n,\n\\Delta_{\\text{norm}}(s_{t})=\\frac{|\\Phi^{*}_{B}(s_{t})-\\Phi^{*}_{F}(s_{t})|}{\\bar{\\Phi}^{*}(s_{t})+\\epsilon},\n(9)\nwhere\nÏµ\n\\epsilon\nis a small constant for numerical stability. Normalization by\nÎ¦\nÂ¯\nâˆ—\n\\bar{\\Phi}^{*}\nensures that discrepancies are penalized more heavily during the early stages (where\nÎ¦\n\\Phi\nis small), as precise guidance is critical initially. We then derive a confidence weight\nw\nt\nâˆˆ\n(\n0\n,\n1\n]\nw_{t}\\in(0,1]\nusing a Gaussian kernel with sensitivity\nÎ±\n\\alpha\n:\nw\nt\n=\nexp\nâ¡\n(\nâˆ’\nÎ±\nâ‹…\n(\nÎ”\nnorm\nâ€‹\n(\ns\nt\n)\n)\n2\n)\n.\nw_{t}=\\exp\\left(-\\alpha\\cdot(\\Delta_{\\text{norm}}(s_{t}))^{2}\\right).\n(10)\nConservative State Update.\nTo prevent the policy from exploiting erroneous estimates in OOD scenarios, we employ a conservative update rule for the maintained progress state\nÎ¦\nâˆ—\nâ€‹\n(\ns\nt\n)\n\\Phi^{*}(s_{t})\ninstead of\nEquation\nËœ\n8\n:\nÎ¦\nâˆ—\nâ€‹\n(\ns\nt\n)\n=\nÎ¦\nâˆ—\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n+\nw\nt\n2\nâ‹…\n(\nÎ¦\nÂ¯\nâˆ—\nâ€‹\n(\ns\nt\n)\nâˆ’\nÎ¦\nâˆ—\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n+\nÎ”\nâ€‹\nÎ¦\nt\nâˆ’\n1\n,\nt\nâ‹†\n)\n.\n\\Phi^{*}(s_{t})=\\Phi^{*}(s_{t-1})+\\frac{w_{t}}{2}\\cdot\\left(\\bar{\\Phi}^{*}(s_{t})-\\Phi^{*}(s_{t-1})+\\Delta\\Phi^{\\star}_{t-1,t}\\right).\n(11)\nThis mechanism acts as a semantic filter: it ignores uncertain updates when\nw\nt\nâ†’\n0\nw_{t}\\to 0\n(retaining\nÎ¦\nâˆ—\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n\\Phi^{*}(s_{t-1})\n) and fully trusts the estimate when consistency is high (\nw\nt\nâ†’\n1\nw_{t}\\to 1\n).\n3\nTraining Data\nAs shown in\nFigure\nËœ\n2\n, RoboBrain 2.5 is trained on a diverse and extensive dataset designed to enhance its capabilities in spatial understanding, temporal modeling and causal reasoning in embodied settings. Specifically, we construct a unified corpus of approximately 12.4M high-quality samples, categorized into three core domains: (1)\nGeneral MLLM Data\nfor robust semantic perception; (2)\nSpatial Reasoning Data\nspanning 2D perception to metric-aware 3D tracing; and (3)\nTemporal Prediction Data\nfor hierarchical planning and dense value estimation. This mixture strategically balances large-scale web knowledge with fine-grained physical world interactions to bridge the gap between high-level reasoning and low-level control.\nFigure 2\n:\nTraining Data Distribution for RoboBrain 2.5.\nThe left pie chart illustrates the hierarchical composition of the dataset, structured into Temporal (red), General (teal), and Spatial (blue) domains. The right bar chart displays the sample count for each specific sub-task on a logarithmic scale, highlighting the extensive scale of Dense Value Estimation, High-Quality General Data, and 3D Spatial Reasoning.\n3.1\nGeneral MLLM Data\nHigh-Quality General Data.\nTo establish a robust foundation for general visual perception and reasoning, the general training dataset for RoboBrain 2.5 incorporates approximately 2.83 million high-quality samples. These are primarily sourced and refined from two state-of-the-art open-source collections: Honey-Data-1M\n[\n82\n]\nand LLaVA-Onevision-1.5-Instruct-Data\n[\n5\n]\n.\n(1) Honey-Data-1M Processing.\nWe utilize Honey-Data-1M\n[\n82\n]\nas a key data source, which provides a diverse set of visual-language instructions designed to enhance multimodal understanding. To align the response style with our embodied agentâ€™s requirements for concise and direct execution commands, we truncated the extensive Chain-of-Thought (CoT) reasoning components, retaining only the final answers to streamline the supervision signal for direct instruction following.\n(2) LLaVA-Onevision Data Refinement.\nWe further integrate LLaVA-Onevision-1.5-Instruct-Data\n[\n5\n]\n, a comprehensive dataset covering a wide array of visual tasks including OCR, math, and general VQA. To strictly focus on vision-centric capabilities, we first filtered out all text-only samples. To address data imbalance, we applied balanced sampling across each visual-based subclass. Furthermore, to optimize training efficiency and context window utilization, we employed a sample packing strategy where shorter training samples are concatenated. This results in a more uniform sequence length distribution, primarily falling within the 2048 to 8192 token range.\n(3) De-duplication and Merging.\nGiven the overlap in data sources between these two repositories, we conducted a rigorous de-duplication process to prevent redundancy and data leakage. We filtered the combined pool based on both image similarity and question-answer textual similarity. The final curated dataset consists of 2.83M unique, high-quality multimodal instruction-following samples.\n3.2\nSpatial Reasoning Data\nVisual Grounding.\nThe visual grounding dataset is constructed to enhance multimodal understanding through precise object-level localization, leveraging the extensive annotations from LVIS\n[\n27\n]\n. We carefully curate 152K high-resolution images from LVIS, ensuring broad coverage of diverse object categories and complex visual scenes. Each object annotation is converted into standardized bounding box coordinates\n(\nx\n1\n,\ny\n1\n,\nx\n2\n,\ny\n2\n)\n(x_{1},y_{1},x_{2},y_{2})\nrepresenting the top-left and bottom-right corners, enabling consistent spatial referencing. To facilitate rich visual dialogue, we generated 86K conversational sequences, each containing multiple rounds of QA pairs that progressively explore visual relationships, attribute reasoning, and contextual understanding. The dataset maintains a balanced distribution across object categories while preserving challenging cases of occlusion, viewpoint variation, and rare instances to support robust visual grounding.\nObject Pointing.\nThe object pointing dataset is constructed to enable RoboBrain 2.5 to identify the locations of specified objects through pointing within an image. We leverage the Pixmo-Points\n[\n22\n]\ndataset, which includes 2.3M point annotations across 223K images as our data source. However, direct utilization of Pixmo-Points data for RoboBrain 2.5 training presents challenges due to densely repeated object instances (\ne.g\n.\n, books on a shelf). To address this, we implement a two-step filtering process: (1) we discard annotations with more than ten labeled points to simplify training, and (2) we use GPT-4o\n[\n31\n]\nas a scene analyzer to select only indoor-relevant objects, such as kitchenware, furniture, and decorations, excluding irrelevant or outdoor scenes. This process yields 190K QA pairs for 64K images with reduced clutter, making the data more suitable for embodied contexts.\nTo construct QA pairs for pointing tasks, we construct 28 human-designed templates, such as â€œ\nPoint out all instances of\n{\nlabel\n}\nin the image.\nâ€ or â€œ\nHelp me find\n{\nlabel\n}\nin the image by pointing to them.\nâ€ Here, {\nlabel\n} refers to object categories from the annotations. Templates are randomly selected to ensure linguistic diversity and improve the modelâ€™s generalization ability in referencing tasks.\nFor object reference pointing, we incorporate object reference data sourced from RoboPoint\n[\n78\n]\n, which includes 347K QA annotations across 288K images. To address the potential issue of excessive points hindering training convergence, we randomly sample up to ten points per question. Additionally, all coordinates are converted into the normalized values to better support RoboBrain 2.5 training.\nAffordance.\nThe affordance dataset focuses on understanding object functionality and spatial vacant areas for placement. For object affordance recognition, we utilize part-level annotations from PACO-LVIS\n[\n63\n]\n, covering 75 object categories and 200 part categories across 46K images. Bounding boxes and segmentation masks are extracted for both whole objects and their functional parts. These annotations are transformed into bounding box coordinates\n(\nx\n1\n,\ny\n1\n,\nx\n2\n,\ny\n2\n)\n(x_{1},y_{1},x_{2},y_{2})\n, serving as ground truth labels for affordance prediction tasks. Questions are constructed using GPT-4o\n[\n31\n]\nto query object functionality and part usage,\ne.g\n.\n, â€œ\nWhich part of a handbag can be grasped to carry it?\nâ€ for the handle of a handbag. For whole-object affordances, questions avoid naming the object directly, such as â€œ\nWhat device can be moved to control the cursor on a screen?\nâ€ for a mouse (computer equipment). This automatic process results in 561K QA pairs.\nFor spatial affordance learning, we include region reference data from RoboPoint\n[\n78\n]\n. This dataset consists of 270K images with 320K QA pairs and 14 spatial relationship labels. Each annotation is converted into a set of the normalized coordinates\n[\n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\nâ€¦\n]\n[(x_{1},y_{1}),(x_{2},y_{2}),...]\n, and ground truth points are resampled to a maximum of ten points per answer for optimization. This dataset enables RoboBrain 2.5 to reason about spatial affordances for object placement in real-world settings.\nSpatial Understanding.\nTo enhance RoboBrain 2.5â€™s spatial reasoning, we present the Spatial Understanding Dataset, comprising 826K samples. This dataset emphasizes object-centric spatial attributes (\ne.g\n.\n, position, orientation) and inter-object relations (\ne.g\n.\n, distance, direction), covering both qualitative and quantitative aspects.\nIt covers 31 distinct spatial concepts, substantially surpassing the\nâˆ¼\n\\sim\n15 typically found in previous datasets.\nWe partially adopt the RefSpatial\n[\n85\n]\npipeline to construct 2D web image and 3D video datasets via automated template- and LLM-based generation:\n(1) 2D web images\naim to provide core spatial concepts and depth perception across diverse indoor and outdoor scenes. To bridge scale and category gaps between these domains, we utilize the large-scale OpenImage\n[\n38\n]\ndataset. Since direct 3D reasoning from 2D images is challenging, we convert them into pseudo-3D scene graphs. Specifically, after filtering 1.7M images to 466K, we first use RAM\n[\n83\n]\nfor object category prediction and GroundingDINO\n[\n49\n]\nfor 2D boxes Detection. Then we enhance using Qwen2.5-VL\n[\n62\n]\nand a heuristic method to generate hierarchical captions given the 2D bounding box, ranging from coarse (\ne.g\n.\n, â€œcupâ€) to fine-grained (\ne.g\n.\n, â€œthe third cup from the leftâ€). This enables unambiguous spatial referring in cluttered environments and captures both coarse and fine-grained spatial references. Next, we use UniDepth V2\n[\n60\n]\nand WildeCamera\n[\n87\n]\nfor depth and camera intrinsics to enable 3D point cloud reconstruction. Finally, combining this with object boxes from GroundingDINO\n[\n49\n]\nand masks from SAM 2.1\n[\n64\n]\n, each scene graph includes object labels, 2D boxes, instance masks, and object-level point clouds, yielding axis-aligned 3D boxes. Object captions serve as nodes, and spatial relations form the edges. QA pairs are generated via templates and LLMs (\ne.g\n.\n, QwQ\n[\n74\n]\n), including object-location questions derived from the hierarchical captions.\n(2) scanning datasets\nintegrates multimodal 3D scene understanding data from five original datasets: MMScan\n[\n50\n]\n, 3RScan\n[\n76\n]\n, ScanQA\n[\n6\n]\n, SQA3D\n[\n51\n]\n, and SpaceR\n[\n58\n]\n. We conduct template-based question filtering through rigorous data processing to ensure task relevance, perform multi-stage quality screening (\ne.g\n.\n, consistency checks, outlier removal), and standardize all formats into a unified representation. This curation enables fine-grained environmental perception with enhanced reliability, supporting tasks ranging from object localization to complex spatial reasoning in 3D scenes.\n(3) 3D embodied videos\nfocus on fine-grained spatial understanding in indoor environments. We leverage the CA-1M\n[\n39\n]\ndataset, filtering 2M frames to 100K high-quality ones. Compared to 2D, the availability of accurate 3D bounding boxes allows us to construct richer scene graphs with more diverse spatial relations, thereby generating more quantitative QA pairs (\ne.g\n.\n, size, distances).\nSpatial Referring.\nAfter enhancing foundational 3D spatial understanding, we extend these capabilities to physical-world interactions by introducing the Spatial Referring Dataset\n[\n85\n]\n, consisting of 802K samples. Unlike prior datasets in visual grounding or object pointing, which often deal with ambiguous or multiple referents, this dataset targets a single unambiguous target, aligning with robotic applications such as precise pick-and-place that demand accurate object identification and localization.\nFollowing the RefSpatial\n[\n85\n]\nconstruction pipeline, for location data, we sample caption-point pairs from scene graphs built on 2D web images (OpenImage\n[\n38\n]\n) and 3D embodied videos (CA-1M\n[\n39\n]\n), using hierarchical captions. For placement data, we leverage fully annotated 3D datasets to generate top-down occupancy maps encoding object positions, orientations, and metric spatial relations (\ne.g\n.\n, â€œ10cm right of the chairâ€), facilitating accurate spatial referring.\n3D Spatial Reasoning (RoboBrain 2.5 New Feature).\nTo equip the model with robust 3D spatial reasoning capabilities for tasks such as 3D spatial referring, measuring, and tracing, we introduce the 3D Spatial Reasoning Dataset, comprising 1.74M samples (8.08M QA pairs).\nUnlike the Spatial Understanding dataset, which focuses on qualitative, metric-agnostic spatial concepts (\ne.g\n.\n, left, far, inside), this part is metric-grounded and supports flexible output in appropriate units (\ne.g\n.\n, cm, inch, m).\nFollowing the TraceSpatial\n[\n86\n]\nconstruction pipeline, we propose a data pipeline that progressively integrates 3D scanning and video sources to perform 3D spatial referring, measuring, and tracing.\n(1) 3D Scanning datasets\nwant to arm the model with a focused metric-grounded spatial reasoning of indoor scenes.\nWe thus leverage the richly annotated CA-1M\n[\n39\n]\nand ScanNet\n[\n21\n]\n.\nAfter fine-grained filtering, similar to the Spatial Understanding part, we construct pseudo-3D scene graphs with more diverse spatial relations, enabled by precise 3D bounding boxes compared to 2D approaches.\nMoreover, we generate 3D occupancy maps that encode positions, orientations, and metric distances (\ne.g\n.\n, â€œ35cm right of the toyâ€) for accurate object-centric spatial trace generation.\n(2) Manipulation videos\nprovide spatial traces aligned with the embodied manipulation in tabletop settings.\nWhile 3D scans enable object-centric tracing, they lack physically plausible manipulations for robotics.\nHence, we curate both real (\ne.g\n.\n, AgiBot-Beta\n[\n19\n]\n, DROID\n[\n36\n]\n) and simulated (\ne.g\n.\n, RoboTwin 2.0\n[\n17\n]\n) tabletop videos. Through a rigorous data cleaning process, such as verifying valid camera poses, coherent task flows, and clean trajectories, we reduce the dataset from 167K to 59K samples for AgiBot-Beta, and from 116K to 24K for DROID.\nWe further leverage Qwen3-VL\n[\n62\n]\nto decompose these tasks into subgoals, enabling precise multi-step spatial tracing for single-/dual-arm across\n3\n3\nrobot configurations.\n3.3\nTemporal Prediction Data\nEgo-View Planning.\nWe construct Ego-View Planning dataset by partially processing the EgoPlan-IT\n[\n18\n]\ndataset, which contains 50K automatically generated samples. For each selected task instance, we extract multiple frames from prior actions to represent task progress, and one frame to capture the current viewpoint. To enhance linguistic variety, we use multiple prompt templates that describe the task goal, video context, and current observation. Each question includes the correct next action along with up to three distractor actions randomly sampled from negative examples. This setup supports multimodal instruction tuning with diverse visual and textual input, aimed at improving egocentric task planning performance.\nShareRobot Planning.\nThe ShareRobot dataset\n[\n33\n]\nis a large-scale, fine-grained resource for robotic manipulation, offering multi-dimensional annotations tailored for task planning. Its planning component provides detailed low-level instructions aligned with individual video frames, effectively transforming high-level task descriptions into structured and executable sub-tasks. Each data instance includes precise planning annotations to support accurate and consistent task execution. The dataset comprises 1M QA pairs from 51K instances, spanning 102 diverse scenes across 12 robot embodiments and 107 atomic tasks filtered according to the Open-X-Embodiment taxonomy\n[\n59\n]\n. All planning data were meticulously annotated by human experts following the RoboVQA\n[\n65\n]\nformat, enabling models to learn robust multi-step planning strategies grounded in diverse real-world scenarios. The scale, quality, and diversity of ShareRobot help improve the modelâ€™s ability to perform fine-grained reasoning and task decomposition in complex embodied environments.\nAGIbot Planning.\nThe AgiBot Planning dataset is a large-scale robotics task planning dataset built upon the AgiBot-World\n[\n12\n]\ndataset, comprising 9,148 QA pairs across 19 manipulation tasks with 109,378 first-person perspective images. Each sample contains 4-17 consecutive frames documenting task progression with multimodal conversational format. AgiBot-Planning provides step-by-step planning instructions that transform high-level goals into executable sub-tasks. Each data point includes current objectives, historical steps, and required subsequent actions. The dataset covers diverse scenarios from household refrigerator operations to supermarket shopping tasks across different environments. The meticulously crafted annotations use standardized conversational formats, enabling models to learn from varied real-world contexts. Through continuous visual sequences and fine-grained action plans, AgiBot-Planning enhances RoboBrain 2.5â€™s ability to perform long-horizon task planning and spatial reasoning in complex embodied scenarios.\nMulti-Robot Planning.\nThe Multi-Robot Planning dataset is constructed by simulating collaborative task scenarios across three environmentsâ€”household, supermarket, and restaurantâ€”based on RoboOS\n[\n69\n,\n68\n]\n. Each sample is generated using structured templates that specify a detailed scene graph, robot specifications, and associated tool lists. For every scenario, we design high-level, long-horizon collaborative task goals that require coordination among multiple robots present in the scene, and generate corresponding workflow graphs that decompose the tasks into subtasks with detailed reasoning explanations. Based on these decompositions, we further generate agent-specific robotic tool plans that translate high-level task goals into precise low-level Observation-Action pairs for each subtask. Specifically, we define 1,659 types of multi-robot collaboration tasks across the three environments and produce 44,142 samples using DeepSeek-V3\n[\n46\n]\n.\nClose-Loop Interaction.\nThe Close-Loop Interaction dataset is designed to facilitate advanced embodied reasoning\n[\n84\n]\n, featuring a large-scale collection of synthesized Observation-Thought-Action (OTA) trajectories that combine first-person visual observations with structured thought tokens. It spans 120 diverse indoor environmentsâ€”including kitchens, bathrooms, bedrooms, and living roomsâ€”containing over 4,000 interactive objects and receptacles. The dataset is constructed within the AI2Thor\n[\n37\n]\nsimulator through a rigorous multi-stage pipeline based on Embodied-Reasoner\n[\n81\n]\n, which includes: (1) crafting task instructions from constrained templates to ensure scene-appropriate validity; (2) deriving key action sequences from an object-affiliation graph encoding functional relationships; and (3) strategically incorporating search actions to emulate realistic exploration. To enrich the depth of reasoning, GPT-4o\n[\n31\n]\ngenerates detailed thought processesâ€”covering situational analysis, spatial reasoning, self-reflection, task planning, and verificationâ€”which are seamlessly integrated between observations and actions, forming coherent reasoning chains that guide models through complex, long-horizon interactive tasks.\nDense Value Estimation (RoboBrain 2.5 New Feature).\nTo empower the dense temporal value estimator with robust generalization capabilities, we construct a comprehensive dataset comprising approximately 35 million value estimation samples derived from over 27 million raw frames, and then down-sample to  3.5M for final training. Following the Dopamine-Reward\n[\n67\n]\npipeline, this corpus is meticulously aggregated from three complementary domains, strategically balanced to bridge the gap between physical reality and semantic understanding:\n(1) Real-World robot data\n, which constitutes the majority (\nâˆ¼\n\\sim\n60%) of the training set, integrating diverse datasets such as AGIBot-World\n[\n12\n]\n, DROID\n[\n36\n]\n, and RoboBrain-X\n[\n25\n]\nto ground the model in physical interaction dynamics across varied environments;\n(2) Simulation data\n(\nâˆ¼\n\\sim\n13%), incorporating benchmarks like LIBERO\n[\n47\n]\n, RoboCasa\n[\n55\n]\n, and RoboTwin\n[\n17\n]\nto foster strong instruction-following capabilities through high-quality, occlusion-free labels;\nand\n(3) Human-Centric data\n(\nâˆ¼\n\\sim\n26%), leveraging the massive scale of EgoDex\n[\n30\n]\nto acquire universal object affordance priors independent of robot morphology. Crucially, this heterogeneous mixture spans a wide spectrum of embodiments, ranging from single-arm industrial robots (\ne.g\n.\n, Franka Emika Panda) to complex bimanual humanoids (\ne.g\n.\n, AGIBot-A2D), preventing overfitting to specific kinematics and ensuring the model focuses on object state changes. We apply the hop-based labeling strategy described in\nSection\nËœ\n2.2\nto this multi-source collection, enabling the model to provide stable, embodiment-invariant progress feedback across a wide spectrum of tasks.\n4\nTraining Strategy\nSimilar to RoboBrain 2.0\n[\n72\n]\n, RoboBrain 2.5 achieves embodied capabilities (spatial understanding, temporal modeling) through a progressive dual-phase training strategy, as shown in\nTable\nËœ\n1\n. Starting from a robust vision-language foundation, we introduce escalating complexity in embodied supervision, enabling the model to evolve from static perception to dynamic reasoning and actionable planning in real-world environments. Specifically, the training pipeline is divided into two distinct phases: (1)\nFoundational Spatiotemporal Learning\n, which establishes broad visual semantics, 2D spatial grounding, and open-loop planning capabilities; and (2)\nSpecific Spatiotemporal Enhancement\n, which fine-tunes the model on quantitative 3D spatial reasoning and dense temporal value estimation to ensure precise, metric-aware physical interaction.\nTable 1\n:\nDetailed configuration for each training stage of the RoboBrain 2.5.\nNVIDIA GPUs\nMoore Threads GPUs\nStage-1\nStage-2\nStage-1\nStage-2\nData\nDataset\nFoundational Learning\nSpecific Learning\nFoundational Learning\nSpecific Learning\n#Samples\n8.3 M\n4.1 M\n8.3 M\n4.1 M\nModel\nTrainable Part\nFull Model\nFull Model\nFull Model\nFull Model\n#Tunable Parameters\n8B\n8B\n8B\n8B\nTraining\nGlobal Batch Size\n1024\n1024\n1024\n1024\nTensor Parallelism (TP)\n2\n2\n2\n2\nPipeline Parallelism (PP)\n2\n2\n2\n2\nLR:\n{\nÏˆ\nv\nViT\n,\nÏ•\nv\nLLM\n}\n\\{\\psi_{v}^{\\text{ViT}},\\phi_{v}^{\\text{LLM}}\\}\n1\nÃ—\n10\nâˆ’\n6\n\\times 10^{-6}\n, 1\nÃ—\n10\nâˆ’\n5\n\\times 10^{-5}\n1\nÃ—\n10\nâˆ’\n6\n\\times 10^{-6}\n, 1\nÃ—\n10\nâˆ’\n5\n\\times 10^{-5}\n1\nÃ—\n10\nâˆ’\n6\n\\times 10^{-6}\n, 1\nÃ—\n10\nâˆ’\n5\n\\times 10^{-5}\n1\nÃ—\n10\nâˆ’\n6\n\\times 10^{-6}\n, 1\nÃ—\n10\nâˆ’\n5\n\\times 10^{-5}\nEpoch\n1\n1\n1\n1\nOptimizer\nAdamW\nAdamW\nAdamW\nAdamW\nWeight Decay\n0.1\n0.1\n0.1\n0.0\nWarmup Ratio\n0.01\n0.01\n0.03\n0.00\nLR Schedule\nCosine\nCosine\nCosine\nCosine\nMax Seq. Length\n16384\n16384\n16384\n16384\nGPU Nums\n64\nÃ—\n\\times\n8\n64\nÃ—\n\\times\n8\n128\nÃ—\n\\times\n8\n128\nÃ—\n\\times\n8\n4.1\nStage 1: Foundational Spatiotemporal Learning\nIn the first stage, we focus on establishing a robust â€œGeneralist Brainâ€ capable of understanding multimodal instructions, grounding objects in 2D space, and mastering high-level planning logic. We utilize the Full Model across\n8.3 million\nsamples, comprising the\nGeneral MLLM Data\n,\nSpatial Reasoning Data\n(excluding metric 3D points/traces), and\nTemporal Prediction Data\n(Planning and pairwise comparisons).\nTo ensure stable convergence on this heterogeneous corpus, we employ a standard next-token prediction loss. The primary objectives of this stage are threefold:\n(1) General Visual Perception:\nLeveraging high-quality general data (\ne.g\n.\n, Honey-Data-1M) to maintain and enhance the modelâ€™s general visual-linguistic capabilities. This ensures the model retains a robust understanding of open-world semantics, complex user queries, and diverse visual scenes, serving as a versatile foundation for specific embodied tasks.\n(2) 2D Grounding & Qualitative 3D Understanding:\nBeyond standard 2D visual grounding and affordance detection, this stage incorporates text-based QA from the 3D Spatial Reasoning dataset. This enables the model to comprehend complex spatial relationships (\ne.g\n.\n, spatial relations, occupancy) and qualitative 3D concepts without the burden of precise metric coordinate regression.\n(3) Planning & Temporal Logic:\nWe integrate diverse planning datasets to teach logical task decomposition. Furthermore, we introduce a\nTemporal Value Comparison\ntask derived from the Dense Value Estimation dataset. Instead of predicting absolute values, the model learns to order keyframes temporally (\ni.e\n.\n, identifying which frame represents a later state), establishing a preliminary awareness of task progress and state evolution.\nThis stage yields a model proficient in general perception, logical planning, and qualitative spatiotemporal reasoning, providing a solid initialization for fine-grained training.\n4.2\nStage 2: Specific Spatiotemporal Enhancement\nTo bridge the gap between semantic understanding and physical actuation, the second stage introduces Specific Spatiotemporal Enhancement, focusing on precise quantitative reasoning. This stage utilizes approximately\n4.1 million\nsamples, targeting the newly introduced\nMetric 3D Spatial Reasoning\nand\nDense Value Estimation\ncapabilities.\n(1) Metric-Aware 3D Tracing.\nWe introduce the specific 3D data focusing on point and trajectory generation to transition the model from qualitative understanding to quantitative perception. This enables the model to predict absolute 3D coordinates, depth-aware traces, and metric distances (\ne.g\n.\n, in centimeters), which are critical for precision manipulation tasks.\n(2) Dense Value Estimation.\nWe transition from pairwise comparison to explicit\nHop\nprediction. The model is trained to act as a robust value function (Critic) by predicting continuous progress values (Hops) frame-by-frame, enabling it to provide fine-grained, closed-loop feedback for policy ranking and error recovery.\n(3) Anti-Forgetting Strategy.\nTo prevent the catastrophic forgetting of general capabilities while learning these specialized metric tasks, we adopt a data replay strategy. We randomly sample 15% of the Stage-1 data and mix it with the Stage-2 specific data. This ensures the model retains its conversation, 2D grounding, and logical planning abilities while mastering fine-grained physical skills for 3D embodied environment.\n5\nInfrastructure\nDuring the training of RoboBrain 2.5, we build upon the infrastructure established in RoboBrain 2.0\n[\n72\n,\n33\n]\nwhile further strengthening and systematizing the core training pipeline. The overall system adopts a multi-dimensional hybrid parallelism strategy, combined with distributed data loading optimizations, and a deeply optimized memory pre-allocation mechanism tailored for multi-modal long-sequence training. These improvements significantly enhance hardware utilization efficiency and overall training throughput.\nOn the data side, our implementation is based on the Megatronâ€“Energon\n[\n40\n]\nframework with substantial in-house optimizations. This design enables unified format representation and online mixed training of heterogeneous modalities, including text, single-image, multi-image, and video samples. At the same time, we strictly preserve intra-dataset sample ordering to satisfy the requirements of instruction alignment and temporal consistency. By adopting a customized WebDataset\n[\n4\n]\nsample format, the system achieves compatibility with diverse data types while substantially reducing offline preprocessing overhead and improving the flexibility and extensibility of the data pipeline.\n5.1\nHybrid Parallelism\nMulti-modal large models exhibit pronounced heterogeneity in both model architecture and computational characteristics\n[\n48\n]\n. The visual component typically consists of a relatively lightweight ViT-based encoder (with adapter modules), whereas the language component is dominated by a large-scale decoder-only architecture. Although the visual encoder has a smaller parameter footprint, its computational cost becomes non-trivial when training with a high proportion of visual or video samples.\nTo address this architectural heterogeneity, we leverage the heterogeneous training experience accumulated in our in-house distributed framework, FlagScale\n[\n20\n]\n, and adopt an\nuneven pipeline parallelism\nstrategy\n[\n56\n]\n. Specifically, the ViT module is placed at the front of the model, and the number of language layers assigned to the first pipeline stage is reduced accordingly. This design balances computational load across pipeline stages, mitigates pipeline bubbles, and improves overall pipeline efficiency.\n5.2\nDynamic pre-Allocated Memory\nIn RoboBrain 2.5 training, sequence lengths vary significantly across samples. Combined with PyTorchâ€™s default CUDA caching memory allocator, this dynamic-shape workload often leads to severe GPU memory fragmentation and, in extreme cases, out-of-memory (OOM) failures. A common workaround is to invoke\ntorch.cuda.empty_cache()\n[\n61\n]\nbefore each iteration; however, this approach disrupts memory reuse and substantially degrades training performance.\nTo resolve this issue, we conduct an in-depth analysis of CUDA memory allocation and reuse behavior and propose a\ndynamic unified padding strategy based on dual data streams\n.\nâ€¢\nBefore training begins, the maximum sequence length observed in the training set is collected;\nâ€¢\nIn the first training iteration, all samples are padded to this maximum length, enabling one-time memory pre-allocation during initialization;\nâ€¢\nIn subsequent iterations, tensors reuse the pre-allocated memory, effectively suppressing memory fragmentation;\nâ€¢\nOnly when the visual token length exceeds the current maximum does the system trigger a full cache cleanup and re-pad samples to the new maximum length.\nThis strategy strikes a practical balance between memory efficiency and training performance, providing both stability and high throughput in large-scale multi-modal long-sequence training scenarios.\n5.3\nCross-Accelerator Training and Inference\nLeveraging FlagScaleâ€™s distributed training capabilities on heterogeneous accelerator clusters, together with VLM-specific kernel and communication optimizations, we successfully complete end-to-end training of RoboBrain 2.5 on a thousand-device cluster composed of\nnon-NVIDIA accelerators\n. The resulting loss convergence behavior closely matches that observed on NVIDIA platforms, with the final convergence gap controlled within 0.62%.\nFurthermore, the trained checkpoints are seamlessly migrated to NVIDIA-based platforms for downstream evaluation. Across a range of mainstream benchmarks, the resulting performance remains highly consistent with models trained natively on NVIDIA hardware. This RoboBrain 2.5 case study demonstrates that FlagOS/FlagScaleâ€™s cross-accelerator training and inference capabilities have matured to a level that is\nreliable, practical, and production-ready\nfor large-scale multi-modal model training.\n6\nEvaluation Results\nWe conducted a comprehensive evaluation of RoboBrain-2.5, significantly expanding the assessment scope of its predecessor to include 3D quantitative spatial reasoning and fine-grained temporal value estimation. To ensure consistency and rigor, we continued to employ FlagEvalMM\n[\n29\n]\n, our flexible framework for systematic multimodal model assessment. Notably, to demonstrate the cross-platform robustness of our training infrastructure, we report performance for RoboBrain-2.5 variants trained on two distinct hardware backends:\nNVIDIA (NV) GPUs\nand\nMoore-Threads (MTT) GPUs\n.\nEvaluations on spatial reasoning benchmarks, which now encompass both foundational 2D tasks (\ne.g\n.\n, CV-Bench\n[\n75\n]\n, RoboSpatial\n[\n66\n]\n) and advanced 3D quantitative measurement (\ne.g\n.\n, MSMU\n[\n14\n]\n, TraceSpatial\n[\n86\n]\n, VABench-V\n[\n79\n]\n), are presented in\nSection\nËœ\n6.1\nand\nSection\nËœ\n6.2\n. Furthermore, we introduce a new dimension of evaluation for temporal value estimation in\nSection\nËœ\n6.3\n, leveraging the General Process Reward Modeling (GPRM) paradigm from Robo-Dopamine\n[\n67\n]\n. We assess the modelâ€™s ability to perceive manipulation progress across diverse data sources, organized into\nReal-Bench\n(real-world robot data including AgiBot\n[\n12\n]\n, DROID\n[\n36\n]\n, and Galaxea\n[\n35\n]\n),\nSim-Bench\n(simulation environments like Libero\n[\n47\n]\nand RoboCasa\n[\n55\n]\n), and\nHuman-Bench\n(human manipulation videos from EgoDex\n[\n30\n]\n). Qualitative examples are provided in\nSection\nËœ\n9\n.\n6.1\n2D Spatial Reasoning Capability\nWe first evaluate RoboBrain-2.5 on five representative\n2D spatial reasoning\nbenchmarks:\nCV-Bench\n[\n75\n]\n,\nCrossPoint\n[\n77\n]\n,\nRoboSpatial\n[\n66\n]\n,\nRefSpatial\n[\n85\n]\n, and\nEmbSpatial\n[\n24\n]\n. Results are summarized in\nTable\nËœ\n2\n. Overall, the RoboBrain-2.5 variants trained on the\nNVIDIA GPU Platform\nand\nMoore-Threads (MTT) GPU Platform\nachieve same average scores of\n75.82\n. Both deliver substantial improvements over general-purpose and embodied baselines.\nTable 2\n:\nPerformance on 2D spatial reasoning benchmarks.\nThe best results are highlighted in\nbold\n, while the second-best results are\nunderlined\n. Results marked with * are sourced from their technical reports.\nModels / Metrics\n2D Spatial Reasoning\nAVG\nCV-Bench\nCrossPoint\nRoboSpatial\nRefSpatial\nEmbSpatial\nAll\nâ†‘\n\\uparrow\nGeneral Baselines\nGemini-3-Pro-Preview\n[\n26\n]\n92.00\nâˆ—\n38.60\n57.96\n65.50\nâˆ—\n76.62\n66.14\nGPT-5.2\n[\n57\n]\n86.84\n33.00\n43.78\n15.00\n68.02\n49.33\nQwen3-VL-8B-Inst.\n[\n8\n]\n92.89\n28.40\n66.90\nâˆ—\n54.20\nâˆ—\n78.50\nâˆ—\n64.18\nEmbodied Baselines\nRoboBrain-2.0 (7B)\n[\n72\n]\n85.75\nâˆ—\n26.00\n54.23\nâˆ—\n32.50\nâˆ—\n76.32\nâˆ—\n54.96\nMimo-Embodied (7B)\n[\n28\n]\n88.82\nâˆ—\n20.02\n61.76\nâˆ—\n48.00\nâˆ—\n76.24\nâˆ—\n58.97\nRoboBrain-2.5 (8B)\nNV\n94.58\n75.40\n73.03\n60.50\n75.58\n75.82\nRoboBrain-2.5 (8B)\nMTT\n93.90\n76.30\n73.00\n59.00\n76.92\n75.82\nâ€¢\nCV-Bench\n[\n75\n]\n.\nCV-Bench assesses vision-centric spatial understanding and visual processing via repurposed 2D/3D vision tasks. RoboBrain-2.5 (8B) trained on NVIDIA achieves the best accuracy of\n94.58\n, with the MTT variant closely following at 93.90. Both consistently outperform strong general baselines such as Qwen3-VL-8B-Inst. (92.89), Gemini-3-Pro-Preview (92.00), and GPT-5.2 (86.84), as well as embodied baselines including RoboBrain-2.0 (7B) (85.75) and Mimo-Embodied (7B) (88.82), indicating a clear gain in foundational 2D spatial perception.\nâ€¢\nCrossPoint\n[\n77\n]\n.\nCrossPoint-Bench evaluates cross-view point correspondence, requiring fine-grained point-level matching across different viewpoints. RoboBrain-2.5 demonstrates a decisive advantage, achieving\n76.30\n(MTT) and 75.40 (NVIDIA), which substantially surpasses all evaluated baselines, including Gemini-3-Pro-Preview (38.60), GPT-5.2 (33.00), Qwen3-VL-8B-Inst. (28.40), RoboBrain-2.0 (7B) (26.00), and Mimo-Embodied (7B) (20.02). This highlights the modelâ€™s strong capability in transitioning from coarse spatial judgment to actionable, coordinate-level correspondence.\nâ€¢\nRoboSpatial\n[\n66\n]\n.\nRoboSpatial measures spatial reasoning in robotics-oriented environments, emphasizing egocentric understanding, reference frames, and interaction-relevant spatial relations. RoboBrain-2.5 achieves the best scores of\n73.03\n(NVIDIA) and 73.00 (MTT), outperforming Qwen3-VL-8B-Inst. (66.90) and Gemini-3-Pro-Preview (57.96), as well as embodied baselines like Mimo-Embodied (7B) (61.76) and RoboBrain-2.0 (7B) (54.23). The consistent gains suggest improved spatial grounding for robot-centric perception and interaction.\nâ€¢\nRefSpatial\n[\n85\n]\n.\nRefSpatial evaluates spatial referring under complex spatial constraints, demanding precise grounding with multi-step spatial reasoning. RoboBrain-2.5 achieves strong results of 60.50 (NVIDIA) and 59.00 (MTT), substantially exceeding Qwen3-VL-8B-Inst. (54.20), Mimo-Embodied (7B) (48.00), RoboBrain-2.0 (7B) (32.50), and GPT-5.2 (15.00), while remaining competitive with the best-performing general baseline (Gemini-3-Pro-Preview,\n65.50\n). This indicates robust spatial referring performance in cluttered, instruction-conditioned settings.\nâ€¢\nEmbSpatial\n[\n24\n]\n.\nEmbSpatial-Bench assesses embodied spatial understanding from an egocentric perspective. RoboBrain-2.5 attains competitive performance with 76.92 (MTT) and 75.58 (NVIDIA), closely matching Gemini-3-Pro-Preview (76.62) and surpassing GPT-5.2 (68.02), while approaching the strongest baseline Qwen3-VL-8B-Inst. (\n78.50\n). These results suggest that RoboBrain-2.5 achieves strong generalization in embodied spatial relations, with minimal sensitivity to the training hardware backend.\n6.2\n3D Spatial Reasoning Capability\nWe further evaluate RoboBrain-2.5 on five\n3D spatial reasoning\nbenchmarks that stress\nmetric-grounded\nand\ntrajectory-aware\nunderstanding:\nMSMU\n[\n14\n]\n,\nQ-Spatial\n[\n45\n]\n,\nTraceSpatial\n[\n86\n]\n,\nVABench-V\n[\n79\n]\n, and\nShareRobot-Bench\n[\n33\n]\n. Results are summarized in\nTable\nËœ\n3\n. Unless otherwise noted, higher is better; specifically for\nVABench-V\nand\nShareRobot-Bench\n, we report distance-based metrics where lower indicates better performance.\nTable 3\n:\nPerformance on five 3D spatial reasoning benchmarks.\nFor\nTraceSpatial\n, we further report fine-grained 3D metrics including\n3D Start\n,\n3D End\n, and\nSuccess\nfor the detailed trace evaluation.\nThe best results among different models are highlighted in\nbold\n, while the second-best results are\nunderlined\n.\nModels / Metrics\n3D Spatial Reasoning\nMSMU\nâ†‘\n\\uparrow\nQ-Spatial\nâ†‘\n\\uparrow\nTraceSpatial\nâ†‘\n\\uparrow\nVABench-V\nâ†“\n\\downarrow\nShareRobot-T\nâ†“\n\\downarrow\n3D Start\n3D End\nSuccess\nGeneral Baselines\nGemini-3-Pro-Preview\n[\n26\n]\n59.44\n81.37\n19\n25\n7\n0.1705\n0.1899\nGPT-5.2\n[\n57\n]\n57.96\n69.16\n3\n8\n0\n0.1962\n0.2379\nQwen3-VL-8B-Inst.\n[\n8\n]\n43.48\n70.74\n30\n20\n6\n0.1979\n0.2347\nEmbodied Baselines\nRoboBrain-2.0 (7B)\n[\n72\n]\n55.01\n63.37\nâ€“\nâ€“\nâ€“\nâ€“\n0.1240\nMimo-Embodied (7B)\n[\n28\n]\n46.36\n65.42\nâ€“\nâ€“\nâ€“\n0.6970\n0.6351\nRoboBrain-2.5 (8B)\nNV\n64.17\n73.53\n83\n63\n44\n0.1281\n0.1164\nRoboBrain-2.5 (8B)\nMTT\n61.66\n78.31\n80\n65\n36\n0.1189\n0.1171\nâ€¢\nMSMU\n[\n14\n]\n.\nMSMU evaluates quantitative 3D spatial measuring and understanding with precise numerical annotations. RoboBrain-2.5 achieves the best performance, with\n64.17\n(NVIDIA) and 61.66 (MTT), surpassing strong general baselines such as Gemini-3-Pro-Preview (59.44) and GPT-5.2 (57.96), as well as embodied baselines like RoboBrain-2.0 (55.01) and Mimo-Embodied (46.36), indicating substantially improved metric-grounded perception.\nâ€¢\nQ-Spatial\n[\n45\n]\n.\nQ-Spatial Benchmark assesses quantitative reasoning about object sizes and distances in images. RoboBrain-2.5 (MTT) achieves a strong score of\n78.31\n, outperforming Qwen3-VL-8B-Inst. (70.74), GPT-5.2 (69.16), RoboBrain-2.0 (63.37), and Mimo-Embodied (65.42), while remaining competitive with the best-performing general baseline (Gemini-3-Pro-Preview,\n81.37\n). This demonstrates robust quantitative spatial reasoning without specialized test-time prompting.\nâ€¢\nTraceSpatial\n[\n86\n]\n.\nTraceSpatial-Bench evaluates multi-step, metric-grounded\nspatial tracing\nin cluttered 3D scenes, where a prediction is considered successful only if the trajectory satisfies correct start/end spatial constraints and remains collision-free.\nWe report three fine-grained 3D metrics:\n3D Start\nmeasures\ngrasp success\n(whether the predicted start point is sufficiently close to the target object point cloud),\n3D End\nmeasures\nplacement success\n(whether the predicted end point falls inside/near the destination objectâ€™s 3D bounding box), and\nSuccess\nmeasures the final\nspatial trace success\nby jointly considering grasp success, placement success, and collision checking along the trace\n[\n86\n]\n.\nâ€¢\nVABench-V\n[\n79\n]\n.\nVABench-V evaluates visual trace generation from natural language instructions with distance-based metrics (RMSE) (lower is better). RoboBrain-2.5 achieves a clear SOTA with a lowest error of\n0.1189\n(MTT) and a close second-best of\n0.1281\n(NVIDIA), substantially improving over Gemini-3-Pro-Preview (0.1705), GPT-5.2 (0.1962), and Qwen3-VL-8B-Inst. (0.1979), demonstrating accurate fine-grained waypoint generation.\nâ€¢\nShareRobot-T\n[\n33\n]\n.\nShareRobot-Traj Benchmark assesses robot-centric spatial grounding for interaction and trajectory-related prediction (RMSE), where lower distance indicates better performance. RoboBrain-2.5 attains the best results with\n0.1164\n(NVIDIA) and a close second of\n0.1171\n(MTT), improving over RoboBrain-2.0 (0.1240) and strongly outperforming general baselines such as Gemini-3-Pro-Preview (0.1899) and GPT-5.2 (0.2379), reflecting more precise interaction-relevant spatial outputs.\n6.3\nTemporal Value Estimation\nTo evaluate\nfine-grained temporal value estimation\nfor manipulation progress, we follow the General Process Reward Modeling (GPRM) paradigm in Robo-Dopamine\n[\n67\n]\n. Concretely, the model is prompted with a task instruction and conditioned on multi-view images of the\ninitial\nand\ngoal\nstates, together with paired multi-view observations of the\nBEFORE\nand\nAFTER\nstates, and predicts a discretized relative progress/regress hop as a value signal\n[\n67\n]\n.\nWe evaluate temporal ordering robustness via two rank-correlation metrics:\nForward VOC\n(\nVOC\n+\n) computed on the original temporal direction, and\nReverse VOC\n(\nVOC\n-\n) computed by\ntime-reversing\nthe video and re-evaluating the model (i.e., the predicted value should consistently invert with the reversed temporal order). We report VOC\n+\n/ VOC\n-\n(both\nâ†‘\n\\uparrow\n) on six data sources spanning real-robot, simulation, and human egocentric videos: AgiBot\n[\n12\n]\n, DROID\n[\n36\n]\n, Galaxea\n[\n35\n]\n, EgoDex\n[\n30\n]\n, LIBERO\n[\n47\n]\n, and RoboCasa\n[\n55\n]\n. Results are summarized in\nTable\nËœ\n4\n.\nTable 4\n:\nTemporal value estimation on six testsets.\nWe report\nVOC\n+\n/ VOC\n-\n(both\nâ†‘\n\\uparrow\n), where VOC\n-\nis computed by reversing the video and re-evaluating the model. The best results among different models are highlighted in\nbold\n, while the second-best results are\nunderlined\n.\nModels / Metrics\nVOC\n+\n/ VOC\n-\n(\nâ†‘\n\\uparrow\n)\nAgiBot\nDROID\nGalaxea\nEgoDex\nLIBERO\nRoboCasa\nGeneral Baselines\nGemini-3-Pro-Preview\n[\n26\n]\n81.36 / 58.70\n90.57 / 44.15\n88.86 / 35.34\n80.48\n/ 50.15\n98.42 / 76.31\n67.89 / 34.28\nGPT-5.2\n[\n57\n]\n90.02\n/ 15.91\n91.45\n/ 15.29\n88.76 / 10.03\n78.12 / 22.79\n96.97 / 19.19\n77.91 / 10.71\nQwen3-VL-8B-Inst.\n[\n8\n]\n82.50 / 5.32\n81.33 / 10.37\n79.98 / 5.51\n63.85 / 12.82\n72.31 / 22.07\n59.11 / -0.03\nEmbodied Models\nRoboBrain-2.5 (8B)\nNV\n83.08 /\n88.58\n90.82 /\n90.07\n93.38\n/\n95.79\n79.14 /\n84.99\n98.97\n/\n98.94\n98.47\n/\n98.75\nRoboBrain-2.5 (8B)\nMTT\n87.36\n/\n87.48\n93.67\n/\n89.26\n94.58\n/\n94.54\n80.67\n/\n81.12\n98.88\n/\n98.91\n98.54\n/\n99.58\nâ€¢\nAgiBot\n[\n12\n]\n.\nOn AgiBot, the\nRoboBrain-2.5\nvariants demonstrate strong and balanced performance. Specifically, the model trained on Moore-Threads (MTT) achieves\n87.36 / 87.48\n, ranking second in Forward VOC while maintaining high consistency. In contrast, while generalist VLMs like GPT-5.2 achieve a higher Forward VOC (\n90.02\n), they exhibit substantially lower Reverse VOC (15.91), indicating a lack of robust bidirectional temporal understanding compared to our embodied models.\nâ€¢\nDROID\n[\n36\n]\n.\nOn DROID,\nRoboBrain-2.5 (MTT)\nattains a clear lead with\n93.67 / 89.26\n, substantially improving over all baselines. While GPT-5.2 achieves the second-best Forward VOC (\n91.45\n), its Reverse VOC drops sharply to 15.29. Similarly, Gemini-3-Pro-Preview shows a large gap between forward (90.57) and reverse (44.15) performance, again highlighting the benefit of RoboBrain-2.5â€™s step-aware progress supervision.\nâ€¢\nGalaxea\n[\n25\n]\n.\nOn Galaxea, both RoboBrain-2.5 variants perform exceptionally well, with the MTT variant reaching\n94.58 / 94.54\nand the NVIDIA variant closely following at\n93.38\n/ 95.79. General baselines show significantly weaker Reverse VOC (e.g., Gemini-3-Pro-Preview at 35.34 and GPT-5.2 at 10.03), suggesting that high forward correlation alone is insufficient without robust time-reversal behavior.\nâ€¢\nEgoDex\n[\n30\n]\n.\nOn EgoDex (human egocentric manipulation videos),\nRoboBrain-2.5 (MTT)\nachieves the best comprehensive result (\n80.67 / 81.12\n). While Gemini-3-Pro-Preview shows competitive Forward VOC (\n80.48\n), its Reverse VOC (50.15) is significantly lower. This indicates that RoboBrain-2.5 generalizes better to human-centric temporal cues while maintaining logical consistency across temporal directions.\nâ€¢\nLIBERO\n[\n47\n]\n.\nOn LIBERO, both RoboBrain-2.5 models achieve near-ceiling performance, with the NVIDIA variant reaching\n98.97 / 98.94\nand the MTT variant close behind at\n98.88\n/ 98.91. Although general baselines show relatively high Forward VOC (e.g., Gemini-3-Pro-Preview at 98.42), their lower Reverse VOC (76.31 or below) reinforces that bidirectional temporal consistency is a stricter criterion for progress-aware value modeling.\nâ€¢\nRoboCasa\n[\n55\n]\n.\nOn RoboCasa,\nRoboBrain-2.5 (MTT)\nachieves the best performance (\n98.54 / 99.58\n), followed closely by the NVIDIA variant. Compared to general baselines (e.g., GPT-5.2 at 77.91 / 10.71), the RoboBrain-2.5 models demonstrate markedly stronger robustness under time reversal, consistent with the design goal of providing reliable, step-aware progress signals for manipulation\n[\n67\n]\n.\n7\nConclusion and Future Works\nIn this work, we introduced\nRoboBrain-2.5\n, a next-generation embodied AI foundation model that significantly bridges the gap between high-level semantic reasoning and low-level physical interaction. By addressing the fundamental limitations of prior generalist modelsâ€”specifically the lack of metric-grounded spatial precision and the absence of dense temporal supervisionâ€”RoboBrain-2.5 achieves a comprehensive upgrade in embodied capabilities. Our contributions are established through two core pillars. First, we proposed\nPrecise 3D Spatial Reasoning\n, moving beyond 2D pixel-relative grounding to depth-aware coordinate prediction. By utilizing a decoupled\n(\nu\n,\nv\n,\nd\n)\n(u,v,d)\nrepresentation and training on high-quality 3D spatial data, the model learns to interpret absolute metric constraints and generate collision-free, trajectory-level manipulation traces. Second, we introduced\nDense Temporal Value Estimation\n, a mechanism that provides fine-grained, step-aware progress and regress feedback. This capability, powered by a hop-based labeling strategy and multi-perspective fusion, enables the model to serve as a robust general-purpose reward function resilient to viewpoint variations. Furthermore, we demonstrated the scalability of our approach through a robust infrastructure capable of cross-accelerator training on both NVIDIA and Moore Threads GPUs. Extensive evaluations confirm that RoboBrain-2.5 sets a new state-of-the-art on both spatial reasoning and temporal value estimation tasks.\nIn future research, we plan to expand the capabilities and efficiency of the RoboBrain model series in four primary directions:\nâ€¢\nUnified Generation and Understanding Paradigm:\nWe aim to evolve RoboBrain into a unified architecture that integrates both spatiotemporal understanding and generative capabilities. By incorporating image and video prediction (i.e., next-stage prediction), the model will serve as an embodied world model. This will enable agents to simulate action outcomes in their â€œmindâ€ before execution, significantly enhancing planning safety and robustness in complex environments.\nâ€¢\nDeployment on Mobile Manipulation and Humanoids:\nWe will extensively validate and deploy our models on diverse real-world platforms, including mobile manipulators and humanoid robots\n[\n16\n,\n44\n,\n42\n,\n43\n,\n41\n]\n. Our focus will be on leveraging\nPrecise 3D Spatial Reasoning\nto achieve training-free manipulation generalization, while utilizing\nDense Temporal Value Estimation\nas a high-fidelity reward signal to drive efficient Reinforcement Learning (RL) in the physical world.\nâ€¢\nScalable Model Family and Specialized Variants:\nTo accommodate varying computational constraints and latency requirements, we plan to release a comprehensive series of models with different parameter scales. This includes lightweight versions optimized for edge-device deployment and high-frequency inference, as well as decoupling the architecture into distinct â€œInstructionâ€ (fast execution) and â€œThinkingâ€ (slow reasoning) versions to balance response speed with reasoning depth.\nâ€¢\nSelf-Evolving Data Engine:\nWe intend to establish a closed-loop data engine where RoboBrain 2.5 acts as a verifier for its own data. By utilizing the dense value estimator to automatically filter and annotate large-scale uncurated videos, the model can iteratively improve itself through self-supervised learning, creating a flywheel effect for continuous capability enhancement.\nReferences\nAbdolmaleki et al. [2025]\nAbbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, et al.\nGemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer.\narXiv preprint arXiv:2510.03342\n, 2025.\nAhn et al. [2024]\nMichael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, et al.\nAutort: Embodied foundation models for large scale orchestration of robotic agents.\narXiv preprint arXiv:2401.12963\n, 2024.\nAlakuijala et al. [2024]\nMinttu Alakuijala, Reginald McLean, Isaac Woungang, Nariman Farsad, Samuel Kaski, Pekka Marttinen, and Kai Yuan.\nVideo-language critic: Transferable reward functions for language-conditioned robotics.\narXiv preprint arXiv:2405.19988\n, 2024.\nAlex Aizman [2020]\nThomas Breuel Alex Aizman, Gavin Maltby.\nWebdataset: High-performance data loading for deep learning, 2020.\nURL\nhttps://webdataset.github.io/webdataset/\n.\nAn et al. [2025]\nXiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al.\nLlava-onevision-1.5: Fully open framework for democratized multimodal training.\narXiv preprint arXiv:2509.23661\n, 2025.\nAzuma et al. [2022]\nDaichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe.\nScanqa: 3d question answering for spatial scene understanding.\nIn\nCVPR\n, pages 19129â€“19139, 2022.\nAzzolini et al. [2025]\nAlisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, et al.\nCosmos-reason1: From physical common sense to embodied reasoning.\narXiv preprint arXiv:2503.15558\n, 2025.\nBai et al. [2025a]\nShuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu.\nQwen3-vl technical report, 2025a.\nURL\nhttps://arxiv.org/abs/2511.21631\n.\nBai et al. [2025b]\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al.\nQwen2. 5-vl technical report.\narXiv preprint arXiv:2502.13923\n, 2025b.\nBai et al. [2025c]\nShuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Zhe Li, Pengxiang Ding, et al.\nEmbodied robot manipulation in the era of foundation models: Planning and learning perspectives.\narXiv preprint arXiv:2512.22983\n, 2025c.\nBai et al. [2025d]\nShuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Wei Zhao, Zhe Li, et al.\nTowards a unified understanding of robot manipulation: A comprehensive survey.\narXiv preprint arXiv:2510.10903\n, 2025d.\nBu et al. [2025]\nQingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al.\nAgibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems.\narXiv preprint arXiv:2503.06669\n, 2025.\nChen et al. [2024a]\nBoyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia.\nSpatialvlm: Endowing vision-language models with spatial reasoning capabilities.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 14455â€“14465, 2024a.\nChen et al. [2025a]\nPingyi Chen, Yujing Lou, Shen Cao, Jinhui Guo, Lubin Fan, Yue Wu, Lin Yang, Lizhuang Ma, and Jieping Ye.\nSd-vlm: Spatial measuring and understanding with depth-encoded vision-language models.\narXiv preprint arXiv:2509.17664\n, 2025a.\nChen et al. [2025b]\nQianzhong Chen, Justin Yu, Mac Schwager, Pieter Abbeel, Fred Shentu, and Philipp Wu.\nSarm: Stage-aware reward modeling for long horizon robot manipulation.\narXiv preprint arXiv:2509.25358\n, 2025b.\nChen et al. [2025c]\nSixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, et al.\nAc-dit: Adaptive coordination diffusion transformer for mobile manipulation.\narXiv preprint arXiv:2507.01961\n, 2025c.\nChen et al. [2025d]\nTianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al.\nRobotwin 2.0: A scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation.\narXiv preprint arXiv:2506.18088\n, 2025d.\nChen et al. [2024b]\nYi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu.\nEgoplan-bench: Benchmarking multimodal large language models for human-level planning, 2024b.\nURL\nhttps://arxiv.org/abs/2312.06722\n.\ncontributors [2024]\nAgiBot World Colosseum contributors.\nAgibot world colosseum.\nhttps://github.com/OpenDriveLab/AgiBot-World\n, 2024.\nContributors [2024]\nFlagScale Contributors.\nFlagscale: A unified meta-framework enabling adaptive heterogeneous computing for the llm ecosystem.\nhttps://github.com/FlagOpen/FlagScale\n, 2024.\nAccessed: 2025-06-26.\nDai et al. [2017]\nAngela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner.\nScannet: Richly-annotated 3d reconstructions of indoor scenes.\nIn\nCVPR\n, 2017.\nDeitke et al. [2024]\nMatt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al.\nMolmo and pixmo: Open weights and open data for state-of-the-art multimodal models.\narXiv preprint arXiv:2409.17146\n, 2024.\nDeitke et al. [2025]\nMatt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al.\nMolmo and pixmo: Open weights and open data for state-of-the-art vision-language models.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 91â€“104, 2025.\nDu et al. [2024]\nMengfei Du, Binhao Wu, Zejun Li, Xuan-Jing Huang, and Zhongyu Wei.\nEmbspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models.\nIn\nACL\n, 2024.\nFlagOpen [2025]\nFlagOpen.\nRobobrain-x0.\nhttps://github.com/FlagOpen/RoboBrain-X0\n, 2025.\nGitHub repository, accessed 2025-11-08.\nGoogle [2025]\nGoogle.\nGemini 3 pro: the frontier of vision ai.\nhttps://blog.google/innovation-and-ai/technology/developers-tools/gemini-3-pro-vision/\n, 2025.\nAccessed: 2025-05-06.\nGupta et al. [2019]\nAgrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A dataset for large vocabulary instance segmentation.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n, pages 5356â€“5364, 2019.\nHao et al. [2025]\nXiaoshuai Hao, Lei Zhou, Zhijian Huang, Zhiwen Hou, Yingbo Tang, Lingfeng Zhang, Guang Li, Zheng Lu, Shuhuai Ren, Xianhui Meng, et al.\nMimo-embodied: X-embodied foundation model technical report.\narXiv preprint arXiv:2511.16518\n, 2025.\nHe et al. [2025]\nZheqi He, Yesheng Liu, Jing shu Zheng, Xuejing Li, Jin-Ge Yao, Bowen Qin, Richeng Xuan, and Xi Yang.\nFlagevalmm: A flexible framework for comprehensive multimodal model evaluation.\n2025.\nURL\nhttps://arxiv.org/abs/2506.09081\n.\nHoque et al. [2025]\nRyan Hoque, Peide Huang, David J Yoon, Mouli Sivapurapu, and Jian Zhang.\nEgodex: Learning dexterous manipulation from large-scale egocentric video.\narXiv preprint arXiv:2505.11709\n, 2025.\nHurst et al. [2024]\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al.\nGpt-4o system card.\narXiv preprint arXiv:2410.21276\n, 2024.\nIntelligence et al. [2025]\nPhysical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al.\npi0.5: a vision-language-action model with open-world generalization.\narXiv preprint arXiv:2504.16054\n, 2025.\nJi et al. [2025a]\nYuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al.\nRobobrain: A unified brain model for robotic manipulation from abstract to concrete.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 1724â€“1734, 2025a.\nJi et al. [2025b]\nYuheng Ji, Yipu Wang, Yuyang Liu, Xiaoshuai Hao, Yue Liu, Yuting Zhao, Huaihai Lyu, and Xiaolong Zheng.\nVisualtrans: A benchmark for real-world visual transformation reasoning.\narXiv preprint arXiv:2508.04043\n, 2025b.\nJiang et al. [2025]\nTao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao.\nGalaxea open-world dataset and g0 dual-system vla model.\narXiv preprint arXiv:2509.00576\n, 2025.\nKhazatsky et al. [2024]\nAlexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al.\nDroid: A large-scale in-the-wild robot manipulation dataset.\narXiv preprint arXiv:2403.12945\n, 2024.\nKolve et al. [2017]\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al.\nAi2-thor: An interactive 3d environment for visual ai.\narXiv preprint arXiv:1712.05474\n, 2017.\nKuznetsova et al. [2020]\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al.\nThe open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.\nIJCV\n, 2020.\nLazarow et al. [2024]\nJustin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan.\nCubify anything: Scaling indoor 3d object detection.\narXiv preprint arXiv:2412.04458\n, 2024.\nLi et al. [2023]\nXuechen Li, Yifan Mai, Percy Liang, and Matei Zaharia.\nEnergon: Scaling megatron-lm training with data and expert parallelism, 2023.\nURL\nhttps://github.com/HazyResearch/megatron-energon\n.\nLi et al. [2024]\nZhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, and Laurence T Yang.\nLamp: Language-motion pretraining for motion generation, retrieval, and captioning.\narXiv preprint arXiv:2410.07093\n, 2024.\nLi et al. [2025a]\nZhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, et al.\nRobomirror: Understand before you imitate for video to humanoid locomotion.\narXiv preprint arXiv:2512.23649\n, 2025a.\nLi et al. [2025b]\nZhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, et al.\nDo you have freestyle? expressive humanoid locomotion via audio control.\narXiv preprint arXiv:2512.23650\n, 2025b.\nLi et al. [2025c]\nZhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, and Chang Xu.\nFrom language to locomotion: Retargeting-free humanoid control via motion latent guidance.\narXiv preprint arXiv:2510.14952\n, 2025c.\nLiao et al. [2024]\nYuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna.\nReasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models.\narXiv preprint arXiv:2409.09788\n, 2024.\nLiu et al. [2024a]\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al.\nDeepseek-v3 technical report.\narXiv preprint arXiv:2412.19437\n, 2024a.\nLiu et al. [2023]\nBo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone.\nLibero: Benchmarking knowledge transfer for lifelong robot learning.\nAdvances in Neural Information Processing Systems\n, 36:44776â€“44791, 2023.\nLiu et al. [2024b]\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 26296â€“26306, 2024b.\nLiu et al. [2024c]\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al.\nGrounding dino: Marrying dino with grounded pre-training for open-set object detection.\nIn\nECCV\n, 2024c.\nLyu et al. [2024]\nRuiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, and Jiangmiao Pang.\nMmscan: A multi-modal 3d scene dataset with hierarchical grounded language annotations.\narXiv preprint arXiv:2406.09401\n, 2024.\nMa et al. [2023a]\nXiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang.\nSqa3d: Situated question answering in 3d scenes.\nIn\nICLR\n, 2023a.\nURL\nhttps://openreview.net/forum?id=IDJx97BC38\n.\nMa et al. [2023b]\nYecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman.\nLiv: Language-image representations and rewards for robotic control.\nIn\nInternational Conference on Machine Learning\n, pages 23301â€“23320. PMLR, 2023b.\nMa et al. [2023c]\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar.\nEureka: Human-level reward design via coding large language models.\narXiv preprint arXiv:2310.12931\n, 2023c.\nMa et al. [2024]\nYecheng Jason Ma, Joey Hejna, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, et al.\nVision language models are in-context value learners.\nIn\nThe Thirteenth International Conference on Learning Representations\n, 2024.\nNasiriany et al. [2024]\nSoroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu.\nRobocasa: Large-scale simulation of everyday tasks for generalist robots.\narXiv preprint arXiv:2406.02523\n, 2024.\nNVIDIA [2021]\nNVIDIA.\nMegatron-lm: Training multi-billion parameter language models using model parallelism, 2021.\nURL\nhttps://github.com/NVIDIA/Megatron-LM\n.\nOpenAI [2025]\nOpenAI.\nUpdate to gpt-5 system card: Gpt-5.2.\nhttps://openai.com/zh-Hans-CN/index/gpt-5-system-card-update-gpt-5-2/\n, 2025.\nAccessed: 2025-05-06.\nOuyang [2025]\nKun Ouyang.\nSpatial-r1: Enhancing mllms in video spatial reasoning.\narXiv preprint arXiv:2504.01805\n, 2025.\nOâ€™Neill et al. [2024]\nAbby Oâ€™Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al.\nOpen x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 6892â€“6903. IEEE, 2024.\nPiccinelli et al. [2025]\nLuigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool.\nUnidepthv2: Universal monocular metric depth estimation made simpler.\narXiv\n, 2025.\nPyTorch Developers [2023]\nPyTorch Developers.\nCuda memory management, 2023.\nURL\nhttps://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management\n.\nQwen Team [2025]\nQwen Team.\nQwen2.5-vl: Multimodal llms from alibaba, 2025.\nURL\nhttps://github.com/QwenLM/Qwen2.5-VL\n.\nRamanathan et al. [2023]\nVignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al.\nPaco: Parts and attributes of common objects.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n, pages 7141â€“7151, 2023.\nRavi et al. [2025]\nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al.\nSam 2: Segment anything in images and videos.\nICLR\n, 2025.\nSermanet et al. [2024]\nPierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi, et al.\nRobovqa: Multimodal long-horizon reasoning for robotics.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n, pages 645â€“652. IEEE, 2024.\nSong et al. [2025]\nChan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield.\nRobospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n, pages 15768â€“15780, 2025.\nTan et al. [2025a]\nHuajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wang, Yuheng Ji, Cheng Chi, Yaoxu Lyu, Zhongxia Zhao, Xiansheng Chen, Peterson Co, et al.\nRobo-dopamine: General process reward modeling for high-precision robotic manipulation.\narXiv preprint arXiv:2512.23703\n, 2025a.\nTan et al. [2025b]\nHuajie Tan, Cheng Chi, Xiansheng Chen, Yuheng Ji, Zhongxia Zhao, Xiaoshuai Hao, Yaoxu Lyu, Mingyu Cao, Junkai Zhao, Huaihai Lyu, et al.\nRoboos-next: A unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration.\narXiv preprint arXiv:2510.26536\n, 2025b.\nTan et al. [2025c]\nHuajie Tan, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Yaoxu Lyu, Mingyu Cao, Zhongyuan Wang, and Shanghang Zhang.\nRoboos: A hierarchical embodied framework for cross-embodiment and multi-agent collaboration.\narXiv preprint arXiv:2505.03673\n, 2025c.\nTan et al. [2025d]\nHuajie Tan, Yuheng Ji, Xiaoshuai Hao, Xiansheng Chen, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang.\nReason-rft: Reinforcement fine-tuning for visual reasoning of vision language models.\nIn\nThe Thirty-ninth Annual Conference on Neural Information Processing Systems\n, 2025d.\nTan et al. [2026]\nHuajie Tan, Peterson Co, Yijie Xu, Shanyu Rong, Yuheng Ji, Cheng Chi, Xiansheng Chen, Qiongyu Zhang, Zhongxia Zhao, Pengwei Wang, et al.\nAction-sketcher: From reasoning to action via visual sketches for long-horizon robotic manipulation.\narXiv preprint arXiv:2601.01618\n, 2026.\nTeam et al. [2025a]\nBAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al.\nRobobrain 2.0 technical report.\narXiv preprint arXiv:2507.02029\n, 2025a.\nTeam et al. [2025b]\nGemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al.\nGemini robotics: Bringing ai into the physical world.\narXiv preprint arXiv:2503.20020\n, 2025b.\nTeam [2025]\nQwen Team.\nQwq-32b: Embracing the power of reinforcement learning, March 2025.\nURL\nhttps://qwenlm.github.io/blog/qwq-32b/\n.\nTong et al. [2024]\nPeter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al.\nCambrian-1: A fully open, vision-centric exploration of multimodal llms.\nNeurIPS\n, 2024.\nWald et al. [2019]\nJohanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias NieÃŸner.\nRio: 3d object instance re-localization in changing indoor environments.\nIn\nICCV\n, pages 7658â€“7667, 2019.\nWang et al. [2025]\nYipu Wang, Yuheng Ji, Yuyang Liu, Enshen Zhou, Ziqiang Yang, Yuxuan Tian, Ziheng Qin, Yue Liu, Huajie Tan, Cheng Chi, et al.\nTowards cross-view point correspondence in vision-language models.\narXiv preprint arXiv:2512.04686\n, 2025.\nYuan et al. [2024]\nWentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox.\nRobopoint: A vision-language model for spatial affordance prediction for robotics, 2024.\nURL\nhttps://arxiv.org/abs/2406.10721\n.\nYuan et al. [2025]\nYifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao.\nEmbodied-r1: Reinforced embodied reasoning for general robotic manipulation.\narXiv preprint arXiv:2508.13998\n, 2025.\nZhai et al. [2025]\nShaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang, Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, and Jiangmiao Pang.\nA vision-language-action-critic model for robotic real-world reinforcement learning.\narXiv preprint arXiv:2509.15937\n, 2025.\nZhang et al. [2025a]\nWenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, et al.\nEmbodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks.\narXiv preprint arXiv:2503.21696\n, 2025a.\nZhang et al. [2025b]\nYi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, and Shi-Min Hu.\nBee: A high-quality corpus and full-stack suite to unlock advanced fully open mllms, 2025b.\nURL\nhttps://arxiv.org/abs/2510.13795\n.\nZhang et al. [2024]\nYoucai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al.\nRecognize anything: A strong image tagging model.\nIn\nCVPR\n, 2024.\nZhou et al. [2024]\nEnshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, and He Wang.\nCode-as-monitor: Constraint-aware visual programming for reactive and proactive robotic failure detection.\narXiv preprint arXiv:2412.04455\n, 2024.\nZhou et al. [2025a]\nEnshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al.\nRoborefer: Towards spatial referring with reasoning in vision-language models for robotics.\narXiv preprint arXiv:2506.04308\n, 2025a.\nZhou et al. [2025b]\nEnshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, et al.\nRobotracer: Mastering spatial trace with reasoning in vision-language models for robotics.\narXiv preprint arXiv:2512.13660\n, 2025b.\nZhu et al. [2023]\nShengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu.\nTame a wild camera: in-the-wild monocular camera calibration.\nNIPS\n, 2023.\n8\nContributions and Author List\nCore Contributors\nâ€¢\nHuajie Tan\nâˆ—â€ \nâ€¢\nEnshen Zhou\nâˆ—\nâ€¢\nZhiyu Li\nâ€¢\nYijie Xu\nâ€¢\nYuheng Ji\nâ€¢\nXiansheng Chen\nâ€¢\nCheng Chi\nâ€¢\nPengwei Wang\nâ€ \nâ€¢\nHuizhu Jia\nâ€¢\nYulong Ao\nâ€¢\nYonghua Lin\nâ€¢\nZhongyuan Wang\nâ€¢\nTiejun Huang\nâ€¢\nShanghang Zhang\nğŸ–‚\n{}^{\\text{\\Letter}}\nContributors\nâ€¢\nMingyu Cao\nâ€¢\nSixiang Chen\nâ€¢\nZhe Li\nâ€¢\nMengzhen Liu\nâ€¢\nZixiao Wang\nâ€¢\nShanyu Rong\nâ€¢\nYaoxu Lyu\nâ€¢\nZhongxia Zhao\nâ€¢\nPeterson Co\nâ€¢\nYibo Li\nâ€¢\nYi Han\nâ€¢\nShaoxuan Xie\nâ€¢\nGuocai Yao\nâ€¢\nSongjing Wang\nâ€¢\nLeiduo Zhang\nâ€¢\nXi Yang\nâ€¢\nYance Jiao\nâ€¢\nDonghai Shi\nâ€¢\nKunchang Xie\nâ€¢\nShaokai Nie\nâ€¢\nChunlei Men\nâ€ \nâ€ \nfootnotetext:\nâˆ—\nEqual Contribution (Co-first Authors).\nâ€ \nâ€ \nfootnotetext:\nâ€ \nProject Leaders.\nâ€ \nâ€ \nfootnotetext:\nğŸ–‚\n{}^{\\text{\\Letter}}\nCorresponding Author. Team Email:\nrobobrain@baai.ac.cn\n\\beginappendix\n9\nQualitative examples\nThis section provides a comprehensive set of qualitative examples that illustrate the capabilities of RoboBrain 2.5 in various embodied AI tasks. As Capabilities like pointing, affordance, planning, etc. are similar to those shown in RoboBrain 2.0\n[\n72\n]\n, the examples in this section\nONLY\ndemonstrate the modelâ€™s proficiency in 3D spatial reasoning, temporal value estimation, showcasing its potential for real-world applications.\n9.1\nExamples on 3D Spatial Reasoning\nIn this section, we provide qualitative visualizations to demonstrate the robustness and precision of RoboBrain 2.5â€™s 3D spatial reasoning capabilities in real-world manipulation scenarios. We focus on three core aspects: compliance with fine-grained spatial constraints, multi-step compositional reasoning for complex manipulation tasks, and generalization across diverse indoor environments and object categories.\nCompliance with Fine-Grained Spatial Constraints.\nRoboBrain 2.5 excels at interpreting spatially constrained instructions and generating accurate 3D manipulation traces that adhere to both relative positional requirements and metric constraints. For instructions like â€œPick up the third picture frame from the left on the piano, and move it to the right of the biggest wooden chairâ€ in\nFigure\nËœ\n3\nor â€œPick up the rightmost vase on the desk, and move it to the spot between the black monitor and the water bottleâ€ in\nFigure\nËœ\n4\n, the model accurately parses ordinal references and spatial relationships to generate collision-free 3D trajectories.\nMulti-Step Compositional Reasoning.\nComplex manipulation tasks often demand multi-step reasoning to decompose high-level goals into executable sub-tasks. RoboBrain 2.5â€™s 3D spatial tracing capability naturally encodes this compositional logic by generating ordered keypoint sequences. For example, instructions like â€œPick up the orange object at right which is on the window sill, and move it to a spot which is on the sinkâ€™s edge and closest to the right wallâ€ require the model to firstly localize the orange object on the window sill, then estimate the sinkâ€™s edge position and its distance to the right wall, and finally generate a smooth 3D trace connecting the two points while avoiding obstacles.\nGeneralization Across Environments and Objects.\nRoboBrain 2.5â€™s 3D spatial reasoning generalizes to diverse indoor settings and object categories. Tasks span kitchen-centric scenarios in\nFigure\nËœ\n4\n, bedroom settings in\nFigure\nËœ\n5\n, and office/study spaces. The model maintains precision across these environments by leveraging metric-grounded spatial representations independent of scene context. Besides, the model handles objects of varying sizes, shapes, and functionalities, which are from small items to larger objects, and adapts its 3D traces to object-specific affordances.\nApplication on RoboTwin 2.0\nTo further validate the generalization capabilities of RoboBrain 2.5 in simulated collaborative environments, we present qualitative results on the RoboTwin 2.0\n[\n17\n]\n, specifically focusing on AgiLex dual-arm manipulation tasks. As illustrated in\nFigure\nËœ\n6\n-\nFigure\nËœ\n9\n, the model demonstrates robust performance in generating precise 3D spatial traces from complex natural language instructions. For example,\nFigure\nËœ\n6\nhighlights the modelâ€™s fine-grained spatial discrimination and reasoning abilities. In tasks such as\nClick Bell\n,\nClick Alarmclock\n, and\nBlocks Ranking\n, RoboBrain 2.5 accurately identifies specific targets based on relative spatial descriptions (\ne.g\n.\n, â€œclosest to milk boxâ€, â€œto the left of globeâ€) and comparative attributes (\ne.g\n.\n, â€œsecond largestâ€, â€œfarthest from the cameraâ€). The generated traces correctly guide the agent to the intended objects among multiple distractors.\nFigure\nËœ\n7\nshowcases the modelâ€™s proficiency in dual-arm coordination and object manipulation. In scenarios like\nHandover Block\n,\nHandover Mic\n,\nHanging Mug\n, and\nMove Can Pot\n, the model predicts coherent spatial traces that effectively handle arm-specific instructions (\ne.g\n.\n, â€œwith the right armâ€) and precise placement goals (\ne.g\n.\n, â€œhang it onto the rackâ€). These examples confirm that RoboBrain 2.5 can successfully ground high-level semantic instructions into collision-free, metric-aware trajectories for high-DoF robotic systems.\nThese examples collectively demonstrate that RoboBrain 2.5â€™s precise 3D spatial reasoning capability effectively bridges the gap between natural language instructions and physical execution, enabling precise, constraint-compliant manipulation in real-world settings.\nFigure 3\n:\nVisualization of TraceSpatial-Bench Rollouts and RoboBrain 2.5â€™s Predicted Traces.\nThe red mask marks the ground-truth starting point, the purple 3D bounding box represents the ground-truth endpoint, and the 2D projection of RoboBrain 2.5â€™s predicted 3D spatial trace is displayed.\nFigure 4\n:\nVisualization of TraceSpatial-Bench Rollouts and RoboBrain 2.5â€™s Predicted Traces.\nThe red mask marks the ground-truth starting point, the purple 3D bounding box represents the ground-truth endpoint, and the 2D projection of RoboBrain 2.5â€™s predicted 3D spatial trace is displayed.\nFigure 5\n:\nVisualization of TraceSpatial-Bench Rollouts and RoboBrain 2.5â€™s Predicted Traces.\nThe red mask marks the ground-truth starting point, the purple 3D bounding box represents the ground-truth endpoint, and the 2D projection of RoboBrain 2.5â€™s predicted 3D spatial trace is displayed.\nFigure 6\n:\nVisualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5â€™s Predicted Traces.\nExamples for AgiLex Dual-Arm tasks: Click Bell; Click Alarm clock; Blocks Ranking.\nFigure 7\n:\nVisualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5â€™s Predicted Traces.\nExamples for AgiLex Dual-Arm tasks: Handover Block; Handover Mic; Hanging Mug; Move Can Pot.\nFigure 8\n:\nVisualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5â€™s Predicted Traces.\nExamples for AgiLex Dual-Arm tasks: Move Playingcard Away; Move Stapler Pad; Open Laptop; Place A2B Left.\nFigure 9\n:\nVisualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5â€™s Predicted Traces.\nExamples for AgiLex Dual-Arm tasks: Place A2B Right; Place Bread Basket; Place Bread Skillet; Place Burger Fries.\n9.2\nExamples on Temporal Value Estimation\nIn this section, we provide additional qualitative visualizations to further substantiate the effectiveness and robustness of our method. We focus on three key aspects: the generalization of RoboBrain 2.5 across diverse semantic tasks, the temporal robustness of progress estimation under varying sampling intervals, and the trajectory visualization of real-world RL.\nDense Value Predictions on Diverse Tasks.\nFigure\nËœ\n10\nillustrates the predicted Hop and Progress curves generated by our RoboBrain 2.5 across a wide spectrum of manipulation tasks, encompassing both real-world scenarios and simulation environments. Specifically, we visualize complex tasks such as deformable object manipulation (e.g.,\nFold the Pants\n), unstructured real-world interaction (e.g.,\nClean the table\n), and precise multi-stage simulation tasks (e.g.,\nStack three Bowls\n,\nOpen the drawer\n). The results demonstrate that our model effectively generalizes across these distinct domains. In successful trajectories, the model consistently predicts positive Hop values corresponding to effective state transitions, which accumulate into a smooth, monotonic Progress curve that accurately reflects task completion. Crucially, the model is able to distinguish between effective progress and background noise, providing a stable signal even in visually cluttered real-world settings.\nRobustness to Temporal Intervals.\nA robust reward model should remain consistent regardless of the video sampling rate or the control frequency of the robot.\nFigure\nËœ\n11\nprovides a comparative analysis of RoboBrain 2.5â€™s progress estimation when inputs are sampled at significantly different intervals (\nÎ”\nâ€‹\nt\n=\n10\n,\n25\n,\n50\n,\n100\n\\Delta t=10,25,50,100\nframes). As shown in the comparison, although the visual disparity between adjacent frames increases drastically with larger\nÎ”\nâ€‹\nt\n\\Delta t\n, the model adaptively predicts larger Hop values to account for the increased semantic distance. Consequently, the reconstructed global progress curves across all sampling rates exhibit high alignment and overlap. This invariance highlights the modelâ€™s ability to decouple physical progress from temporal duration, ensuring that the value estimation remains reliable whether the agent operates at high frequency or processes sparse keyframes.\nVisualization of Different Progress Estimation Modes.\nFurthermore, to elucidate the robustness of our progress estimation,\nFigure\nËœ\n12\nvisualizes the three complementary perspectives used in our Multi-Perspective Progress Fusion strategy. Specifically, (a)\nIncremental Prediction\nrecursively accumulates frame-wise hop values to capture fine-grained local dynamics; (b)\nForward-Anchored Prediction\nestimates progress relative to the initial state, providing a stable baseline during early execution; and (c)\nBackward-Anchored Prediction\nmeasures progress against the goal state, offering high sensitivity near task completion. As demonstrated, all three modes yield consistent, monotonic progress curves on unseen validation tasks, confirming that fusing these perspectives effectively mitigates error accumulation while maintaining global consistency.\nReal-World RL Rollout Visualization.\nFinally,\nFigure\nËœ\n13\nvisualizes the robustness of the policy learned for the â€œInsert Blockâ€ task. The policy used in this rollout was trained for approximately 20 minutes and achieved a success rate of over 95%. To evaluate the policyâ€™s reactivity and the reward modelâ€™s accuracy, we introduced an artificial disturbance during execution. As shown in the sequence, a human operator manually moves the target slot while the robot is in motion (a). This intervention causes the robot to miss the target and fall into misalignment (b). Crucially, the inset plots show that RoboBrain 2.5 immediately reflects this setback: the estimated\nProgress\ndrops sharply, correctly identifying that the state has regressed from the goal. This accurate negative feedback guides the agent to adjust its trajectory. The robot successfully recovers from the misalignment (c), repositions itself above the target (d), aligns with the slot (e), and completes the insertion (f). This demonstrates that RoboBrain 2.5 provides dense, semantically meaningful rewards that enable the agent to recover from unexpected external perturbations.\nFigure 10\n:\nRoboBrain 2.5 Progress Predictions across Diverse Tasks.\nWe visualize the frame-wise\nHop\n(instantaneous change) and accumulated\nProgress\npredicted by RoboBrain 2.5 on unseen validation tasks.\nFigure 11\n:\nProgress Estimation Consistency across Sampling Intervals.\nWe plot the reconstructed progress curves for the same trajectory using different frame strides (10, 25, 50, and 100 frames). The high overlap between curves demonstrates that our RoboBrain 2.5 is robust to temporal granularity and does not simply overfit to a specific frame rate.\nFigure 12\n:\nRoboBrain 2.5 Progress Predictions across three modes.\nWe visualize the frame-wise\nHop\n(instantaneous change) and accumulated\nProgress\npredicted by RoboBrain 2.5 with incremental, forward-anchored and backward-anchored mode.\nFigure 13\n:\nRobustness to Artificial Disturbance during Real-World Execution.\nWe visualize a rollout of the converged policy (success rate\n>\n95\n%\n>95\\%\n) under human interference. Each sub-figure shows the third-person view, the ego-centric view, and the real-time RoboBrain 2.5 inference (Top:\nHop\n, Bottom:\nProgress\n).\n(a) Artificial Disturbance Position:\nA human hand intervenes and shifts the target board while the robot attempts to approach.\n(b) Fall Into Misalignment:\nThe robot misses the new position. Note that the RoboBrain 2.5\nProgress\ncurve drops significantly (indicated by the red dot in the bottom inset), reflecting the failure state.\n(c) Misalignment Recovery:\nThe policy reacts to the visual feedback and the drop in reward, adjusting the end-effector position.\n(d) Move to the top:\nThe robot realigns directly above the target slot.\n(e) Align with the Slot:\nPrecise fine-tuning before insertion.\n(f) Successful Insertion:\nThe task is completed, with the progress estimation reaching its peak.\n10\nProof of Bounded Global Progress\nIn this subsection, we provide a formal proof that iteratively applying the predicted relative progress hops guarantees that the reconstructed global progress\nÎ¦\nâ‹†\nâ€‹\n(\ns\n)\n\\Phi^{\\star}(s)\nremains strictly within the bounds\n[\n0\n,\n1\n]\n[0,1]\n, provided that the initial state is bounded and the model predictions lie within\n[\nâˆ’\n1\n,\n1\n]\n[-1,1]\n.\nFirst, we define the general recursive update rule. Based on the definition of the hop label\nâ„‹\nâ€‹\n(\ns\np\n,\ns\nq\n)\n\\mathcal{H}(s_{p},s_{q})\nin Equation\n2\n, we derive the recursive update rule for estimating the global progress of the next state\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n\\Phi^{\\star}(s_{t})\ngiven the current state\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n\\Phi^{\\star}(s_{t-1})\nand the predicted hop\nH\n=\nâ„‹\nâ€‹\n(\ns\nt\nâˆ’\n1\n,\ns\nt\n)\nH=\\mathcal{H}(s_{t-1},s_{t})\n. We assume the normalization where\nÎ¦\nâ€‹\n(\ns\n0\n)\n=\n0\n\\Phi(s_{0})=0\nand\nÎ¦\nâ€‹\n(\ns\nM\n)\n=\n1\n\\Phi(s_{M})=1\n. Rearranging the equation, the update rule is:\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\n{\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n+\nH\nâ‹…\n[\n1\nâˆ’\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n]\nif\nâ€‹\nH\nâ‰¥\n0\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\n+\nH\nâ‹…\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\nif\nâ€‹\nH\n<\n0\n\\Phi^{\\star}(s_{t})=\\begin{cases}\\Phi^{\\star}(s_{t-1})+H\\cdot[1-\\Phi^{\\star}(s_{t-1})]&\\text{if }H\\geq 0\\\\\n\\Phi^{\\star}(s_{t-1})+H\\cdot\\Phi^{\\star}(s_{t-1})&\\text{if }H<0\\end{cases}\n(12)\nGiven that the initial progress\nÎ¦\nâ‹†\nâ€‹\n(\ns\n0\n)\n=\n0\n\\Phi^{\\star}(s_{0})=0\nand the predicted hop\nH\nâˆˆ\n[\nâˆ’\n1\n,\n1\n]\nH\\in[-1,1]\n, the reconstructed global progress\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n\\Phi^{\\star}(s_{t})\nsatisfies\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\nâˆˆ\n[\n0\n,\n1\n]\n\\Phi^{\\star}(s_{t})\\in[0,1]\nfor all steps\nt\nt\n.\nWe proceed by mathematical induction as follow:\n(1) Base Case (\nt\n=\n0\nt=0\n):\nBy definition,\nÎ¦\nâ‹†\nâ€‹\n(\ns\n0\n)\n=\n0\n\\Phi^{\\star}(s_{0})=0\n, which satisfies\n0\nâˆˆ\n[\n0\n,\n1\n]\n0\\in[0,1]\n.\n(2) Inductive Step:\nAssume that for step\nt\nâˆ’\n1\nt-1\n, the hypothesis holds:\n0\nâ‰¤\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\nâ‰¤\n1\n0\\leq\\Phi^{\\star}(s_{t-1})\\leq 1\n.\nLet\nG\n=\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\nâˆ’\n1\n)\nG=\\Phi^{\\star}(s_{t-1})\nfor brevity, where\nG\nâˆˆ\n[\n0\n,\n1\n]\nG\\in[0,1]\n. We analyze the next state\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n\\Phi^{\\star}(s_{t})\nunder two cases (\ni.e.,\nPositive Hop and Negative Hop) based on the sign of the predicted hop\nH\nH\n.\nâ€¢\nCase 1: Positive Hop (Progress),\n0\nâ‰¤\nH\nâ‰¤\n1\n0\\leq H\\leq 1\n.\nFrom\nEquation\nËœ\n12\n, the update is written as:\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\nG\n+\nH\nâ€‹\n(\n1\nâˆ’\nG\n)\n\\Phi^{\\star}(s_{t})=G+H(1-G)\n(13)\nRearranging terms to view this as a convex combination:\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\nH\n+\nG\nâ€‹\n(\n1\nâˆ’\nH\n)\n\\Phi^{\\star}(s_{t})=H+G(1-H)\n(14)\nLower Bound:\nSince\nG\nâ‰¥\n0\nG\\geq 0\n,\nH\nâ‰¥\n0\nH\\geq 0\n, and\n(\n1\nâˆ’\nH\n)\nâ‰¥\n0\n(1-H)\\geq 0\n, it follows that\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\nâ‰¥\n0\n\\Phi^{\\star}(s_{t})\\geq 0\n.\nUpper Bound:\nSince\nG\nâ‰¤\n1\nG\\leq 1\n, we substitute the maximum value of\nG\nG\n:\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n\\displaystyle\\Phi^{\\star}(s_{t})\n=\nH\n+\nG\nâ€‹\n(\n1\nâˆ’\nH\n)\n\\displaystyle=H+G(1-H)\nâ‰¤\nH\n+\n1\nâ‹…\n(\n1\nâˆ’\nH\n)\n\\displaystyle\\leq H+1\\cdot(1-H)\n=\nH\n+\n1\nâˆ’\nH\n\\displaystyle=H+1-H\n=\n1\n\\displaystyle=1\nThus,\n0\nâ‰¤\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\nâ‰¤\n1\n0\\leq\\Phi^{\\star}(s_{t})\\leq 1\nwhen\nH\nâ‰¥\n0\nH\\geq 0\n.\nâ€¢\nCase 2: Negative Hop (Regress),\nâˆ’\n1\nâ‰¤\nH\n<\n0\n-1\\leq H<0\n.\nFrom\nEquation\nËœ\n12\n, the update is:\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\nG\n+\nH\nâ‹…\nG\n=\nG\nâ€‹\n(\n1\n+\nH\n)\n\\Phi^{\\star}(s_{t})=G+H\\cdot G=G(1+H)\n(15)\nLower Bound:\nSince\nH\nâˆˆ\n[\nâˆ’\n1\n,\n0\n)\nH\\in[-1,0)\n, the term\n(\n1\n+\nH\n)\nâ‰¥\n0\n(1+H)\\geq 0\n. Since\nG\nâ‰¥\n0\nG\\geq 0\n, the product\nG\nâ€‹\n(\n1\n+\nH\n)\nâ‰¥\n0\nG(1+H)\\geq 0\n.\nUpper Bound:\nSince\nH\n<\n0\nH<0\n, the term\n(\n1\n+\nH\n)\n<\n1\n(1+H)<1\n. Combining this with\nG\nâ‰¤\n1\nG\\leq 1\n:\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\n=\nG\nâ€‹\n(\n1\n+\nH\n)\nâ‰¤\n1\nâ‹…\n(\n1\n)\n=\n1\n\\Phi^{\\star}(s_{t})=G(1+H)\\leq 1\\cdot(1)=1\nThus,\n0\nâ‰¤\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\nâ‰¤\n1\n0\\leq\\Phi^{\\star}(s_{t})\\leq 1\nwhen\nH\n<\n0\nH<0\n.\nConclusion.\nSince the property holds for the base case and is preserved in both update scenarios during the inductive step, we conclude that\nÎ¦\nâ‹†\nâ€‹\n(\ns\nt\n)\nâˆˆ\n[\n0\n,\n1\n]\n\\Phi^{\\star}(s_{t})\\in[0,1]\nfor all\nt\nt\n.",
    "preview_text": "We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io\n\n\\contribution\nPlease see\nContributions and Author List\nfor more author details.\nRoboBrain 2.5: Depth in Sight, Time in Mind.\nBAAI RoboBrain Team\nAbstract\nWe introduce\nRoboBrain 2.5\n, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks\nPrecise 3D Spatial Reasoning\nby shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes\nDense Temporal Value Estimation\nthat provides dense, step-aware progress prediction an",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "VLA",
        "whole body control"
    ],
    "one_line_summary": "RoboBrain 2.5æ˜¯ä¸€ä¸ªå…·èº«AIåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç²¾ç¡®çš„3Dç©ºé—´æ¨ç†å’Œå¯†é›†æ—¶é—´ä»·å€¼ä¼°è®¡ï¼Œæå‡å¤æ‚ç²¾ç»†æ“ä½œçš„ç‰©ç†åŸºç¡€å’Œæ‰§è¡Œæ„ŸçŸ¥èƒ½åŠ›ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-20T17:21:54Z",
    "created_at": "2026-01-27T15:53:14.346507",
    "updated_at": "2026-01-27T15:53:14.346514",
    "recommend": 0
}