{
    "id": "2601.07333v1",
    "title": "OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image",
    "authors": [
        "Tessa Pulli",
        "Jean-Baptiste Weibel",
        "Peter HÃ¶nig",
        "Matthias Hirschmanner",
        "Markus Vincze",
        "Andreas Holzinger"
    ],
    "abstract": "å…­ç»´ç‰©ä½“å§¿æ€ä¼°è®¡åœ¨æœºå™¨äººæŠ€æœ¯å’Œå¢å¼ºç°å®ç­‰åº”ç”¨åœºæ™¯ç†è§£ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚ä¸ºé€‚åº”åŠ¨æ€å˜åŒ–ç‰©ä½“é›†åˆçš„éœ€æ±‚ï¼Œç°ä»£é›¶æ ·æœ¬ç‰©ä½“å§¿æ€ä¼°è®¡å™¨æ— éœ€é’ˆå¯¹ç‰¹å®šç‰©ä½“è¿›è¡Œè®­ç»ƒï¼Œä»…ä¾èµ–CADæ¨¡å‹å³å¯å·¥ä½œã€‚ç„¶è€Œè¿™ç±»æ¨¡å‹ä¸€æ—¦éƒ¨ç½²åéš¾ä»¥è·å–ï¼Œä¸”æŒç»­å˜åŒ–å¢é•¿çš„ç‰©ä½“é›†åˆä½¿å¾—å¯é è¯†åˆ«ç›®æ ‡å®ä¾‹æ¨¡å‹å˜å¾—æ›´åŠ å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºOSCARæ–¹æ³•â€”â€”ä¸€ç§åŸºäºè¯­è¨€æç¤ºä¸å•å¼ å›¾åƒçš„å¼€é›†CADæ¨¡å‹æ£€ç´¢ç³»ç»Ÿã€‚è¿™ç§æ— éœ€è®­ç»ƒçš„æ–°æ–¹æ³•èƒ½å¤Ÿä»æœªæ ‡æ³¨çš„ä¸‰ç»´ç‰©ä½“æ•°æ®åº“ä¸­æ£€ç´¢åŒ¹é…çš„ç‰©ä½“æ¨¡å‹ã€‚\n\nåœ¨æ¨¡å‹å…¥åº“é˜¶æ®µï¼ŒOSCARé€šè¿‡å›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹ä¸ºæ•°æ®åº“æ¨¡å‹çš„å¤šè§†è§’æ¸²æŸ“å›¾è‡ªåŠ¨ç”Ÿæˆæè¿°æ€§æ ‡æ³¨ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒGroundedSAMé¦–å…ˆæ£€æµ‹è¾“å…¥å›¾åƒä¸­çš„æŸ¥è¯¢ç‰©ä½“ï¼Œéšåä¸ºæ„Ÿå…´è¶£åŒºåŸŸå’Œæ•°æ®åº“æ ‡æ³¨åŒæ—¶è®¡ç®—å¤šæ¨¡æ€åµŒå…¥å‘é‡ã€‚OSCARé‡‡ç”¨ä¸¤é˜¶æ®µæ£€ç´¢ç­–ç•¥ï¼šé¦–å…ˆåˆ©ç”¨CLIPè¿›è¡ŒåŸºäºæ–‡æœ¬çš„è¿‡æ»¤ä»¥ç¡®å®šå€™é€‰æ¨¡å‹ï¼Œå†é€šè¿‡DINOv2è¿›è¡ŒåŸºäºå›¾åƒçš„ç²¾ç»†åŒ–ç­›é€‰ï¼Œæœ€ç»ˆé€‰å–è§†è§‰ç›¸ä¼¼åº¦æœ€é«˜çš„ç‰©ä½“ã€‚\n\nå®éªŒè¡¨æ˜ï¼ŒOSCARåœ¨è·¨é¢†åŸŸä¸‰ç»´æ¨¡å‹æ£€ç´¢åŸºå‡†MI3DORä¸Šè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†OSCARåœ¨å…­ç»´ç‰©ä½“å§¿æ€ä¼°è®¡ä¸­è‡ªåŠ¨åŒ–è·å–ç‰©ä½“æ¨¡å‹çš„ç›´æ¥é€‚ç”¨æ€§ï¼šå½“æ— æ³•è·å¾—å®Œå…¨ç›¸åŒçš„å®ä¾‹æ¨¡å‹æ—¶ï¼Œå¯é‡‡ç”¨æœ€ç›¸ä¼¼ç‰©ä½“æ¨¡å‹è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚åœ¨YCB-Vç‰©ä½“æ•°æ®é›†ä¸Šçš„æ£€ç´¢å®éªŒæ˜¾ç¤ºï¼ŒOSCARè¾¾åˆ°90.48%çš„å¹³å‡ç²¾åº¦ã€‚è¿›ä¸€æ­¥ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨Megaposeå§¿æ€ä¼°è®¡ç®—æ³•æ—¶ï¼Œé‡‡ç”¨æœ€ç›¸ä¼¼ç‰©ä½“æ¨¡å‹èƒ½è·å¾—æ¯”åŸºäºé‡å»ºçš„æ–¹æ³•æ›´ä¼˜çš„ç»“æœã€‚",
    "url": "https://arxiv.org/abs/2601.07333v1",
    "html_url": "https://arxiv.org/html/2601.07333v1",
    "html_content": "OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image\nTessa Pulli\nJean-Baptiste Weibel\nPeter HÃ¶nig\nMatthias Hirschmanner\nMarkusÂ Vincze\nAndreas Holzinger\nAutomation and Control Institute, TU Wien, Wien, Austria\n{last_name}@acin.tuwien.ac.at\nBOKU University, Human-Centered AI Lab, FTEC, Department for Ecosystem Management, Climate and Biodiversity, Wien, Austria\n{first_name}.{last_name}@boku.ac.at\nInstitute for Human Centered Computing, Faculty of Informatics and Biomedical Engineering, TU Graz, Graz, Austria\na.holzinger@tugraz.at\nAbstract\n6D object pose estimation plays a crucial role in scene understanding for applications such as robotics and augmented reality.\nTo support the needs of ever-changing object sets in such context, modern zero-shot object pose estimators were developed to not require object-specific training but only rely on CAD models.\nSuch models are hard to obtain once deployed, and a continuously changing and growing set of objects makes it harder to reliably identify the instance model of interest.\nTo address this challenge, we introduce an\nO\npen-\nS\net\nCA\nD\nR\netrieval from a Language\nPrompt and a Single Image (OSCAR), a novel training-free method that retrieves a matching object model from an unlabeled 3D object database.\nDuring onboarding, OSCAR generates multi-view renderings of database models and annotates them with descriptive captions using an image captioning model.\nAt inference, GroundedSAM detects the queried object in the input image, and multi-modal embeddings are computed for both the Region-of-Interest and the database captions. OSCAR employs a two-stage retrieval: text-based filtering using CLIP identifies candidate models, followed by image-based refinement using DINOv2 to select the most visually similar object.\nIn our experiments we demonstrate that OSCAR outperforms all state-of-the-art methods on the cross-domain 3D model retrieval benchmark MI3DOR.\nFurthermore, we demonstrate OSCARâ€™s direct applicability in automating object model sourcing for 6D object pose estimation.\nWe propose using the most similar object model for pose estimation if the exact instance is not available and show that OSCAR achieves an average precision of 90.48% during object retrieval on the YCB-V object dataset.\nMoreover, we demonstrate that the most similar object model can be utilized for pose estimation using Megapose achieving better results than a reconstruction-based approach.\nkeywords:\n3D Model Retrieval , 6D Object Pose Estimation , Image-based Object Retrieval\nâ€ \nâ€ \njournal:\nIMAVIS\n1\nIntroduction\n6D object pose estimation is a key task in robotics, and Augmented Reality.\nInstance-level approaches\n(Wang\net al.\n,\n2021\n)\nare commonly used and have shown impressive results on known objects. They however, require a re-training to handle any additional unseen instances.\nRecently, zero-shot pose estimation methods emerged to estimate 6D object poses of novel instances\n(LabbÃ©\net al.\n,\n2022\n; Ausserlechner\net al.\n,\n2024\n; Wen\net al.\n,\n2024\n; Lin\net al.\n,\n2024\n; Thalhammer\net al.\n,\n2023\n)\n.\nExisting zero-shot methods can be divided into model-based\n(LabbÃ©\net al.\n,\n2022\n; Ausserlechner\net al.\n,\n2024\n; Wen\net al.\n,\n2024\n; Lin\net al.\n,\n2024\n)\nand model-free approaches\n(Wen\net al.\n,\n2024\n; Shugurov\net al.\n,\n2022\n; Caraffa\net al.\n,\n2024\n; Lee\net al.\n,\n2024\n)\n, depending on whether a 3D mesh is required.\nModel-based methods typically yield better performance results\n(Hodan\net al.\n,\n2024\n)\nbut assume the availability of a ground-truth 3D object model.\nAcquiring these CAD models is a time-consuming process that requires special equipment or skilled personnel, and considerable pre-processing before they can be included as a reference model, limiting the feasibility of modeling once the autonomous agent is deployed.\nFurthermore, maintaining and utilizing a large, dynamic database of 3D object models presents significant challenges for open-set 6D pose estimation.\nEven when models are available, they are often unlabeled or categorized only by coarse classes.\nThe manual effort required to accurately label and maintain thousands of object models in large databases like in Megapose\n(LabbÃ©\net al.\n,\n2022\n)\n, is prone to error.\nWhile CAD model retrieval is an established research direction\n(GÃ¼meli\net al.\n,\n2022\n; Gao\net al.\n,\n2024\n)\n, 3D object model retrieval for 6D object pose estimation is largely unexplored.\nTo address this problem, we propose a novel method that retrieves a corresponding 3D object model from an unlabeled database using an image and text input.\nFigure 1\n:\nOverview: We propose\nOSCAR\nto retrieve a 3D object model from an unlabeled database of 3D objects with a single image input and a language prompt.\nIn this paper, we introduce\nOSCAR\n, short for\nO\npen-\nS\net\nCA\nD\nR\netrieval from a Language Prompt and a Single Image (see Fig.\n1\n).\nDuring the onboarding stage, each 3D model of the database is rendered from multiple viewpoints, and an image captioning model generates descriptive texts for each view.\nThese textual descriptions are stored alongside the models to enable language-guided retrieval without manual annotation.\nDuring inference, we segment the Region-of-Interest (ROI) from an RGB image using GroundedSAM\n(Ren\net al.\n,\n2024\n)\nwith a language prompt.\nOSCAR relies on a two-stage object retrieval:\nFirstly, we compute the textual embeddings for each description of the rendered views, together with the image embedding of the RoI using CLIP.\nCandidates whose cosine similarity passes a defined threshold are selected.\nFor each candidate, the ROI embedding is compared to embeddings of pre-rendered views of the CAD model using DINOv2.\nWith this strategy, we select the model with the highest similarity, considering textual embeddings and image embeddings.\nIn summary, our contributions are the following:\n1.\nWe demonstrate that the fusion of vision-language embeddings (CLIP) and self-supervised visual embeddings (DINOv2) is superior to geometry or purely visual feature-based retrieval methods, achieving state-of-the-art performance on the cross-domain 3D model retrieval benchmark MI3DOR.\n2.\nWe show, that utilizing a clean, retrieved 3D model yields superior 6D pose estimation accuracy and better efficiency compared to approaches relying on on-the-fly reconstructed object models.\ndemonstrate the effective integration of OSCAR with MegaPose, achieving superior 6D pose estimation accuracy over reconstruction baselines, thereby proving the importance of high-quality models in object pose estimation.\n3.\nWe embed these findings in a novel training-free framework, OSCAR, combining multi-modal 3D object model retrieval from an unlabeled 3D object database using an RGB image and a language prompt.\nWe evaluate OSCAR on the cross-domain 3D model retrieval benchmark MI3DOR\n(Song\net al.\n,\n2021\n)\nand show that we outperform existing methods, e.g.\n(Zhou\net al.\n,\n2019\n; Song\net al.\n,\n2022\n; Fu\net al.\n,\n2025\n)\n.\nThe paper proceeds as follows: We first review the related work in 6D object pose estimation, 3D reconstruction, and 3D model instance retrieval in Section II. In Section II, we detail our novel OSCAR framework, including the multi-modal onboarding stage and the two-stage retrieval process. Section IV presents the experimental setup, covering the datasets and evaluation metrics used. We then present our state-of-the-art results on the MI3DOR benchmark and our integration with MegaPose in Section V. Finally, we conclude the paper in Section VI.\n2\nRelated Work\nIn this section, we discuss the state-of-the-art in three key areas relevant to this work: 6D object pose estimation, 3D reconstruction, and 3D model instance retrieval.\n2.1\n6D Object Pose Estimation\n6D Object Pose Estimation plays a crucial role in robotics and Augmented Reality, enabling precise object manipulation and interaction.\nInstance-level methods\n(Wang\net al.\n,\n2021\n)\nachieve strong performance on objects encountered during training but struggle to generalize to novel instances.\nZero-shot pose estimation approaches have been developed to address this challenge by estimating 6D poses of novel object instances\n(LabbÃ©\net al.\n,\n2022\n; Ausserlechner\net al.\n,\n2024\n; Wen\net al.\n,\n2024\n; Lin\net al.\n,\n2024\n)\n.\nThe BOP challenge\n(Hodan\net al.\n,\n2024\n)\ncategorizes 6D object pose estimation into model-based\n(LabbÃ©\net al.\n,\n2022\n; Ausserlechner\net al.\n,\n2024\n; Wen\net al.\n,\n2024\n; Lin\net al.\n,\n2024\n)\nand model-free approaches\n(Wen\net al.\n,\n2024\n; Shugurov\net al.\n,\n2022\n; Caraffa\net al.\n,\n2024\n; Lee\net al.\n,\n2024\n; Lei\net al.\n,\n2025\n)\n, depending on whether a 3D mesh is required.\nModel-based methods typically yield better performance but rely on the availability of a ground-truth 3D object model\n(Burde\net al.\n,\n2025\n)\n.\nAcquiring these CAD models is a time-consuming process that requires special equipment, skilled personnel, and considerable pre-processing before it can be integrated into a 6D object pose estimation method.\nThey assume that database models are pre-labeled and demand manual effort to manage an object database.\nIn a practical setting, such as household environments, it is infeasible to acquire CAD models for each object.\nA promising alternative is model-free approaches, which do not require a mesh at inference but reconstruct these models on the fly through reference images\n(Wen\net al.\n,\n2024\n)\nor videos\n(Sun\net al.\n,\n2022\n)\n.\n2.2\n3D Reconstruction\nSeveral approaches combine object pose estimators with 3D reconstruction methods.\nTraditional multi-view stereo and structure-from-motion methods\n(Wang\net al.\n,\n2024\n)\nprovide accurate results but rely on dense observations from multiple viewpoints, which often require controlled settings and are computationally expensive.\nRecent learning-based methods employ neural implicit representations to reconstruct 3D shapes from multiple views\n(Mildenhall\net al.\n,\n2021\n;\nAPS-NeuS\n)\n.\nDespite improved visual quality, these approaches typically require a large number of input images and runtimes up to several hours, limiting their applicability in real-time scenarios.\nDiffusion-based NVS\n(Xiang\net al.\n,\n2025\n; Long\net al.\n,\n2024\n)\nmethods overcome this limitation as they require only a few images to reconstruct a watertight mesh.\nWhile promising in terms of speed, these methods often synthesize unseen parts of the object and do not preserve true scale or geometry.\nGiven these limitations, we propose an alternative approach to retrieve the most similar object from a pre-existing 3D model database, rather than performing on-the-fly reconstruction.\n2.3\n3D Model Instance Retrieval\n3D model retrieval and alignment is a relevant problem in several domains and aims to find the most similar 3D model in a database given a query.\nTraditional approaches can be broadly categorized into model-based, view-based, and feature-fusion methods.\nModel-based methods directly operate on 3D data such as meshes, point clouds\n(Qi\net al.\n,\n2017\n; Zhu\net al.\n,\n2022\n)\n, or voxels\n(Wang\net al.\n,\n2019\n)\n, learning geometric features that encode spatial and structural information.\nThese methods, for instance, are applied in indoor scene acquisition\n(GÃ¼meli\net al.\n,\n2022\n; Gao\net al.\n,\n2024\n)\nto replace noisy 3D meshes with clean CAD models.\nWhile these approaches perform well with point clouds, they are not directly applicable to RGB-only scenes.\nView-based methods address this limitation by rendering multiple 2D views of a 3D model and applying image-based feature extraction.\nMVCNN\n(Su\net al.\n,\n2015\n)\nemploys 2D CNNs to process each rendered view and aggregates them through view pooling into a compact 3D descriptor.\nGVCNN\n(Feng\net al.\n,\n2018\n)\nfurther improves on this by grouping view features and hierarchically pooling them to obtain richer global representations.\nLater, MVTN\n(Hamdi\net al.\n,\n2021\n)\nleverages differentiable rendering to learn optimal viewpoints for 3D shape recognition.\nThese approaches effectively bridge the gap between 2D and 3D domains, but they still require synthetic rendering pipelines or pre-existing 3D models, which can be impractical for real-world scenarios.\nBeyond geometry-based methods, recent work explores cross-modal retrieval, where the query comes from a different modality such as images or sketches\n(Bai\net al.\n,\n2023\n)\n.\nSketch-based retrieval methods like PAGML\n(Bai\net al.\n,\n2023\n)\nemploy precise alignment-guided metric learning to reduce discrepancies between sketch and 3D shape domains.\nSimilarly, CNOS\n(Nguyen\net al.\n,\n2023\n)\nreformulates instance recognition as a retrieval problem, comparing visual embeddings from RGB images against a database of known objects.\nWhile effective for category-level retrieval, such purely visual methods struggle in open-set conditions, where unseen objects must be recognized.\nTo address these challenges, advances in multi-modal representation learning have introduced joint imageâ€“text embeddings that align visual and linguistic information in a shared semantic space.\nModels such as CLIP\n(Radford\net al.\n,\n2021\n)\n, BLIP\n(Li\net al.\n,\n2022\n)\n, and DINO\n(Oquab\net al.\n,\n2024\n)\nlearn cross-modal representations that generalize well across domains and modalities.\nThese embeddings have proven particularly valuable in robotic and household settings, where visual tasks, such as identifying or retrieving objects are often guided by natural-language instructions\n(Frering\net al.\n,\n2025\n)\nand have also been used in object retrieval tasks\n(Song\net al.\n,\n2025\n)\n.\nThis progress has led to dedicated benchmarks like MI3DOR\n(Song\net al.\n,\n2021\n)\n(Multi-modal Image-to-3D Object Retrieval), which specifically tests the generalization capacity of retrieval methods in open-set, cross-domain scenarios. The current state-of-the-art on MI3DOR is largely dominated by methods like S2Mix\n(Fu\net al.\n,\n2025\n)\n, SC-IFA\nSong\net al.\n(\n2022\n)\n, and DLEA\nZhou\net al.\n(\n2019\n)\n, which primarily employ domain adaptation or feature fusion techniques to handle the large domain gap between 2D query images and 3D model views. Despite their efforts, these methods often rely on complex training procedures or fail to fully leverage the dense semantic information available in multi-view renderings.\nBuilding on these insights, we investigate the use of text, image, and joint multimodal embeddings to retrieve corresponding 3D models or instances from an object database.\n3\nMethod\nWith an input RGB image\nI\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nI\\in\\mathbb{R}^{H\\times W\\times 3}\n, a language prompt\nL\nL\n, and\na database of 3D CAD models\nğ’®\n=\n{\ns\n1\n,\ns\n2\n,\nâ€¦\n,\ns\nN\n}\n\\mathcal{S}=\\{s_{1},s_{2},\\dots,s_{N}\\}\n, we aim to retrieve the CAD model\ns\nâˆ—\ns^{*}\nwith the highest similarity to the query object.\nAn overview of the approach is given in Fig.\n2\n.\nFigure 2\n:\nConcept overview:\nDuring the onboarding stage, each 3D model is rendered from multiple views and automatically captioned to enable language-guided retrieval without manual labels. During the inference stage, a Region-of-Interest (ROI) is generated from an RGB image and a language prompt using GroundedSAM. First, CLIP embeddings of the ROI and model captions are compared via cosine similarity and filtered according to a threshold. The DINOv2 embeddings are computed for these candidate images. Based on the cosine similarity, the object model with the highest similarity is selected.\n3.1\nOnboarding Stage\nFollowing the approach proposed by CAP3D\n(Luo\net al.\n,\n2023\n)\n, we render for each CAD model of the database\ns\ni\nâˆˆ\nğ’®\ns_{i}\\in\\mathcal{S}\nsynthetic images\nâ„›\ni\n=\n{\nr\ni\nâ€‹\n1\n,\nr\ni\nâ€‹\n2\n,\nâ€¦\n,\nr\ni\nâ€‹\nK\n}\n\\mathcal{R}_{i}=\\{r_{i1},r_{i2},\\dots,r_{iK}\\}\nfrom\nK\n=\n8\nK=8\nviewpoints, with two views from an elevation angle\nK\n1\n,\n2\n,\n3\n,\n4\n=\nâˆ’\n15\nâˆ˜\nK_{1,2,3,4}=-15^{\\circ}\nand\nK\n5\n,\n6\n,\n7\n,\n8\n=\n+\n15\nâˆ˜\nK_{5,6,7,8}=+15^{\\circ}\n, while the azimuth angles are distributed evenly around the object.\nEach rendered image\nr\ni\nâ€‹\nj\nr_{ij}\nis captioned using LLaVA-v1.5-7B\n(Liu\net al.\n,\n2023\n)\n, producing\nğ’\ni\n=\n{\nc\ni\nâ€‹\n1\n,\nc\ni\nâ€‹\n2\n,\nâ€¦\n,\nc\ni\nâ€‹\nK\n}\n\\mathcal{C}_{i}=\\{c_{i1},c_{i2},\\dots,c_{iK}\\}\ndescriptions.\nWhen a novel object is added to the database, OSCAR verifies whether the corresponding CAD model is already associated with a rendered view and a textual description.\nIf such data is missing, the onboarding pipeline automatically renders the required viewpoints and generates the associated captions.\n3.2\nObject Retrieval\nWe assume a given input RGB image\nI\nâˆˆ\nâ„\nH\nÃ—\nW\nÃ—\n3\nI\\in\\mathbb{R}^{H\\times W\\times 3}\nand a natural language prompt\nL\nâˆˆ\nâ„’\nL\\in\\mathcal{L}\n.\nDuring the detection stage, we utilize GroundedSAM\n(Ren\net al.\n,\n2024\n)\nto generate a bounding box around the RoI and segment the corresponding object and a gray background to match the rendered images.\nThe object retrieval stage consists of two phases: (1) filtering of candidate objects based on descriptions and query embedding, and (2) the retrieval of the most similar object based on DINOv2\n(Oquab\net al.\n,\n2024\n)\nembeddings.\nThe objective of the first stage is to filter candidate objects at the textual level.\nIn our experiments, we find that image embeddings are particularly effective for datasets containing distinct objects, as they capture fine-grained texture similarities.\nHowever, when objects share similar textures, image embeddings may match entirely different categories.\nTo mitigate this issue, we propose a sanity check in which CLIP text embeddings are first used to filter plausible candidates based on semantic similarity.\nOnly then are image embeddings applied to refine the selection, ensuring that the final match is both semantically and visually consistent.\nFiltering of Candidates\nAt inference time, textual descriptions\nc\ni\nâ€‹\nj\nc_{ij}\nare encoded into embeddings using a CLIP text encoder\nf\nL\nf_{L}\n, giving text embeddings\nğ­\ni\nâ€‹\nj\n=\nf\nL\nâ€‹\n(\nc\ni\nâ€‹\nj\n)\nâˆˆ\nâ„\nd\n\\mathbf{t}_{ij}=f_{L}(c_{ij})\\in\\mathbb{R}^{d}\nAt the same time, we encode the ROI using CLIP as an image encoder\nf\nI\nf_{I}\n, producing an image embedding\nğª\nğ‚ğ‹ğˆğ\n=\nf\nI\nâ€‹\n(\nI\nROI\n)\nâˆˆ\nâ„\nd\n\\mathbf{q_{CLIP}}=f_{I}(I_{\\text{ROI}})\\in\\mathbb{R}^{d}\nWe compute the cross-modal cosine similarity between the image embedding\nğª\n\\mathbf{q}\nand each caption embedding\nğ­\ni\nâ€‹\nj\n\\mathbf{t}_{ij}\n. For each object\ns\ni\ns_{i}\n, we aggregate over its captions:\nsim\ntext\nâ€‹\n(\ns\ni\n)\n=\nmax\nj\nâ¡\nâŸ¨\nğª\nğ‚ğ‹ğˆğ\n,\nğ­\ni\nâ€‹\nj\nâŸ©\nâ€–\nğª\nğ‚ğ‹ğˆğ\nâ€–\nâ€‹\nâ€–\nğ­\ni\nâ€‹\nj\nâ€–\n.\n\\mathrm{sim}_{\\text{text}}(s_{i})\\;=\\;\\max_{j}\\;\\frac{\\langle\\mathbf{q_{CLIP}},\\mathbf{t}_{ij}\\rangle}{\\|\\mathbf{q_{CLIP}}\\|\\,\\|\\mathbf{t}_{ij}\\|}.\nWe then form a candidate set\nğ’®\nâ€²\n\\mathcal{S}^{\\prime}\nby selecting all objects with\nsim\ntext\nâ€‹\n(\ns\ni\n)\nâ‰¥\nÏ„\ntext\n\\mathrm{sim}_{\\text{text}}(s_{i})\\geq\\tau_{\\text{text}}\n(threshold\nÏ„\ntext\n\\tau_{\\text{text}}\nwith\nÏ„\ntext\n=\n0.37\n\\tau_{\\text{text}}=0.37\n.\nIf\nğ’®\nâ€²\n\\mathcal{S}^{\\prime}\nis empty (no candidate passes\nÏ„\ntext\n\\tau_{\\text{text}}\n), we fall back to the top-\nk\nk\ntext candidates.\nImage-based Retrieval\nFor each candidate\ns\ni\nâˆˆ\nğ’®\nâ€²\ns_{i}\\in\\mathcal{S}^{\\prime}\nwe compare the ROI against pre-rendered views\n{\nr\ni\nâ€‹\nk\n}\n\\{r_{ik}\\}\nof the corresponding CAD model. Each rendered view is embedded by the DINOv2 image encoder:\nğ¯\ni\nâ€‹\nk\n=\nf\nI\nâ€‹\n(\nr\ni\nâ€‹\nk\n)\nâˆˆ\nâ„\nd\n.\n\\mathbf{v}_{ik}=f_{I}(r_{ik})\\in\\mathbb{R}^{d}.\nWe score a candidate by the best imageâ€“image similarity across its views:\nsim\nimg\nâ€‹\n(\ns\ni\n)\n=\nmax\nk\nâ¡\nâŸ¨\nğª\nğƒğˆğğ\n,\nğ¯\ni\nâ€‹\nk\nâŸ©\nâ€–\nğª\nğƒğˆğğ\nâ€–\nâ€‹\nâ€–\nğ¯\ni\nâ€‹\nk\nâ€–\n.\n\\mathrm{sim}_{\\text{img}}(s_{i})\\;=\\;\\max_{k}\\;\\frac{\\langle\\mathbf{q_{DINO}},\\mathbf{v}_{ik}\\rangle}{\\|\\mathbf{q_{DINO}}\\|\\,\\|\\mathbf{v}_{ik}\\|}.\nThe final retrieved image (and thus the CAD model) is chosen as\nc\nâˆ—\n=\narg\nâ¡\nmax\ns\ni\nâˆˆ\nğ’®\nâ€²\nâ¡\nsim\nimg\nâ€‹\n(\ns\ni\n)\n,\nc^{\\ast}\\;=\\;\\arg\\max_{s_{i}\\in\\mathcal{S}^{\\prime}}\\mathrm{sim}_{\\text{img}}(s_{i}),\nand\nc\nâˆ—\nc^{\\ast}\nis mapped to its corresponding CAD model.\n4\nExperiments\nWe evaluate our approach and its components against state-of-the-art baselines on image-based 3D object retrieval and 6D pose estimation.\nThe following experiments were conducted on a system equipped with an NVIDIA GeForce RTX 4090 GPU and an AMD Ryzen 9 5900X CPU.\n4.1\nDatasets\nWe assess OSCAR on the 3D model datasets (MI3DOR\n(Song\net al.\n,\n2021\n)\n), and on three datasets for object retrieval in the context of 6D object pose estimation (YCB-V\n(Xiang\net al.\n,\n2018\n)\n, Housecat6D\n(Jung\net al.\n,\n2024\n)\n, and YCB-V combined with GSO\n(Downs\net al.\n,\n2022\n)\n).\nMI3DOR is designed for monocular image-based 3D model retrieval and includes 2D images and 3D models across 21 categories.\nThe 2D images, serving as queries, are adapted from ImageNet.\nTo show OSCARâ€™s capabilities for 6D object pose estimation in domestic environments, we consider the following datasets: YCB-V, consisting of 21 household objects, and Housecat6D, containing 194 diverse objects across 10 household categories.\nAdditionally, we assess OSCARâ€™s scalability and robustness by using the Google Scanned Objects (GSO) dataset as distractor objects.\nWe combine the YCB-V dataset with the GSO objects, which are 1030 CAD models of common household objects.\n4.2\nEvaluation Metrics\nIn the following, we introduce the metrics to evaluate the object retrieval capabilities and the application in object pose estimation.\n4.3\nMean Average Precision (mAP@k)\nTo evaluate our object retrieval strategy, we adopt the mean Average Precision at top-\nk\nk\n(mAP@\nk\nk\n).\nFor each query, the retrieved results are ranked by their cosine similarity score.\nThe Average Precision (AP) for a query is defined as:\nm\nâ€‹\nA\nâ€‹\nP\nâ€‹\n@\nâ€‹\nK\n=\n1\nn\nâ€‹\nâˆ‘\ni\n=\n1\nn\nA\nâ€‹\nP\nâ€‹\n@\nâ€‹\nk\n,\nmAP@K=\\frac{1}{n}\\sum_{i=1}^{n}AP@k,\nwhere\nn\nn\ndenotes the total number of queries, and\nA\nâ€‹\nP\nâ€‹\n@\nâ€‹\nk\nAP@k\nrepresents the average precision of the top-\nK\nK\nranked items for the query\nu\nu\n.\nThis metric accounts for both the relevance and the ordering of recommendations, as higher-ranked relevant items contribute more significantly to the score.\n4.4\nEarth Moverâ€™s Distance\nThe general definition of the Earth Moverâ€™s Distance (EMD) between two discrete distributions is\nEMD\nâ€‹\n(\nÎ±\n,\nÎ²\n)\n=\nmin\nÏ€\nâˆˆ\nÎ \nâ€‹\n(\nÎ±\n,\nÎ²\n)\nâ€‹\nâˆ‘\ni\n=\n1\nN\nâˆ‘\nj\n=\n1\nM\nÏ€\ni\n,\nj\nâ€‹\nc\nâ€‹\n(\nx\ni\n,\ny\nj\n)\n,\n\\mathrm{EMD}(\\alpha,\\beta)=\\min_{\\pi\\in\\Pi(\\alpha,\\beta)}\\sum_{i=1}^{N}\\sum_{j=1}^{M}\\pi_{i,j}\\,c(x_{i},y_{j}),\nwhere\nÏ€\nâˆˆ\nâ„\nN\nÃ—\nM\n\\pi\\in\\mathbb{R}^{N\\times M}\nis a non-negative matrix representing the optimal correspondence between points\nx\ni\nâˆˆ\nX\nx_{i}\\in X\nand\ny\nj\nâˆˆ\nY\ny_{j}\\in Y\n, and\nc\nâ€‹\n(\nâ‹…\n,\nâ‹…\n)\nc(\\cdot,\\cdot)\nis a distance function, typically chosen as\nc\nâ€‹\n(\nx\ni\n,\ny\nj\n)\n=\n1\n2\nâ€‹\nâ€–\nx\ni\nâˆ’\ny\nj\nâ€–\n2\n2\nc(x_{i},y_{j})=\\frac{1}{2}\\|x_{i}-y_{j}\\|_{2}^{2}\n.\nIntuitively, this represents the weighted distance between all pairs of points according to the optimal correspondence\nÏ€\n\\pi\n.\n4.5\nTranslation Error\nThe translation error is defined as the Euclidean distance between the predicted translation vector\nt\n^\n\\hat{t}\nand the ground truth translation\nt\nt\n:\ne\nt\n=\nâˆ¥\n1\nn\nâ€‹\nâˆ‘\ni\n=\n1\nn\n=\n3\n(\nt\n1\n,\ni\nâˆ’\nt\n2\n,\ni\n)\nâˆ¥\ne_{t}=\\lVert\\frac{1}{n}\\sum_{i=1}^{n=3}(t_{1,i}-t_{2,i})\\rVert\n4.6\nRotation Error\nThe rotation error is computed as the angular difference between the predicted rotation matrix\nR\n^\n\\hat{R}\nand the ground truth\nR\nR\n:\ne\nR\n=\nâˆ¥\nR\n1\nâ€‹\nR\n2\nâŠ¤\nâˆ¥\n2\n2\ne_{R}=\\lVert R_{1}\\,R_{2}^{\\top}\\rVert_{2}^{2}\n5\nResults\nThe following section presents the main results and ablation studies.\n5.1\nObject Retrieval\nTable\n1\nreports the results of several object retrieval methods on the MI3DOR dataset.\nWe employ six commonly used evaluation criteria to\ncomprehensively assess retrieval performance, namely nearest neighbor (NN), first tier (FT), second tier (ST), F-measure (F),\ndiscounted cumulative gain (DCG), and average normalized modified retrieval rank (ANMRR).\nWe refer to\nbenchmark3d\nfor a more detailed description of these metrics.\nOur experiments show, that OSCAR outperforms all other state-of-the-art approaches, despite being the only fully training-free method in this comparison.\nTable 1\n:\nObject Retrieval performance on the MI3DOR benchmark. OSCAR is the only fully training-free method compared. Best result in\nbold)\nMethod\nNN\nâ†‘\n\\uparrow\nFT\nâ†‘\n\\uparrow\nST\nâ†‘\n\\uparrow\nF\nâ†‘\n\\uparrow\nDCG\nâ†‘\n\\uparrow\nANMRR\nâ†“\n\\downarrow\nCORAL\n(Sun\net al.\n,\n2016\n)\n0.362\n0.174\n0.256\n0.060\n0.199\n0.816\nMEDA\n(Wang\net al.\n,\n2018\n)\n0.430\n0.344\n0.501\n0.046\n0.361\n0.646\nJGSA\n(Zhang\net al.\n,\n2017\n)\n0.612\n0.443\n0.599\n0.116\n0.473\n0.541\nJAN\n(Long\net al.\n,\n2017\n)\n0.446\n0.343\n0.495\n0.085\n0.364\n0.647\nRevGrad\n(Ganin and Lempitsky,\n2015\n)\n0.650\n0.505\n0.643\n0.112\n0.542\n0.474\nDLEA\n(Zhou\net al.\n,\n2019\n)\n0.764\n0.558\n0.716\n0.143\n0.597\n0.421\nSC-IFA\n(Song\net al.\n,\n2022\n)\n0.721\n0.584\n0.721\n0.163\n0.637\n0.363\nS2Mix\n(Fu\net al.\n,\n2025\n)\n0.841\n0.670\n0.807\n0.166\n0.713\n0.304\nOSCAR (Ours)\n0.894\n0.708\n0.850\n0.238\n0.844\n0.205\n5.2\nObject Pose Estimation\nWe investigate the benefits of OSCAR in the context of novel object pose estimation. Once a vision system is deployed, two options are available. The novel object can be autonomously reconstructed, and the reconstructed model can be used later on for pose estimation. In-the-wild autonomous reconstruction limits the model accuracy. Alternatively, a similar model can be retrieved using OSCAR and substituted for the instance model in the pose estimation process. We propose to evaluate the trade-off between these two strategies.\nTo validate this, we evaluate the performance of MegaPose\n(LabbÃ©\net al.\n,\n2022\n)\nusing three types of 3D object models:\n(i) top-ranked confusion CAD models retrieved by OSCAR from a combined database of YCB-V\n(Xiang\net al.\n,\n2018\n)\nand GSO\n(Downs\net al.\n,\n2022\n)\n, (ii) 3D models reconstructed from real-world images, and (iii) ground-truth object models (see Fig.\n3\n).\nImplementation Details\nTo evaluate the next-best-model strategy, we provide MegaPose with the most frequently retrieved\nconfused model\n, i.e., the CAD model that OSCAR incorrectly selects most often for that object class.\nFor reconstructed models, we rely on the dataset provided by Burde et al.\n(Burde\net al.\n,\n2025\n)\n, which demonstrates that Nerfacto\n(Mildenhall\net al.\n,\n2021\n)\noffers the best trade-off between pose recall and runtime.\nFollowing their findings, we use the meshes reconstructed with Nerfacto from a set of 50 input images.\nAs a baseline, we use the ground-truth object models for pose estimation with MegaPose.\nAll experiments are conducted on a test scene from the YCB-V dataset\n(Xiang\net al.\n,\n2018\n)\n. Figure\n3\nshows the CAD models utilized for each experiment.\nFigure 3\n:\nVisualization of object meshes: Ground-truth object models (top), most frequently confused CAD model retrieved by OSCAR (middle), and Nerfacto-reconstructed models (bottom).\nResults\nThe pose estimation results in Table\n5\nshow that CAD models retrieved using OSCAR achieve more accurate pose estimations compared to the reconstruction-based approach.\nEven when using ground-truth models, however, certain objects remain challenging (e.g., the canned meat), highlighting the difficulties of the task.\nNevertheless, the estimated poses for objects such as the sugar box and the bowl are sufficiently accurate to enable grasping.\nIn contrast, when using reconstructed object models, none of the pose estimates proved reliableâ€”MegaPose consistently failed to produce usable results.\nFigure\n5\nillustrates this comparison, overlaying the estimated poses for CAD models retrieved with OSCAR (Fig.\n4(b)\n) and for reconstructed meshes (Fig.\n4(c)\n).\n(a)\nGround-truth models\n(b)\nCADs retrieved with Oscar\n(c)\nReconstructed CAD models\nFigure 4\n:\nComparison of pose estimation results across three approaches: (a) ground-truth object models, (b) CAD models retrieved by OSCAR, and (c) reconstructed 3D models generated with Nerfacto.\nObject\nTrl. error\n[\nmm\n]\n[\\mathrm{mm}]\nRot. error\n[\nâˆ˜\n]\n[\\!^{\\circ}]\nGT\nOSCAR\nReconst.\nGT\nOSCAR\nReconst.\nSugar\n16.29\n20.19\n1621.95\n42.44\n4.30\n93.24\nTuna\n29.53\n806.40\n4760.08\n22.76\n51.17\n94.53\nBowl\n25.06\n85.55\n2077.02\n99.83\n115.23\n68.59\nSpam\n185.26\n665.33\n3219.92\n87.83\n57.17\n78.39\nFigure 5:\nRotation and translation errors for individual objects using OSCAR and reconstructed models against the ground-truth models as baselines. Lower values indicate better performance.\nRuntime Analysis\nIn our runtime analysis, we evaluate the time required to onboard a novel object instance into the database.\nFor comparison with OSCAR, we use a 50-image subset of the image dataset published by\n(Burde\net al.\n,\n2025\n)\n.\nWe reconstruct the YCB-V Cracker Box using Nerfacto within the Nerfstudio framework on an NVIDIA GeForce RTX 4090 GPU.\nTab.\n2\noutlines the steps involved in generating a 3D mesh, including preprocessing with COLMAP, reconstruction with Nerfacto, and the final mesh extraction from Nerfstudio.\nOSCAR onboards every new object by rendering synthetic views of each CAD model and generating textual descriptions.\nThe time required to add a novel instance to the combined GSO and Housecat6D dataset is also reported in Table\n2\n.\nCompared to the reconstruction-based pipeline, OSCARâ€™s onboarding is 31.8x faster, making it significantly more suitable for time-sensitive applications.\nStage\nRec.\nOSCAR\nCOLMAP\n01:07\nâ€“\nReconstruction\n09:04\nâ€“\nModel extraction\n05:43\nâ€“\nText generation\nâ€“\n0:14\nRendering\nâ€“\n0:16\nTotal\n15:54\n00:30\nTable 2:\nComparison of system runtime for onboarding a single novel object to the database [mm:ss]\n5.3\nAblation Studies: Modalities\nThis section presents an evaluation of the impact of the design choices of our retrieval strategy, in particular investigating different retrieval modalities.\n5.3.1\nImage-based retrieval\nIn the following section, we evaluate the performance of our retrieval framework under different image encoders.\nImplementation Details\nWe evaluate object retrieval using two different image encoders: CLIP\n(Radford\net al.\n,\n2021\n)\n, which leverages joint visionâ€“language pretraining, and DINOv2\n(Oquab\net al.\n,\n2024\n)\n, a purely vision-based self-supervised model.\nFor each object instance, the region of interest (ROI) is extracted either as a bounding box or a segmentation with a gray background to match the rendered images.\nWe report mean Average Precision at different\nk\nk\nfor (\nk\n=\n1\n,\n3\n,\n5\n,\n10\nk=1,3,5,10\n) to capture both top-1 retrieval accuracy and broader candidate recall.\nResults\nThe results in Table\n3\nshow that DINOv2 achieves higher top-1 accuracy, particularly on YCB-V (77.38%) and HCat6D (41.81%) with segmentation masks, while CLIP with segmentation excels at higher\nk\nk\n, reaching 92.61% mAP@10 on YCB-V and performing best on the larger YCB-V&GSO dataset.\nSegmentation generally improves retrieval, especially for CLIP, though bounding boxes sometimes perform better on HCat6D on similar-looking objects.\nOverall, DINOv2 favors top-1 precision, whereas CLIP with segmentation is more robust for broader candidate retrieval in diverse datasets.\nMethod\nYCB-V\nHcat6D\nYCBV&GSO\nmAP@1\nCLIP + bbox\n59.78\n34.31\n40.08\nCLIP + segm\n69.53\n28.30\n41.31\nDinov2 + bbox\n70.98\n33.41\n31.61\nDinov2 + segm\n77.38\n41.81\n36.41\nmAP@3\nCLIP + bbox\n63.82\n45.32\n45.13\nCLIP + segm\n81.16\n33.47\n47.12\nDinov2 + bbox\n75.35\n39.74\n36.51\nDinov2 + segm\n81.54\n44.71\n40.65\nmAP@5\nCLIP + bbox\n63.05\n51.11\n45.29\nCLIP + segm\n87.68\n33.63\n46.12\nDinov2 + bbox\n74.44\n39.42\n36.67\nDinov2 + segm\n80.23\n43.10\n41.33\nmAP@10\nCLIP + bbox\n60.03\n59.91\n43.75\nCLIP + segm\n92.61\n30.86\n44.73\nDinov2 + bbox\n69.08\n35.41\n35.44\nDinov2 + segm\n75.20\n39.21\n40.37\nTable 3:\nComparison of Top-\nk\nk\nmean Average Precision for image-to-image across YCB-V, Housecat6D, and the combined YCB-V&GSO datasets. Best results for each\nk\nk\nand each dataset are printed\nbold\n.\n5.3.2\nText-based Retrieval\nType\nPrompt\nExample Output\nblind\nImagine youâ€™re describing the object to a blind person. Be extremely detailed about the objectâ€™s appearance, colors, shape, material, any text or logos, and any unique markings.\nThe object is a blue can with a black top. The can is cylindrical in shape and has a black top that is slightly curved. The can is made of metal, and it is filled with a substance, possibly coffee. The can has a logo on it, which is a picture of a coffee cup. The can is also labeled with the word â€Majorâ€ and the number â€100.â€ The overall appearance of the can is that of a standard coffee.\ncomma\nDescribe the object in a comma-separated list, focusing on its visual appearance, color, material, and any visible text or brand names. Be concise.\nBlue, silver, black, and white can.\ncaption\nWrite a detailed visual caption of the image, mentioning colors, materials, brand names, and visible labels.\nThe image features a large blue can of coffee, possibly a can of Maxwell House coffee. The can is prominently displayed in the foreground, occupying a significant portion of the image. The can is made of metal and has a label on it, indicating the brand and the contents inside. The overall scene is a close-up of the coffee can, showcasing its size and design.\nattributes\nExtract visual attributes of the main object in the image: object type, brand name, color, material, and label text.\nThe main object in the image is a blue can with a label. The can is made of metal and has a black label. The brand name is â€Major.â€ The can is filled with a product, possibly coffee, as indicated by the label text.\nFigure 6\n:\nExample image (top) and table (bottom) comparing four prompting strategies. The table lists the prompt type, prompt text, and an example output.\nFor the text-based retrieval strategy, we consider four strategies describing the objects with different levels of detail.\nFor each rendered image, we generate a description with LLaVa-v1.5-7B\n(Liu\net al.\n,\n2023\n)\n.\nTable\n6\nshows the used prompts with example outputs.\nAll methods receive ground-truth bounding boxes as input and operate under the assumption of access to a 3D object database.\nTable\n4\nshows that prompt choice strongly affects retrieval performance.\nThe blind prompt performs best on a distinct dataset such as YCB-V, while the attributes prompt achieves higher scores on the larger datasets Housecat6D and YCB-V&GSO.\nCaption- and comma-style prompts generally underperform, indicating that detailed or structured descriptions provide more discriminative cues for text-based 3D object retrieval.\nMethod\nYCB-V\nHcat6D\nYCBV&GSO\nmAP@1\nblind\n68.12\n16.87\n30.33\ncomma\n15.88\n4.97\n6.42\ncaption\n43.52\n16.11\n31.39\nattributes\n32.95\n25.89\n46.11\nmAP@3\nblind\n72.75\n20.82\n34.91\ncomma\n18.04\n5.87\n7.18\ncaption\n46.38\n19.71\n34.11\nattributes\n34.88\n28.98\n50.51\nmAP@5\nblind\n72.50\n21.20\n34.55\ncomma\n15.57\n5.89\n7.25\ncaption\n46.38\n20.25\n33.80\nattributes\n35.04\n29.37\n50.40\nmAP@10\nblind\n69.63\n20.43\n32.71\ncomma\n18.90\n5.80\n7.10\ncaption\n44.50\n19.57\n32.20\nattributes\n33.99\n28.40\n48.58\nTable 4:\nComparison of mAP-\nk\nk\nfor different language prompts across YCB-V, Housecat6D, and the combined YCB-V&GSO datasets. Best results for each\nk\nk\nand each dataset are printed\nbold\n.\n5.4\nAblation Studies: Finetuning\nBased on the results from both image-based and text-based retrieval, we observed that each modality has notable limitations.\nTo leverage the strengths of both, we first filter candidate instances by matching textual descriptions to the input image, and then identify the best-matching reference image using DINOv2.\nFigures\n7\nand\n8\npresent the results of our ablation studies focusing on the impact of threshold selection and top-k filtering on model accuracy across different datasets.\n5.4.1\nThresholding\nThe threshold defines at which clip-based cosine similarity a corresponding object is selected as candidate.\nFigure\n7\nillustrates the effect of varying the threshold parameter on the accuracy for three datasets: YCBV&GSO, YCBV, and Housecat6D.\nWe observe that for YCBV&GSO, accuracy remains relatively stable at lower thresholds (0.10â€“0.30), followed by an increase around thresholds 0.35â€“0.37, reaching a peak at 60%, before declining at higher thresholds. A similar trend is evident for YCBV, with accuracy peaking at 86.72% at a threshold of 0.38, while Hcat6D demonstrates a more gradual improvement, plateauing around 48%.\nThese trends suggest that the performance can be maximized with a threshold of 0.37.\n0.1\n0.1\n0.2\n0.2\n0.3\n0.3\n0.4\n0.4\n0.5\n0.5\n0.6\n0.6\n0.7\n0.7\n0\n20\n20\n40\n40\n60\n60\n80\n80\n100\n100\nThreshold\nAverage Precision (%)\nYCBV&GSO\nYCBV\nHCAT\nFigure 7\n:\nAverage Precision vs. Threshold for Filtering candidate objects for YCB-V, YCB-V&GSO and Housecat6D datasets\n5.4.2\nTok-k Filtering\nIf none of the retrieved objects surpasses the confidence threshold, we employ a top-k filtering strategy to select candidate models for evaluation.\nFigure\n8\nillustrates how the average precision varies as the number of top-k candidates increases for the three datasets: YCB-V, Housecat6D, and YCB-V&GSO.\nOverall, performance remains relatively stable across most values of k, indicating that the retrieval method is robust to moderate changes in the number of candidates considered.\nFor YCB-V, accuracy peaks around the top-10 candidates (90.8%) and gradually decreases for larger k, suggesting that additional retrieved models tend to introduce more noise than useful matches.\nA similar pattern is observed for YCB-V&GSO and Housecat6D, where performance slightly improves up to top-10 and then declines marginally for larger k.\ntop5\ntop10\ntop15\ntop20\ntop25\ntop30\ntop40\n40\n40\n60\n60\n80\n80\n100\n100\nTop-k Filtering\nAverage Precision (%)\nYCBV&GSO\nYCBV\nHCAT\nFigure 8\n:\nAverage Precision vs Top-k Filtering for YCB-V, YCB-V&GSO and Housecat6D datasets\n6\nConclusion\nWe presented OSCAR, a novel method that retrieves a matching object model from an unlabeled 3D object database using a single RGB image and a natural language prompt.\nWe demonstrated that OSCAR retrieves object models reliably by combining text embeddings and image embeddings in a two-stage process.\nFurthermore, we proposed a novel pose estimation strategy alternative to time-costly reconstruction-based approaches.\nIntegrating OSCAR with existing pose estimation techniques yields improved accuracy and efficiency, demonstrating the effectiveness of retrieval paradigms in practical scenarios where the exact CAD model or multi-view data are unavailable.\nFuture work will aim to embed OSCAR in a pose estimation framework.\nAuthor contributions: CRediT\nTessa Pulli\n: Conceptualization, Investigation, Writing-original-draft.\nJean-Baptiste Weibel\n: Writing-review-editing.\nPeter HÃ¶nig\n: Writing-review-editing.\nMatthias Hirschmanner\n: Writing-review-editing.\nMarkus Vincze\n: Writing-review-editing, Supervision, Project-administration, Funding-acquisition.\nAndreas Holzinger\n: Writing-review-editing.\nAcknowledgement\nWe gratefully acknowledge the support of the EU-program EC Horizon 2020 for Research and\nInnovation under project No. I 6114, project iChores.\nDeclaration of generative AI and AI-assisted technologies in the manuscript preparation process\nDuring the preparation of this work the authors used ChatGPT and Google Gemini in order to improve the language and readability of the manuscript, and to assist in writing Python code for visualizing experimental results.\nAfter using these tools, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article.\nReferences\nP. Ausserlechner, D. Haberger, S. Thalhammer, J. Weibel, and M. Vincze (2024)\nZs6d: zero-shot 6d object pose estimation using vision transformers\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\npp.Â 463â€“469\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nS. Bai, J. Bai, H. Xu, J. Tuo, and M. Liu (2023)\nPAGML: precise alignment guided metric learning for sketch-based 3d shape retrieval\n.\nImage and Vision Computing\n136\n,\npp.Â 104756\n.\nExternal Links:\nDocument\n,\nISSN 0262-8856\n,\nLink\nCited by:\nÂ§2.3\n.\nV. Burde, A. Benbihi, P. Burget, and T. Sattler (2025)\nComparative evaluation of 3d reconstruction methods for object pose estimation\n.\nIn\n2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n,\npp.Â 7669â€“7681\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.1\n,\nÂ§5.2\n,\nÂ§5.2\n.\nA. Caraffa, D. Boscaini, A. Hamza, and F. Poiesi (2024)\nFreeze: training-free zero-shot 6d pose estimation with geometric and vision foundation models\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 414â€“431\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nL. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and V. Vanhoucke (2022)\nGoogle scanned objects: a high-quality dataset of 3d scanned household items\n.\nIn\n2022 International Conference on Robotics and Automation (ICRA)\n,\npp.Â 2553â€“2560\n.\nCited by:\nÂ§4.1\n,\nÂ§5.2\n.\nY. Feng, Z. Zhang, X. Zhao, R. Ji, and Y. Gao (2018)\nGvcnn: group-view convolutional neural networks for 3d shape recognition\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 264â€“272\n.\nCited by:\nÂ§2.3\n.\nL. Frering, G. Steinbauer-Wagner, and A. Holzinger (2025)\nIntegrating belief-desire-intention agents with large language models for reliable humanâ€“robot interaction and explainable artificial intelligence\n.\nEngineering Applications of Artificial Intelligence\n141\n,\npp.Â 109771\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.3\n.\nX. Fu, D. Song, Y. Yang, Y. Zhang, and B. Wang (2025)\nS2Mix: style and semantic mix for cross-domain 3d model retrieval\n.\nJournal of Visual Communication and Image Representation\n107\n,\npp.Â 104390\n.\nExternal Links:\nDocument\n,\nISSN 1047-3203\n,\nLink\nCited by:\nÂ§1\n,\nÂ§2.3\n,\nTable 1\n.\nY. Ganin and V. Lempitsky (2015)\nUnsupervised domain adaptation by backpropagation\n.\nIn\nInternational conference on machine learning\n,\npp.Â 1180â€“1189\n.\nExternal Links:\nDocument\nCited by:\nTable 1\n.\nD. Gao, D. Rozenberszki, S. Leutenegger, and A. Dai (2024)\nDiffcad: weakly-supervised probabilistic cad model retrieval and alignment from an rgb image\n.\nACM Transactions on Graphics (TOG)\n43\n(\n4\n),\npp.Â 1â€“15\n.\nCited by:\nÂ§1\n,\nÂ§2.3\n.\nC. GÃ¼meli, A. Dai, and M. NieÃŸner (2022)\nRoca: robust cad model retrieval and alignment from a single image\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 4022â€“4031\n.\nCited by:\nÂ§1\n,\nÂ§2.3\n.\nA. Hamdi, S. Giancola, and B. Ghanem (2021)\nMvtn: multi-view transformation network for 3d shape recognition\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 1â€“11\n.\nCited by:\nÂ§2.3\n.\nT. Hodan, M. Sundermeyer, Y. Labbe, V. N. Nguyen, G. Wang, E. Brachmann, B. Drost, V. Lepetit, C. Rother, and J. Matas (2024)\nBop challenge 2023 on detection segmentation and pose estimation of seen and unseen rigid objects\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 5610â€“5619\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nH. Jung, S. Wu, P. Ruhkamp, G. Zhai, H. Schieber, G. Rizzoli, P. Wang, H. Zhao, L. Garattoni, S. Meier, D. Roth, N. Navab, and B. Busam (2024)\nHouseCat6D-a large-scale multi-modal category level 6d object perception dataset with household objects in realistic scenarios\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 22498â€“22508\n.\nCited by:\nÂ§4.1\n.\nY. LabbÃ©, L. Manuelli, A. Mousavian, S. Tyree, S. Birchfield, J. Tremblay, J. Carpentier, M. Aubry, D. Fox, and J. Sivic (2022)\nMegaPose: 6d pose estimation of novel objects via render & compare\n.\nIn\nCORL\n,\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§5.2\n.\nJ. Lee, Y. Cabon, R. BrÃ©gier, S. Yoo, and J. Revaud (2024)\nMfos: model-free & one-shot object pose estimation\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n38\n,\npp.Â 2911â€“2919\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nH. Lei, X. Liu, Y. Zhou, G. Niu, C. Yi, Y. Zhou, X. Liang, and F. Liu (2025)\nMMFEIR: multi-attention mutual feature enhance and instance reconstruction for category-level 6D object pose estimation\n.\nImage and Vision Computing\n162\n,\npp.Â 105657\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.1\n.\nJ. Li, D. Li, C. Xiong, and S. Hoi (2022)\nBlip: bootstrapping language-image pre-training for unified vision-language understanding and generation\n.\nIn\nInternational conference on machine learning\n,\npp.Â 12888â€“12900\n.\nCited by:\nÂ§2.3\n.\nJ. Lin, L. Liu, D. Lu, and K. Jia (2024)\nSAM-6D: segment anything model meets zero-shot 6D object pose estimation\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 27906â€“27916\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nH. Liu, C. Li, Y. Li, and Y. J. Lee (2023)\nImproved baselines with visual instruction tuning\n.\narXiv:2310.03744\n.\nCited by:\nÂ§3.1\n,\nÂ§5.3.2\n.\nM. Long, H. Zhu, J. Wang, and M. I. Jordan (2017)\nDeep transfer learning with joint adaptation networks\n.\nIn\nInternational conference on machine learning\n,\npp.Â 2208â€“2217\n.\nExternal Links:\nDocument\nCited by:\nTable 1\n.\nX. Long, Y. Guo, C. Lin, Y. Liu, Z. Dou, L. Liu, Y. Ma, S. Zhang, M. Habermann, C. Theobalt,\net al.\n(2024)\nWonder3d: single image to 3d using cross-domain diffusion\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 9970â€“9980\n.\nCited by:\nÂ§2.2\n.\nT. Luo, C. Rockwell, H. Lee, and J. Johnson (2023)\nScalable 3d captioning with pretrained models\n.\nIn\nAdvances in Neural Information Processing Systems\n,\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.)\n,\nVol.\n36\n,\npp.Â 75307â€“75337\n.\nCited by:\nÂ§3.1\n.\nB. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng (2021)\nNerf: representing scenes as neural radiance fields for view synthesis\n.\nCommunications of the ACM\n65\n(\n1\n),\npp.Â 99â€“106\n.\nCited by:\nÂ§2.2\n,\nÂ§5.2\n.\nV. N. Nguyen, T. Groueix, G. Ponimatkin, V. Lepetit, and T. Hodan (2023)\nCnos: a strong baseline for cad-based novel object segmentation\n.\nIn\nProceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 2134â€“2140\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.3\n.\nM. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby,\net al.\n(2024)\nDINOv2: learning robust visual features without supervision\n.\nTransactions on Machine Learning Research Journal\n,\npp.Â 1â€“31\n.\nCited by:\nÂ§2.3\n,\nÂ§3.2\n,\nÂ§5.3.1\n.\nC. R. Qi, H. Su, K. Mo, and L. J. Guibas (2017)\nPointnet: deep learning on point sets for 3d classification and segmentation\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 652â€“660\n.\nCited by:\nÂ§2.3\n.\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\net al.\n(2021)\nLearning transferable visual models from natural language supervision\n.\nIn\nInternational conference on machine learning\n,\npp.Â 8748â€“8763\n.\nCited by:\nÂ§2.3\n,\nÂ§5.3.1\n.\nT. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, Z. Zeng, H. Zhang, F. Li, J. Yang, H. Li, Q. Jiang, and L. Zhang (2024)\nGrounded sam: assembling open-world models for diverse visual tasks\n.\nExternal Links:\n2401.14159\nCited by:\nÂ§1\n,\nÂ§3.2\n.\nI. Shugurov, F. Li, B. Busam, and S. Ilic (2022)\nOSOP: a multi-stage one shot object pose estimation framework\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 6835â€“6844\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nD. Song, W. Nie, W. Li, M. Kankanhalli, and A. Liu (2021)\nMonocular image-based 3-d model retrieval: a benchmark\n.\nIEEE Transactions on Cybernetics\n52\n(\n8\n),\npp.Â 8114â€“8127\n.\nCited by:\nÂ§1\n,\nÂ§2.3\n,\nÂ§4.1\n.\nD. Song, Z. Qiang, C. Zhang, L. Wang, Q. Liu, Y. Yang, and A. Liu (2025)\nAdaptive CLIP for open-domain 3D model retrieval\n.\nInformation Processing & Management\n62\n(\n2\n),\npp.Â 103989\n.\nExternal Links:\nDocument\nCited by:\nÂ§2.3\n.\nD. Song, Y. Yang, W. Nie, X. Li, and A. Liu (2022)\nCross-domain 3d model retrieval based on contrastive learning and label propagation\n.\nIn\nProceedings of the 30th ACM international conference on multimedia\n,\npp.Â 286â€“295\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§2.3\n,\nTable 1\n.\nH. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller (2015)\nMulti-view convolutional neural networks for 3d shape recognition\n.\nIn\nProceedings of the IEEE international conference on computer vision\n,\npp.Â 945â€“953\n.\nCited by:\nÂ§2.3\n.\nB. Sun, J. Feng, and K. Saenko (2016)\nReturn of frustratingly easy domain adaptation\n.\nIn\nProceedings of the AAAI conference on artificial intelligence\n,\nVol.\n30\n.\nExternal Links:\nDocument\nCited by:\nTable 1\n.\nJ. Sun, Z. Wang, S. Zhang, X. He, H. Zhao, G. Zhang, and X. Zhou (2022)\nOnepose: one-shot object pose estimation without cad models\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 6825â€“6834\n.\nCited by:\nÂ§2.1\n.\nS. Thalhammer, J. Weibel, M. Vincze, and J. Garcia-Rodriguez (2023)\nSelf-supervised vision transformers for 3d pose estimation of novel objects\n.\nImage and Vision Computing\n139\n,\npp.Â 104816\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n.\nC. Wang, M. Cheng, F. Sohel, M. Bennamoun, and J. Li (2019)\nNormalNet: a voxel-based cnn for 3d object classification and retrieval\n.\nNeurocomputing\n323\n,\npp.Â 139â€“147\n.\nCited by:\nÂ§2.3\n.\nG. Wang, F. Manhardt, F. Tombari, and X. Ji (2021)\nGdr-net: geometry-guided direct regression network for monocular 6d object pose estimation\n.\nIn\nCVPR\n,\npp.Â 16611â€“16621\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nJ. Wang, N. Karaev, C. Rupprecht, and D. Novotny (2024)\nVggsfm: visual geometry grounded deep structure from motion\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 21686â€“21697\n.\nCited by:\nÂ§2.2\n.\nJ. Wang, W. Feng, Y. Chen, H. Yu, M. Huang, and P. S. Yu (2018)\nVisual domain adaptation with manifold embedded distribution alignment\n.\nIn\nProceedings of the 26th ACM international conference on Multimedia\n,\npp.Â 402â€“410\n.\nExternal Links:\nDocument\nCited by:\nTable 1\n.\nB. Wen, W. Yang, J. Kautz, and S. Birchfield (2024)\nFoundationPose: unified 6d pose estimation and tracking of novel objects\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n,\npp.Â 17868â€“17879\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nJ. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang (2025)\nStructured 3d latents for scalable and versatile 3d generation\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 21469â€“21480\n.\nCited by:\nÂ§2.2\n.\nY. Xiang, T. Schmidt, V. Narayanan, and D. Fox (2018)\nPoseCNN: a convolutional neural network for 6d object pose estimation in cluttered scenes\n.\nCited by:\nÂ§4.1\n,\nÂ§5.2\n,\nÂ§5.2\n.\nJ. Zhang, W. Li, and P. Ogunbona (2017)\nJoint geometrical and statistical alignment for visual domain adaptation\n.\nIn\nProceedings of the IEEE conference on computer vision and pattern recognition\n,\npp.Â 1859â€“1867\n.\nExternal Links:\nDocument\nCited by:\nTable 1\n.\nH. Zhou, A. Liu, and W. Nie (2019)\nDual-level embedding alignment network for 2d image-based 3d object retrieval\n.\nIn\nProceedings of the 27th ACM international conference on multimedia\n,\npp.Â 1667â€“1675\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§2.3\n,\nTable 1\n.\nF. Zhu, J. Xu, and C. Yao (2022)\nLocal information fusion network for 3d shape classification and retrieval\n.\nImage and Vision Computing\n121\n,\npp.Â 104405\n.\nExternal Links:\nDocument\n,\nISSN 0262-8856\n,\nLink\nCited by:\nÂ§2.3\n.",
    "preview_text": "6D object pose estimation plays a crucial role in scene understanding for applications such as robotics and augmented reality. To support the needs of ever-changing object sets in such context, modern zero-shot object pose estimators were developed to not require object-specific training but only rely on CAD models. Such models are hard to obtain once deployed, and a continuously changing and growing set of objects makes it harder to reliably identify the instance model of interest. To address this challenge, we introduce an Open-Set CAD Retrieval from a Language Prompt and a Single Image (OSCAR), a novel training-free method that retrieves a matching object model from an unlabeled 3D object database. During onboarding, OSCAR generates multi-view renderings of database models and annotates them with descriptive captions using an image captioning model. At inference, GroundedSAM detects the queried object in the input image, and multi-modal embeddings are computed for both the Region-of-Interest and the database captions. OSCAR employs a two-stage retrieval: text-based filtering using CLIP identifies candidate models, followed by image-based refinement using DINOv2 to select the most visually similar object. In our experiments we demonstrate that OSCAR outperforms all state-of-the-art methods on the cross-domain 3D model retrieval benchmark MI3DOR. Furthermore, we demonstrate OSCAR's direct applicability in automating object model sourcing for 6D object pose estimation. We propose using the most similar object model for pose estimation if the exact instance is not available and show that OSCAR achieves an average precision of 90.48\\% during object retrieval on the YCB-V object dataset. Moreover, we demonstrate that the most similar object model can be utilized for pose estimation using Megapose achieving better results than a reconstruction-based approach.\n\nOSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image\nTessa Pulli\nJean-Baptiste Weibel\nPeter ",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "6D object pose estimation",
        "CAD retrieval",
        "zero-shot",
        "CLIP",
        "DINOv2",
        "GroundedSAM",
        "Megapose"
    ],
    "one_line_summary": "OSCARæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡è¯­è¨€æç¤ºå’Œå•å¼ å›¾åƒä»æ— æ ‡ç­¾3Dæ•°æ®åº“ä¸­æ£€ç´¢åŒ¹é…çš„CADæ¨¡å‹ï¼Œç”¨äº6Dç‰©ä½“å§¿æ€ä¼°è®¡ã€‚",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-12T08:59:22Z",
    "created_at": "2026-01-21T12:09:09.118557",
    "updated_at": "2026-01-21T12:09:09.118564"
}