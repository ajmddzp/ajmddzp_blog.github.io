{
    "id": "2601.11143v1",
    "title": "Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model",
    "authors": [
        "Minho Lee",
        "Hyeonseok Kim",
        "Jin Tak Kim",
        "Sangshin Park",
        "Jeong Hyun Lee",
        "Jungsan Cho",
        "Jemin Hwangbo"
    ],
    "abstract": "大型液压机器人的仿真到现实（sim-to-real）迁移在机器人学中面临重大挑战，这源于其固有的控制响应迟缓与复杂的流体动力学特性。复杂动力学由多缸互联结构及各缸流体速率差异所导致。这些特性使得对所有关节进行精细仿真变得困难，难以适用于强化学习（RL）应用。本研究提出一种基于液压动力学驱动的解析执行器模型，用以表征复杂的执行机构。该模型能在1微秒内预测全部12个执行器的关节扭矩，满足强化学习环境对快速处理的需求。我们将该模型与基于神经网络的执行器模型进行比较，并展示了本模型在数据受限场景下的优势。通过强化学习结合本模型训练出的运动策略，已成功部署于重量超过300公斤的液压四足机器人。此项工作首次在重型液压四足机器人上实现了基于强化学习的稳定鲁棒指令跟踪运动策略迁移，展现了先进的仿真到现实迁移能力。",
    "url": "https://arxiv.org/abs/2601.11143v1",
    "html_url": "https://arxiv.org/html/2601.11143v1",
    "html_content": "Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model\nMinho Lee\n1\n, Hyeonseok Kim\n2\n, Jin Tak Kim\n3\n, Sangshin Park\n3\n, Jeong Hyun Lee\n1\n, Jungsan Cho\n3\n,\nand Jemin Hwangbo\n∗,1\nThis work was supported by the Korea Research Institute for defense Technology planning and advancement(KRIT) grant funded by the Korean government(DAPA(Defense Acquisition Program Administration)). (No. 20-107-C00-007-02(KRIT-CT-22-001), Development of Walking-Driving Hybrid Locomotion Control for Multi-Legged Robot Platform, 2024)\n∗\n*\ncorresponding author\n1\nRobotics and Artificial Intelligence Lab, KAIST, Daejeon, South Korea\n2\nRobotics Team, HYUNDAI Rotem, Uiwang, Gyeonggi-do, South Korea\n3\nHydraulic Robot Laboratory, Human-Centric Robotics R&D Department, Korea Institue of Industrial Technology, Ansan, Gyeonggi-do, South Korea\nmyno1126@kaist.ac.kr,\nhyunseok0427@hyundai-rotem.co.kr, jintagi@kitech.re.kr, pss@kitech.re.kr, josualee@kaist.ac.kr, chojs@kitech.re.kr, jhwangbo@kaist.ac.kr\nAbstract\nThe simulation-to-reality (sim-to-real) transfer of large-scale hydraulic robots presents a significant challenge in robotics because of the inherent slow control response and complex fluid dynamics. The complex dynamics result from the multiple interconnected cylinder structure and the difference in fluid rates of the cylinders. These characteristics complicate detailed simulation for all joints, making it unsuitable for reinforcement learning (RL) applications. In this work, we propose an analytical actuator model driven by hydraulic dynamics to represent the complicated actuators. The model predicts joint torques for all 12 actuators in under 1 microsecond, allowing rapid processing in RL environments. We compare our model with neural network-based actuator models and demonstrate the advantages of our model in data-limited scenarios. The locomotion policy trained in RL with our model is deployed on a hydraulic quadruped robot, which is over 300 kg. This work is the first demonstration of a successful transfer of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot, demonstrating advanced sim-to-real transferability.\nI\nINTRODUCTION\nQuadruped robots have been widely used for navigating through various obstacles and complex terrains\n[\n22\n,\n15\n,\n6\n,\n8\n,\n12\n,\n11\n,\n9\n]\n. They have demonstrated stability\n[\n8\n]\nand terrain adaptability\n[\n22\n,\n14\n,\n11\n,\n9\n]\n, showing their potential for various applications.\nMany hydraulic quadruped robots have been operated with model-based control\n[\n17\n,\n16\n,\n8\n,\n12\n,\n1\n]\n. Model-based control is a control strategy that relies on a mathematical model of the system’s dynamics. The model predicts the robot’s future behavior and optimizes the control inputs accordingly, enabling effective decision-making and real-time adaptation. For example, the hydraulic quadruped robot HyQ\n[\n16\n]\nutilized model-based control to achieve stable locomotion, demonstrating the effectiveness of model-based control in such systems\n[\n20\n]\n. Model-based control provides accurate and reliable control using an accurate system dynamics model. However, it can be computationally intensive and less adaptable to dynamic or unpredictable environments.\nRecently, the control of quadruped robots has increasingly relied on reinforcement learning (RL) approaches\n[\n4\n,\n19\n,\n7\n,\n9\n]\n. RL-based control offers greater flexibility, adapting more effectively to changing conditions and learning near-optimal policies without an explicit model, making it more robust in unpredictable scenarios\n[\n14\n,\n2\n]\n. Despite the advantages of RL-based control, model-based control has been widely used in hydraulic robots\n[\n1\n]\ndue to its ability to manage the complex dynamics of hydraulic systems. The low simulation-to-reality (sim-to-real) transferability in hydraulic systems makes it challenging to deploy RL-based controllers on real-world hardware platforms.\nFigure 1:\nActuator model framework\nSim-to-real transfer is considered more difficult in hydraulic systems than in electric motor systems because of the inherent slow response and the complex fluid dynamics of hydraulic systems\n[\n17\n,\n16\n,\n8\n,\n1\n]\n. Hydraulic robots comprise multiple interconnected cylinders, making accurate simulation difficult due to fluid compressibility and variations in pressure delivery speed in real systems. Furthermore, the large size and heavy weight of the robot can cause undesirable bending and compression of mechanical parts, further complicating the sim-to-real transfer. Moreover, collecting data from such a system without a reliable controller is challenging, as accidents can result in serious damage to the robot.\nWhile previous approaches have informed our work, our method differs in two key ways. First, we propose a simple analytical actuator model derived from hydraulic actuator dynamics, which facilitates sim-to-real transfer. This model accurately captures the behavior of hydraulic systems with negligible computational cost(\n<\n1\n​\nμ\n​\ns\n<1\\mu s\nfor all 12 actuators simultaneously). Recent studies often rely on neural network-based actuator models\n[\n4\n,\n10\n,\n21\n]\n. While these models can provide accurate predictions within the domains they are trained on, they tend to generalize poorly to out-of-distribution scenarios. This poses a serious risk in large-scale hydraulic systems, where untested conditions during data collection can lead to hazardous failures. Moreover, collecting isolated actuator data is impractical for legged robots, as joint loads vary depending on configuration and motion history. These dependencies make it difficult to replicate realistic operating conditions outside the full robotic system. We capture the real-world dynamics of hydraulic actuators—including external loads and fluid effects—by collecting locomotion data directly from a quadruped robot. Building on this, we develop an analytical model capable of handling a broad range of operating conditions to support robust reinforcement learning training.\nAlso, we validate our approach on a fully hydraulic quadruped robot. Unlike prior studies that focus on single-actuator hardware demonstrations\n[\n3\n,\n10\n]\n, our work showcases the model’s effectiveness in a complete, real-world multi-actuator system. In this study, we make the following key contributions:\n•\nProposal of an analytical actuator model with accurate torque prediction, showing high sim-to-real transferability.\n•\nDevelopment of an RL-based locomotion controller for a hydraulic quadrupedal robot weighing over 300kg.\n•\nDemonstration of hydraulic-powered RL locomotion at 1m/s, including adaptability to dynamic conditions.\nTo the best of our knowledge, this is the first demonstration of locomotion at 1 m/s on a hydraulic quadruped robot weighing over 300 kg using an RL-based locomotion controller.\nII\nRELATED WORKS\nRL-based control approaches have recently gained widespread adoption in the quadruped robot community. Previous work\n[\n4\n]\ndemonstrated the successful use of RL for learning agile and dynamic motor skills in legged robots, achieving high performance in complex tasks such as walking, running, and recovering from falls. More recent developments\n[\n13\n]\nadopted the RL-trained policies to traverse various terrains, enhancing the sim-to-real transferability and improving robustness in quadruped locomotion. However, it is challenging to apply RL directly to hydraulic quadruped robots due to the difficulty in sim-to-real transferability in hydraulic systems.\nSeveral attempts have been made to improve the sim-to-real transferability between theoretical estimations and actual hydraulic systems\n[\n17\n,\n3\n,\n10\n,\n18\n,\n1\n,\n21\n]\n. One line of work\n[\n3\n]\nproposed a nonlinear adaptive robust control strategy for a hydraulic actuator to estimate system parameters and handle nonlinearities. However, this approach is heavily dependent on the accuracy of the system model and is computationally demanding. Another work\n[\n10\n]\npresented a neural network inverse model of a hydraulic actuator to track a force trajectory accurately. However, their method was limited to a single actuator system and was verified only in a disturbance-free environment. Recent advancements\n[\n21\n]\nintroduced adaptive online neural predictive control for hydraulic actuators, demonstrating low tracking error and robustness to model uncertainties. However, the use of neural networks requires a large amount of high-quality training data, and real-time application remains challenging due to their high computational cost.\nIII\nMETHODS\nIII-A\nHydraulic quadruped robot\nFigure 2:\nA hybrid-locomotion platform combined with wheeled & quadruped system\nFigure 3:\nStructure of the hydraulic actuator, where two hydraulic cylinders function as a single unit to generate torque on a revolute joint. The illustration details how chains transfer force from the hydraulic cylinders to the joint. Each leg has three such actuators (roll, pitch, and knee), all following the same structural design.\nOur robot is a hybrid locomotion platform used to validate the proposed actuator model and the locomotion performance of a policy trained with the actuator model. Our robot weighs over 300kg and has a length exceeding 1.8 meters. It is a ground vehicle developed to navigate complex terrains while carrying heavy supplies. It features a four-legged design and a two-wheel differential drive system, as shown in Fig.\n2\n. The robot can handle various environments, driving at high speeds on flat ground and transitioning to walking when driving is impractical.\nThe leg mechanism comprises a 3-degree-of-freedom hydraulically actuated joint. Each rotary joint integrates a sprocket and chain mechanism, as shown in Fig.\n3\n, which addresses torque nonlinearity issues while having a wide range of motion. Each leg has three revolute actuators—roll, pitch, and knee joint—all of which share the same structural design. The torque for each joint is controlled by adjusting the pressures and displacements within two hydraulic cylinders arranged in a bidirectional pulling structure, which together form a single revolute actuator. A total of 24 hydraulic cylinders control the movement, all of which are connected to a single pump that supplies the fluid from outside the robot. In this study, we focus on the quadrupedal locomotion of the robot. All experiments are conducted with the driving system deactivated.\nIII-B\nHydraulic actuator control\nFigure 4:\nControl diagram of the joint with (a) torque PID control and (b) position PID control\nFigure 5:\nData distribution of simulation and real data\nEach joint operates under two distinct low-level control modes: torque and position PID control, each suited for different tasks. The block diagrams for both controls are shown in Fig.\n4\n, where\nu\nu\nis the servo valve input,\ni\ni\nis the electric current,\nQ\nQ\nis the flow rate, and\nq\nq\nis the current position of the joint.\nTorque PID control adjusts hydraulic pressure to achieve a target torque, making it computationally efficient and suitable for simulations where torque output is assumed to match the target. However, real-world discrepancies arise due to measurement noise and delays, reducing reliability in dynamic tasks like locomotion. Position PID control regulates actuator position for greater stability and precision in real-world applications, as it relies on accurate encoders. While less sensitive to measurement errors, it requires a detailed actuator model for effective simulation, making implementation more complex.\nWe address these challenges by proposing an actuator model that simplifies hydraulic actuator dynamics, enabling efficient simulation of position control with minimal computational overhead. Our actuator model predicts torque output based on its physical state, allowing rapid and realistic simulation of position control. This solution bridges the gap between simulation and real-world performance, enabling effective RL-based training without complex actuator modeling.\nIII-C\nActuator model\nPrevious work\n[\n10\n]\nproposed a first-order differential equation for a single hydraulic cylinder as\nf\n˙\n=\ng\n​\n(\nx\n)\n​\nx\n˙\n+\nh\n​\n(\nf\n,\nx\n)\n​\nx\ns\n\\displaystyle\\dot{f}=g(x)\\dot{x}+h(f,x)x_{s}\n(1)\nx\n˙\ns\n=\nθ\n​\n(\nx\ns\n)\n+\nψ\n​\n(\nx\ns\n)\n​\nu\n\\displaystyle\\dot{x}_{s}=\\theta(x_{s})+\\psi(x_{s})u\n(2)\nwhere,\ng\n​\n(\nx\n)\n\\displaystyle g(x)\n=\n−\nA\n2\n​\nβ\n​\n(\n1\nV\n0\n​\nA\n+\nA\n​\nx\n+\n1\nV\n0\n​\nB\n+\nA\n​\n(\nL\n−\nx\n)\n)\n\\displaystyle=-A^{2}\\beta\\left(\\frac{1}{V_{0A}+Ax}+\\frac{1}{V_{0B}+A(L-x)}\\right)\nh\n​\n(\nf\n,\nx\n)\n\\displaystyle h(f,x)\n=\nC\nd\n​\nw\n​\nβ\n​\nA\n​\nP\nS\n−\nP\nT\nρ\n−\nsgn\n​\n(\nx\ns\n)\n​\nf\nρ\n​\nA\n\\displaystyle=C_{d}w\\beta A\\sqrt{\\frac{P_{S}-P_{T}}{\\rho}-\\text{sgn}(x_{s})\\frac{f}{\\rho A}}\n⋅\n(\n1\nV\n0\n​\nA\n+\nA\n​\nx\n+\n1\nV\n0\n​\nB\n+\nA\n​\n(\nL\n−\nx\n)\n)\n.\n\\displaystyle\\quad\\cdot\\left(\\frac{1}{V_{0A}+Ax}+\\frac{1}{V_{0B}+A(L-x)}\\right).\nHere,\nf\nf\nis the force,\nx\nx\nis the position of the cylinder,\nx\ns\nx_{s}\nis the valve opening,\nu\nu\nis the control input (i.e., the reference position),\nθ\n​\n(\nx\ns\n)\n\\theta(x_{s})\n, and\nψ\n​\n(\nx\ns\n)\n\\psi(x_{s})\nare nonlinear functions related to the valve’s model and controller. The function\nsgn\n​\n(\nx\ns\n)\n\\text{sgn}(x_{s})\nis the sign function,\nL\nL\nis the cylinder stroke,\nA\nA\nis the cylinder area,\nw\nw\nis the area gradient,\nβ\n\\beta\nis the bulk modulus of the hydraulic fluid,\nρ\n\\rho\nis the oil density,\nV\n0\n​\nA\nV_{0A}\nand\nV\n0\n​\nB\nV_{0B}\nare the dead volume of each cylinder channel,\nC\nd\nC_{d}\nis the discharge coefficient of the valve,\nP\nS\nP_{S}\nis the supply pressure, and\nP\nT\nP_{T}\nis the return pressure.\nBy rewriting (\n1\n) into delta notations, the following equation can be obtained:\nΔ\n​\nf\n=\ng\n​\n(\nx\n)\n​\nΔ\n​\nx\n+\nh\n​\n(\nf\n,\nx\n)\n​\nx\ns\n​\nΔ\n​\nt\n.\n\\Delta f=g(x)\\Delta x+h(f,x)x_{s}\\Delta t\\,.\n(3)\nHowever, (\n3\n) is formulated for a single cylinder system, which does not account for the dynamic interactions present in a multi-cylinder environment. Direct application of (\n3\n) to systems with multiple interconnected cylinders, as in our robot, can lead to significant errors due to hydraulic coupling effects. In our case, multiple actuators share a common hydraulic circuit, where fluid compressibility and varying pressure delivery speeds cause discrepancies in the return pressures between the cylinders. Nonlinearity and the number of state variables also require high computational costs, which makes it challenging to apply in real-time simulations with 400 parallel environments. Additionally, in environments where forces are applied in a consistent direction, resistive forces and large impacts could be neglected. In contrast, the direction of force application continuously changes during the locomotion of our robot, making resistive forces and dynamic impacts non-negligible factors that require additional consideration.\nFigure 6:\nOverall concurrent training framework\nTo address these limitations, we extend the actuator model beyond the single-cylinder formulation (\n3\n) by introducing additional terms, including\nk\n3\nk_{3}\nto model resistive forces and\nk\n4\nk_{4}\nto account for corrective forces during sudden impacts. Furthermore, we simplify the original formulation while preserving its ability to capture essential hydraulic dynamics, thereby enhancing its practicality for real-world applications. Specifically, we propose the following model:\nΔ\n​\nf\n=\nk\n1\nΔ\n​\nx\n−\nk\n2\n​\nf\n−\nk\n3\n​\nx\n˙\n+\nk\n4\n​\nΔ\n​\nx\n⋅\nmax\n​\n(\n−\nf\n⋅\nsgn\n​\n(\nΔ\n​\nx\n)\n,\n0\n)\n.\n\\displaystyle\\begin{split}\\Delta f=k_{1}&\\Delta x-k_{2}f-k_{3}\\dot{x}\\\\\n&+k_{4}\\Delta x\\cdot\\text{max}\\left(-f\\cdot\\text{sgn}(\\Delta x),0\\right)\\,.\\end{split}\n(4)\nIn (\n3\n),\ng\n​\n(\nx\n)\ng(x)\nrepresents the position-dependent mechanical leverage effect as a function of\nx\nx\n. However, its variation remains minimal within the robot’s operating range, which can be approximated as a constant\nk\n1\nk_{1}\n.\nh\n​\n(\nf\n,\nx\n)\n​\nx\ns\n​\nd\n​\nt\nh(f,x)x_{s}dt\nrepresents the force-dependent pressure drop across the hydraulic system.\nf\nf\nis scaled 1000 times larger than\nx\nx\n. The applied force primarily influences this pressure drop, while other factors remain constant or vary negligibly. Thus, it can be approximated as a negative linear function of force, with\nk\n2\nk_{2}\ncapturing the system-dependent hydraulic characteristics. This simplification eliminates the need to solve nonlinear differential equations, substantially reducing computational cost while maintaining sufficient accuracy for RL training.\nThe term\nk\n3\n​\nx\n˙\nk_{3}\\dot{x}\nis introduced to model the velocity-dependent resistive force, which was negligible in (\n1\n) when rapid changes in the direction of force application were minimal. However, in our experiments, the actuator’s movement direction changes continuously, making fluid resistance a significant factor. Therefore, incorporating this resistive term is essential to accurately capture the actuator’s real-world behavior.\nWe introduce the corrective term\nk\n4\nk_{4}\nto account for sudden impact scenarios. The control input is conflicted by the physical dynamics of the actuator when the direction of motion is opposite to the current force. This happens when the actuator is attempting to resist an excessive external load from an impact, yet it is forced to move in a direction opposite to the force. The system’s control objective is misaligned with its actual physical response, resulting in discrepancies between the intended control and the resulting motion in the simulation without appropriate correction. Our robot experiences high external loads and reaction forces, and these effects are not adequately accounted for in the previous model (\n1\n). To address this issue, we introduce a corrective force that adjusts the control input in these scenarios. This term prevents excessive reductions in control force, ensuring that the model’s responses remain consistent with the system’s actual dynamics, even in the presence of large external disturbances or impacts.\nThe derivation of the corrective term begins with a reconsideration of the fundamental hydraulic pressure dynamics governing the actuator. The actuator force\nf\nf\ncan be represented as\nf\n=\np\n⋅\nA\nf=p\\cdot A\nwith the pressure\np\np\nand the effective actuator area\nA\nA\n. The hydraulic pressure dynamics within the actuator chamber, subject to fluid compressibility and instantaneous displacement demands, is given by:\nd\n​\np\nd\n​\nt\n=\nB\nV\n​\n(\nQ\ni\n​\nn\n−\nA\n​\nx\n˙\n)\n.\n\\frac{dp}{dt}=\\frac{B}{V}\\left(Q_{in}-A\\dot{x}\\right).\n(5)\nB\nB\ndenotes the fluid bulk modulus,\nV\nV\nis the actuator chamber volume,\nQ\ni\n​\nn\nQ_{in}\nis the volumetric inflow. In sudden impact scenarios, a rapid displacement command\nΔ\n​\nx\n\\Delta x\nleads to an instantaneous and significant inflow demand, described by\nQ\ni\n​\nn\n≈\nA\n​\nΔ\n​\nx\n/\nΔ\n​\nt\nQ_{in}\\approx A\\Delta x/\\Delta t\n.\nWe hypothesize that under impact conditions, the commanded displacement rate\nΔ\n​\nx\n/\nΔ\n​\nt\n\\Delta x/\\Delta t\ncan be considered significantly larger than the actual actuator velocity\nx\n˙\n\\dot{x}\n. The actuator piston motion is constrained by flow capacity and oil viscosity, which limit its acceleration under sudden commands. Consequently, while the commanded velocity reflects the instantaneous displacement demand, the actual piston velocity remains much smaller at the initial moment of impact. Hence, the second term can be regarded as negligible, leading to the approximation\nΔ\n​\np\n≈\n(\nB\n​\nA\n/\nV\n)\n​\nΔ\n​\nx\n\\Delta p\\approx(BA/V)\\Delta x\n.\nFurthermore, we assume that the pressure variation caused by an impact should scale with the applied impact force. To capture this effect, we introduce a dimensionless normalization with respect to the maximum actuator force, thereby hypothesizing that the corrective term can represent the nonlinear and transient nature of hydraulic responses under sudden impact conditions. The resulting equation is expressed as:\nΔ\n​\nf\nimp\n=\nB\n​\nA\n2\nV\n​\nf\nmax\n​\nf\n​\nΔ\n​\nx\n.\n\\Delta f_{\\text{imp}}=\\frac{BA^{2}}{Vf_{\\text{max}}}f\\Delta x.\n(6)\nThe final adjusted term, designed to activate under actual impact conditions (i.e., when\nf\n⋅\nΔ\n​\nx\n<\n0\nf\\cdot\\Delta x<0\n), is given by\nk\n4\n​\nΔ\n​\nx\n⋅\nmax\n​\n(\n−\nf\n⋅\nsgn\n​\n(\nΔ\n​\nx\n)\n,\n0\n)\nk_{4}\\Delta x\\cdot\\text{max}\\left(-f\\cdot\\text{sgn}(\\Delta x),0\\right)\n, as shown in (\n4\n). This corrective force plays a critical role in obtaining a stable RL policy. During RL exploration in simulation, the robot frequently encounters situations involving sudden impacts. The RL actor undergoes various attempts and experiences numerous falls throughout the process of learning locomotion policies, resulting in frequent and substantial impacts. This is shown in Fig.\n5\n, where simulation data exhibits a broader distribution compared to real data, particularly when the torque and control input are in opposite directions. If the actuator model is constructed without properly accounting for impacts, it will predict entirely incorrect values when encountering these situations, thereby hindering the RL training process. Additionally, our analytical model can estimate\nk\n4\nk_{4}\neven with a small amount of data, whereas learning-based methods rely on large and diverse input data. Since curve fitting requires significantly fewer data points than neural network training, our model remains reliable despite lower occurrences of impact cases.\nBy applying\nΔ\n​\nf\n=\nf\nnext\n−\nf\n\\Delta f=f_{\\text{next}}-f\nand the relationships\nτ\n=\nR\n​\nf\n\\tau=Rf\n,\nΔ\n​\nx\n=\nx\ndes\n−\nx\n\\Delta x=x_{\\text{des}}-x\n,\nx\n˙\n=\nR\n​\nq\n˙\n\\dot{x}=R\\dot{q}\n, and\nx\n=\nR\n​\nq\nx=Rq\n, where\nR\nR\nis the radius of the sprocket, the final expression of the actuator model is obtained from (\n4\n) as:\nτ\nnext\n=\n\\displaystyle\\tau_{\\text{next}}=\nk\n1\n​\nR\n2\n​\n(\nq\ndes\n−\nq\n)\n+\n(\n1\n−\nk\n2\n)\n​\nτ\n−\nk\n3\n​\nR\n2\n​\nq\n˙\n\\displaystyle k_{1}R^{2}(q_{\\text{des}}-q)+(1-k_{2})\\tau-k_{3}R^{2}\\dot{q}\n+\nk\n4\n​\nR\n​\n(\nq\ndes\n−\nq\n)\n⋅\nmax\n​\n(\n−\nτ\n⋅\nsgn\n​\n(\nq\ndes\n−\nq\n)\n,\n0\n)\n.\n\\displaystyle+k_{4}R(q_{\\text{des}}-q)\\cdot\\text{max}\\left(-\\tau\\cdot\\text{sgn}(q_{\\text{des}}-q),0\\right).\n(7)\nTherefore, (\nIII-C\n) shows that our model can predict\nτ\nnext\n\\tau_{\\text{next}}\ncorresponding to the current torque\nτ\n\\tau\n, current joint state\nq\nq\nand\nq\n˙\n\\dot{q}\n, and the target position\nq\ndes\nq_{\\text{des}}\n.\nThe coefficients of the actuator model from (\nIII-C\n) are obtained by fitting the data from real-world robot operations. We collected data from operating the robot for 20 seconds with a locomotion policy trained without an actuator model. The locomotion policy was operated at a frequency of 100Hz, while joint positions, target positions, joint velocities and torques were recorded at a higher frequency of 1000Hz. The high-frequency data collection allowed us to capture detailed and precise information about the actuator dynamics in real-robot operations.\nWe implemented three baseline neural network architectures—MLP, LSTM, and GRU—under the same dataset and training conditions as the proposed actuator model to ensure a fair comparison. Each network was configured with size (48, 64, 12), and hyperparameters such as hidden layer size were tuned to achieve the best accuracy. Training was performed for 1,000 iterations until convergence, providing sufficient optimization.\nIII-D\nEstimator and policy training with actuator model\nTABLE I:\nGlobal Rewards\nReward\nExpression\nCommand\nr\nv\n=\nk\ncmd\n{\nexp\n(\n−\n∥\ncmd\nx\n​\ny\n−\nv\nx\n​\ny\n∥\n2\n)\n⋅\n(\n1\n+\nexp\n⁡\n(\n−\n0.5\n​\n‖\ncmd\nx\n​\ny\n−\nv\nx\n​\ny\n‖\n)\n)\n+\nexp\n(\n−\n1.5\n(\ncmd\nz\n−\nω\nz\n)\n2\n)\n}\n\\begin{aligned} r_{v}=k_{\\text{cmd}}&\\{\\exp(-\\|\\text{cmd}_{xy}-v_{xy}\\|^{2})\\\\\n&\\cdot(1+\\exp(-0.5||\\text{cmd}_{xy}-v_{xy}||))\\\\\n&+\\exp(-1.5(\\text{cmd}_{z}-\\omega_{z})^{2})\\}\\end{aligned}\nYaw Command Error\nr\nyaw\n=\n{\n10\n​\nc\nf\n​\nk\nyaw\n​\n(\ncmd\nz\n−\nω\nz\n)\n2\n,\nif\n​\nω\nz\n=\n0\n,\nc\nf\n​\nk\nyaw\n​\n(\ncmd\nz\n−\nω\nz\n)\n2\n,\nelse\n.\n\\begin{aligned} r_{\\text{yaw}}=\\begin{cases}10c_{f}k_{\\text{yaw}}(\\text{cmd}_{z}-\\omega_{z})^{2},&\\text{if }\\omega_{z}=0,\\\\\nc_{f}k_{\\text{yaw}}(\\text{cmd}_{z}-\\omega_{z})^{2},&\\text{else}.\\end{cases}\\end{aligned}\nBase Height\nr\nh\n=\nk\nh\n​\nexp\n⁡\n(\n−\n40\n​\n|\nh\n0\n−\nh\n|\n)\nr_{h}=k_{h}\\exp(-40|h_{0}-h|)\nBase Motion\nr\nm\n=\nk\nm\n​\n(\nv\nz\n2\n+\n0.02\n​\n(\n|\nω\nx\n|\n+\n|\nω\ny\n|\n)\n)\nr_{m}=k_{m}(v_{z}^{2}+0.02(|\\omega_{x}|+|\\omega_{y}|))\nTorque\nr\nτ\n=\nc\nf\n​\nk\nτ\n​\n‖\nτ\n‖\n2\nr_{\\tau}=c_{f}k_{\\tau}||\\tau||^{2}\nTorque Clip\nr\nτ\nclip\n=\nc\nf\n​\nk\nτ\nclip\n​\n‖\nτ\nclip\n‖\n2\nr_{\\tau_{\\text{clip}}}=c_{f}k_{\\tau_{\\text{clip}}}||\\tau_{\\text{clip}}||^{2}\nNominal Position\nr\nq\n=\n{\n10\n​\nc\nf\n​\nk\nq\n​\n‖\nq\nt\n−\nq\nnom\n‖\n,\nif\n​\n|\ncmd\n|\n=\n0\n,\nc\nf\n​\nk\nq\n​\n‖\nq\nt\n−\nq\nnom\n‖\n,\nelse\n.\n\\begin{aligned} r_{q}=\\begin{cases}10c_{f}k_{q}||q_{t}-q_{\\text{nom}}||,&\\text{if }|\\text{cmd}|=0,\\\\\nc_{f}k_{q}||q_{t}-q_{\\text{nom}}||,&\\text{else}.\\end{cases}\\end{aligned}\nJoint Velocity\nr\nq\n˙\n=\nc\nf\n​\nk\nq\n˙\n​\n‖\nq\n˙\nt\n‖\n2\nr_{\\dot{q}}=c_{f}k_{\\dot{q}}||\\dot{q}_{t}||^{2}\nJoint Acceleration\nr\nq\n¨\n=\nc\nf\n​\nk\nq\n¨\n​\n‖\nq\n˙\nt\n−\nq\n˙\nt\n−\n1\n‖\n2\nr_{\\ddot{q}}=c_{f}k_{\\ddot{q}}||{\\dot{q}_{t}}-{\\dot{q}_{t-1}}||^{2}\nAction smoothness\nr\ns\n=\nc\nf\nk\ns\n(\n0.5\n|\n|\nq\nt\nd\n​\ne\n​\ns\n−\n2\nq\nt\n−\n1\nd\n​\ne\n​\ns\n+\nq\nt\n−\n2\nd\n​\ne\n​\ns\n|\n|\n2\n+\n|\n|\nq\nt\nd\n​\ne\n​\ns\n−\nq\nt\n−\n1\nd\n​\ne\n​\ns\n|\n|\n2\n)\n\\begin{aligned} r_{s}=c_{f}k_{s}(0.5||q_{t}^{des}-&2q_{t-1}^{des}+q_{t-2}^{des}||^{2}\\\\\n&+||q_{t}^{des}-q_{t-1}^{des}||^{2})\\end{aligned}\nTABLE II:\nLocal Rewards\nReward\nExpression\nFlight Phase\nr\nfl\n=\nc\nf\n​\nk\nfl\n​\n(\nif\n​\n∀\ni\n,\ncontact\ni\n=\nFalse\n)\nr_{\\text{fl}}=c_{f}k_{\\text{fl}}\\>(\\text{if }\\forall i,\\text{contact}_{i}=\\text{False})\nAirtime\nr\nair, i\n=\n{\nk\na\n​\nmin\n⁡\n(\nmax\n⁡\n(\nT\ns\n,\ni\n−\nT\na\n,\ni\n,\n−\n0.25\n)\n,\n0.25\n)\nif\n​\n‖\ncmd\n‖\n=\n0\n,\nk\na\n​\nmin\n⁡\n(\nT\na\n,\ni\n,\n0.2\n)\nif\n​\nT\na\n,\ni\n<\n0.25\n,\nk\na\n​\nmin\n⁡\n(\nT\ns\n,\ni\n,\n0.2\n)\nif\n​\nT\ns\n,\ni\n<\n0.25\n,\n0\nelse .\n\\begin{aligned} r_{\\text{air, i}}=\\begin{cases}k_{a}\\min(\\max(T_{s,i}-T_{a,i},-0.25),0.25)&\\text{if }||\\text{cmd}||=0,\\\\\nk_{a}\\min(T_{a,i},0.2)&\\text{if }T_{a,i}<0.25,\\\\\nk_{a}\\min(T_{s,i},0.2)&\\text{if }T_{s,i}<0.25,\\\\\n0&\\text{else .}\\\\\n\\end{cases}\\end{aligned}\nFoot Slip\nr\nslip\n,\ni\n=\nc\nf\n​\nk\nslip\n​\n‖\nv\nfoot\n,\ni\n,\nx\n​\ny\n‖\n2\n​\n(\nif\ncontact\ni\n=\nTrue\n)\n\\begin{aligned} r_{\\text{slip},i}=c_{f}k_{\\text{slip}}||v_{\\text{foot},i,xy}||^{2}\\>(\\text{if }\\text{contact}_{i}=\\text{True})\\\\\n\\end{aligned}\nFoot Clearance 1\nr\nc1\n,\ni\n=\nk\nc1\n‖\nv\nfoot\n,\ni\n‖\n​\n(\nh\ni\n−\nh\ntar\n)\n2\n(\nif\ncontact\ni\n=\nFalse\n&\n‖\ncmd\n‖\n>\n0\n)\n\\begin{aligned} r_{\\text{c1},i}=k_{\\text{c1}}&||v_{\\text{foot},i}||(h_{i}-h_{\\text{tar}})^{2}\\\\\n&(\\text{if }\\text{contact}_{i}=\\text{False}\\>\\&\\>||\\text{cmd}||>0)\\end{aligned}\nFoot Clearance 2\nr\nc2\n,\ni\n=\nk\nc2\n​\nr\nc1\n,\nt\n−\n1\n,\ni\n(\nif\ncontact\ni\n=\nTrue\n)\nr_{\\text{c2},i}=k_{\\text{c2}}r_{\\text{c1},t-1,i}\\quad(\\text{if }\\text{contact}_{i}=\\text{True})\nGRF Smoothness\nr\ngrf, i\n=\nc\nf\nk\ngrf\n{\n0.5\n(\nf\nt\n,\ni\ng\n​\nr\n​\nf\n−\n2\nf\nt\n−\n1\n,\ni\ng\n​\nr\n​\nf\n+\nf\nt\n−\n2\n,\ni\ng\n​\nr\n​\nf\n)\n2\n+\n(\nf\nt\n,\ni\ng\n​\nr\n​\nf\n−\nf\nt\n−\n1\n,\ni\ng\n​\nr\n​\nf\n)\n2\n}\n\\begin{aligned} r_{\\text{grf, i}}=c_{f}k_{\\text{grf}}\\{0.5(f_{t,i}^{grf}&-2f_{t-1,i}^{grf}+f_{t-2,i}^{grf})^{2}\\\\\n&+(f_{t,i}^{grf}-f_{t-1,i}^{grf})^{2}\\}\\end{aligned}\nAction Clip\nr\nact, j\n=\nc\nf\n​\nk\nact\n​\n|\nq\nt\n,\nj\nd\n​\ne\n​\ns\n,\nclip\n|\nr_{\\text{act, j}}=c_{f}k_{\\text{act}}|q_{t,j}^{des,\\text{clip}}|\nJoint Limit\nr\nl\n,\nj\n=\nc\nf\n​\nk\nl\n​\n(\nif\n​\nq\nt\n,\nj\n>\nlimit\n)\nr_{l,j}=c_{f}k_{l}\\>(\\text{if }q_{t,j}>\\text{limit})\nThe locomotion policy of the quadruped robot is trained with RL in 400 parallel environments with flat terrain. The initial state is assigned randomly for each environment, and curriculum learning is employed with factor\nc\nf\nc_{f}\nto facilitate smooth state transitions. Concurrent training\n[\n7\n]\nis introduced to train a robust locomotion policy and the state estimator for our robot, as the overall training framework is shown in Fig.\n6\n.\nReward functions and coefficients are applied as shown in Table\nI\nand\nII\n. Global rewards are calculated for the whole robot, and local rewards are calculated for each joint or foot. The total reward is calculated by summing all the global and local rewards. The simulation terminates during training when the robot makes contact with the ground, except for its feet and shanks. Observation and kinematics randomization are applied to be robust against disturbances from real-world experiments.\nTABLE III:\nRMSE and MAPE Loss for Actuator model and MLP, LSTM, GRU\nModels\nCommand 0.4 m/s\nCommand 1.0 m/s (1)\nCommand 1.0 m/s (2)\nCommand 1.0 m/s (3)\nCommand 1.0 m/s (4)\nRMSE\n|\nτ\nj\n|\n>\n50\n|\\tau_{j}|>50\nRMSE\n|\nτ\nj\n|\n>\n50\n|\\tau_{j}|>50\nRMSE\n|\nτ\nj\n|\n>\n50\n|\\tau_{j}|>50\nRMSE\n|\nτ\nj\n|\n>\n50\n|\\tau_{j}|>50\nRMSE\n|\nτ\nj\n|\n>\n50\n|\\tau_{j}|>50\nRMSE\nMAPE [%]\nRMSE\nMAPE [%]\nRMSE\nMAPE [%]\nRMSE\nMAPE [%]\nRMSE\nMAPE [%]\nActutator model (Ours)\n7.45\n8.06\n4.22\n8.44\n9.15\n4.77\n7.90\n8.67\n4.48\n10.6\n11.4\n5.65\n10.8\n11.7\n5.72\nMLP\n36.0\n41.7\n20.9\n47.1\n53.5\n27.3\n38.9\n44.8\n22.8\n57.5\n64.4\n33.2\n57.7\n63.8\n32.6\nLSTM\n33.3\n37.2\n19.8\n41.1\n46.5\n23.8\n35.3\n39.7\n20.6\n50.1\n55.8\n28.2\n51.2\n56.9\n28.7\nGRU\n27.4\n30.7\n15.6\n34.4\n38.3\n19.9\n28.9\n32.2\n16.4\n41.8\n46.1\n24.0\n42.1\n45.9\n24.0\nThe RL training network consists of three parts: actor, critic, and estimator. All three networks are designed using MLP. The estimator estimates the current robot state\nx\nt\nx_{t}\nfrom the observation\no\nt\no_{t}\n, and the actor receives the estimated robot state\nx\n~\nt\n\\widetilde{x}_{t}\nwith\no\nt\no_{t}\nand outputs an action\na\nt\na_{t}\n, which is the desired joint position. The observation state\no\nt\no_{t}\nconsists of\no\nt\n=\n(\nω\n,\nq\nt\n,\nq\n˙\nt\n,\nq\nt\n−\n1\nd\n​\ne\n​\ns\n,\ncmd\n)\n\\displaystyle o_{t}=(\\omega,q_{t},\\dot{q}_{t},q_{t-1}^{des},\\text{cmd})\n(8)\nwhere\nω\n\\omega\nstands for the base angular velocity,\nq\nt\nq_{t}\nand\nq\n˙\nt\n\\dot{q}_{t}\nare the joint states of time t,\nq\nt\n−\n1\nd\n​\ne\n​\ns\nq_{t-1}^{des}\nis the previous action which is the desired joint position from the previous time step, and\ncmd\nis the velocity command. The critic outputs a value from\no\nt\no_{t}\nand\nx\nt\nx_{t}\n, and the policy is trained with PPO. The estimator is trained by supervised learning with\nx\nt\nx_{t}\nand\nx\n~\nt\n\\widetilde{x}_{t}\n. The locomotion policy and the estimator are operated at 100Hz simultaneously, while the simulation and the position controller in the real robot are operated at 1000Hz. The training is conducted in Raisim\n[\n5\n]\nusing an AMD Ryzen Threadripper 3990X and NVidia RTX 3090, with 20,000 iterations taking around 10 hours.\nIV\nRESULTS\nFigure 7:\nTorque prediction of the actuator model of joint LFR, LFP, LFK\nFigure 8:\nReward shown according to the training iteration.\nIV-A\nEvaluation of actuator model\nWe compared the proposed actuator model’s prediction results with the baseline neural networks (MLP, LSTM, and GRU) under the conditions described in Section III-C. In total, five real-robot locomotion experiments were conducted. One trial was performed with a forward command of 0.4 m/s, while the remaining four were conducted at a forward command of 1.0 m/s. During these experiments, the locomotion policy generated target positions at 100Hz, and\no\nt\no_{t}\nwere recorded at 1000Hz. We compared the predicted torques with the measured torques at each timestep using the collected data. RMS and Mean Absolute Percentage (MAP) errors with a threshold of 50 N\n⋅\n\\cdot\nm for the actual torque values were calculated. This threshold is introduced because the error becomes disproportionately more prominent in the MAPE metric when the actual torque values are small. Table\nIII\nsummarizes the results for RMS and MAP errors, whereas Fig.\n7\nalso shows that our model has the highest accuracy in predicting the torque values. Our model outperforms the trained neural networks in terms of torque prediction accuracy across all experimental data, demonstrating its performance in sim-to-real transfer. This accuracy enables the RL-based controller to operate reliably across a broader range of conditions, ensuring stable and robust locomotion. Unlike neural networks, which struggle with out-of-distribution data, our actuator model provides consistent torque predictions even in dynamic environments, allowing the RL policy to adapt to disturbances and variations in terrain.\nFigure 9:\nLocomotion results with 1.0 m/s command, trained with our actuator model.\nFigure 10:\nTorque control results, actual vs target torque with torque control\nIn addition, we implemented the proposed actuator model and baseline models(MLP, LSTM, GRU) in the simulation and trained locomotion policies with RL. The reward values recorded during training are shown in Fig.\n8\n. Notably, our actuator model outperformed the exploration process in RL, as the latter failed to achieve appropriate reward values and did not converge to a stable result.\nThe proposed model’s performance arises from its ability to generalize beyond the narrow operational range of the training data. Although the dataset for the actuator model was limited, our model successfully predicted the corresponding torques in unstable situations. The baseline models were also trained with the same data but faced difficulties, often failing to generate reasonable outputs in unstable situations and consequently breaking down during RL training. To learn dynamic locomotion successfully, the actuator model must handle exceptional cases such as falls, where large impact forces frequently occur. Unlike the neural networks trained solely on standard walking data, the proposed actuator model incorporates the\nk\n4\nk_{4}\nterm, which applies corrective forces under impact conditions based on the actuator’s physical state. This mechanism enabled the model to remain accurate in out-of-distribution scenarios and effectively respond to dynamic disturbances, thereby ensuring robustness during training and real-world applications. Our model’s performance demonstrated its ability to generalize beyond the training data and handle complex, non-ideal situations, which is critical for robust quadruped robot locomotion.\nWe also compared the computational cost of training. The proposed actuator model required 10 hours to train the locomotion policy with RL, whereas the baseline models required between 36 and 48 hours under the same training conditions. This result highlights that our model achieves superior accuracy while being more than three times faster to train, thereby offering advantages from both performance and efficiency perspectives.\nIV-B\nEvaluation of the locomotion performance\nWe compared the robot’s locomotion performance with the trained policies using three different control approaches. These included torque PID control trained in RL without an actuator model, position PID control trained in RL with stiff PD estimation instead of an actuator model, and position PID control trained in RL with the proposed actuator model. All experiments were conducted indoors on a flat floor, where the robot walked about 30 meters in a straight trajectory to provide a controlled evaluation environment for the trained policies. The robot’s walking performance was evaluated in two main areas: command tracking and locomotion stability. The supplementary material includes detailed experimental videos that illustrate these approaches.\nIV-B\n1\nCommand Tracking\nThe robot was tested with tracking commands of 0.4 m/s and 1.0 m/s. The torque control demonstrated satisfactory performance at a speed of 0.4 m/s, with the command being tracked effectively. However, as the command speed increased, torque control struggled to keep the torque output in sync with the target torque, as shown in Fig.\n10\n, causing the robot to lose balance and tip forward. The robot struggled to follow the target torque in torque control, resulting in failures during dynamic locomotion. In contrast, the position controls followed the commands up to 1.0 m/s, demonstrating improved robustness to higher speeds. Notably, the locomotion controller trained with the actuator model showed sufficient capability, as in Fig.\n9\n, to increase the command speed beyond 1.0 m/s. This highlights the advantage of using an accurate actuator model, which not only enhances locomotion stability but also enables smoother control transitions, even in untrained conditions. However, this could not be thoroughly tested due to spatial constraints in the experimental setup and limitations related to the crane’s speed, which was used to prevent the robot from tipping over. Nevertheless, it is expected that the locomotion controller trained with the actuator model would have been able to handle even higher command speeds under more favorable conditions.\nIV-B\n2\nLocomotion stability\nThe torque control exhibited significant issues with locomotion stability as the command speed increased, resulting in the robot failing to maintain stability. The actuator delays shown in Fig.\n10\ncaused the robot to walk with pronounced impacts, resulting in noticeable vibrations and sounds. The robot frequently lost stability and fell when its feet encountered obstacles or the ground. The position control without the actuator model showed stability during locomotion but faced challenges due to its stiff nature of high-gain PD estimation. This stiffness restricted the control’s ability to lift the legs sufficiently, especially when encountering obstacles such as uneven surfaces. This limitation resulted in instability and compromised locomotion performance in more complex environments, as holding the foot slightly above the ground. Moreover, the stiff nature of the controller resulted in a less natural walking pattern, with sudden vibrations to the robot’s body. However, the position control trained with the proposed actuator model demonstrated superior locomotion stability. The locomotion was natural and smooth, with the robot’s feet reaching sufficient height for stable movements, as exactly shown inside the simulation environment. The learned policy demonstrated the ability to perform turning and lateral walking on flat terrain without requiring retraining, further supporting its robustness under the current training conditions. Overall locomotion was stable, and the robot exhibited more fluid and adaptive movements, even in dynamic conditions with sudden disturbances. This adaptivity was facilitated by the RL-based controller, which leveraged the accurate torque estimation of the actuator model to develop more robust control policies.\nThese results demonstrate that the proposed actuator model not only accurately simulates the behavior of hydraulic actuators but also enables the seamless transfer of simulation-based policies to real-world robotic systems. The stable and robust locomotion confirms the effectiveness of our actuator model under a wide range of conditions, both in simulation and real-world experiments.\nV\nCONCLUSION\nThis study demonstrated the effectiveness of the proposed actuator model by showing both accurate torque prediction and improvement of real-world locomotion. Our actuator model enabled the development of a robust RL-locomotion controller, demonstrating successful sim-to-real transfer in heavy hydraulic robots. The RL policy trained with the actuator model outperformed traditional methods, delivering smooth and stable locomotion both in simulation and on the real robot, even at higher speeds and under dynamic conditions.\nHowever, the model’s performances were not tested in scenarios that deviate significantly from the training data, such as abrupt directional changes or extreme external forces. Additionally, the long-term durability of the actuator model remains unassessed, and the experimental setup limited testing beyond 1.0 m/s.\nFuture work will focus on expanding real-world testing to include a broader range of conditions, such as higher speeds and diverse terrains, and refining the actuator model by incorporating factors such as actuator wear and hydraulic variations to ensure long-term reliability. We also aim to evaluate the model’s adaptability to different robotic platforms and tasks, increasing its generalizability beyond the quadruped robot used in this study. In the long term, we aim to develop controllers that combine the robustness of RL with the interpretability of model-based approaches, contributing to more reliable deployment in real-world systems.\nReferences\n[1]\nB. Cho, S. Kim, S. Shin, J. Oh, H. Park, and H. Park\n(2023)\nEnergy-efficient hydraulic pump control for legged robots using model predictive control\n.\n28\n(\n1\n),\npp. 3–14\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§I\n,\n§I\n,\n§II\n.\n[2]\nS. Choi, G. Ji, J. Park, H. Kim, J. Mun, J. H. Lee, and J. Hwangbo\n(2023)\nLearning quadrupedal locomotion on deformable terrain\n.\nScience Robotics\n8\n(\n74\n),\npp. eade2256\n.\nExternal Links:\nDocument\n,\nLink\n,\nhttps://www.science.org/doi/pdf/10.1126/scirobotics.ade2256\nCited by:\n§I\n.\n[3]\nC. Guan and S. Pan\n(2008)\nNonlinear adaptive robust control of single-rod electro-hydraulic actuator with unknown nonlinear parameters\n.\n16\n(\n3\n),\npp. 434–445\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§II\n.\n[4]\nJ. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter\n(2019)\nLearning agile and dynamic motor skills for legged robots\n.\nScience Robotics\n4\n(\n26\n),\npp. eaau5872\n.\nExternal Links:\nDocument\n,\nLink\n,\nhttps://www.science.org/doi/pdf/10.1126/scirobotics.aau5872\nCited by:\n§I\n,\n§I\n,\n§II\n.\n[5]\nJ. Hwangbo, J. Lee, and M. Hutter\n(2018)\nPer-contact iteration method for solving contact dynamics\n.\n3\n(\n2\n),\npp. 895–902\n.\nExternal Links:\nDocument\nCited by:\n§\nIII-D\n.\n[6]\nS. Jeon, M. Jung, S. Choi, B. Kim, and J. Hwangbo\n(2024)\nLearning whole-body manipulation for quadrupedal robot\n.\nIEEE Robotics and Automation Letters\n9\n(\n1\n),\npp. 699–706\n.\nExternal Links:\nDocument\nCited by:\n§I\n.\n[7]\nG. Ji, J. Mun, H. Kim, and J. Hwangbo\n(2022)\nConcurrent training of a control policy and a state estimator for dynamic and robust legged locomotion\n.\nIEEE Robotics and Automation Letters\n7\n(\n2\n),\npp. 4630–4637\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§\nIII-D\n.\n[8]\nH. Khan, S. Kitano, M. Frigerio, M. Camurri, V. Barasuol, R. Featherstone, D. G. Caldwell, and C. Semini\n(2015)\nDevelopment of the lightweight hydraulic quadruped robot — minihyq\n.\nIn\n2015 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)\n,\nVol.\n,\npp. 1–6\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§I\n,\n§I\n.\n[9]\nH. Kim, H. Oh, J. Park, Y. Kim, D. Youm, M. Jung, M. Lee, and J. Hwangbo\n(2025)\nHigh-speed control and navigation for quadrupedal robots on complex and discrete terrain\n.\nScience Robotics\n10\n(\n102\n),\npp. eads6192\n.\nExternal Links:\nDocument\n,\nLink\n,\nhttps://www.science.org/doi/pdf/10.1126/scirobotics.ads6192\nCited by:\n§I\n,\n§I\n.\n[10]\nS. Kim, B. Cho, S. Shin, J. Oh, J. Hwangbo, and H. Park\n(2021)\nForce control of a hydraulic actuator with a neural network inverse model\n.\n6\n(\n2\n),\npp. 2814–2821\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§I\n,\n§II\n,\n§\nIII-C\n.\n[11]\nY. Kim, J. H. Lee, C. Lee, J. Mun, D. Youm, J. Park, and J. Hwangbo\n(2024)\nLearning semantic traversability with egocentric video and automated annotation strategy\n.\nIEEE Robotics and Automation Letters\n9\n(\n11\n),\npp. 10423–10430\n.\nExternal Links:\nDocument\nCited by:\n§I\n.\n[12]\nS. Kuindersma, R. Deits, M. Fallon, A. Valenzuela, H. Dai, F. Permenter, T. Koolen, P. Marion, and R. Tedrake\n(2016-03-01)\nOptimization-based locomotion planning, estimation, and control design for the atlas humanoid robot\n.\nAutonomous Robots\n40\n(\n3\n),\npp. 429–455\n.\nExternal Links:\nISSN 1573-7527\n,\nDocument\n,\nLink\nCited by:\n§I\n,\n§I\n.\n[13]\nA. Kumar, Z. Fu, D. Pathak, and J. Malik\n(2021)\nRma: rapid motor adaptation for legged robots\n.\nCited by:\n§II\n.\n[14]\nJ. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter\n(2020)\nLearning quadrupedal locomotion over challenging terrain\n.\nScience Robotics\n5\n(\n47\n),\npp. eabc5986\n.\nExternal Links:\nDocument\n,\nLink\n,\nhttps://www.science.org/doi/pdf/10.1126/scirobotics.abc5986\nCited by:\n§I\n,\n§I\n.\n[15]\nX. Rong, Y. Li, J. Ruan, and B. Li\n(2012)\nDesign and simulation for a hydraulic actuated quadruped robot\n.\nJournal of Mechanical Science and Technology\n26\n(\n4\n),\npp. 1171–1177\n.\nExternal Links:\nDocument\nCited by:\n§I\n.\n[16]\nC. Semini, N. G. Tsagarakis, B. Vanderborght, Y. Yang, and D. G. Caldwell\n(2008)\nHyQ - hydraulically actuated quadruped robot: hopping leg prototype\n.\nIn\n2008 2nd IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics\n,\nVol.\n,\npp. 593–599\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§I\n.\n[17]\nM.R. Sirouspour and S.E. Salcudean\n(2001)\nNonlinear control of hydraulic robots\n.\n17\n(\n2\n),\npp. 173–182\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§I\n,\n§II\n.\n[18]\nK. B. Tcheumchoua, S. Nam, and W. K. Chung\n(2022)\nTorque control of hydraulic pressure servo valve driven actuator with deep neural network\n.\nIn\n2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n,\nVol.\n,\npp. 12512–12519\n.\nExternal Links:\nDocument\nCited by:\n§II\n.\n[19]\nV. Tsounis, M. Alge, J. Lee, F. Farshidian, and M. Hutter\n(2020)\nDeepGait: planning and control of quadrupedal gaits using deep reinforcement learning\n.\nIEEE Robotics and Automation Letters\n5\n(\n2\n),\npp. 3699–3706\n.\nExternal Links:\nDocument\nCited by:\n§I\n.\n[20]\nB. Ugurlu, I. Havoutis, C. Semini, and D. G. Caldwell\n(2013)\nDynamic trot-walking with the hydraulic quadruped robot—hyq: analytical trajectory generation and active compliance control\n.\nIn\n2013 IEEE/RSJ International Conference on Intelligent Robots and Systems\n,\npp. 6044–6051\n.\nCited by:\n§I\n.\n[21]\nY. Wang, C. Zhao, and Z. Shi\n(2024)\nAdaptive online neural predictive control of hydraulic actuator using fpga acceleration\n.\n71\n(\n4\n),\npp. 3924–3934\n.\nExternal Links:\nDocument\nCited by:\n§I\n,\n§II\n.\n[22]\nA. W. Winkler, C. Mastalli, I. Havoutis, M. Focchi, D. G. Caldwell, and C. Semini\n(2015)\nPlanning and execution of dynamic whole-body locomotion for a hydraulic quadruped on challenging terrain\n.\nIn\n2015 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp. 5148–5154\n.\nExternal Links:\nDocument\nCited by:\n§I\n.",
    "preview_text": "The simulation-to-reality (sim-to-real) transfer of large-scale hydraulic robots presents a significant challenge in robotics because of the inherent slow control response and complex fluid dynamics. The complex dynamics result from the multiple interconnected cylinder structure and the difference in fluid rates of the cylinders. These characteristics complicate detailed simulation for all joints, making it unsuitable for reinforcement learning (RL) applications. In this work, we propose an analytical actuator model driven by hydraulic dynamics to represent the complicated actuators. The model predicts joint torques for all 12 actuators in under 1 microsecond, allowing rapid processing in RL environments. We compare our model with neural network-based actuator models and demonstrate the advantages of our model in data-limited scenarios. The locomotion policy trained in RL with our model is deployed on a hydraulic quadruped robot, which is over 300 kg. This work is the first demonstration of a successful transfer of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot, demonstrating advanced sim-to-real transferability.\n\nLearning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model\nMinho Lee\n1\n, Hyeonseok Kim\n2\n, Jin Tak Kim\n3\n, Sangshin Park\n3\n, Jeong Hyun Lee\n1\n, Jungsan Cho\n3\n,\nand Jemin Hwangbo\n∗,1\nThis work was supported by the Korea Research Institute for defense Technology planning and advancement(KRIT) grant funded by the Korean government(DAPA(Defense Acquisition Program Administration)). (No. 20-107-C00-007-02(KRIT-CT-22-001), Development of Walking-Driving Hybrid Locomotion Control for Multi-Legged Robot Platform, 2024)\n∗\n*\ncorresponding author\n1\nRobotics and Artificial Intelligence Lab, KAIST, Daejeon, South Korea\n2\nRobotics Team, HYUNDAI Rotem, Uiwang, Gyeonggi-do, South Korea\n3\nHydraulic Robot Laboratory, Human-Centric Robotics R&D Department, Korea Institue of Industrial Technology, Ansan, Gye",
    "is_relevant": true,
    "relevance_score": 8.0,
    "extracted_keywords": [
        "Reinforcement Learning",
        "locomotion",
        "whole body control"
    ],
    "one_line_summary": "该论文提出一种基于液压动力学的执行器模型，用于在重型液压四足机器人上实现强化学习驱动的稳定运动控制，解决了仿真到现实转移的挑战。",
    "detailed_summary": "## 论文摘要\n\n**论文标题：** 使用执行器模型学习重型液压机器人的四足运动\n\n### 1. 研究背景和动机\n液压四足机器人因其强大的负载能力和地形适应性，在复杂环境中具有重要应用潜力。然而，其固有的**慢控制响应**和**复杂的流体动力学**（如多缸互联、流体压缩性）使得精确仿真极为困难，严重阻碍了**强化学习（RL）** 在仿真到现实（Sim-to-Real）迁移中的应用。现有基于神经网络的方法在数据有限或分布外场景下泛化能力差，且计算成本高，难以应用于重型液压系统。\n\n### 2. 核心方法和技术创新\n本文的核心创新是提出了一种**基于液压动力学的解析式执行器模型**，以高效、准确地模拟复杂的液压执行器动态。该模型在简化现有复杂微分方程的基础上，引入了四个关键物理参数：\n- **`k₁`**： 近似位置相关的机械杠杆效应。\n- **`k₂`**： 捕捉系统依赖的液压特性（力相关的压力降）。\n- **`k₃`**： 建模速度相关的流体阻力（针对连续变向运动）。\n- **`k₄`**： **关键创新项**，用于模拟**突然冲击**下的修正力，解决了在运动方向与受力方向相反时模型失准的问题，这对于RL训练中频繁出现的跌倒和冲击场景至关重要。\n\n该模型能够以**低于1微秒**的速度预测所有12个执行器的关节扭矩，计算效率极高，可直接集成到需要大量并行环境的RL训练中。\n\n### 3. 主要实验结果\n- **模型精度**： 在超过300公斤的真实液压四足机器人（混合行走-驱动平台）上收集数据验证。与MLP、LSTM、GRU等神经网络模型相比，本文的解析模型在**均方根误差（RMSE）** 和**平均绝对百分比误差（MAPE）** 上均表现更优，尤其在扭矩较大（>50 N·m）或存在冲击的场景下优势明显。\n- **训练效率**： 使用该模型训练RL运动策略仅需约10小时，比基线神经网络模型（36-48小时）快**3倍以上**。\n- **现实部署**： 将基于该模型训练的RL策略部署到真实机器人上，首次实现了**重达300公斤以上的液压四足机器人以1 m/s速度进行稳定、鲁棒的命令跟踪行走**。与传统的扭矩PID控制或未使用精确执行器模型的位置控制相比，本文方法表现出更平滑、更稳定的步态，并能适应动态扰动。\n\n### 4. 研究意义和价值\n本研究首次成功地将RL控制器迁移到重型液压四足机器人上，实现了高性能的Sim-to-Real转移。其价值在于：\n- **技术突破**： 提出的**轻量级解析执行器模型**有效桥接了液压系统仿真与现实的鸿沟，为数据有限场景下的RL应用提供了可靠、高效的解决方案。\n- **实际应用**： 证明了RL方法在控制复杂、重型液压机器人方面的可行性与优越性，为未来在**野外运输、灾难救援**等需要高负载和动态移动能力的场景中部署智能机器人奠定了基础。\n- **方法论贡献**： 模型中对冲击力（`k₄`项）的显式建模，为解决机器人学习控制中常见的“分布外”动态问题提供了新思路，增强了策略在非理想、高扰动环境下的鲁棒性。",
    "qa_pairs": [
        {
            "question": "这篇论文的核心创新点是什么，他想解决什么问题，怎么解决的？",
            "answer": "## 论文核心分析\n\n### **一、 研究问题**\n论文旨在解决**大型液压四足机器人仿真到现实（Sim-to-Real）迁移的难题**。具体挑战包括：\n- **液压系统动力学复杂**：流体可压缩性、多缸互联、压力传递延迟等，使得精确仿真和建模极其困难。\n- **控制响应慢**：相比电机，液压系统固有的迟滞特性阻碍了高性能控制。\n- **数据收集风险高**：在没有可靠控制器的情况下，从重型（>300 kg）机器人上收集数据极易导致设备损坏。\n- **现有方法的局限性**：基于模型的控制计算量大、适应性差；基于神经网络的方法需要大量数据、泛化能力弱且计算成本高。\n\n### **二、 核心创新点**\n论文的核心创新在于**提出并验证了一种用于强化学习（RL）训练的高效、解析式液压执行器模型**，从而首次在重型液压四足机器人上成功实现了基于RL的稳定、快速（1 m/s）命令跟踪运动。\n\n1.  **创新的解析式执行器模型**：\n    - **本质**：一个基于液压动力学推导的、高度简化的**分析模型**，而非黑箱神经网络。\n    - **关键改进**：在单缸模型基础上，引入了两个关键项以适配多缸互联的机器人腿部动态：\n        - **速度相关阻力项 (`k3 * ẋ`)**：捕捉连续运动中不可忽略的流体阻力。\n        - **冲击校正项 (`k4` 项)**：专门处理机器人学习过程中频繁发生的脚部冲击和方向突变情况，这是RL策略收敛的关键。\n    - **公式**：`Δf = k1Δx - k2f - k3ẋ + k4Δx·max(-f·sgn(Δx), 0)` （最终转换为关节扭矩预测公式 (7)）。\n\n2.  **数据高效与计算高效**：\n    - **数据需求极低**：仅需20秒的真实机器人运动数据，通过曲线拟合即可确定模型参数 (`k1, k2, k3, k4`)。\n    - **计算速度极快**：可**在1微秒内预测全部12个执行器的扭矩**，满足RL训练中400个并行环境实时仿真的要求。\n\n3.  **完整的系统集成与验证**：\n    - **首次演示**：这是首次在超过300公斤的**全液压四足机器人**上，成功部署基于RL训练出的运动策略，并实现1 m/s的稳定行走。\n    - **方法集成**：将提出的执行器模型嵌入到**并发训练框架**中，同时训练运动策略和状态估计器，提升了系统的整体鲁棒性。\n\n### **三、 解决方案路径**\n1.  **建模**：从液压缸基础动力学方程出发，进行合理简化和物理扩展，推导出轻量级解析模型。\n2.  **参数辨识**：在真实机器人上短时运行一个基础策略，收集关节状态、目标位置和扭矩数据，通过拟合确定模型参数。\n3.  **仿真训练**：在Raisim仿真器中，使用该执行器模型模拟位置PID控制下的液压关节响应，在400个并行环境中用PPO算法训练运动策略和状态估计器。\n4.  **实物迁移**：将训练好的策略直接部署到真实的300公斤液压四足机器人上，无需额外调整。\n5.  **对比验证**：\n    - **与神经网络模型对比**：在相同数据下，本文模型在扭矩预测精度（RMSE, MAPE）和**处理分布外数据**（如冲击）的能力上显著优于MLP、LSTM、GRU等模型。\n    - **与不同控制方式对比**：证明了“**基于执行器模型的位置PID控制**”在命令跟踪（可达1 m/s以上）和运动稳定性上，优于传统的扭矩PID控制和没有精确模型的高增益位置PD控制。\n\n### **四、 实际价值与意义**\n- **技术突破**：为**重型液压腿式机器人**的**数据驱动控制（特别是RL）** 开辟了一条可行的道路，解决了其Sim-to-Real的核心瓶颈。\n- **工程实用性**：模型**简单、高效、可靠**，降低了对计算资源和海量数据的需求，更适用于实际机器人开发与部署。\n- **性能提升**：实现了重型液压四足机器人更**自然、平滑、自适应**的动态行走，展示了RL在复杂系统控制中的潜力。\n- **为后续研究奠基**：提供了可扩展的框架，未来可在此基础上测试更复杂地形、更高速度，并探索与基于模型控制的融合。\n\n**总结**：本文的核心创新是**一个为RL量身定制的、物理启发的轻型液压执行器解析模型**。它通过巧妙的简化与关键动态项的补充，以极低的计算和数据成本，精准地捕捉了复杂液压系统的核心动态，从而成功桥接了重型液压机器人仿真与现实的鸿沟，实现了高性能学习控制的首次落地。",
            "timestamp": "2026-01-20T17:57:48.285906",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "基于他的前作，梳理这个方向的整个发展脉络，每一步相比于之前的工作都改进了什么，着重于几个不同的发展方向。",
            "answer": "## 论文分析：基于前作的四足液压机器人强化学习控制发展脉络\n\n本文《Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model》是液压驱动四足机器人控制领域的一个重要进展。其核心贡献在于**首次在超过300公斤的重型液压四足机器人上，成功实现了基于强化学习（RL）的稳定、鲁棒且能跟踪速度指令（1 m/s）的步态控制，并完成了高效的仿真到现实（sim-to-real）迁移**。\n\n基于本文及其引用的相关工作，该方向的发展脉络可以梳理为以下几个关键阶段和不同的发展方向：\n\n### 发展脉络与核心改进\n\n#### 方向一：**控制范式演进：从模型控制到学习控制**\n\n1.  **早期阶段：基于模型的传统控制**\n    *   **代表工作**：HyQ系列机器人、MIT Atlas等（文献[8, 12, 16, 20]）。\n    *   **核心方法**：依赖精确的机器人动力学和液压系统数学模型（如流体力学方程），进行实时优化和控制（如模型预测控制MPC）。\n    *   **优点**：在模型准确时，控制精度高、稳定。\n    *   **局限性**：\n        *   **计算复杂**：实时求解复杂模型计算量大。\n        *   **环境适应性差**：难以应对未建模的动态或不可预测的环境变化（如复杂地形）。\n        *   **模型依赖性强**：对液压系统复杂的非线性动力学（如流体压缩性、耦合效应）建模极其困难，模型误差会直接影响性能。\n\n2.  **近期突破：基于强化学习的控制（针对电机驱动机器人）**\n    *   **代表工作**：ETH Zurich团队的工作（文献[4, 14, 19]）等。\n    *   **核心方法**：在仿真环境中，通过试错让智能体学习步态策略，无需精确的显式动力学模型。策略以端到端方式从传感器输入映射到关节动作。\n    *   **相比于模型控制的改进**：\n        *   **强大的环境适应性**：能学习在复杂、不平坦地形上行走（文献[14]）。\n        *   **高动态性**：能实现跑、跳、摔倒恢复等敏捷运动（文献[4]）。\n        *   **降低模型依赖**：不依赖于精确的解析模型，更注重从数据中学习行为。\n    *   **在液压机器人上应用的障碍**：**仿真到现实的巨大鸿沟**。电机模型相对简单，而液压系统的复杂动力学使得仿真环境与真实机器人行为差异巨大，直接迁移学习策略必然失败。\n\n#### 方向二：**解决液压系统Sim-to-Real鸿沟：执行器建模的演进**\n\n这是本文工作的核心背景。为了将RL成功应用于液压机器人，必须建立能够准确反映真实液压执行器行为的仿真模型。\n\n1.  **初期尝试：复杂解析模型与单执行器验证**\n    *   **代表工作**：文献[3, 10]等。\n    *   **核心方法**：\n        *   建立基于物理定律（如流体连续性方程、阀控方程）的非线性微分方程模型（如本文公式(1)(2)）。\n        *   使用非线性自适应鲁棒控制[3]或神经网络逆模型[10]进行单执行器的力/轨迹跟踪控制。\n    *   **局限性**：\n        *   **计算成本高**：非线性方程求解慢，难以集成到需要大量并行仿真的RL训练中。\n        *   **系统简化**：多在**单执行器、无干扰**的实验台上验证，未考虑多执行器互联、机器人全身动力学耦合的真实场景。\n        *   **数据需求与泛化**（针对神经网络模型）：需要大量高质量数据，且对分布外（OOD）情况泛化能力差，在大型机器人上收集失败数据风险高。\n\n2.  **本文的改进：面向RL的轻量化解析执行器模型**\n    *   **核心创新**：提出了一个**兼顾精度与效率的解析执行器模型**（公式(7)），作为仿真环境与真实硬件之间的“桥梁”。\n    *   **相比于前作的改进**：\n        *   **计算效率极高**：预测12个执行器的扭矩仅需<1微秒，比求解完整微分方程快数个数量级，使大规模并行RL训练（400个环境）变得可行。\n        *   **模型简化但关键特征保留**：\n            *   将位置相关项 `g(x)` 简化为常数 `k1`。\n            *   将力相关压力降项 `h(f,x)xs dt` 简化为与力负相关的线性项 `-k2 f`。\n            *   **新增关键项**：引入了**速度相关阻力项 `-k3 x˙`** 和**冲击校正项 `k4 ...`**。后者专门用于处理学习过程中机器人频繁摔倒、脚与地面发生剧烈冲击的情况，这是此前模型忽略但RL训练中至关重要的动态。\n        *   **数据效率高**：仅需20秒的真实机器人行走数据，通过曲线拟合即可确定模型参数 (`k1-k4`)。相比需要海量数据的神经网络模型，在**数据有限**的场景下优势明显。\n        *   **全系统验证**：模型在**完整的、300公斤重的多执行器四足机器人**上成功验证，实现了从仿真策略到真实机器人的直接部署，而不仅仅是单执行器实验。\n\n#### 方向三：**训练框架与策略鲁棒性的增强**\n\n1.  **基础RL训练**：使用PPO等算法在仿真中学习步态。\n2.  **并发训练**：本文采用了文献[7]的**并发训练框架**，同时训练**策略网络（Actor）** 和**状态估计器（Estimator）**。这使机器人能基于不完全的观测（如无精确的全局状态）学习鲁棒策略，更好地应对真实世界中的传感器噪声和状态估计误差。\n3.  **课程学习与域随机化**：在训练中逐步增加难度（课程学习），并对机器人的动力学参数（如质量、惯性）进行随机化。这迫使策略学习适应一个参数分布，而非单一模型，从而提高了对真实世界不确定性的**鲁棒性**。\n\n### 总结：几个不同的发展方向\n\n1.  **控制范式迁移方向**：从**完全依赖先验知识的模型控制**，转向**以数据驱动、具备强大环境适应能力的强化学习控制**。本文是这一范式在**重型液压**这一极具挑战性平台上成功落地的里程碑。\n2.  **建模方向**：针对液压系统sim-to-real难题，从追求**高保真度但笨重的第一性原理模型**，发展到**为RL训练量身定制的、在关键动态上保真、在计算上高效的简化解析模型**。本文模型在精度、速度和数据需求间取得了最佳平衡。\n3.  **系统集成与验证方向**：从**在简化、隔离的单执行器硬件上验证算法**，发展到**在完整的、复杂的多自由度机器人系统上进行端到端的集成与验证**。本文证明了学习到的策略能直接控制一个大型复杂机电液系统。\n4.  **训练方法论方向**：从单纯训练策略，发展到**并发训练策略与状态估计器**，并结合**课程学习、域随机化**等技术，系统性提升学习策略的鲁棒性和现实迁移能力。\n\n**本文的实际价值**在于，它提供了一套切实可行的技术路径，让强化学习这种强大的数据驱动方法能够应用于传统上被认为必须由复杂模型控制的重型液压机器人，为这类机器人在复杂、非结构化环境（如救灾、野外勘探、建筑工地）中实现更自主、更适应性的运动能力打开了大门。",
            "timestamp": "2026-01-20T17:58:45.913956",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他的前作有哪些？使用表格仔细讲讲他的每篇前作，他和前作的区别是什么，主要改善是什么？着重于具体相比于之前文章的改动",
            "answer": "## 论文前作分析\n\n根据提供的论文内容，作者团队（以KAIST的Jemin Hwangbo实验室为核心）在四足机器人强化学习控制领域有一系列相关研究。以下是基于文中引用梳理出的主要前作及其与本篇论文的区别与改进。\n\n### 前作汇总与分析\n\n| 前作文献 | 发表年份 | 核心贡献/焦点 | **与本篇论文（液压机器人+执行器模型）的主要区别** | **本篇论文的主要改善与创新** |\n| :--- | :--- | :--- | :--- | :--- |\n| **Hwangbo et al.** [4] | 2019 | **开创性工作**：首次系统性地展示了使用RL为（电动）四足机器人学习敏捷动态运动技能（跑、跳、恢复）。 | 1. **机器人平台**：针对**电动**执行器的四足机器人（如ANYmal）。<br>2. **执行器模型**：使用简单的力矩控制模型或理想化模型，未深入解决复杂执行器的sim-to-real鸿沟。<br>3. **挑战**：方法直接应用于**液压**机器人时，会因液压系统复杂的流体动力学和慢响应而失败。 | 1. **平台迁移**：将RL控制成功应用于**大型、重型（>300kg）液压**四足机器人，解决了更具挑战性的硬件平台问题。<br>2. **核心创新**：提出了**基于液压动力学的解析执行器模型**，专门用于在仿真中精确、高效地模拟液压执行器的位置控制行为，这是实现sim-to-real转移的关键。 |\n| **Lee et al.** [14] | 2020 | 专注于让四足机器人在**具有挑战性的自然地形**（岩石、草地、斜坡）上学习 locomotion。提升了在复杂环境中的鲁棒性和适应性。 | 1. **研究重点**：侧重于**外部环境**的复杂性与适应性。<br>2. **执行器假设**：依然基于相对理想的电动执行器模型，未处理执行器内部复杂动力学。 | 1. **研究重点转移**：侧重于**机器人本体**的复杂动力学——即**液压执行器**的建模问题。这是实现任何复杂地形应用的前提。<br>2. **解决内部不确定性**：通过精确的执行器模型，首先保证了机器人本体在平地上运动的精确控制和稳定性，为后续复杂地形应用打下基础。 |\n| **Ji et al.** [7] | 2022 | 提出了**并发训练**框架，同时训练控制策略和状态估计器，以应对状态估计不准和外部扰动，提升动态运动的鲁棒性。 | 1. **方法层面**：提出了一个通用的训练框架（并发训练）。<br>2. **未特定化平台**：框架可应用于多种机器人，但未针对液压系统的特殊难题提供解决方案。 | 1. **框架应用与深化**：**采纳并应用了并发训练框架**（见图6），但将其与**新的液压执行器模型**紧密结合。<br>2. **解决特定问题**：本工作的核心创新（执行器模型）解决了并发训练框架在液压平台上无法实施的根本问题——即缺乏一个快速、准确、可迁移的仿真环境。 |\n| **Choi et al.** [2] | 2023 | 研究四足机器人在**可变形地形**（如泥地、沙地）上的 locomotion 学习，进一步扩展了环境适应性。 | 1. **环境交互**：关注机器人与**柔软、时变**地面的交互动力学。<br>2. **本体模型**：机器人本体（执行器）仍被视为相对简单的模型。 | 1. **先解决“内功”**：本文认为，对于液压机器人，**本体执行器的精确建模**是比复杂地形交互更优先、更基础的挑战。只有在仿真中准确复现了机器人本体的响应，学习到的策略才有可能转移到实体。<br>2. **基础性贡献**：本文的工作是为液压机器人应用 [2] 这类高级地形适应技术**铺平道路**，提供了必需的本体建模工具。 |\n| **Kim et al.** [10] | 2021 | **与本工作直接相关**：提出了用于液压执行器的**神经网络逆模型**，以实现精确的力轨迹跟踪。但仅在**单执行器**硬件上验证，且环境无扰动。 | 1. **模型类型**：采用**数据驱动的神经网络**作为执行器模型。<br>2. **验证范围**：仅在**单个、孤立**的执行器上进行测试，未集成到完整的多足机器人系统中。<br>3. **数据需求与泛化**：神经网络需要大量高质量数据，且在分布外场景下泛化能力差，对于学习过程中充满“摔倒”等异常情况的RL训练风险高。 | 1. **模型类型**：提出**解析模型**，基于物理方程推导并简化，仅需少量数据（20秒机器人运动数据）拟合参数。<br>2. **验证范围**：在**完整的12关节、300kg四足机器人**上实现端到端的成功部署，验证了其在实际复杂系统中的作用。<br>3. **优势**：<br>   - **计算效率极高**（<1μs预测12个关节力矩），适合RL所需的大量并行仿真。<br>   - **泛化能力强**：通过引入 `k4` 项等，能更好地处理RL探索中常见的、数据集中少见的“冲击”场景，提高了训练稳定性和成功率。<br>   - **数据效率高**：在数据有限的场景下（收集真实机器人数据昂贵且危险）明显优于神经网络模型。 |\n\n### 总结：核心改进路径\n\n作者团队的研究呈现出一条清晰的演进路径：\n1.  **从电动到液压**：早期工作（[4], [14]）奠定了RL控制四足机器人的方法论，但主要面向执行器模型相对简单的**电动**平台。\n2.  **从理想模型到真实建模**：认识到液压系统的sim-to-real是主要瓶颈后，开始攻关执行器建模。前作[10]尝试了神经网络方法，但存在**数据依赖强、泛化弱、仅限单执行器**的局限。\n3.  **本文的突破**：\n    *   **方法论上**：从**数据驱动的黑箱模型**转向**基于物理的解析白箱模型**，在**准确性、计算速度、数据效率和泛化能力**上取得了更好平衡。\n    *   **系统集成上**：从**单执行器验证**迈向**全尺寸重型多关节机器人**的完整RL控制与成功部署。\n    *   **成果上**：首次实现了重型液压四足机器人**基于RL的、稳定跟踪指令的1m/s行走**，标志着在解决液压机器人sim-to-real难题上迈出了关键一步。\n\n**因此，本文最核心的改善在于：它提供了一个专门为重型液压机器人RL训练量身定制的、高效且鲁棒的执行器建模解决方案，从而打通了将先进的RL控制方法应用于这类复杂机械系统的“最后一公里”。**",
            "timestamp": "2026-01-20T17:59:37.638681",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文提出了哪些关键技术方法，请列表格具体详细说明技术细节，需要包含具体的数学原理推导，以及具体参数。",
            "answer": "## 论文关键技术方法分析\n\n本文针对**重型液压四足机器人**的仿真到现实（Sim-to-Real）迁移难题，提出了一套创新的技术方案，其核心是**一个基于液压动力学的解析式执行器模型**。该模型旨在高效、准确地模拟复杂液压执行器的动态特性，从而支撑强化学习（RL）策略的训练，并最终实现高性能的机器人运动控制。\n\n以下表格详细说明了论文提出的关键技术方法及其细节：\n\n| 关键技术方法 | 技术细节与数学原理推导 | 具体参数与说明 |\n| :--- | :--- | :--- |\n| **1. 解析式液压执行器模型** | **核心目标**：建立一个计算高效、能准确预测关节扭矩的模型，用于RL仿真训练。<br><br>**模型基础**：从一个描述单液压缸力动态的一阶微分方程出发：<br>`\\dot{f} = g(x)\\dot{x} + h(f, x)x_s`<br>其中，`g(x)` 代表与位置相关的机械杠杆效应，`h(f, x)x_s` 代表与力相关的系统压降。<br><br>**模型简化与扩展**：作者指出，对于其多缸互联的机器人系统，直接应用上述模型会因液压耦合效应产生误差，且非线性计算成本高。因此，他们进行了关键简化和物理扩展，提出了最终模型：<br>`Δf = k₁Δx - k₂f - k₃\\dot{x} + k₄Δx · max(-f · sgn(Δx), 0)`<br><br>**各项物理意义**：<br>- **`k₁Δx`**：近似常数化的位置相关项，代表由目标位移差产生的力（类似弹簧效应）。<br>- **`-k₂f`**：力相关的阻尼项，模拟液压系统的压力损失动态。<br>- **`-k₃\\dot{x}`**：速度相关的阻力项，模拟流体阻力，在运动方向频繁变化的步态中至关重要。<br>- **`k₄Δx · max(-f · sgn(Δx), 0)`**：**冲击修正项**。这是本文的关键创新之一。当执行器运动方向 (`Δx`) 与当前受力 (`f`) 方向相反时（即承受冲击），此项被激活，提供一个额外的修正力，防止模型在RL训练中遇到冲击时预测出完全错误的值。其推导基于液压腔压力动力学 `dp/dt = B/V (Q_in - A\\dot{x})`，并在冲击假设下简化得到 `Δf_imp ∝ fΔx`。 | - **模型输入**：当前力 `f`、当前位置 `x`、目标位置变化 `Δx`、当前速度 `\\dot{x}`。<br>- **模型输出**：下一时刻的力变化 `Δf`。<br>- **关键系数**：`k₁, k₂, k₃, k₄`。这些系数通过**曲线拟合**从真实机器人20秒的 locomotion 数据（1000Hz采样）中获取，而非从孤立执行器数据中学习。<br>- **计算效率**：预测全部12个执行器的关节扭矩耗时 **<1微秒**，满足RL并行仿真（400个环境）的实时性要求。 |\n| **2. 从力模型到关节扭矩模型的转换** | 将执行器力模型（公式4）映射到机器人关节空间。利用机器人机械结构（链轮链条）的几何关系：<br>`τ = R·f`, `x = R·q`, `\\dot{x} = R·\\dot{q}`, `Δx = R(q_des - q)`<br>其中 `R` 是链轮半径，`q` 是关节角度，`τ` 是关节扭矩。<br><br>代入公式(4)并整理，得到**最终用于仿真的关节扭矩预测模型**：<br>`τ_next = k₁R²(q_des - q) + (1 - k₂)τ - k₃R²\\dot{q} + k₄R(q_des - q) · max(-τ · sgn(q_des - q), 0)` | - **输入**：当前关节扭矩 `τ`、当前关节位置 `q` 与速度 `\\dot{q}`、目标关节位置 `q_des`。<br>- **输出**：下一仿真步的预测关节扭矩 `τ_next`。<br>- **作用**：该模型在仿真中替代了理想的位置PID控制器或扭矩控制器，使得仿真中的位置控制能反映出真实液压系统的延迟、阻尼和冲击响应。 |\n| **3. 基于执行器模型的RL训练框架** | **整体架构**：采用 **“并发训练”** 框架，同时训练 locomotion 策略和状态估计器，以提升系统对真实世界扰动的鲁棒性。<br><br>**网络结构**：<br>1.  **估计器 (Estimator)**：MLP网络，从观测 `o_t` 估计机器人状态 `x_t`。<br>2.  **执行器 (Actor)**：MLP网络，以估计状态 `\\tilde{x}_t` 和观测 `o_t` 为输入，输出目标关节位置 `q_des`（即动作 `a_t`）。<br>3.  **评论家 (Critic)**：MLP网络，评估状态-动作对的价值。<br><br>**训练细节**：<br>- **算法**：近端策略优化（PPO）。<br>- **仿真环境**：Raisim，400个并行环境。<br>- **观测空间 `o_t`**：基座角速度 `ω`、关节位置 `q_t`、关节速度 `\\dot{q}_t`、上一时刻动作 `q_{t-1}^{des}`、速度指令 `cmd`。<br>- **奖励函数**：精心设计的多项奖励组合（见表I和表II），涵盖指令跟踪、基座高度与运动、关节扭矩与运动平滑性、足端接触与滑移等多个方面，并采用课程学习（因子 `c_f`）来平滑训练过程。 | - **训练平台**：AMD Ryzen Threadripper 3990X CPU 和 NVidia RTX 3090 GPU。<br>- **训练时间**：使用提出的解析模型，完成20,000次迭代约需 **10小时**。相比之下，基于神经网络（MLP/LSTM/GRU）的执行器模型需要36-48小时，凸显了本文模型的计算效率优势。<br>- **策略频率**：100Hz（与真实机器人控制器一致）。<br>- **仿真步频**：1000Hz（以匹配真实系统的高频动态）。 |\n| **4. 与数据驱动模型的对比与优势** | **对比基准**：在**相同数据集**上训练了三种神经网络模型——MLP、LSTM、GRU，作为执行器模型的对比基准。<br><br>**性能对比**：<br>1.  **预测精度**：在多个真实机器人实验数据（0.4 m/s 和 1.0 m/s 速度指令）上，本文解析模型的RMSE和MAPE误差均**显著低于**所有神经网络模型（见表III）。尤其在扭矩绝对值较大（`|τ_j|>50 N·m`）时，优势更明显。<br>2.  **泛化与鲁棒性**：神经网络在训练数据分布内表现尚可，但对**分布外情况**（如RL探索中的跌倒、剧烈冲击）泛化能力差，导致RL训练失败。本文模型的**冲击修正项 (`k₄`)** 使其能物理合理地处理这些罕见但关键的情况。<br>3.  **数据效率**：解析模型通过**曲线拟合**少量数据（20秒）即可获得可靠参数，而神经网络需要大量且覆盖所有可能情况的数据，对于重型液压机器人这种数据收集昂贵且危险的系统而言，本文方法更具实用性。 | - **数据**：仅使用一段20秒的真实机器人 locomotion 数据（1000Hz）进行拟合/训练。<br>- **关键优势总结**：<br>    - **高精度**：在真实数据上预测误差最小。<br>    - **强泛化**：通过物理原理项（特别是 `k₄`）处理分布外冲击。<br>    - **高效率**：计算极快（<1μs），训练时间短（10小时）。<br>    - **高数据效率**：仅需少量真实数据。 |\n\n### **总结：技术创新与实际价值**\n\n1.  **核心技术创新**：提出了一个**融合物理原理与数据拟合的轻量级解析执行器模型**。它并非完全黑箱，其结构源自液压动力学方程，并通过引入**冲击修正项 (`k₄`)** 专门解决了RL训练和实际运行中动态冲击的建模难题。这是区别于纯神经网络模型和传统复杂液压模型的关键。\n\n2.  **实际价值体现**：\n    *   **首次实现**：在超过300公斤的重型液压四足机器人上，**首次成功**实现了基于RL的、稳定且能跟踪1 m/s速度指令的运动控制sim-to-real迁移。\n    *   **解决工程痛点**：有效应对了重型液压系统sim-to",
            "timestamp": "2026-01-20T18:00:42.799400",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "他使用了哪些评价指标与数据集，列表格具体讲讲他的评价指标的细节与数据集的细节",
            "answer": "## 论文评价指标与数据集分析\n\n### 一、 评价指标\n\n论文的评价主要分为两个部分：**1) 执行器模型本身的预测性能**；**2) 基于该模型训练的强化学习（RL）策略在真实机器人上的运动性能**。具体指标如下：\n\n#### 1. 执行器模型预测性能评价指标\n这部分用于比较提出的**解析式执行器模型**与三种**神经网络基线模型（MLP, LSTM, GRU）** 的扭矩预测精度。\n\n| 指标名称 | 缩写 | 计算方式与细节 | 目的与意义 |\n| :--- | :--- | :--- | :--- |\n| **均方根误差** | RMSE | 计算模型预测扭矩与真实测量扭矩在所有时间步上的均方根误差。公式：$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(\\tau_{pred,i} - \\tau_{true,i})^2}$ | 衡量模型预测的整体偏差，对大的预测误差更为敏感。 |\n| **阈值后均方根误差** | RMSE \\|τ_j\\|>50 | 仅对**真实扭矩绝对值大于50 N·m**的数据点计算RMSE。 | 过滤掉扭矩值较小（如接近零）时可能产生的、在百分比误差中会被放大的噪声，专注于评价在**高负载/高扭矩工况**下的预测精度，这对机器人稳定运动至关重要。 |\n| **平均绝对百分比误差** | MAPE [%] | 计算模型预测扭矩与真实测量扭矩的相对误差百分比的平均值。公式：$MAPE = \\frac{100\\%}{N}\\sum_{i=1}^{N}\\left|\\frac{\\tau_{pred,i} - \\tau_{true,i}}{\\tau_{true,i}}\\right|$ | 衡量预测误差的相对大小，便于在不同量级的数据集间进行比较。 |\n\n**关键细节**：\n*   **评价场景**：在5组不同的真实机器人运动实验数据上进行测试（1组0.4 m/s，4组1.0 m/s）。\n*   **数据频率**：策略指令频率为100 Hz，但用于评价的关节状态（位置、速度、扭矩）和指令数据以1000 Hz记录，确保了评价的高保真度。\n*   **对比基准**：与MLP、LSTM、GRU在**相同数据集和训练条件**下进行公平比较，网络结构经过调优。\n\n#### 2. 运动策略性能评价指标\n这部分是**定性**与**定量**相结合的评价，主要通过在真实机器人（>300 kg液压四足机器人）上的实验来验证。\n\n| 评价维度 | 具体指标与细节 | 目的与意义 |\n| :--- | :--- | :--- |\n| **指令跟踪能力** | - **跟踪速度**：成功跟踪0.4 m/s和1.0 m/s的前进指令。<br>- **跟踪质量**：观察机器人基座的实际速度与指令速度的吻合程度。 | 验证策略能否准确执行高层速度指令，是移动机器人的基本功能。论文通过对比**扭矩PID控制**、**无执行器模型的位置PID控制**和**使用本文执行器模型的位置PID控制**三种策略的表现，突出本方法的优势。 |\n| **运动稳定性** | - **运动流畅性**：观察机器人身体是否平稳、有无剧烈振动或异常声响。<br>- **抗扰动性**：在平坦地面上进行直线行走、转弯和横向行走，观察其应对自身动力学扰动和环境微小不平整的能力。<br>- **足端抬升**：观察步态中足端是否能充分抬离地面，避免刮擦。 | 验证策略在动态运动中的鲁棒性和自然度。论文指出，基于本执行器模型的策略实现了“自然且平滑”的运动，而其他方法存在振动、僵硬或易失稳的问题。 |\n| **训练收敛与效率** | - **训练奖励曲线**：比较不同执行器模型下，RL策略在训练过程中获得的总奖励随迭代次数的变化（见图8）。<br>- **训练时间**：记录达到稳定策略所需的训练时间。 | 从学习过程角度评价模型的有效性。本文模型使RL训练成功收敛且奖励更高，而基线模型训练失败或收敛效果差。**本文模型训练耗时约10小时，而基线模型需36-48小时**，凸显了其计算效率。 |\n\n### 二、 数据集细节\n\n论文中用于**训练和评估执行器模型**的数据集来源于**真实机器人平台**的实操数据。\n\n| 数据集属性 | 具体细节 |\n| :--- | :--- |\n| **数据来源** | 一台重量超过300公斤的大型液压四足机器人（混合行走-驱动平台）。 |\n| **数据收集方式** | 1. **初始策略**：使用一个**未包含执行器模型**的RL训练出的初步运动策略来操作机器人。<br>2. **收集过程**：让机器人在该策略控制下运动**20秒**。<br>3. **安全考量**：由于机器人重量大、成本高，在无可靠控制器时收集数据风险极高，因此采用一个能基本运动的策略来收集数据是务实之举。 |\n| **记录变量** | - 关节位置 (`q`)<br>- 关节目标位置 (`q_des`，即控制指令)<br>- 关节速度 (`q_dot`)<br>- 关节扭矩 (`τ`) |\n| **记录频率** | **1000 Hz**。采用远高于控制频率（100 Hz）的采样率，以捕捉液压系统复杂的动态细节，特别是高频的冲击和流体动力学效应。 |\n| **数据规模与特点** | - **规模**：20秒时长，1000Hz采样，共约20,000个数据点（针对12个关节）。<br>- **特点**：数据反映了**真实、耦合的机器人运动状态**，而非单个执行器的孤立测试数据。这包含了腿部配置变化、运动历史依赖以及多执行器间的液压耦合效应，对于学习一个适用于足式机器人的执行器模型至关重要。<br>- **分布**：如图5所示，仿真探索数据（尤其是扭矩与指令方向相反的情况）分布比真实数据更广，这要求模型必须具备良好的**外推能力**以应对训练数据未覆盖的“异常情况”（如摔倒时的巨大冲击）。 |\n\n**总结**：本文的评价体系紧密围绕**sim-to-real转移**的核心挑战。通过使用小规模但高保真的真实机器人数据集，验证了所提解析执行器模型在**预测精度**、**外推泛化能力**和**计算效率**上均优于神经网络模型。最终，**在真实重型液压机器人上实现1 m/s的稳定、可指令跟踪的运动**，是衡量整个方法（模型+RL训练）成功与否的最有力、最直接的“评价指标”。",
            "timestamp": "2026-01-20T18:01:31.373278",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文在哪些数据集上进行了实验？主要的评估指标和性能提升是多少？",
            "answer": "## 论文实验数据集、评估指标与性能提升分析\n\n### 一、 实验数据集\n论文中使用的数据**并非来自公开标准数据集**，而是**专门为研究收集的真实机器人操作数据**。具体细节如下：\n\n- **数据来源**：在一台**超过300公斤的重型液压四足机器人**上直接采集。\n- **采集方式**：\n    - 使用一个**未包含执行器模型的初步训练出的运动策略**，在真实机器人上运行。\n    - 机器人执行了**总时长20秒**的行走任务。\n- **数据内容与频率**：\n    - 记录了**关节位置、目标位置、关节速度和关节扭矩**。\n    - **运动策略（输出目标位置）的运行频率为100 Hz**。\n    - **传感器数据的记录频率为1000 Hz**，以捕捉执行器动态的细节。\n- **数据用途**：该数据集**专门用于拟合和验证论文提出的解析式执行器模型**，并与基于神经网络（MLP, LSTM, GRU）的模型进行对比。\n\n### 二、 主要评估指标\n论文从**执行器模型精度**和**最终运动控制性能**两个层面进行评估。\n\n#### 1. 执行器模型评估指标\n用于比较提出的解析模型与神经网络模型（MLP, LSTM, GRU）的预测精度。\n- **均方根误差 (RMSE)**：衡量预测扭矩与真实扭矩之间的总体偏差。\n- **平均绝对百分比误差 (MAPE)**：衡量预测误差的相对大小。\n    - **特别处理**：计算MAPE时，**仅考虑真实扭矩绝对值大于50 N·m的数据点**。这是因为当真实扭矩值很小时，MAPE会被不成比例地放大，此阈值确保了评估的公平性和有效性。\n- **计算效率**：模型在强化学习环境中**预测所有12个执行器扭矩所需的时间（<1微秒）**，以及**训练整个运动策略所需的总时间**。\n\n#### 2. 运动控制性能评估指标\n在真实机器人上部署训练好的策略后，通过实验定性及定量评估。\n- **命令跟踪能力**：机器人**实际速度**与**给定速度指令**（0.4 m/s 和 1.0 m/s）的匹配程度。\n- **运动稳定性**：机器人行走的**平滑度、自然性、抗干扰能力**，以及是否出现剧烈振动、失稳或跌倒。\n- **对比基线**：与另外两种控制方法进行对比：\n    1. **扭矩PID控制**：在RL中训练，但**未使用执行器模型**。\n    2. **位置PID控制（高增益PD估计）**：在RL中训练，使用**僵硬的PD估计**而非执行器模型。\n    3. **位置PID控制（使用提出的执行器模型）**：论文的核心方法。\n\n### 三、 性能提升结果\n\n#### 1. 执行器模型精度显著提升\n- **预测误差大幅降低**：如表III所示，在五种不同的真实机器人行走实验（1次0.4 m/s，4次1.0 m/s）中，**提出的解析模型在RMSE和MAPE上均远低于所有神经网络模型**。\n    - **例如**：在某个1.0 m/s的测试中，解析模型的RMSE为9.15，而MLP、LSTM、GRU的RMSE分别为53.5、46.5、38.3。**解析模型的误差仅为最佳神经网络（GRU）的约24%**。\n- **泛化能力更强**：解析模型在训练数据分布之外的“冲击”情况下（扭矩与运动方向相反）仍能保持准确预测，这得益于其物理启发的`k4`修正项。而神经网络在训练数据未充分覆盖的此类“分布外”场景下表现不佳。\n- **计算效率极高**：\n    - **推理速度**：预测所有12个执行器扭矩**耗时小于1微秒**，满足RL中400个并行环境实时模拟的需求。\n    - **训练速度**：使用该模型训练RL运动策略**仅需10小时**，而使用神经网络模型需要**36至48小时**，**提速3倍以上**。\n\n#### 2. 机器人运动性能全面提升\n- **成功实现高速稳定行走**：**首次**在超过300公斤的重型液压四足机器人上，通过**RL训练的策略实现了1 m/s的稳定行走**。\n- **命令跟踪对比**：\n    - **扭矩控制**：在0.4 m/s时表现尚可，但在1.0 m/s时**因执行器延迟导致扭矩无法跟踪目标，机器人失稳前倾**。\n    - **位置控制（无模型/僵硬PD）**：能跟踪1 m/s指令，但步态不自然。\n    - **位置控制（使用提出的模型）**：**能稳定、平滑地跟踪1 m/s指令**，并展现出处理更高速度的潜力。\n- **运动稳定性对比**：\n    - **扭矩控制**：在高速下**稳定性差，振动和冲击明显，易跌倒**。\n    - **位置控制（无模型/僵硬PD）**：虽能行走，但**控制器“僵硬”**，导致抬腿高度不足、遇到微小障碍易失稳、身体存在突然振动。\n    - **位置控制（使用提出的模型）**：**运动自然、平滑、稳定**。足端能达到足够高度，步态流畅，并能完成转向、侧向行走等未专门训练的动作，**适应性和鲁棒性显著增强**。\n\n**总结**：论文通过使用一个**高精度、高效率、物理可解释的解析执行器模型**，成功解决了重型液压机器人“仿真到现实”迁移的核心难题。该模型不仅在预测精度和训练速度上**大幅超越数据驱动的神经网络方法**，更关键的是，它使得训练出的RL运动策略在真实机器人上实现了**从0到1的突破**——达到了此前未有的**1 m/s稳定行走性能**，并且在运动质量和适应性上**显著优于传统的扭矩控制和简单的位置控制方法**。",
            "timestamp": "2026-01-20T18:02:14.692394",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        },
        {
            "question": "论文的主要局限性有哪些？未来可能的改进方向是什么？",
            "answer": "## 论文主要局限性分析\n\n根据论文内容，作者明确指出了当前研究的局限性，主要集中在以下几个方面：\n\n### 局限性\n- **测试场景有限**：模型的性能未在**显著偏离训练数据**的场景下进行测试，例如：\n    - 急剧的方向改变\n    - 极端的外部力干扰\n- **速度上限未充分验证**：由于实验场地和**安全吊车速度**的限制，未能充分测试超过 **1.0 m/s** 的更高速度下的性能。\n- **长期耐久性未评估**：未对执行器模型在**长期运行**中的可靠性、磨损或性能退化进行评估。\n- **模型泛化性待验证**：研究仅针对特定的**300公斤级液压四足机器人平台**，模型在其他机器人平台或不同任务上的适应性和泛化能力尚未验证。\n\n## 未来可能的改进方向\n\n论文在结论部分提出了明确的未来工作方向，主要集中在扩展测试、模型改进和系统整合：\n\n### 1. 扩展真实世界测试范围\n- **更高速度**：在更安全或更大的实验场地测试超过1.0 m/s的极限速度。\n- **多样化地形**：在**非结构化、复杂地形**（如碎石、斜坡、楼梯）上验证 locomotion 策略的鲁棒性。\n- **动态扰动**：测试机器人对突然推搡、负载变化等**动态干扰**的适应能力。\n\n### 2. 精炼执行器模型\n- **纳入更多物理因素**：\n    - **执行器磨损**：考虑长期运行导致的液压密封、阀门特性变化。\n    - **液压系统变化**：如油温变化、油液污染、泵性能衰减对系统动态的影响。\n- **提高模型保真度**：在保持计算效率的前提下，可能引入更精细的**流体动力学**或**热力学**效应。\n\n### 3. 提升系统通用性与可靠性\n- **平台迁移**：将模型和控制框架适配到**不同规模、不同构型**的液压机器人平台，验证其通用性。\n- **任务扩展**：超越直线行走，开发**动态转向、跳跃、负载搬运**等更复杂的技能。\n- **长期自主性**：研究在**能源有限**和**部件性能衰减**情况下的长期可靠运行策略。\n\n### 4. 探索混合控制范式\n- **结合模型与学习**：开发**融合RL鲁棒性与模型预测控制（MPC）可解释性**的混合控制器，以提升决策透明度和可靠性。\n- **在线适应**：研究模型参数的**在线估计与自适应**机制，以应对系统特性的缓慢变化或未知扰动。\n\n### 5. 数据效率与安全学习\n- **减少数据依赖**：尽管本文模型已比神经网络更数据高效，未来可探索**基于物理的迁移学习**或**元学习**，进一步减少对真实机器人数据的需求。\n- **安全约束下的RL**：在训练和部署中更严格地集成**安全约束**，防止在探索或测试中对昂贵硬件造成损坏。\n\n---\n**总结**：本文的核心局限性在于验证场景的广度与深度不足，以及模型的长期和跨平台可靠性未知。未来的改进将沿着 **“更严苛的测试 → 更精细的模型 → 更通用的系统 → 更智能的混合控制”** 这一路径展开，旨在推动重型液压腿式机器人走向更实用、更可靠的现实世界部署。",
            "timestamp": "2026-01-20T18:02:39.959429",
            "thinking": null,
            "is_reasoning": false,
            "parent_qa_id": null
        }
    ],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-16T10:01:09Z",
    "created_at": "2026-01-20T17:49:59.515763",
    "updated_at": "2026-01-20T17:49:59.515774",
    "recommend": 0
}