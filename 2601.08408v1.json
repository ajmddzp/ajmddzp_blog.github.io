{
    "id": "2601.08408v1",
    "title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2",
    "authors": [
        "Yizhan Feng",
        "Hichem Snoussi",
        "Jing Teng",
        "Jian Liu",
        "Yuyang Wang",
        "Abel Cherouat",
        "Tian Wang"
    ],
    "abstract": "在复杂场景中实现实时视觉理解与交互的需求对无人机系统日益关键。然而，大型视觉语言模型的高计算成本与无人机边缘设备有限的计算资源之间存在显著矛盾。为应对这一挑战，本文提出一种基于BLIP-2的轻量级多模态任务平台，该平台集成了YOLO-World与YOLOv8-Seg模型。通过最小化适配且无需针对无人机数据进行任务特定微调，该方案拓展了BLIP-2在无人机应用中的多任务处理能力。首先，BLIP-2与YOLO模型的深度融合使其能够利用YOLO的精确感知结果完成目标检测与实例分割等基础任务，从而促进更深层次的视觉注意力理解与推理。其次，设计了基于K-Means聚类的内容感知关键帧采样机制，融合智能帧选择与时序特征拼接技术，使轻量级BLIP-2架构具备有效处理视频级交互任务的能力。第三，实施了面向多任务适配的统一提示优化方案，该方案通过策略性地将YOLO模型输出的结构化事件日志作为上下文信息注入BLIP-2输入层，结合为过滤技术细节设计的输出约束机制，有效引导模型为各类任务生成准确且符合上下文语境的输出结果。",
    "url": "https://arxiv.org/abs/2601.08408v1",
    "html_url": "https://arxiv.org/html/2601.08408v1",
    "html_content": "",
    "preview_text": "The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.",
    "is_relevant": false,
    "relevance_score": 2.0,
    "extracted_keywords": [
        "BLIP-2",
        "YOLO",
        "multimodal learning",
        "UAV",
        "video understanding",
        "edge computing",
        "object detection",
        "instance segmentation",
        "key frame sampling",
        "prompt optimization"
    ],
    "one_line_summary": "该论文提出了一种基于BLIP-2的轻量级多模态任务平台，用于无人机视频理解，通过集成YOLO模型和优化机制来提升边缘设备上的实时性能。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-13T10:26:10Z",
    "created_at": "2026-01-20T17:49:42.532630",
    "updated_at": "2026-01-20T17:49:42.532637"
}