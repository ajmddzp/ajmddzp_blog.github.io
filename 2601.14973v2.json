{
    "id": "2601.14973v2",
    "title": "HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV",
    "authors": [
        "Faryal Batool",
        "Iana Zhura",
        "Valerii Serpiva",
        "Roohan Ahmed Khan",
        "Ivan Valuev",
        "Issatay Tokmurziyev",
        "Dzmitry Tsetserukou"
    ],
    "abstract": "åœ¨ç´§æ€¥åœºæ™¯ä¸­å®ç°å¯é çš„äººæœºåä½œï¼Œéœ€è¦èƒ½å¤Ÿæ£€æµ‹äººç±»ã€æ¨æ–­å¯¼èˆªç›®æ ‡å¹¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­å®‰å…¨è¿è¡Œçš„è‡ªä¸»ç³»ç»Ÿã€‚æœ¬æ–‡æå‡ºHumanDiffusionï¼Œä¸€ç§è½»é‡çº§çš„å›¾åƒæ¡ä»¶æ‰©æ•£è§„åˆ’å™¨ï¼Œå¯ç›´æ¥æ ¹æ®RGBå›¾åƒç”Ÿæˆäººç±»æ„ŸçŸ¥çš„å¯¼èˆªè½¨è¿¹ã€‚è¯¥ç³»ç»Ÿç»“åˆåŸºäºYOLO-11çš„äººç±»æ£€æµ‹ä¸æ‰©æ•£é©±åŠ¨çš„è½¨è¿¹ç”ŸæˆæŠ€æœ¯ï¼Œä½¿å››æ—‹ç¿¼æ— äººæœºæ— éœ€ä¾èµ–å…ˆéªŒåœ°å›¾æˆ–è®¡ç®—å¯†é›†å‹è§„åˆ’æµç¨‹ï¼Œå³å¯æ¥è¿‘ç›®æ ‡äººå‘˜å¹¶æä¾›åŒ»ç–—æ´åŠ©ã€‚è½¨è¿¹åœ¨åƒç´ ç©ºé—´ä¸­è¿›è¡Œé¢„æµ‹ï¼Œç¡®ä¿è¿åŠ¨å¹³æ»‘å¹¶ä¿æŒä¸äººç±»æŒç»­çš„å®‰å…¨è·ç¦»ã€‚æˆ‘ä»¬åœ¨ä»¿çœŸå’ŒçœŸå®å®¤å†…æ¨¡æ‹Ÿç¾éš¾åœºæ™¯ä¸­å¯¹HumanDiffusionè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨åŒ…å«300ä¸ªæ ·æœ¬çš„æµ‹è¯•é›†ä¸Šï¼Œè¯¥æ¨¡å‹åœ¨åƒç´ ç©ºé—´è½¨è¿¹é‡å»ºä¸­å®ç°äº†0.02çš„å‡æ–¹è¯¯å·®ã€‚çœŸå®ç¯å¢ƒå®éªŒæ˜¾ç¤ºï¼Œåœ¨å­˜åœ¨éƒ¨åˆ†é®æŒ¡çš„äº‹æ•…å“åº”ä¸æœç´¢å®šä½ä»»åŠ¡ä¸­ï¼Œæ•´ä½“ä»»åŠ¡æˆåŠŸç‡è¾¾åˆ°80%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œäººç±»æ¡ä»¶æ‰©æ•£è§„åˆ’ä¸ºæ—¶é—´ç´§è¿«çš„æ´åŠ©åœºæ™¯ä¸­çš„äººç±»æ„ŸçŸ¥æ— äººæœºå¯¼èˆªæä¾›äº†å®ç”¨ä¸”é²æ£’çš„è§£å†³æ–¹æ¡ˆã€‚",
    "url": "https://arxiv.org/abs/2601.14973v2",
    "html_url": "https://arxiv.org/html/2601.14973v2",
    "html_content": "Figure 1\n.\nHumanDiffusion architecture. YOLO provides human-based goal points, then the image and startâ€“goal information are encoded and fused to condition a UNet-based diffusion model. The model generates a clean pixel-space trajectory, which is converted to a 3D world-frame path for execution by the rescuer drone with a gripper.\nHumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV\nFaryal Batool\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nFaryal.Batool@skoltech.ru\n,\nIana Zhura\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\niana.zhura@skoltech.ru\n,\nValerii Serpiva\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nvalerii.serpiva@skoltech.ru\n,\nRoohan Ahmed Khan\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nra.khan@skoltech.ru\n,\nIvan Valuev\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nivan.valuev@skoltech.ru\n,\nIssatay Tokmurziyev\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nIssatay.Tokmurziyev@skoltech.ru\nand\nDzmitry Tsetserukou\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nd.tsetserukou@skoltech.ru\n(2026)\nAbstract.\nReliable humanâ€“robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents\nHumanDiffusion\n, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11â€“based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans.\nWe evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of\n80%\nacross accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.\nhuman-robot interaction, diffusion models, image-conditioned navigation, human-guided goal generation, search and rescue\nâ€ \nâ€ \nconference:\n;  ;\nâ€ \nâ€ \ncopyright:\nacmlicensed\nâ€ \nâ€ \njournalyear:\n2026\nâ€ \nâ€ \nconference:\nIEEE/ACM International Conference on Human-Robot Interaction; March 16â€“19, 2026; Edinburgh, Scotland, UK\nâ€ \nâ€ \nbooktitle:\nIEEE/ACM International Conference on Human-Robot Interaction (HRI â€™26), March 16â€“19, 2026, Edinburgh, Scotland, UK\nâ€ \nâ€ \nccs:\nHuman-centered computingÂ Human robot interaction\nâ€ \nâ€ \nccs:\nComputing methodologiesÂ Robotic planning\nâ€ \nâ€ \nccs:\nComputing methodologiesÂ Neural networks\nâ€ \nâ€ \nccs:\nComputing methodologiesÂ Computer vision\n1.\nIntroduction\nSearch and rescue (SAR) missions often operate under severe time constraints and limited situational awareness, where the locations of victims or medical personnel are unknown. Unmanned aerial vehicles (UAVs) are well suited to such settings due to their ability to access confined or hazardous environments and provide rapid assistance\n(Khosravi\net al.\n,\n2025\n)\n. While modern SAR systems commonly integrate vision-based human detection\n(V\net al.\n,\n2025\n; Giusti\net al.\n,\n2016\n)\nwith autonomous navigation, most rely on predefined goals, explicit maps, or planning frameworks with significant computational overhead such as A*, RRT*, or Model Predictive Control (MPC)\n(\nkÃ¶hler2025mpcframeworkefficientnavigation\n; Primatesta\net al.\n,\n2021\n)\n. These assumptions limit applicability in dynamic or partially observable environments where human locations must be inferred online.\nWe propose a lightweight\nhuman-conditioned diffusion planner\nthat generates global trajectories directly from RGB images, leveraging YOLO-based human detector. The center of the detected human bounding box is treated as an implicit goal, eliminating the need for maps and waypoints in goal inference. Although designed for real-world SAR, we evaluate the system in controlled indoor environments as a proof-of-concept, focusing on medical handover tasks such as retrieving supplies from one individual and delivering them to another.\nOur results show that diffusion-based planners can work well as perception-driven global planners. This provides a basis for future work that combines local obstacle avoidance and supports scalable humanâ€“robot collaboration in emergency response. The main contributions of this paper are:\nâ€¢\nEnd-to-end image-conditioned diffusion planning:\nWe develop a lightweight diffusion model that generates global trajectories conditioned solely on RGB images and the inferred startâ€“goal pair, enabling map-free navigation.\nâ€¢\nIntegrated proof-of-concept system:\nWe implement a fully functional indoor UAV pipeline supporting person identification, human-based goal generation, and autonomous trajectory execution with a gripper for object handover.\nâ€¢\nSim-to-real deployment:\nWe train a diffusion planner solely on simulated RGB data and A*-generated trajectories and successfully deploy it in two real-world indoor assistance scenarios.\n2.\nRelated Works\nThis section reviews prior work in UAV-based SAR, human-centered navigation, classical and learning-based UAV planning, diffusion-based trajectory generation, and multimodal visionâ€“language UAV systems.\n2.1.\nHuman-Centered UAV Navigation for Search and Rescue\nPrior UAV-SAR research emphasizes robust human perception, with YOLO-based detectors widely adopted for detecting small, distant, or partially occluded humans\n(Ciccone and Ceruti,\n2025\n; RamÃ­rez-Ayala\net al.\n,\n2023\n; Abbas\net al.\n,\n2024\n)\n. Surveys further highlight human detection as a core capability for aerial SAR platforms\n(Quero and Martinez-Carranza,\n2025\n; Zhang\net al.\n,\n2025a\n)\n. Several works combine detection with filtering-based tracking to enable UAV-based human following\n(Bany Abdelnabi and Rabadi,\n2024\n; GÃ³mez\net al.\n,\n2023\n)\n. However, these approaches assume persistent visual contact and focus on local interaction, without connecting human perception to global navigation or planning. In contrast, our approach directly uses human detection to infer navigation goals and generate global trajectories.\n2.2.\nUAV Navigation and Planning\nConventional UAV navigation pipelines rely on occupancy maps, classical planners such as A* or RRT*, and control strategies including MPC or learning-based controllers such as Reinforcement Learning for trajectory execution\n(Meng\net al.\n,\n2025\n; Li\net al.\n,\n2022\n; Zhou and Liu,\n2022\n; Primatesta\net al.\n,\n2021\n; Zhou\net al.\n,\n2021\n; Khan\net al.\n,\n2025\n; Aschu\net al.\n,\n2024\n)\n. While effective in structured environments, these approaches typically depend on explicit maps, predefined waypoints, or accurate state estimation, which limits their applicability in scenarios where only onboard vision is available and navigation goals must be inferred online from perception.\n2.3.\nDiffusion Models for Trajectory Planning\nDiffusion models have recently gained significant attention for motion planning and trajectory synthesis. Vision-based diffusion planners such as NoMaD\n(Sridhar\net al.\n,\n2024\n)\nand DiPPeR\n(Liu\net al.\n,\n2024\n)\ngenerate navigation trajectories conditioned on RGB observations, cost fields, or object-level goals, demonstrating strong generalization in unknown and cluttered environments compared to classical planners. Recent surveys further highlight the rapid adoption of diffusion models for robotic motion generation and decision-making\n(Wolf\net al.\n,\n2025\n; Ubukata\net al.\n,\n2024\n; Zhura\net al.\n,\n2025\n)\n.\n2.4.\nVisionâ€“Language Models for UAV Reasoning\nRecent work integrates visionâ€“language models (VLMs) with UAVs to enable semantic reasoning and high-level mission understanding. UAV-VLRR combines VLM-based scene interpretation with NMPC for SAR tasks\n(Yaqoot\net al.\n,\n2025\n)\n, while FlightGPT and UAV-VLA explores language-guided UAV navigation\n(Cai\net al.\n,\n2025\n; Sautenkov\net al.\n,\n2025\n)\n. Other studies investigate VLM-guided object detection and navigation\n(Li\net al.\n,\n2025\n; Zhang\net al.\n,\n2025b\n; Gaitan\net al.\n,\n2025\n)\n. Despite their strong reasoning capabilities, these systems do not couple human-aware perception with generative trajectory planning or automatic goal inference.\nOverall, prior methods either detect humans without planning, perform local following, rely on predefined goals, or employ diffusion planners without human-conditioned goal inference. Our work bridges this gap by unifying human detection with diffusion-based trajectory generation, enabling end-to-end, map-free global planning for search and rescue, driven directly by detected humans.\n3.\nSystem Architecture\nThe HumanDiffusion framework as shown in Figure\n1\nconsists of two core modules: (i) a YOLO-based perception system that detects humans and outputs a dynamic goal point, and (ii) a diffusion-based trajectory generator that predicts a path in pixel space conditioned on the RGB image, start and inferred goal location.\n3.1.\nPerception and Human Goal Inference\nThe YOLO-11 detector identifies humans from incoming RGB frames, and the center of the selected bounding box is used as the navigation goal. This goal updates continuously, allowing the UAV to track a moving human, while the start point is obtained from onboard localization and provided to the diffusion model as an additional conditioning signal.\n3.2.\nDiffusion-Based Trajectory Generator\nThe trajectory generator is based on a conditional UNet-based diffusion model inspired by\n(Liang\net al.\n,\n2024\n)\n. The model predicts a pixel-space trajectory mask by iteratively denoising a noisy sample. The input is a three-channel mask\nx\n0\nâˆˆ\nâ„\nB\nÃ—\n3\nÃ—\nH\nÃ—\nW\nx_{0}\\in\\mathbb{R}^{B\\times 3\\times H\\times W}\n,\nwhere\nB\nB\nis batch size,\nH\nH\nand\nW\nW\nare spatial dimensions, and the channels represent the start point, goal point, and trajectory mask.\nForward Diffusion Process.\nNoise is gradually added to the clean mask using a squared-cosine schedule:\nx\nt\n=\nÎ±\nÂ¯\nt\nâ€‹\nx\n0\n+\n1\nâˆ’\nÎ±\nÂ¯\nt\nâ€‹\nÏµ\n,\nx_{t}=\\sqrt{\\overline{\\alpha}_{t}}\\,x_{0}+\\sqrt{1-\\overline{\\alpha}_{t}}\\,\\epsilon,\nwhere\nx\nt\nx_{t}\nis the noisy sample at timestep\nt\nt\n,\nÎ±\nÂ¯\nt\n=\nâˆ\ns\n=\n1\nt\nÎ±\ns\n\\overline{\\alpha}_{t}=\\prod_{s=1}^{t}\\alpha_{s}\nis the cumulative noise factor,\nÎ±\nt\n=\n1\nâˆ’\nÎ²\nt\n\\alpha_{t}=1-\\beta_{t}\nwith\nÎ²\nt\n\\beta_{t}\nthe noise variance,\nand\nÏµ\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\n\\epsilon\\sim\\mathcal{N}(0,I)\nis the Gaussian noise.\nReverse Denoising Process.\nDuring denoising, the conditional UNet predicts the clean mask\nx\n^\n0\n\\hat{x}_{0}\n, which is used in the Denoising Diffusion Probabilistic Model (DDPM) posterior:\nx\nt\nâˆ’\n1\n=\nÎ¼\nt\nâ€‹\n(\nx\nt\n,\nx\n^\n0\n)\n+\nÏƒ\nt\nâ€‹\nz\n,\nz\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\n,\nx_{t-1}=\\mu_{t}(x_{t},\\hat{x}_{0})+\\sigma_{t}z,\\quad z\\sim\\mathcal{N}(0,I),\nwhere\nÎ¼\nt\n\\mu_{t}\nand\nÏƒ\nt\n\\sigma_{t}\nare the posterior mean and standard deviation and and\nz\nâˆ¼\nğ’©\nâ€‹\n(\n0\n,\nI\n)\nz\\sim\\mathcal{N}(0,I)\nis the Gaussian noise. This process is repeated from\nt\n=\nT\nt=T\nto\nt\n=\n0\nt=0\nwhile the start and goal channels are inpainted to ensure that the generated trajectory remains aligned with the specified boundary conditions.\nTraining Objective.\nThe model is trained to reconstruct the trajectory mask and enforce accurate endpoints. The total loss is:\nâ„’\n=\nÎ»\npath\nâ€‹\nL\npath\n+\nÎ»\nendpoint\nâ€‹\nL\nendpoint\n,\n\\mathcal{L}=\\lambda_{\\text{path}}\\,L_{\\text{path}}+\\lambda_{\\text{endpoint}}\\,L_{\\text{endpoint}},\nwhere\nÎ»\npath\n\\lambda_{\\text{path}}\nand\nÎ»\nendpoint\n\\lambda_{\\text{endpoint}}\nare the weights for trajectory reconstruction and endpoint accuracy respectively.\nThe trajectory reconstruction loss is:\nL\npath\n=\nw\nt\nâ€‹\n1\nN\nâ€‹\nâˆ‘\nB\n,\nH\n,\nW\n(\nT\nB\n,\nH\n,\nW\npred\nâˆ’\nT\nB\n,\nH\n,\nW\ngt\n)\n2\n,\nL_{\\text{path}}=w_{t}\\frac{1}{N}\\sum_{B,H,W}\\left(T^{\\text{pred}}_{B,H,W}-T^{\\text{gt}}_{B,H,W}\\right)^{2},\nwhere\nT\npred\nT^{\\text{pred}}\nand\nT\ngt\nT^{\\text{gt}}\nare the predicted and ground-truth masks, respectively,\nw\nt\nw_{t}\nis the trajectory-channel weight,\nand and\nN\n=\nB\nÃ—\nH\nÃ—\nW\nN=B\\times H\\times W\nis the total\nnumber of pixels across the batch. The endpoint loss is:\nL\nendpoint\n=\n1\n2\nâ€‹\n[\nw\ns\nâ€‹\n1\nN\nâ€‹\nâˆ‘\nB\n,\nH\n,\nW\n(\nS\nB\n,\nH\n,\nW\npred\nâˆ’\nS\nB\n,\nH\n,\nW\ngt\n)\n2\n+\nw\ng\nâ€‹\n1\nN\nâ€‹\nâˆ‘\nB\n,\nH\n,\nW\n(\nG\nB\n,\nH\n,\nW\npred\nâˆ’\nG\nB\n,\nH\n,\nW\ngt\n)\n2\n]\n.\nL_{\\text{endpoint}}=\\frac{1}{2}\\left[\\begin{aligned} &w_{s}\\frac{1}{N}\\sum_{B,H,W}\\left(S^{\\text{pred}}_{B,H,W}-S^{\\text{gt}}_{B,H,W}\\right)^{2}\\\\[4.0pt]\n&+\\quad\\quad w_{g}\\frac{1}{N}\\sum_{B,H,W}\\left(G^{\\text{pred}}_{B,H,W}-G^{\\text{gt}}_{B,H,W}\\right)^{2}\\end{aligned}\\right].\nwhere\nS\npred\n,\nG\npred\nS^{\\text{pred}},G^{\\text{pred}}\nand\nS\ngt\n,\nG\ngt\nS^{\\text{gt}},G^{\\text{gt}}\ndenote predicted and ground-truth start and goal masks, respectively, with channel weights\nw\ns\nw_{s}\nand\nw\ng\nw_{g}\n. The factor\n1\n2\n\\tfrac{1}{2}\nnormalizes their combined contribution.\nThe path loss ensures global geometric fidelity of the predicted trajectory, whereas the endpoint loss enforces accurate boundary conditions.\n3.3.\nTraining Pipeline and Dataset Generation\nWe generate 9,800 ground-truth trajectories using the A* planner on simulated environments from\n(LHW,\n2024\n)\n. Among these, 8,000 samples are used for training, 1,500 for evaluation during training without doing any backpropagation, and 300 for testing. The dataset spans multiple indoor scenarios and startâ€“goal pairs to promote generalization. Across multiple configurations, training with 100 diffusion steps and 30 epochs yielded the best performance.\n4.\nExperimental Evaluation\nThe proposed HumanDiffusion pipeline was evaluated in real-world trials using a custom-built quadrotor equipped with an Intel RealSenseÂ D455 depth camera and an Intel NUC for onboard computation. The YOLO-11 detector identifed humans and generated dynamically updated goal points, while the diffusion-based planner produced pixel-space trajectories at rate of 0.2â€“0.3â€‰s per frame. Both models ran off-board on a remote server and communicated with the onboard NUC via ROS. A custom gripper was attached for payload handling.\nThe diffusion model outputs 2D trajectories in image pixel space, which are projected into 3D waypoints using depth measurements and calibrated camera intrinsics. No explicit obstacle map is constructed; instead, the planner operates vision-driven, mapless navigation framework.\n4.1.\nResults and Failure Analysis\nThe system was evaluated on a simulated test dataset and two real-world human-assistance scenarios: (1)\nAccident Response\nand (2)\nSearch-and-Locate in Occluded Environments\n. Each scenario was executed 10 times, resulting in an overall success rate of 80%.\nFailures were categorized as follows: (i)\nPerception loss\n(2 trials), caused by camera limitations or severe human occlusion; (ii)\nController tracking errors\n(1 trial), due to transient state-estimation drift; and (iii)\nCommunication dropouts\n(1 trial), which delayed trajectory updates.\nAll experiments were conducted in accordance with laboratory safety and ethical guidelines. The UAV was equipped with propeller guards, operated at a low flight speed of 0.3â€‰m/s, and maintained a fixed stopping distance of 1â€‰m from participants. No physical contact occurred during the trials. All three participants provided informed consent prior to experimentation.\n4.2.\nEvaluation on Test Dataset\nThe model was first evaluated on a test dataset comprising of 300 simulated images with corresponding ground-truth trajectories. Performance was measured using the mean squared error (MSE) between predicted and ground-truth trajectory masks in pixel space. The evaluation yielded an MSE of\n0.02\n, indicating close agreement between predicted and reference trajectories. Representative qualitative results are shown in Figure\n2\n. Minor zigzag artifacts appear in the predicted trajectories due to the original annotations being generated at a resolution of\n512\nÃ—\n512\n512\\times 512\nand subsequently downsampled to\n64\nÃ—\n64\n64\\times 64\nfor training.\n4.3.\nEvaluation on Real Indoor Scenarios\n4.3.1.\nScenario 1: Accident Response\nIn this scenario, the UAV is provided only with approximate locations of the hospital and accident site. Upon arriving at the hospital, the drone detects a doctor using YOLO-11, extracts the bounding-box center as the navigation goal, and approaches while respecting the safety margin. After receiving a medical kit, the UAV autonomously navigates to the accident site, identifies an injured person, and completes the handover.\nAcross 10 trials, the system successfully completed 9 full delivery cycles. Figure\n3\nillustrates a representative trajectory, including human detection and safe-distance stopping behavior.\n4.3.2.\nScenario 2: Search-and-Locate in Occluded Environments\nThis scenario evaluated the systemâ€™s ability to track a partially occluded human concealed behind obstacles such as small hills or vegetation. When the human temporarily left the cameraâ€™s field of view, the UAV continued navigation toward the last known goal position. Once visibility was restored, the goal was updated and the UAV completed delivery of water or medical supplies.\nThe system succeeded in 7 out of 10 trials. Figure\n4\nshows an example of successful target reacquisition following temporary occlusion.\nOverall, HumanDiffusion achieved an 80% success rate in real-world trials. Performance in ScenarioÂ 2 was slightly lower due to communication delays and limited arena size, where the UAV occasionally detected the human only after reaching the operational boundary, leaving insufficient space for redirection under the enforced 1â€‰m stopping margin. Despite these constraints, the results demonstrate that HumanDiffusion enables reliable, safe, and human-aware assistance through diffusion-based trajectory planning combined with real-time human detection.\nFigure 2\n.\nComparison between diffusion predicted and annotated ground truth trajectories.\nFigure 3\n.\nHuman detection results for ScenarioÂ 1 with the corresponding diffusion-planned trajectories and the executed 3D flight path showing start position and goal updates.\nFigure 4\n.\nHuman detection results for ScenarioÂ 2 with the corresponding diffusion-planned trajectories and the executed 3D flight path showing start position and goal updates.\n5.\nConclusion and Future Work\nThis work introduced HumanDiffusion, a vision based, diffusion driven trajectory generation framework for human assistance with autonomous aerial robots. By combining YOLO-11 based human detection with a conditional diffusion model, the system produces smooth, safe, and goal consistent trajectories directly in pixel space. Real-world evaluations showed reliable performance across accident response and search-and-locate scenarios, achieving an overall success rate of 80%. These results indicate that diffusion based planning is a robust alternative to classical navigation methods in dynamic humanâ€“robot interaction settings.\nFuture work will extend HumanDiffusion with gesture guided human selection, multi human handling with prioritization and target switching as in\n(Zhou\net al.\n,\n2019\n; Rollo\net al.\n,\n2023\n)\n, and improved collision awareness in dynamic environments. These directions move the system toward fully autonomous, context aware aerial assistance suitable for real-world deployment.\nAcknowledgment\nResearch reported in this publication was financially supported by the RSFâ€“DST grant No.Â 24-41-02039.\nReferences\nY. Abbas, N. Al Mudawi, B. Alabdullah, T. Sadiq, A. Algarni, H. Rahman, and A. Jalal (2024)\nUnmanned aerial vehicles for human detection and recognition using neural-network model\n.\nFrontiers in Neurorobotics\n18\n.\nExternal Links:\nISSN 1662-5218\nCited by:\nÂ§2.1\n.\nD. Aschu, R. Peter, S. Karaf, A. Fedoseev, and D. Tsetserukou (2024)\nMARLander: a local path planning for drone swarms using multiagent deep reinforcement learning\n.\nIn\nProc. IEEE Int. Conf. on Systems, Man, and Cybernetics (SMC)\n,\nVol.\n,\npp.Â 2943â€“2948\n.\nCited by:\nÂ§2.2\n.\nA. A. Bany Abdelnabi and G. Rabadi (2024)\nHuman detection from unmanned aerial vehiclesâ€™ images for search and rescue missions: a state-of-the-art review\n.\nIEEE Access\n12\n(\n),\npp.Â 152009â€“152035\n.\nCited by:\nÂ§2.1\n.\nH. Cai, J. Dong, J. Tan, J. Deng, S. Li, Z. Gao, H. Wang, Z. Su, A. Sumalee, and R. Zhong (2025)\nFlightGPT: towards generalizable and interpretable uav vision-and-language navigation with vision-language models\n.\nNote:\narxiv:2505.12835\nCited by:\nÂ§2.4\n.\nF. Ciccone and A. Ceruti (2025)\nReal-time search and rescue with drones: a deep learning approach for small-object detection based on yolo\n.\nDrones\n9\n(\n8\n).\nCited by:\nÂ§2.1\n.\nN. C. Gaitan, B. I. Batinas, and C. Ursu (2025)\nAI-enhanced rescue drone with multi-modal vision and cognitive agentic architecture\n.\nAI\n6\n(\n10\n).\nCited by:\nÂ§2.4\n.\nA. Giusti, J. Guzzi, D. C. CireÅŸan, F. He, J. P. RodrÃ­guez, F. Fontana, M. Faessler, C. Forster, J. Schmidhuber, G. D. Caro, D. Scaramuzza, and L. M. Gambardella (2016)\nA machine learning approach to visual perception of forest trails for mobile robots\n.\nIEEE Robotics and Automation Letters\n1\n(\n2\n),\npp.Â 661â€“667\n.\nCited by:\nÂ§1\n.\nJ. GÃ³mez, O. Aycard, and J. Baber (2023)\nEfficient detection and tracking of human using 3d lidar sensor\n.\nSensors\n23\n(\n10\n).\nCited by:\nÂ§2.1\n.\nR. A. Khan, V. Serpiva, D. Aschalew, A. Fedoseev, and D. Tsetserukou (2025)\nAgilePilot: drl-based drone agent for real-time motion planning in dynamic environments by leveraging object detection\n.\nIn\nProc. IEEE Int. Conf. on Unmanned Aircraft Systems (ICUAS)\n,\nVol.\n,\npp.Â 185â€“192\n.\nCited by:\nÂ§2.2\n.\nM. Khosravi, R. Arora, S. Enayati, and H. Pishro-Nik (2025)\nA search and detection autonomous drone system: from design to implementation\n.\nIEEE Transactions on Automation Science and Engineering\n22\n(\n),\npp.Â 3485â€“3501\n.\nCited by:\nÂ§1\n.\nK. LHW (2024)\nVLN-go2-matterport dataset\n.\nNote:\nhttps://huggingface.co/datasets/Kennylhw/VLN-Go2-Matterport\nAccessed: 2025-02-08\nCited by:\nÂ§3.3\n.\nJ. Li, C. Liao, W. Zhang, H. Fu, and S. Fu (2022)\nUAV path planning model based on r5dos model improved a-star algorithm\n.\nApplied Sciences\n12\n(\n22\n).\nCited by:\nÂ§2.2\n.\nY. Li, L. Yang, M. Yang, F. Yan, T. Liu, C. Guo, and R. Chen (2025)\nNavBLIP: a visual-language model for enhancing unmanned aerial vehicles navigation and object detection\n.\nFrontiers in Neurorobotics\nVolume 18 - 2024\n.\nCited by:\nÂ§2.4\n.\nJ. Liang, A. Payandeh, D. Song, X. Xiao, and D. Manocha (2024)\nDTG : diffusion-based trajectory generation for mapless global navigation\n.\nIn\nProc. IEEE Int. Conf. on Intelligent Robots and Systems (IROS)\n,\nVol.\n,\npp.Â 5340â€“5347\n.\nCited by:\nÂ§3.2\n.\nJ. Liu, M. Stamatopoulou, and D. Kanoulas (2024)\nDiPPeR: diffusion-based 2d path planner applied on legged robots\n.\nIn\n2024 IEEE International Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 9264â€“9270\n.\nCited by:\nÂ§2.3\n.\nW. Meng, X. Zhang, L. Zhou, H. Guo, and X. Hu (2025)\nAdvances in uav path planning: a comprehensive review of methods, challenges, and future directions\n.\nDrones\n9\n(\n5\n).\nCited by:\nÂ§2.2\n.\nS. Primatesta, A. Pagliano, G. Guglieri, and A. Rizzo (2021)\nModel predictive sample-based motion planning for unmanned aircraft systems\n.\nIn\nProc. IEEE Int. Conf. on Unmanned Aircraft Systems (ICUAS)\n,\nVol.\n,\npp.Â 111â€“119\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nC. O. Quero and J. Martinez-Carranza (2025)\nUnmanned aerial systems in search and rescue: a global perspective on current challenges and future applications\n.\nInternational Journal of Disaster Risk Reduction\n118\n,\npp.Â 105199\n.\nCited by:\nÂ§2.1\n.\nO. RamÃ­rez-Ayala, I. GonzÃ¡lez-HernÃ¡ndez, S. Salazar, J. Flores, and R. Lozano (2023)\nReal-time person detection in wooded areas using thermal images from an aerial perspective\n.\nSensors\n23\n(\n22\n).\nCited by:\nÂ§2.1\n.\nF. Rollo, A. Zunino, G. Raiola, F. Amadio, A. Ajoudani, and N. Tsagarakis (2023)\nFollowMe: a robust person following framework based on visual re-identification and gestures\n.\nIn\nProc. IEEE Int. Conf. on Advanced Robotics and Its Social Impacts (ARSO)\n,\nVol.\n,\npp.Â 84â€“89\n.\nCited by:\nÂ§5\n.\nO. Sautenkov, Y. Yaqoot, A. Lykov, M. A. Mustafa, G. Tadevosyan, A. Akhmetkazy, M. A. Cabrera, M. Martynov, S. Karaf, and D. Tsetserukou (2025)\nUAV-vla: vision-language-action system for large scale aerial mission generation\n.\nIn\nACM/IEEE Int. Conf. on Human-Robot Interaction (HRI)\n,\nVol.\n,\npp.Â 1588â€“1592\n.\nCited by:\nÂ§2.4\n.\nA. Sridhar, D. Shah, C. Glossop, and S. Levine (2024)\nNoMaD: goal masked diffusion policies for navigation and exploration\n.\nIn\nProc. IEEE Int. Conf. Conference on Robotics and Automation (ICRA)\n,\nVol.\n,\npp.Â 63â€“70\n.\nCited by:\nÂ§2.3\n.\nT. Ubukata, J. Li, and K. Tei (2024)\nDiffusion model for planning: a systematic literature review\n.\nNote:\narxiv:2408.10266\nCited by:\nÂ§2.3\n.\nP. K. V, G. L. Reddy, H. Mahadev, V. Harish Kumar, and K. K. Prasad (2025)\nReal-time detection of unmanned aerial vehicles using yolov8\n.\nIn\nProc. IEEE Int. Conf. on Inventive Research in Computing Applications (ICIRCA)\n,\nVol.\n,\npp.Â 216â€“223\n.\nCited by:\nÂ§1\n.\nR. Wolf, Y. Shi, S. Liu, and R. Rayyes (2025)\nDiffusion models for robotic manipulation: a survey\n.\nFrontiers in Robotics and AI\n12\n.\nCited by:\nÂ§2.3\n.\nY. Yaqoot, M. A. Mustafa, O. Sautenkov, A. Lykov, V. Serpiva, and D. Tsetserukou (2025)\nUAV-vlrr: vision-language informed nmpc for rapid response in uav search and rescue\n.\nIn\nIEEE Intelligent Vehicles Symposium (IV)\n,\nVol.\n,\npp.Â 1195â€“1200\n.\nCited by:\nÂ§2.4\n.\nX. Zhang, Y. Feng, N. Wang, G. Lu, and S. Mei (2025a)\nAerial person detection for search and rescue: survey and benchmarks\n.\nJournal of Remote Sensing\n5\n,\npp.Â 0474\n.\nCited by:\nÂ§2.1\n.\nY. Zhang, H. Yu, J. Xiao, and M. Feroskhan (2025b)\nGrounded vision-language navigation for uavs with open-vocabulary goal understanding\n.\nNote:\narxiv:2506.10756\nCited by:\nÂ§2.4\n.\nK. Zhou, Y. Yang, A. Cavallaro, and T. Xiang (2019)\nOmni-scale feature learning for person re-identification\n.\nIn\nProc. IEEE Int. Conf. on Computer Vision (ICCV)\n,\nVol.\n,\npp.Â 3701â€“3711\n.\nCited by:\nÂ§5\n.\nQ. Zhou and G. Liu (2022)\nUAV path planning based on the combination of a-star algorithm and rrt-star algorithm\n.\nIn\nProc. IEEE Int. Conf. on Unmanned Systems (ICUS)\n,\nVol.\n,\npp.Â 146â€“151\n.\nCited by:\nÂ§2.2\n.\nX. Zhou, Z. Wang, H. Ye, C. Xu, and F. Gao (2021)\nEGO-planner: an esdf-free gradient-based local planner for quadrotors\n.\nIEEE Robotics and Automation Letters\n6\n(\n2\n),\npp.Â 478â€“485\n.\nCited by:\nÂ§2.2\n.\nI. Zhura, S. Karaf, F. Batool, N. D. W. Mudalige, V. Serpiva, A. A. Abdulkarim, A. Fedoseev, D. Seyidov, H. Amjad, and D. Tsetserukou (2025)\nSwarmDiffusion: end-to-end traversability-guided diffusion for embodiment-agnostic navigation of heterogeneous robots\n.\nNote:\narxiv:2512.02851\nCited by:\nÂ§2.3\n.",
    "preview_text": "Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11 based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.\n\nFigure 1\n.\nHumanDiffusion architecture. YOLO provides human-based goal points, then the image and startâ€“goal information are encoded and fused to condition a UNet-based diffusion model. The model generates a clean pixel-space trajectory, which is converted to a 3D world-frame path for execution by the rescuer drone with a gripper.\nHumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV\nFaryal Batool\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nFaryal.Batool@skoltech.ru\n,\nIana Zhura\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\niana.zhura@skoltech.ru\n,\nValerii Serpiva\nSkolkovo Institute of Science and Technology\nMoscow\nRussia\nvalerii.serpiva@skoltech.ru\n,\nRoohan Ahmed",
    "is_relevant": false,
    "relevance_score": 1.0,
    "extracted_keywords": [
        "âŒ medical"
    ],
    "one_line_summary": "è®ºæ–‡åŒ…å«è´Ÿé¢å…³é”®è¯ã€Œmedicalã€ï¼Œè‡ªåŠ¨æ ‡è®°ä¸ºä¸ç›¸å…³",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T13:22:22Z",
    "created_at": "2026-01-27T15:53:20.790932",
    "updated_at": "2026-01-27T15:53:20.790938"
}