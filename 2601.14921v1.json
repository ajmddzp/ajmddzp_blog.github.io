{
    "id": "2601.14921v1",
    "title": "Vision-Language Models on the Edge for Real-Time Robotic Perception",
    "authors": [
        "Sarat Ahmad",
        "Maryam Hafeez",
        "Syed Ali Raza Zaidi"
    ],
    "abstract": "视觉语言模型（VLMs）为机器人感知与交互提供了多模态推理能力，但其在现实系统中的部署仍受限于延迟、机载资源不足以及云端卸载的隐私风险。6G网络中的边缘智能，特别是开放式无线接入网与多接入边缘计算，通过将计算任务迁移至数据源附近，为解决这些挑战提供了可行路径。本研究以宇树G1人形机器人为实体测试平台，探索了在开放式无线接入网/多接入边缘计算基础设施上部署视觉语言模型的方法。我们设计了一套基于WebRTC的多模态数据流传输管道至边缘节点，并在实时条件下对比评估了边缘部署与云端部署的LLaMA-3.2-11B-Vision-Instruct模型性能。实验结果表明，边缘部署在保持接近云端精度的同时，将端到端延迟降低了5%。我们进一步评估了专为资源受限环境优化的紧凑模型Qwen2-VL-2B-Instruct，该模型实现了亚秒级响应，将延迟削减过半，但需以精度下降为代价。",
    "url": "https://arxiv.org/abs/2601.14921v1",
    "html_url": "https://arxiv.org/html/2601.14921v1",
    "html_content": "Vision-Language Models on the Edge for Real-Time Robotic Perception\nSarat Ahmad, Maryam Hafeez, Syed Ali Raza Zaidi\nAbstract\nVision–Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.\nFigure 1:\nOverview of the system architecture for edge-deployed VLM in robotic perception, illustrating the interactions among key system components.\nI\nIntroduction\nThe integration of vision and language understanding has led to the emergence of Vision–Language Models (VLMs), which couple visual perception with natural language reasoning for tasks such as image captioning, visual question answering (VQA), and embodied scene interpretation\n[\n27\n]\n. Beyond perception, VLMs show strong potential for robotic control, where their reasoning and generalization capabilities enable robots to infer human intentions, interpret emotions, and decompose complex environments into actionable steps\n[\n5\n]\n. By processing multimodal sensor streams such as RGB, depth, and speech in real time\n[\n25\n]\n, VLMs enable embodied reasoning that underpins autonomous robotic capabilities, including navigation\n[\n23\n]\n, manipulation, and human–robot interaction in both industrial and social contexts.\nDespite rapid progress, realizing the full potential of VLMs in robotics depends on network infrastructures that can satisfy stringent requirements for latency, bandwidth, and privacy. Cloud-based inference, which transmits large volumes of sensitive multimodal data, is limited by scalability challenges, excessive wide-area latency, and privacy risks\n[\n21\n]\n. Offloading to remote servers also creates dependency on stable connectivity, which is often unreliable in dynamic environments. Moreover, streaming raw sensory data including video, audio, and LiDAR consumes significant bandwidth and raises serious privacy concerns. These limitations highlight the need to shift inference closer to the data source. In this context, 6G-enabled edge intelligence, with compute resources embedded directly within the radio access network, offers a promising approach to support real-time perception, reasoning, and actuation in robotic systems\n[\n19\n]\n.\nWithin this context, ORAN and MEC provide a natural architectural foundation\n[\n11\n]\n. By co-locating compute resources with radio access network components, ORAN edge nodes enable local breakout of traffic, reducing backhaul load and mitigating privacy risks. MEC further ensures the low-latency guarantees required for robotics and other interactive applications, making edge infrastructure a compelling platform for deploying VLMs in real-world human–robot interaction scenarios\n[\n14\n]\n.\nDespite this potential, few studies have empirically examined how large VLMs perform when deployed on MEC/ORAN infrastructure in real robotic systems. Existing works often rely on simulated environments\n[\n7\n]\n, virtualized edge nodes\n[\n8\n]\n, or lightweight models, leaving open critical questions about the latency–accuracy trade-offs of deploying state-of-the-art VLMs at the edge versus the cloud. Moreover, evaluations that combine standardized benchmarks with real-world robot-collected data are rare, limiting insights into how deployment decisions affect real-time interaction.\nTo address this gap, we evaluate how VLMs perform under realistic robotic deployment conditions at the edge. Specifically, we examine whether large-scale models can retain their reasoning performance when migrated from cloud servers to resource-constrained edge hardware, and explore how compact models operate under strict latency constraints. This evaluation is motivated by the need to understand practical deployment trade-offs for autonomous robotics, where real-time perception and decision-making are critical.\nThe contributions of our work are threefold:\n1.\nSystem integration:\nWe design and implement a real-time robotic perception pipeline that deploys a VLM on an ORAN/MEC edge node, demonstrating the feasibility of large-scale multimodal inference in a realistic robotic setting.\n2.\nEmpirical evaluation:\nWe present a systematic comparison of edge versus cloud deployment of LLaMA-3.2-11B-Vision-Instruct using both a standardized benchmark and a robot-collected dataset, reporting task accuracy and end-to-end latency, directly quantifying the impact of deployment location.\n3.\nLatency–accuracy trade-off:\nWe analyze the performance of a Qwen2-VL-2B-Instruct on the edge, highlighting how lightweight architectures can deliver sub-second responsiveness at reduced accuracy, thereby illustrating the design trade-offs relevant for latency-critical human–robot interaction.\nII\nRelated Works\nLarge Language Models (LLMs) and VLMs are increasingly integrated into robotics to enhance perception, reasoning, and human–robot interaction. SayCan\n[\n1\n]\nused PaLM to map natural-language commands to robotic actions via affordances, while PaLM-E\n[\n9\n]\nextended this to unified multimodal reasoning over vision, language, and control. More recent efforts include multimodal models for vision-and-language navigation (VLN)\n[\n28\n]\n, Vision–Language–Action (VLA) models with locomotion skills for legged robots\n[\n5\n]\n, and applications in social robotics ranging from task execution\n[\n26\n]\nto multi-turn, open-domain dialogue\n[\n15\n]\n.\nDespite these advances, deploying LLMs/VLMs in robotics remains challenging. State-of-the-art models, often with billions of parameters, are computationally intensive and exhibit high inference latency, which conflicts with the millisecond-to-second response times required for safe navigation and interaction\n[\n10\n]\n. Mobile and humanoid robots are further constrained by size, weight, and power budgets, limiting access to high-end GPUs and large memory. High latency not only degrades user experience but can render systems unsafe if robots fail to react promptly.\nCloud offloading provides one solution but introduces excessive latency, dependence on network connectivity, and privacy risks\n[\n14\n]\n. Transmitting multimodal streams (video, audio, LiDAR) to remote servers is bandwidth-intensive and unreliable in environments such as disaster zones\n[\n10\n]\n. Moreover, sending raw sensory data raises significant privacy concerns\n[\n14\n]\n. These limitations highlight the need to move inference closer to the data source.\nSeveral strategies have been explored to mitigate these challenges. Model compression and distillation reduce memory footprint and inference cost, though often at the expense of accuracy in complex reasoning tasks\n[\n13\n]\n. Split computing and end–edge co-design distribute computation between the robot and nearby servers, lowering device-level requirements but remaining sensitive to network instability\n[\n14\n]\n. Beyond model-level techniques, the systems community has advanced edge computing frameworks for robotics. In particular, MEC integrates compute and storage into the RAN, enabling local breakout and reducing backhaul dependence. MEC has been applied in robotics for SLAM\n[\n8\n]\n, collaborative perception\n[\n12\n]\n, and real-time control\n[\n24\n,\n3\n]\n, where low-latency decision-making is essential.\nWhile these are important advancements, real-world evaluations of edge-assisted robotic systems remain scarce. Many studies rely on virtualized edge environments without physical robots\n[\n8\n]\n, or report limited metrics such as CPU usage while neglecting latency or task success\n[\n7\n]\n. In contrast, our work deploys a state-of-the-art VLM on a realistic MEC/ORAN edge platform using the Unitree G1 humanoid robot, and evaluates real-time performance on both standardized and robot-collected datasets in terms of latency and accuracy.\nIII\nSystem Design\nFigure\n1\npresents an overview of the system architecture. The design integrates three key components: the Unitree G1 humanoid robot as the embodied multimodal data source, a WebRTC-based communication and control layer for real-time streaming, and the deployment of a VLM on the ORAN edge for low-latency inference and response.\nFigure 2:\nOverview of the dataset collected with the Unitree G1 robot in a laboratory environment, comprising 200 Q&A pairs distributed across five domains of human–robot interaction\nIII-A\nUnitree G1 Robot\nWe employ the Unitree G1-EDU humanoid robot\n[\n18\n]\nas the embodied platform for perception and interaction. The G1 integrates multiple onboard sensors and actuators, and is recognized for its agility, precise balance control, and high-performance actuation, which enable operation in realistic laboratory environments. Within our system, the G1 also functions as a user equipment (UE) in the network, serving as the primary interface between the physical environment and the edge-deployed VLM.\nFor audio input, a four-microphone array captures user speech and transcribe it into text via an Automatic Speech Recognition (ASR) module. Responses generated by the VLM are converted back into speech using a Text-to-Speech (TTS) module, providing natural bidirectional audio interaction. For visual perception, the robot is equipped with an Intel RealSense D435i depth camera mounted overhead. The camera provides synchronized RGB and depth streams, allowing the system to capture both the appearance and three-dimensional structure of the environment.\nIn our pipeline, the G1 serves as the primary multimodal data source, continuously streaming RGB video and textual queries to the edge server and the React-based operator interface. The robot also provides a natural human–robot interaction modality: users issue queries through spoken commands or the companion application, while the robot delivers responses in spoken form. This setup enables evaluation of edge-deployed VLMs in an interactive, embodied robotic scenario under realistic operating conditions.\nIII-B\nReact Application\nThe React-based web application serves as the operator’s primary control interface, enabling real-time interaction with the robot and the edge-deployed VLM. The application renders live video streams from the Unitree G1’s RealSense camera, providing continuous visual feedback of the robot’s environment. In addition to monitoring, the interface supports an active chat module through which users can submit natural-language queries for inference on the edge server. To support transparency and system evaluation, the application also displays system-level feedback, including end-to-end latency, model response time, and accuracy of answers against annotated ground truth.\nFigure 3:\nEvaluation results: (a) Accuracy and latency of VLMs on the Robo2VLM benchmark; (b) Accuracy and latency on the robot-collected dataset; (c) End-to-end latency distributions for locally deployed LLaMA-3.2.\nIII-C\nVLM Deployment on Edge\nIII-C\n1\nModel Selection and Configuration\nWe deploy\nLlama-3.2-11B-Vision-Instruct\n[\n2\n]\n, an open-source VLM, to enable real-time robotic perception. The model takes an RGB frame and a textual query as input and generates a natural-language response describing or reasoning about the scene. To meet edge resource constraints, we apply 4-bit NF4 quantization with double quantization, reducing memory footprint while maintaining accuracy. Inference is optimized for latency using greedy decoding, a maximum generation length of 50 tokens, and early stopping when the model predicts an End-of-Sequence (EOS) token, ensuring concise responses without truncation.\nAdditionally, we also deploy\nQwen2-VL-2B-Instruct\n[\n22\n]\n, a compact 2B parameter VLM optimized for resource-constrained environments, on the same edge node. Including Qwen2 enables a systematic comparison between large-scale and lightweight architectures, allowing us to examine latency–accuracy trade-offs under edge constraints. This comparison is particularly relevant for highly interactive tasks such as real-time robotic perception, where responsiveness is critical.\nOur edge deployment leverages an NVIDIA L4 GPU with 24 GB of VRAM, optimized for inference workloads using FP16/INT8 acceleration. This hardware profile reflects a realistic ORAN/MEC node configuration, offering a balance of efficiency and compute capacity to support real-time multimodal inference.\nIII-C\n2\nDeployment Pipeline\nThe edge node hosts a FastAPI\n[\n6\n]\ninference service providing a local access point that receives frames and queries from G1 robot. Each request follows a standardized sequence: (1) image decoding and preprocessing, resize and color conversion (2) vision encoding and multimodal fusion, (3) transformer-based language generation, and (4) text decoding and return of the final response. This design prioritizes interactive, per-frame inference over batch throughput.\nIII-C\n3\nCloud Baseline\nFor comparison, we deploy the identical Llama-3.2 model via the NVIDIA NIM cloud API\n[\n16\n]\n, using the same prompts and preprocessing steps as in the edge deployment. This configuration represents the traditional cloud-centric paradigm, where sensor data is transmitted over the wide-area network to remote datacenters for inference. By isolating deployment location as one of the key differences, this setup provides a baseline for quantifying the latency and accuracy benefits of ORAN edge deployment.\nIII-D\nCommunication & Control Layer\nTo support low-latency interaction between the robot, the edge-deployed VLM, and the React application, we adopt a communication and control stack built on WebRTC\n[\n17\n]\n. As a browser-native, peer-to-peer framework for real-time multimedia communication, WebRTC enables the transport of video and auxiliary data streams over UDP.\nIII-D\n1\nSignaling Server\nA lightweight signaling server coordinates the initial session setup between the robot and the React application. It exchanges Session Description Protocol (SDP) offers and Interactive Connectivity Establishment (ICE) candidates, enabling peers to negotiate media capabilities and discover the optimal media path without relying on centralized relay nodes.\nIII-D\n2\nMedia Streams and Data Channels\nRGB frames are transmitted over a WebRTC video track using the Secure Real-time Transport Protocol (SRTP), with adaptive bitrate control and jitter buffering to minimize end-to-end latency. Textual payloads, including ASR transcripts, operator queries, and control metadata, are exchanged via a WebRTC data channel, which provides encrypted and reliable delivery of non-media information with low latency.\nIV\nEvaluation\nThis section introduces the evaluation metrics, describes the datasets employed, and presents discussion of the experimental results.\nIV-A\nEvaluation Metrics\nWe evaluate system performance using two primary metrics: accuracy, which quantifies task effectiveness, and latency, which measures system responsiveness.\nIV-A\n1\nAccuracy\nAccuracy is defined as the proportion of model predictions that match the gold-standard answers. Following standard VQA evaluation protocols, yes/no and multiple-choice queries are scored by exact match, while short free-form responses are normalized (e.g., lowercasing, punctuation removal) prior to comparison. To isolate the effect of deployment location, we measure the accuracy of Llama-3.2 in both cloud and edge configurations. In addition, we include Qwen2 on the edge as a lightweight alternative, illustrating the latency–accuracy trade-off under constrained resources.\nIV-A\n2\nLatency\nEnd-to-end (E2E) latency is defined as the elapsed time between frame capture at the robot and receipt of the final textual response from the VLM. This measurement encompasses frame transmission, preprocessing, inference, and response decoding. By timestamping these events, we obtain precise measurements of system responsiveness. Latency distributions are reported for cloud and edge deployments of Llama-3.2, with Qwen2 on edge serving as a low-latency comparison.\nIV-B\nDataset\nWe evaluate our system using the Robo2VLM\n[\n4\n]\n, a large scale benchmark designed to assess VLMs in real-world robotic manipulation scenarios. Robo2VLM formulates VQA tasks from in-the-wild robot interaction datasets, pairing multimodal observations with natural-language queries. The questions span key aspects of embodied perception and control, including object recognition, spatial reasoning, manipulation intent, and outcome prediction. Each query is paired with a gold-standard answer, enabling reproducible and standardized evaluation of VLM accuracy in robotics contexts.\nIn addition to the benchmark dataset, we construct a robot-collected dataset using the Unitree G1 humanoid robot in a laboratory environment. This dataset consists of 200 question–answer pairs grounded in real-time interactions. It extends beyond manipulation-focused queries to incorporate human-centered tasks such as social navigation, gesture recognition, and human presence detection\n[\n20\n]\n. This design reflects the role of humanoid robots in navigating physical environments while engaging with human users (see Figure\n2\n). Robotic responses were collected under realistic conditions, including cluttered spaces, low-light settings, and user-driven queries issued directly in front of the robot. Importantly, the live interaction enables precise measurement of end-to-end latency by timestamping the interval between query initiation and response delivery.\nIV-C\nResults\nFigure\n3\nsummarizes the evaluation results for accuracy and end-to-end latency across benchmark and robot-collected datasets. Figure\n4\nfurther breaks down accuracy by dataset category, highlighting model strengths and weaknesses across different human–robot interaction domains. In the following, we present the empirical results and discuss their implications.\nWe begin with the Robo2VLM benchmark (Figure\n3\n(a)), where LLaMA-3.2 achieves 41% accuracy, while Qwen2 reaches 28.02%, approximately 13% lower. This performance gap reflects the difficulty of VQA tasks derived from in-the-wild robot interaction datasets, which demand fine-grained multimodal reasoning that compact models struggle to capture.\nComparing edge to cloud deployment (Figure\n3\n(b)), we find that edge LLaMA-3.2 achieves a 5% latency reduction (1600.03 ms vs. 1685.20 ms) while also slightly improving accuracy. Although the absolute latency gain is modest, the relative improvement translates into a higher accuracy-per-millisecond ratio. For human–robot interaction, even tens of milliseconds can be significant, pushing system responsiveness closer to sub-1s thresholds for natural dialogue turn-taking, and improving safety in scenarios such as navigation, gesture recognition, and obstacle avoidance.\nQwen2 offers a different operating point, reducing latency to less than half of the cloud baseline and achieving sub-second responsiveness due to its smaller parameter size and lightweight inference path. This speed gain comes at the cost of multimodal reasoning depth and accuracy (77.08%). Together, these results highlight a fundamental trade-off: large models such as LLaMA are\nperformance optimal\n, while compact models such as Qwen2 are\nefficiency optimal\n. In effect, the two models trace a Pareto frontier where each is optimal under different resource and latency constraints.\nTo better understand responsiveness, Figure\n3\n(c) decomposes latency for locally deployed LLaMA into key components. The results show that text generation dominates total inference time, contributing over 85% of end-to-end latency, while image decoding while image decoding and preprocessing contribute negligibly. This identifies autoregressive decoding as the primary bottleneck. System-level speedups should therefore target the generation stage, for example, through quantization, speculative decoding, or model distillation, rather than vision preprocessing.\nFigure 4:\nPer-category accuracy of VLMs on the robot-collected dataset, across human–robot interaction domains.\nTask category results (Figure\n4\n) further clarify the trade-offs. The edge-deployed LLaMA achieves balanced accuracy across all domains, demonstrating stronger multimodal grounding and reasoning. In contrast, Qwen2 performs competitively on perception-driven tasks such as human presence detection and instruction following, but lags in reasoning-intensive domains such as spatial relations and social navigation. This suggests the potential for hybrid strategies: lightweight VLMs could handle fast perceptual grounding, while larger VLMs are invoked selectively for reasoning-heavy queries.\nFinally, beyond raw latency and accuracy, edge inference brings critical system-level advantages. By avoiding the offloading of raw video and audio streams to the cloud, edge deployment reduces bandwidth consumption and mitigates privacy risks. Thus, even when latency gains are modest, edge intelligence provides robustness and trustworthiness that are essential for embodied robotic systems.\nIn summary, our results demonstrate that deploying VLMs at the edge enables near-cloud accuracy while reducing latency and improving system-level efficiency. Compact models like Qwen2 deliver interactive responsiveness with reduced accuracy, while large models like LLaMA provide stronger reasoning capacity at higher computational cost. These findings underscore the value of real-world edge evaluations: they expose nuanced latency–accuracy–efficiency trade-offs and highlight adaptive deployment strategies as a path forward for VLM-driven robotics.\nV\nConclusion\nThis work demonstrated the feasibility of deploying large VLMs on ORAN/MEC edge infrastructure for real-time robotic perception. Using the Unitree G1 humanoid robot as a testbed, we compared cloud-based and edge-based deployments of LLaMA-3.2-11B-Vision-Instruct, alongside a compact edge model, Qwen2-VL-2B-Instruct. Through experiments on both standardized benchmarks and a robot-collected dataset, we showed that edge deployment of LLaMA-3.2 preserves near-cloud accuracy while reducing latency, while compact models such as Qwen2 achieve sub-second responsiveness at the cost of reasoning depth. These findings establish ORAN edge infrastructure as a viable platform for multimodal AI in robotics, addressing key challenges of latency and privacy. Future work will focus on two main directions: (i) model–system co-design for latency reduction, targeting autoregressive decoding as the dominant bottleneck; and (ii) adaptive hybrid deployment strategies, where lightweight models handle fast perceptual grounding while larger models are selectively invoked for reasoning-intensive tasks.\nAcknowledgment\nThis research was supported by UK Research and Innovation (UKRI) through the EPSRC under two grants: the Technology Missions Fund project CHEDDAR (EP/Y037421/1), and Award UKRI851, focused on strategic decision-making and cooperation among AI agents in telecom safety and governance. This study does not involve human subjects or sensitive data, and raises no ethical or policy concerns\nReferences\n[1]\nM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman,\net al.\n(2022)\nDo as i can, not as i say: grounding language in robotic affordances\n.\narXiv preprint arXiv:2204.01691\n.\nCited by:\n§II\n.\n[2]\nM. AI\nLlama-3.2-11b-vision-instruct (hugging face)\n.\nNote:\nhttps://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct\n[Online; accessed 13-Oct-2025]\nCited by:\n§\nIII-C\n1\n.\n[3]\nC. Asavasirikulkij, C. Mathong, T. Sinthumongkolchai, R. Chancharoen, and W. Asdomwised\n(2021)\nLow latency peer to peer robot wireless communication with edge computing\n.\nIn\n2021 IEEE 11th International Conference on System Engineering and Technology (ICSET)\n,\npp. 100–105\n.\nCited by:\n§II\n.\n[4]\nK. Chen, S. Xie, Z. Ma, P. R. Sanketi, and K. Goldberg\n(2025)\nRobo2vlm: visual question answering from large-scale in-the-wild robot manipulation datasets\n.\narXiv preprint arXiv:2505.15517\n.\nCited by:\n§\nIV-B\n.\n[5]\nA. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Bıyık, H. Yin, S. Liu, and X. Wang\n(2024)\nNavila: legged robot vision-language-action model for navigation\n.\narXiv preprint arXiv:2412.04453\n.\nCited by:\n§I\n,\n§II\n.\n[6]\nF. Contributors\nFASTAPI/FASTAPI: FASTAPI framework, high performance, easy to learn, fast to code, ready for production\n.\nNote:\nhttps://github.com/fastapi/fastapi\n[Online; accessed 13-Oct-2025]\nCited by:\n§\nIII-C\n2\n.\n[7]\nD. Dechouniotis, D. Spatharakis, and S. Papavassiliou\n(2022)\nEdge robotics experimentation over next generation iiot testbeds\n.\nIn\nNOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium\n,\npp. 1–3\n.\nCited by:\n§I\n,\n§II\n.\n[8]\nS. Dey and A. Mukherjee\n(2016)\nRobotic slam: a review from fog computing and mobile edge computing perspective\n.\nIn\nAdjunct Proceedings of the 13th International Conference on Mobile and Ubiquitous Systems: Computing Networking and Services\n,\npp. 153–158\n.\nCited by:\n§I\n,\n§II\n,\n§II\n.\n[9]\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang,\net al.\n(2023)\nPalm-e: an embodied multimodal language model\n.\nCited by:\n§II\n.\n[10]\nR. Firoozi, J. Tucker, S. Tian, A. Majumdar, J. Sun, W. Liu, Y. Zhu, S. Song, A. Kapoor, K. Hausman,\net al.\n(2025)\nFoundation models in robotics: applications, challenges, and the future\n.\nThe International Journal of Robotics Research\n44\n(\n5\n),\npp. 701–739\n.\nCited by:\n§II\n.\n[11]\nY. C. Hu, M. Patel, D. Sabella, N. Sprecher, and V. Young\n(2015)\nMobile edge computing—a key technology towards 5g\n.\nETSI white paper\n11\n(\n11\n),\npp. 1–16\n.\nCited by:\n§I\n.\n[12]\nT. Klaas, J. Lambrecht, and E. Funk\n(2020)\nSemantic local planning for mobile robots through path optimization services on the edge: a scenario-based evaluation\n.\nIn\n2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)\n,\nVol.\n1\n,\npp. 711–718\n.\nCited by:\n§II\n.\n[13]\nJ. Lin, J. Tang, H. Tang, S. Yang, W. Chen, W. Wang, G. Xiao, X. Dang, C. Gan, and S. Han\n(2024)\nAwq: activation-aware weight quantization for on-device llm compression and acceleration\n.\nProceedings of machine learning and systems\n6\n,\npp. 87–100\n.\nCited by:\n§II\n.\n[14]\nZ. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang\n(2025)\nPushing large language models to the 6g edge: vision, challenges, and opportunities\n.\nIEEE Communications Magazine\n63\n(\n9\n),\npp. 52–59\n.\nCited by:\n§I\n,\n§II\n,\n§II\n.\n[15]\nM. Mauliana, A. Ashok, D. Czernochowski, and K. Berns\n(2025)\nExploring llm-powered multi-session human-robot interactions with university students\n.\nFrontiers in Robotics and AI\n12\n,\npp. 1585589\n.\nCited by:\n§II\n.\n[16]\nNVIDIA\nLlama 3.2 11b vision instruct — NIM\n.\nNote:\nhttps://build.nvidia.com/meta/llama-3.2-11b-vision-instruct\n[Online; accessed 13-Oct-2025]\nCited by:\n§\nIII-C\n3\n.\n[17]\nW. Project\nWebRTC — github organization\n.\nNote:\nhttps://github.com/webrtc\n[Online; accessed 13-Oct-2025]\nCited by:\n§\nIII-D\n.\n[18]\nU. Robotics\nG1 developer guide\n.\nNote:\nhttps://support.unitree.com/home/en/G1_developer\n[Online; accessed 13-Oct-2025]\nCited by:\n§\nIII-A\n.\n[19]\nA. Sharshar, L. U. Khan, W. Ullah, and M. Guizani\n(2025)\nVision-language models for edge networks: a comprehensive survey\n.\nIEEE Internet of Things Journal\n.\nCited by:\n§I\n.\n[20]\nZ. Shi, E. Zhao, N. Dennler, J. Wang, X. Xu, K. Shrestha, M. Fu, D. Seita, and M. Matarić\n(2025)\nHRIBench: benchmarking vision-language models for real-time human perception in human-robot interaction\n.\narXiv preprint arXiv:2506.20566\n.\nCited by:\n§\nIV-B\n.\n[21]\nM. M. H. Shuvo, S. K. Islam, J. Cheng, and B. I. Morshed\n(2022)\nEfficient acceleration of deep learning inference on resource-constrained edge devices: a review\n.\nProceedings of the IEEE\n111\n(\n1\n),\npp. 42–91\n.\nCited by:\n§I\n.\n[22]\nQ. Team\nQwen2-vl-2b-instruct (hugging face)\n.\nNote:\nhttps://huggingface.co/Qwen/Qwen2-VL-2B-Instruct\n[Online; accessed 13-Oct-2025]\nCited by:\n§\nIII-C\n1\n.\n[23]\nS. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek\n(2020)\nRobots that use language\n.\nAnnual Review of Control, Robotics, and Autonomous Systems\n3\n(\n1\n),\npp. 25–55\n.\nCited by:\n§I\n.\n[24]\nT. T. Tran, Y. Zhang, W. Liao, Y. Lin, M. Li, and H. Huang\n(2020)\nAn autonomous mobile robot system based on serverless computing and edge computing\n.\nIn\n2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS)\n,\npp. 334–337\n.\nCited by:\n§II\n.\n[25]\nJ. Wu, W. Gan, Z. Chen, S. Wan, and P. S. Yu\n(2023)\nMultimodal large language models: a survey\n.\nIn\n2023 IEEE International Conference on Big Data (BigData)\n,\npp. 2247–2256\n.\nCited by:\n§I\n.\n[26]\nJ. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser\n(2023)\nTidybot: personalized robot assistance with large language models\n.\nAutonomous Robots\n47\n(\n8\n),\npp. 1087–1102\n.\nCited by:\n§II\n.\n[27]\nF. Zeng, W. Gan, Y. Wang, N. Liu, and P. S. Yu\n(2023)\nLarge language models for robotics: a survey\n.\narXiv preprint arXiv:2311.07226\n.\nCited by:\n§I\n.\n[28]\nB. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,\net al.\n(2023)\nRt-2: vision-language-action models transfer web knowledge to robotic control\n.\nIn\nConference on Robot Learning\n,\npp. 2165–2183\n.\nCited by:\n§II\n.",
    "preview_text": "Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.\n\nVision-Language Models on the Edge for Real-Time Robotic Perception\nSarat Ahmad, Maryam Hafeez, Syed Ali Raza Zaidi\nAbstract\nVision–Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-c",
    "is_relevant": true,
    "relevance_score": 4.0,
    "extracted_keywords": [
        "VLM"
    ],
    "one_line_summary": "该论文研究在边缘计算（如ORAN/MEC）上部署视觉语言模型（VLM）以降低机器人感知的延迟，但未直接涉及强化学习、扩散模型、全身控制等关键词。",
    "detailed_summary": "",
    "qa_pairs": [],
    "is_hidden": false,
    "is_starred": false,
    "flag": true,
    "published_date": "2026-01-21T12:09:48Z",
    "created_at": "2026-01-27T15:53:20.327393",
    "updated_at": "2026-01-27T15:53:20.327399",
    "recommend": 0
}